[2024-06-11 11:25:42,695] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-11 11:25:45,658] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-06-11 11:25:45,705] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/wzjsz/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/wangzj/LLaMA-Factory-hx/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/nfs04/wangzj/models/Qwen1.5-1.8B --cutoff_len 1024 --finetuning_type moe --moe_every_k_layers 1 --moe_num_experts 2 --topk 1 --use_load_balancing_loss --use_polarization_loss --polarization_func entropy --polarization_coef 0.01 --router_aux_loss_coef 0.01 --do_train --dataset ar_2b,de_2b,ru_2b --preprocessing_num_workers 16 --cutoff_len 512 --cache_path /home/nfs03/wangzj/dataset/pretrain/arderu6b --output_dir /home/nfs04/wangzj/checkpoints/moe/test --overwrite_output_dir --per_device_train_batch_size 16 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 1 --save_total_limit 100 --save_steps 10000 --save_only_model --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-06-11 11:25:47,348] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-11 11:25:49,409] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-06-11 11:25:49,409] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-06-11 11:25:49,409] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-06-11 11:25:49,409] [INFO] [launch.py:163:main] dist_world_size=4
[2024-06-11 11:25:49,409] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-06-11 11:25:55,230] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-11 11:25:55,230] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-11 11:25:55,230] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-11 11:25:55,233] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-11 11:26:02,895] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-11 11:26:02,895] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-11 11:26:02,896] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-06-11 11:26:02,897] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-11 11:26:02,898] [INFO] [comm.py:637:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
06/11/2024 11:26:03 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
06/11/2024 11:26:03 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory-hx/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun11_11-26-02_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
06/11/2024 11:26:03 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
06/11/2024 11:26:03 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
06/11/2024 11:26:03 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory-hx/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun11_11-26-02_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
06/11/2024 11:26:03 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
06/11/2024 11:26:03 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory-hx/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun11_11-26-02_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-06-11 11:26:03,919 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2027] 2024-06-11 11:26:03,919 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2027] 2024-06-11 11:26:03,919 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-06-11 11:26:03,919 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-06-11 11:26:03,919 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-06-11 11:26:03,919 >> loading file tokenizer.json
06/11/2024 11:26:03 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory-hx/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun11_11-26-02_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[WARNING|logging.py:314] 2024-06-11 11:26:04,231 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:727] 2024-06-11 11:26:04,232 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/config.json
[INFO|configuration_utils.py:792] 2024-06-11 11:26:04,234 >> Model config Qwen2Config {
  "_name_or_path": "/home/nfs04/wangzj/models/Qwen1.5-1.8B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3334] 2024-06-11 11:26:04,338 >> loading weights file /home/nfs04/wangzj/models/Qwen1.5-1.8B/model.safetensors
[INFO|modeling_utils.py:1459] 2024-06-11 11:26:04,406 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:827] 2024-06-11 11:26:04,408 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}

06/11/2024 11:26:07 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/11/2024 11:26:07 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
06/11/2024 11:26:07 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs04/wangzj/models/Qwen1.5-1.8B', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=0.01, router_aux_loss_coef=0.01, use_polarization_loss=True, polarization_func='entropy', use_load_balancing_loss=True)
06/11/2024 11:26:07 - INFO - llmtuner.model.loader - trainable params: 811696128 || all params: 2648524800 || trainable%: 30.6471
06/11/2024 11:26:07 - WARNING - llmtuner.data.loader - Loading dataset from disk will ignore other data arguments.
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 5872167
})
06/11/2024 11:26:07 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/11/2024 11:26:07 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
[INFO|modeling_utils.py:4070] 2024-06-11 11:26:07,936 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4078] 2024-06-11 11:26:07,936 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/nfs04/wangzj/models/Qwen1.5-1.8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-06-11 11:26:07,942 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/generation_config.json
[INFO|configuration_utils.py:827] 2024-06-11 11:26:07,942 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "max_new_tokens": 2048
}

06/11/2024 11:26:07 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/11/2024 11:26:07 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
06/11/2024 11:26:07 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/11/2024 11:26:07 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
06/11/2024 11:26:08 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs04/wangzj/models/Qwen1.5-1.8B', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=0.01, router_aux_loss_coef=0.01, use_polarization_loss=True, polarization_func='entropy', use_load_balancing_loss=True)
06/11/2024 11:26:08 - INFO - llmtuner.model.loader - trainable params: 811696128 || all params: 2648524800 || trainable%: 30.6471
06/11/2024 11:26:08 - WARNING - llmtuner.data.loader - Loading dataset from disk will ignore other data arguments.
06/11/2024 11:26:08 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs04/wangzj/models/Qwen1.5-1.8B', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=0.01, router_aux_loss_coef=0.01, use_polarization_loss=True, polarization_func='entropy', use_load_balancing_loss=True)
06/11/2024 11:26:08 - INFO - llmtuner.model.loader - trainable params: 811696128 || all params: 2648524800 || trainable%: 30.6471
06/11/2024 11:26:08 - WARNING - llmtuner.data.loader - Loading dataset from disk will ignore other data arguments.
06/11/2024 11:26:08 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs04/wangzj/models/Qwen1.5-1.8B', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=0.01, router_aux_loss_coef=0.01, use_polarization_loss=True, polarization_func='entropy', use_load_balancing_loss=True)
06/11/2024 11:26:08 - INFO - llmtuner.model.loader - trainable params: 811696128 || all params: 2648524800 || trainable%: 30.6471
06/11/2024 11:26:08 - WARNING - llmtuner.data.loader - Loading dataset from disk will ignore other data arguments.
Loading cached shuffled indices for dataset at /home/nfs03/wangzj/dataset/pretrain/arderu6b/cache-32ee8c5574b2e83c.arrow
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 5872167
})
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): Qwen2ForCausalLM(
      (model): Qwen2Model(
        (embed_tokens): Embedding(151936, 2048)
        (layers): ModuleList(
          (0): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (1): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (2): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (3): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (4): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (5): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (6): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (7): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (8): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (9): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (10): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (11): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (12): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (13): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (14): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (15): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (16): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (17): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (18): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (19): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (20): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (21): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (22): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (23): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
        )
        (norm): Qwen2RMSNorm()
      )
      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
    )
  )
)
[INFO|trainer.py:586] 2024-06-11 11:26:08,962 >> Using auto half precision backend
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 5872167
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 5872167
})
[2024-06-11 11:26:09,225] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
[2024-06-11 11:26:13,557] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-06-11 11:26:13,560] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-06-11 11:26:13,560] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-11 11:26:13,566] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-06-11 11:26:13,567] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-06-11 11:26:13,567] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-06-11 11:26:13,567] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-06-11 11:26:13,567] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-06-11 11:26:13,567] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: False
[2024-06-11 11:26:13,567] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-06-11 11:26:17,164] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-06-11 11:26:17,165] [INFO] [utils.py:803:see_memory_usage] MA 6.1 GB         Max_MA 6.48 GB         CA 6.77 GB         Max_CA 7 GB 
[2024-06-11 11:26:17,165] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 38.98 GB, percent = 10.4%
[2024-06-11 11:26:17,378] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-06-11 11:26:17,378] [INFO] [utils.py:803:see_memory_usage] MA 7.61 GB         Max_MA 9.88 GB         CA 10.55 GB         Max_CA 11 GB 
[2024-06-11 11:26:17,379] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 38.98 GB, percent = 10.4%
[2024-06-11 11:26:17,379] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-06-11 11:26:17,578] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-06-11 11:26:17,579] [INFO] [utils.py:803:see_memory_usage] MA 7.61 GB         Max_MA 7.61 GB         CA 10.55 GB         Max_CA 11 GB 
[2024-06-11 11:26:17,580] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 39.35 GB, percent = 10.5%
[2024-06-11 11:26:17,581] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-06-11 11:26:17,581] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-06-11 11:26:17,581] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-06-11 11:26:17,581] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2024-06-11 11:26:17,582] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-06-11 11:26:17,583] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-11 11:26:17,583] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-11 11:26:17,583] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-06-11 11:26:17,583] [INFO] [config.py:978:print]   amp_params ................... False
[2024-06-11 11:26:17,583] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-11 11:26:17,583] [INFO] [config.py:978:print]   bfloat16_enabled ............. True
[2024-06-11 11:26:17,583] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-06-11 11:26:17,583] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-06-11 11:26:17,583] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5aa3c86bf0>
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   dump_state ................... False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... None
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   fp16_auto_cast ............... None
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   fp16_enabled ................. False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 8
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 1
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   loss_scale ................... 1.0
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-06-11 11:26:17,584] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   pld_params ................... False
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   train_batch_size ............. 512
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  16
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   world_size ................... 4
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-11 11:26:17,585] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-06-11 11:26:17,585] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 512, 
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1748] 2024-06-11 11:26:17,585 >> ***** Running training *****
[INFO|trainer.py:1749] 2024-06-11 11:26:17,585 >>   Num examples = 5,872,167
[INFO|trainer.py:1750] 2024-06-11 11:26:17,585 >>   Num Epochs = 1
[INFO|trainer.py:1751] 2024-06-11 11:26:17,585 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1754] 2024-06-11 11:26:17,585 >>   Total train batch size (w. parallel, distributed & accumulation) = 512
[INFO|trainer.py:1755] 2024-06-11 11:26:17,586 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1756] 2024-06-11 11:26:17,586 >>   Total optimization steps = 11,469
[INFO|trainer.py:1757] 2024-06-11 11:26:17,587 >>   Number of trainable parameters = 811,696,128
06/11/2024 11:26:17 - WARNING - llmtuner.extras.callbacks - Previous log file in this folder will be deleted.
  0%|          | 0/11469 [00:00<?, ?it/s]  0%|          | 1/11469 [00:32<104:40:16, 32.86s/it]                                                     {'loss': 3.7908, 'learning_rate': 4.9999999062095436e-05, 'epoch': 0.0}
  0%|          | 1/11469 [00:33<104:40:16, 32.86s/it]                                                     {'polar__loss': 0.5434960126876831, 'experts_info_per_layer': '0.0 0.0 0.0 0.0 0.01 0.01 0.03 0.03 0.02 0.08 0.07 0.03 0.04 0.1 0.1 0.07 0.2 0.16 0.19 0.24 0.43 0.59 0.45 0.57', 'epoch': 0.0}
  0%|          | 1/11469 [00:34<104:40:16, 32.86s/it]                                                     {'ld__loss': 1.104486346244812, 'experts_info_per_layer': '0.54 0.55 0.67 0.69 0.76 0.32 0.73 0.7 0.67 0.17 0.8 0.43 0.68 0.74 0.48 0.54 0.67 0.63 0.46 0.62 0.63 0.91 0.63 0.9', 'epoch': 0.0}
  0%|          | 1/11469 [00:34<104:40:16, 32.86s/it]  0%|          | 2/11469 [01:05<103:27:31, 32.48s/it]                                                     {'loss': 3.7285, 'learning_rate': 4.999999624838181e-05, 'epoch': 0.0}
  0%|          | 2/11469 [01:05<103:27:31, 32.48s/it]                                                     {'polar__loss': 0.5162488222122192, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.02 0.02 0.04 0.04 0.03 0.14 0.1 0.04 0.05 0.13 0.11 0.1 0.27 0.2 0.24 0.3 0.52 0.65 0.55 0.93', 'epoch': 0.0}
  0%|          | 2/11469 [01:06<103:27:31, 32.48s/it]                                                     {'ld__loss': 1.1395543813705444, 'experts_info_per_layer': '0.53 0.59 0.68 0.72 0.77 0.31 0.76 0.71 0.69 0.12 0.81 0.4 0.72 0.77 0.49 0.52 0.73 0.65 0.38 0.5 0.69 0.91 0.54 0.99', 'epoch': 0.0}
  0%|          | 2/11469 [01:06<103:27:31, 32.48s/it]  0%|          | 3/11469 [01:37<103:00:06, 32.34s/it]                                                     {'loss': 3.6867, 'learning_rate': 4.9999991558859326e-05, 'epoch': 0.0}
  0%|          | 3/11469 [01:37<103:00:06, 32.34s/it]                                                     {'polar__loss': 0.5121451616287231, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.02 0.02 0.05 0.05 0.04 0.16 0.15 0.05 0.06 0.15 0.14 0.14 0.3 0.19 0.27 0.27 0.48 0.54 0.59 0.93', 'epoch': 0.0}
  0%|          | 3/11469 [01:38<103:00:06, 32.34s/it]                                                     {'ld__loss': 1.1560428142547607, 'experts_info_per_layer': '0.57 0.64 0.72 0.74 0.75 0.28 0.77 0.73 0.73 0.12 0.85 0.33 0.71 0.77 0.45 0.47 0.8 0.63 0.31 0.45 0.76 0.87 0.59 0.99', 'epoch': 0.0}
  0%|          | 3/11469 [01:38<103:00:06, 32.34s/it]  0%|          | 4/11469 [02:09<102:50:26, 32.29s/it]                                                     {'loss': 3.4431, 'learning_rate': 4.999998499352835e-05, 'epoch': 0.0}
  0%|          | 4/11469 [02:09<102:50:26, 32.29s/it]                                                     {'polar__loss': 0.5097412467002869, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.02 0.02 0.05 0.06 0.05 0.26 0.14 0.08 0.05 0.14 0.17 0.13 0.26 0.15 0.24 0.29 0.43 0.58 0.6 0.95', 'epoch': 0.0}
  0%|          | 4/11469 [02:10<102:50:26, 32.29s/it]                                                     {'ld__loss': 1.1539242267608643, 'experts_info_per_layer': '0.54 0.6 0.71 0.74 0.77 0.26 0.78 0.74 0.75 0.08 0.85 0.27 0.68 0.74 0.37 0.41 0.7 0.6 0.31 0.52 0.67 0.88 0.5 0.99', 'epoch': 0.0}
  0%|          | 4/11469 [02:10<102:50:26, 32.29s/it]  0%|          | 5/11469 [02:41<102:47:45, 32.28s/it]                                                     {'loss': 3.464, 'learning_rate': 4.999997655238937e-05, 'epoch': 0.0}
  0%|          | 5/11469 [02:41<102:47:45, 32.28s/it]                                                     {'polar__loss': 0.5070565938949585, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.02 0.02 0.06 0.06 0.06 0.27 0.15 0.11 0.05 0.16 0.19 0.18 0.27 0.14 0.3 0.23 0.37 0.52 0.57 0.97', 'epoch': 0.0}
  0%|          | 5/11469 [02:42<102:47:45, 32.28s/it]                                                     {'ld__loss': 1.1853077411651611, 'experts_info_per_layer': '0.57 0.68 0.73 0.74 0.77 0.26 0.79 0.72 0.76 0.07 0.86 0.22 0.67 0.73 0.36 0.34 0.81 0.56 0.2 0.4 0.8 0.87 0.63 1.0', 'epoch': 0.0}
  0%|          | 5/11469 [02:42<102:47:45, 32.28s/it]  0%|          | 6/11469 [03:13<102:46:13, 32.28s/it]                                                     {'loss': 3.4674, 'learning_rate': 4.999996623544301e-05, 'epoch': 0.0}
  0%|          | 6/11469 [03:13<102:46:13, 32.28s/it]                                                     {'polar__loss': 0.49825531244277954, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.04 0.04 0.05 0.06 0.06 0.32 0.15 0.17 0.06 0.11 0.21 0.16 0.21 0.15 0.3 0.3 0.46 0.63 0.68 0.96', 'epoch': 0.0}
  0%|          | 6/11469 [03:15<102:46:13, 32.28s/it]                                                     {'ld__loss': 1.1727265119552612, 'experts_info_per_layer': '0.54 0.57 0.71 0.7 0.79 0.21 0.76 0.76 0.77 0.07 0.84 0.14 0.7 0.7 0.28 0.36 0.65 0.51 0.29 0.58 0.57 0.9 0.38 1.0', 'epoch': 0.0}
  0%|          | 6/11469 [03:15<102:46:13, 32.28s/it]  0%|          | 7/11469 [03:46<102:50:20, 32.30s/it]                                                     {'loss': 3.3909, 'learning_rate': 4.999995404269006e-05, 'epoch': 0.0}
  0%|          | 7/11469 [03:46<102:50:20, 32.30s/it]                                                     {'polar__loss': 0.4954527020454407, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.03 0.04 0.05 0.06 0.07 0.36 0.12 0.17 0.05 0.16 0.26 0.22 0.26 0.16 0.36 0.26 0.44 0.57 0.65 0.97', 'epoch': 0.0}
  0%|          | 7/11469 [03:47<102:50:20, 32.30s/it]                                                     {'ld__loss': 1.1834797859191895, 'experts_info_per_layer': '0.57 0.62 0.71 0.71 0.77 0.22 0.75 0.74 0.76 0.06 0.82 0.14 0.68 0.71 0.25 0.29 0.76 0.44 0.25 0.43 0.72 0.88 0.53 1.0', 'epoch': 0.0}
  0%|          | 7/11469 [03:47<102:50:20, 32.30s/it]  0%|          | 8/11469 [04:18<102:56:05, 32.33s/it]                                                     {'loss': 3.3484, 'learning_rate': 4.9999939974131415e-05, 'epoch': 0.0}
  0%|          | 8/11469 [04:18<102:56:05, 32.33s/it]                                                     {'polar__loss': 0.4885690212249756, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.04 0.06 0.05 0.07 0.07 0.37 0.12 0.19 0.06 0.14 0.3 0.19 0.2 0.19 0.33 0.33 0.5 0.65 0.74 0.97', 'epoch': 0.0}
  0%|          | 8/11469 [04:19<102:56:05, 32.33s/it]                                                     {'ld__loss': 1.1842190027236938, 'experts_info_per_layer': '0.55 0.56 0.72 0.68 0.81 0.19 0.71 0.78 0.74 0.05 0.8 0.14 0.67 0.72 0.2 0.3 0.65 0.41 0.27 0.6 0.54 0.92 0.36 1.0', 'epoch': 0.0}
  0%|          | 8/11469 [04:19<102:56:05, 32.33s/it]  0%|          | 9/11469 [04:51<102:56:28, 32.34s/it]                                                     {'loss': 3.3629, 'learning_rate': 4.999992402976815e-05, 'epoch': 0.0}
  0%|          | 9/11469 [04:51<102:56:28, 32.34s/it]                                                     {'polar__loss': 0.4850148558616638, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.03 0.06 0.04 0.06 0.05 0.35 0.13 0.19 0.06 0.14 0.27 0.22 0.24 0.21 0.36 0.36 0.54 0.69 0.74 0.98', 'epoch': 0.0}
  0%|          | 9/11469 [04:52<102:56:28, 32.34s/it]                                                     {'ld__loss': 1.1833579540252686, 'experts_info_per_layer': '0.56 0.57 0.69 0.72 0.8 0.18 0.71 0.7 0.69 0.05 0.81 0.13 0.68 0.68 0.23 0.27 0.67 0.43 0.24 0.51 0.64 0.93 0.4 1.0', 'epoch': 0.0}
  0%|          | 9/11469 [04:52<102:56:28, 32.34s/it]  0%|          | 10/11469 [05:23<102:55:01, 32.33s/it]                                                      {'loss': 3.295, 'learning_rate': 4.9999906209601446e-05, 'epoch': 0.0}
  0%|          | 10/11469 [05:23<102:55:01, 32.33s/it]                                                      {'polar__loss': 0.4831728935241699, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.04 0.06 0.04 0.06 0.05 0.3 0.12 0.21 0.06 0.14 0.3 0.22 0.25 0.23 0.35 0.37 0.51 0.69 0.78 0.98', 'epoch': 0.0}
  0%|          | 10/11469 [05:24<102:55:01, 32.33s/it]                                                      {'ld__loss': 1.1817048788070679, 'experts_info_per_layer': '0.56 0.58 0.69 0.71 0.8 0.19 0.72 0.73 0.69 0.06 0.8 0.11 0.66 0.68 0.2 0.27 0.66 0.41 0.25 0.52 0.62 0.92 0.42 1.0', 'epoch': 0.0}
  0%|          | 10/11469 [05:24<102:55:01, 32.33s/it]  0%|          | 11/11469 [05:55<102:52:30, 32.32s/it]                                                      {'loss': 3.3047, 'learning_rate': 4.999988651363265e-05, 'epoch': 0.0}
  0%|          | 11/11469 [05:55<102:52:30, 32.32s/it]                                                      {'polar__loss': 0.4834807515144348, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.03 0.05 0.04 0.05 0.05 0.28 0.1 0.21 0.05 0.14 0.27 0.24 0.3 0.25 0.42 0.36 0.53 0.7 0.74 0.99', 'epoch': 0.0}
  0%|          | 11/11469 [05:56<102:52:30, 32.32s/it]                                                      {'ld__loss': 1.1935656070709229, 'experts_info_per_layer': '0.56 0.63 0.68 0.74 0.77 0.2 0.72 0.68 0.69 0.06 0.79 0.13 0.62 0.67 0.24 0.26 0.74 0.39 0.18 0.37 0.77 0.93 0.54 1.0', 'epoch': 0.0}
  0%|          | 11/11469 [05:56<102:52:30, 32.32s/it]  0%|          | 12/11469 [06:28<102:51:23, 32.32s/it]                                                      {'loss': 3.2979, 'learning_rate': 4.9999864941863236e-05, 'epoch': 0.0}
  0%|          | 12/11469 [06:28<102:51:23, 32.32s/it]                                                      {'polar__loss': 0.47553497552871704, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.04 0.06 0.04 0.06 0.07 0.33 0.13 0.25 0.06 0.16 0.31 0.25 0.29 0.27 0.45 0.36 0.54 0.7 0.76 0.99', 'epoch': 0.0}
  0%|          | 12/11469 [06:29<102:51:23, 32.32s/it]                                                      {'ld__loss': 1.2073628902435303, 'experts_info_per_layer': '0.56 0.59 0.72 0.67 0.8 0.17 0.73 0.73 0.72 0.04 0.81 0.09 0.64 0.68 0.2 0.25 0.69 0.37 0.18 0.42 0.74 0.93 0.53 1.0', 'epoch': 0.0}
  0%|          | 12/11469 [06:29<102:51:23, 32.32s/it]  0%|          | 13/11469 [07:00<103:03:14, 32.38s/it]                                                      {'loss': 3.2277, 'learning_rate': 4.999984149429483e-05, 'epoch': 0.0}
  0%|          | 13/11469 [07:00<103:03:14, 32.38s/it]                                                      {'polar__loss': 0.4751058518886566, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.03 0.05 0.04 0.06 0.06 0.32 0.12 0.21 0.06 0.13 0.25 0.23 0.3 0.32 0.52 0.43 0.59 0.75 0.74 0.97', 'epoch': 0.0}
  0%|          | 13/11469 [07:01<103:03:14, 32.38s/it]                                                      {'ld__loss': 1.205578088760376, 'experts_info_per_layer': '0.56 0.61 0.69 0.74 0.76 0.21 0.7 0.67 0.69 0.04 0.8 0.12 0.63 0.66 0.27 0.29 0.76 0.3 0.13 0.31 0.76 0.94 0.53 1.0', 'epoch': 0.0}
  0%|          | 13/11469 [07:01<103:03:14, 32.38s/it]  0%|          | 14/11469 [07:32<102:57:00, 32.35s/it]                                                      {'loss': 3.284, 'learning_rate': 4.999981617092918e-05, 'epoch': 0.0}
  0%|          | 14/11469 [07:32<102:57:00, 32.35s/it]                                                      {'polar__loss': 0.47072935104370117, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.03 0.05 0.04 0.05 0.05 0.38 0.13 0.29 0.06 0.14 0.29 0.26 0.29 0.31 0.48 0.42 0.58 0.75 0.79 0.99', 'epoch': 0.0}
  0%|          | 14/11469 [07:34<102:57:00, 32.35s/it]                                                      {'ld__loss': 1.212729811668396, 'experts_info_per_layer': '0.56 0.6 0.69 0.73 0.76 0.19 0.7 0.69 0.69 0.03 0.81 0.08 0.61 0.67 0.23 0.23 0.74 0.33 0.14 0.36 0.73 0.94 0.5 1.0', 'epoch': 0.0}
  0%|          | 14/11469 [07:34<102:57:00, 32.35s/it]  0%|          | 15/11469 [08:05<102:49:04, 32.32s/it]                                                      {'loss': 3.2663, 'learning_rate': 4.9999788971768194e-05, 'epoch': 0.0}
  0%|          | 15/11469 [08:05<102:49:04, 32.32s/it]                                                      {'polar__loss': 0.4693753719329834, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.03 0.05 0.04 0.06 0.06 0.41 0.14 0.31 0.06 0.15 0.31 0.25 0.29 0.32 0.46 0.38 0.57 0.73 0.79 0.99', 'epoch': 0.0}
  0%|          | 15/11469 [08:06<102:49:04, 32.32s/it]                                                      {'ld__loss': 1.2130403518676758, 'experts_info_per_layer': '0.55 0.57 0.71 0.7 0.79 0.17 0.69 0.75 0.73 0.02 0.82 0.07 0.63 0.68 0.19 0.24 0.69 0.34 0.17 0.43 0.68 0.94 0.46 1.0', 'epoch': 0.0}
  0%|          | 15/11469 [08:06<102:49:04, 32.32s/it]  0%|          | 16/11469 [08:37<102:41:22, 32.28s/it]                                                      {'loss': 3.2342, 'learning_rate': 4.9999759896813903e-05, 'epoch': 0.0}
  0%|          | 16/11469 [08:37<102:41:22, 32.28s/it]                                                      {'polar__loss': 0.46523284912109375, 'experts_info_per_layer': '0.0 0.0 0.0 0.0 0.04 0.05 0.04 0.05 0.08 0.38 0.15 0.36 0.07 0.15 0.32 0.28 0.27 0.31 0.46 0.42 0.58 0.77 0.82 0.99', 'epoch': 0.0}
  0%|          | 16/11469 [08:38<102:41:22, 32.28s/it]                                                      {'ld__loss': 1.210660696029663, 'experts_info_per_layer': '0.55 0.53 0.69 0.68 0.8 0.17 0.69 0.74 0.72 0.02 0.82 0.06 0.62 0.67 0.18 0.23 0.65 0.36 0.18 0.51 0.58 0.94 0.37 1.0', 'epoch': 0.0}
  0%|          | 16/11469 [08:38<102:41:22, 32.28s/it]  0%|          | 17/11469 [09:13<106:41:18, 33.54s/it]                                                      {'loss': 3.2024, 'learning_rate': 4.99997289460685e-05, 'epoch': 0.0}
  0%|          | 17/11469 [09:13<106:41:18, 33.54s/it]                                                      {'polar__loss': 0.46008893847465515, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.03 0.05 0.04 0.06 0.07 0.4 0.16 0.3 0.05 0.15 0.31 0.29 0.32 0.38 0.54 0.49 0.61 0.79 0.79 0.99', 'epoch': 0.0}
  0%|          | 17/11469 [09:15<106:41:18, 33.54s/it]                                                      {'ld__loss': 1.2376937866210938, 'experts_info_per_layer': '0.56 0.61 0.69 0.78 0.76 0.2 0.7 0.68 0.7 0.02 0.84 0.07 0.6 0.65 0.21 0.2 0.76 0.31 0.12 0.27 0.8 0.95 0.53 1.0', 'epoch': 0.0}
  0%|          | 17/11469 [09:15<106:41:18, 33.54s/it]  0%|          | 18/11469 [09:50<109:33:46, 34.44s/it]                                                      {'loss': 3.2009, 'learning_rate': 4.99996961195343e-05, 'epoch': 0.0}
  0%|          | 18/11469 [09:50<109:33:46, 34.44s/it]                                                      {'polar__loss': 0.456162691116333, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.03 0.04 0.04 0.06 0.07 0.44 0.19 0.29 0.06 0.14 0.29 0.26 0.28 0.39 0.56 0.57 0.65 0.82 0.82 0.99', 'epoch': 0.0}
  0%|          | 18/11469 [09:52<109:33:46, 34.44s/it]                                                      {'ld__loss': 1.233972430229187, 'experts_info_per_layer': '0.56 0.59 0.67 0.8 0.78 0.21 0.68 0.67 0.7 0.01 0.85 0.06 0.61 0.64 0.22 0.22 0.73 0.29 0.13 0.29 0.75 0.97 0.49 1.0', 'epoch': 0.0}
  0%|          | 18/11469 [09:52<109:33:46, 34.44s/it]  0%|          | 19/11469 [10:26<111:30:41, 35.06s/it]                                                      {'loss': 3.1864, 'learning_rate': 4.9999661417213764e-05, 'epoch': 0.0}
  0%|          | 19/11469 [10:26<111:30:41, 35.06s/it]                                                      {'polar__loss': 0.4580352306365967, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.04 0.05 0.04 0.06 0.08 0.45 0.18 0.32 0.07 0.16 0.36 0.32 0.28 0.33 0.5 0.41 0.61 0.81 0.83 0.99', 'epoch': 0.0}
  0%|          | 19/11469 [10:29<111:30:41, 35.06s/it]                                                      {'ld__loss': 1.2305829524993896, 'experts_info_per_layer': '0.57 0.53 0.7 0.75 0.81 0.19 0.69 0.72 0.72 0.01 0.86 0.05 0.62 0.66 0.17 0.17 0.69 0.33 0.15 0.43 0.65 0.96 0.41 1.0', 'epoch': 0.0}
  0%|          | 19/11469 [10:29<111:30:41, 35.06s/it]  0%|          | 20/11469 [11:03<113:30:11, 35.69s/it]                                                      {'loss': 3.1716, 'learning_rate': 4.999962483910951e-05, 'epoch': 0.0}
  0%|          | 20/11469 [11:03<113:30:11, 35.69s/it]                                                      {'polar__loss': 0.45419639348983765, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.04 0.05 0.04 0.06 0.09 0.43 0.21 0.32 0.07 0.19 0.32 0.31 0.32 0.33 0.53 0.46 0.66 0.82 0.81 0.99', 'epoch': 0.0}
  0%|          | 20/11469 [11:05<113:30:11, 35.69s/it]                                                      {'ld__loss': 1.2358168363571167, 'experts_info_per_layer': '0.55 0.56 0.7 0.77 0.81 0.19 0.7 0.72 0.71 0.01 0.88 0.05 0.6 0.68 0.19 0.19 0.72 0.34 0.13 0.39 0.67 0.96 0.42 1.0', 'epoch': 0.0}
  0%|          | 20/11469 [11:05<113:30:11, 35.69s/it]  0%|          | 21/11469 [11:40<113:54:19, 35.82s/it]                                                      {'loss': 3.1516, 'learning_rate': 4.999958638522427e-05, 'epoch': 0.0}
  0%|          | 21/11469 [11:40<113:54:19, 35.82s/it]                                                      {'polar__loss': 0.4468405842781067, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.04 0.06 0.04 0.08 0.1 0.45 0.2 0.36 0.06 0.21 0.38 0.35 0.36 0.38 0.56 0.45 0.66 0.84 0.84 0.99', 'epoch': 0.0}
  0%|          | 21/11469 [11:41<113:54:19, 35.82s/it]                                                      {'ld__loss': 1.253281593322754, 'experts_info_per_layer': '0.55 0.56 0.71 0.75 0.81 0.18 0.68 0.74 0.73 0.01 0.86 0.04 0.61 0.7 0.14 0.16 0.74 0.31 0.12 0.39 0.69 0.97 0.47 1.0', 'epoch': 0.0}
  0%|          | 21/11469 [11:41<113:54:19, 35.82s/it]  0%|          | 22/11469 [12:16<114:03:16, 35.87s/it]                                                      {'loss': 3.1607, 'learning_rate': 4.999954605556093e-05, 'epoch': 0.0}
  0%|          | 22/11469 [12:16<114:03:16, 35.87s/it]                                                      {'polar__loss': 0.4408499598503113, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.05 0.06 0.04 0.09 0.11 0.45 0.26 0.37 0.07 0.22 0.42 0.35 0.32 0.42 0.52 0.51 0.68 0.85 0.85 0.99', 'epoch': 0.0}
  0%|          | 22/11469 [12:17<114:03:16, 35.87s/it]                                                      {'ld__loss': 1.2567533254623413, 'experts_info_per_layer': '0.55 0.54 0.72 0.75 0.83 0.17 0.69 0.78 0.74 0.01 0.89 0.04 0.64 0.69 0.13 0.15 0.71 0.3 0.15 0.48 0.59 0.97 0.38 1.0', 'epoch': 0.0}
  0%|          | 22/11469 [12:17<114:03:16, 35.87s/it]  0%|          | 23/11469 [12:52<114:35:38, 36.04s/it]                                                      {'loss': 3.1405, 'learning_rate': 4.999950385012252e-05, 'epoch': 0.0}
  0%|          | 23/11469 [12:52<114:35:38, 36.04s/it]                                                      {'polar__loss': 0.43489202857017517, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.03 0.05 0.04 0.08 0.09 0.46 0.29 0.32 0.09 0.23 0.31 0.41 0.43 0.49 0.7 0.61 0.71 0.87 0.78 0.99', 'epoch': 0.0}
  0%|          | 23/11469 [12:54<114:35:38, 36.04s/it]                                                      {'ld__loss': 1.329444169998169, 'experts_info_per_layer': '0.56 0.67 0.7 0.85 0.77 0.2 0.72 0.67 0.71 0.01 0.91 0.05 0.64 0.68 0.2 0.14 0.88 0.25 0.04 0.1 0.96 0.97 0.65 1.0', 'epoch': 0.0}
  0%|          | 23/11469 [12:54<114:35:38, 36.04s/it]  0%|          | 24/11469 [13:29<115:04:14, 36.20s/it]                                                      {'loss': 3.1087, 'learning_rate': 4.9999459768912206e-05, 'epoch': 0.0}
  0%|          | 24/11469 [13:29<115:04:14, 36.20s/it]                                                      {'polar__loss': 0.4386916160583496, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.04 0.06 0.04 0.09 0.11 0.44 0.28 0.35 0.08 0.24 0.34 0.37 0.35 0.41 0.58 0.57 0.7 0.87 0.83 0.99', 'epoch': 0.0}
  0%|          | 24/11469 [13:30<115:04:14, 36.20s/it]                                                      {'ld__loss': 1.2604320049285889, 'experts_info_per_layer': '0.54 0.57 0.71 0.78 0.81 0.18 0.71 0.73 0.72 0.01 0.9 0.04 0.61 0.69 0.16 0.15 0.75 0.31 0.12 0.36 0.7 0.97 0.45 1.0', 'epoch': 0.0}
  0%|          | 24/11469 [13:30<115:04:14, 36.20s/it]  0%|          | 25/11469 [14:05<114:59:44, 36.17s/it]                                                      {'loss': 3.1441, 'learning_rate': 4.9999413811933294e-05, 'epoch': 0.0}
  0%|          | 25/11469 [14:05<114:59:44, 36.17s/it]                                                      {'polar__loss': 0.43696486949920654, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.04 0.07 0.05 0.09 0.13 0.44 0.3 0.36 0.09 0.26 0.38 0.39 0.36 0.42 0.56 0.52 0.68 0.87 0.83 0.98', 'epoch': 0.0}
  0%|          | 25/11469 [14:06<114:59:44, 36.17s/it]                                                      {'ld__loss': 1.2660033702850342, 'experts_info_per_layer': '0.55 0.57 0.73 0.76 0.83 0.16 0.7 0.77 0.76 0.01 0.92 0.05 0.63 0.71 0.14 0.14 0.71 0.32 0.12 0.43 0.66 0.97 0.43 1.0', 'epoch': 0.0}
  0%|          | 25/11469 [14:06<114:59:44, 36.17s/it]  0%|          | 26/11469 [14:41<114:54:45, 36.15s/it]                                                      {'loss': 3.1016, 'learning_rate': 4.999936597918923e-05, 'epoch': 0.0}
  0%|          | 26/11469 [14:41<114:54:45, 36.15s/it]                                                      {'polar__loss': 0.4295456111431122, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.04 0.06 0.04 0.09 0.11 0.5 0.34 0.32 0.09 0.25 0.33 0.42 0.47 0.46 0.7 0.59 0.76 0.85 0.78 0.98', 'epoch': 0.0}
  0%|          | 26/11469 [14:42<114:54:45, 36.15s/it]                                                      {'ld__loss': 1.3048748970031738, 'experts_info_per_layer': '0.56 0.65 0.73 0.83 0.81 0.18 0.71 0.73 0.73 0.01 0.91 0.05 0.64 0.67 0.18 0.14 0.84 0.28 0.07 0.2 0.86 0.96 0.58 1.0', 'epoch': 0.0}
  0%|          | 26/11469 [14:42<114:54:45, 36.15s/it]  0%|          | 27/11469 [15:17<114:46:40, 36.11s/it]                                                      {'loss': 3.1294, 'learning_rate': 4.9999316270683606e-05, 'epoch': 0.0}
  0%|          | 27/11469 [15:17<114:46:40, 36.11s/it]test.sh: line 34: 2059909 Killed                  deepspeed --num_gpus 4 --master_port=9902 src/train_bash.py --deepspeed /home/wangzj/LLaMA-Factory-hx/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/nfs04/wangzj/models/Qwen1.5-1.8B --cutoff_len 1024 --finetuning_type moe --moe_every_k_layers 1 --moe_num_experts 2 --topk 1 --use_load_balancing_loss --use_polarization_loss --polarization_func entropy --polarization_coef 0.01 --router_aux_loss_coef 0.01 --do_train --dataset ar_2b,de_2b,ru_2b --preprocessing_num_workers 16 --cutoff_len 512 --cache_path /home/nfs03/wangzj/dataset/pretrain/arderu6b --output_dir /home/nfs04/wangzj/checkpoints/moe/test --overwrite_output_dir --per_device_train_batch_size 16 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 1 --save_total_limit 100 --save_steps 10000 --save_only_model --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
                                                      {'polar__loss': 0.42648863792419434, 'experts_info_per_layer': '0.0 0.0 0.0 0.01 0.04 0.05 0.05 0.09 0.12 0.51 0.34 0.32 0.1 0.25 0.37 0.38 0.43 0.46 0.69 0.66 0.76 0.9 0.82 1.0', 'epoch': 0.0}
  0%|          | 27/11469 [15:18<114:46:40, 36.11s/it]                                                      {'ld__loss': 1.2940925359725952, 'experts_info_per_layer': '0.54 0.63 0.72 0.83 0.82 0.19 0.71 0.69 0.74 0.01 0.92 0.05 0.62 0.69 0.17 0.16 0.8 0.27 0.08 0.24 0.82 0.98 0.53 1.0', 'epoch': 0.0}
  0%|          | 27/11469 [15:18<114:46:40, 36.11s/it]