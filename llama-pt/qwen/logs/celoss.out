[2024-06-27 23:36:52,981] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:36:57,409] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-06-27 23:36:57,433] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/wzjsz/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/nfs04/wangzj/models/Qwen1.5-1.8B --adapter_name_or_path /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe --train_only_router --moe_router_type top2 --topk 2 --ce_loss_coef 0.05 --do_train --dataset slimpajam_1b,de_2b --max_samples 50000 --preprocessing_num_workers 16 --cutoff_len 512 --finetuning_type moe --output_dir /home/nfs04/wangzj/checkpoints/moe/test --overwrite_output_dir --per_device_train_batch_size 8 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 100 --save_steps 10000 --save_only_model --learning_rate 2e-4 --num_train_epochs 1.0 --plot_loss --fp16
[2024-06-27 23:36:58,991] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:37:01,140] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-06-27 23:37:01,140] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-06-27 23:37:01,140] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-06-27 23:37:01,140] [INFO] [launch.py:163:main] dist_world_size=4
[2024-06-27 23:37:01,140] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-06-27 23:37:09,907] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:37:09,909] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:37:09,909] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:37:09,909] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:37:19,581] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-27 23:37:19,581] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-06-27 23:37:19,581] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-27 23:37:19,581] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-27 23:37:19,582] [INFO] [comm.py:637:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
06/27/2024 23:37:19 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
06/27/2024 23:37:19 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
06/27/2024 23:37:19 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
06/27/2024 23:37:19 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-37-19_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
06/27/2024 23:37:19 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-37-19_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
06/27/2024 23:37:19 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-37-19_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
06/27/2024 23:37:19 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
06/27/2024 23:37:19 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-37-19_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:37:19,617 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:37:19,617 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:37:19,617 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:37:19,617 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:37:19,617 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:37:19,617 >> loading file tokenizer.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-06-27 23:37:19,885 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:727] 2024-06-27 23:37:19,887 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:792] 2024-06-27 23:37:19,889 >> Model config Qwen2Config {
  "_name_or_path": "/home/nfs04/wangzj/models/Qwen1.5-1.8B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:3334] 2024-06-27 23:37:20,080 >> loading weights file /home/nfs04/wangzj/models/Qwen1.5-1.8B/model.safetensors
[INFO|modeling_utils.py:1459] 2024-06-27 23:37:20,102 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:827] 2024-06-27 23:37:20,104 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}

06/27/2024 23:37:25 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/27/2024 23:37:25 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
06/27/2024 23:37:25 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
06/27/2024 23:37:25 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/27/2024 23:37:25 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
06/27/2024 23:37:25 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
[INFO|modeling_utils.py:4070] 2024-06-27 23:37:25,314 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4078] 2024-06-27 23:37:25,314 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/nfs04/wangzj/models/Qwen1.5-1.8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-06-27 23:37:25,318 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/generation_config.json
[INFO|configuration_utils.py:827] 2024-06-27 23:37:25,318 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "max_new_tokens": 2048
}

06/27/2024 23:37:25 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/27/2024 23:37:25 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
06/27/2024 23:37:25 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
06/27/2024 23:37:25 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/27/2024 23:37:25 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
06/27/2024 23:37:25 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
06/27/2024 23:37:27 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
06/27/2024 23:37:27 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
06/27/2024 23:37:27 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
06/27/2024 23:37:27 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
06/27/2024 23:37:27 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
06/27/2024 23:38:04 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Using custom data configuration default-74e4f6a24fa3d738
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Downloading and preparing dataset json/default to /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 7174 examples [00:00, 50900.85 examples/s]Generating train split: 15798 examples [00:00, 44627.86 examples/s]Generating train split: 20479 examples [00:00, 44667.73 examples/s]Generating train split: 27414 examples [00:00, 43452.19 examples/s]Generating train split: 34106 examples [00:00, 42940.62 examples/s]Generating train split: 40614 examples [00:00, 43783.78 examples/s]Generating train split: 49643 examples [00:01, 43617.26 examples/s]Generating train split: 58314 examples [00:01, 40426.07 examples/s]Generating train split: 67766 examples [00:01, 44384.53 examples/s]Generating train split: 74619 examples [00:01, 44528.87 examples/s]Generating train split: 79342 examples [00:01, 44821.79 examples/s]Generating train split: 88543 examples [00:01, 45560.82 examples/s]Generating train split: 97257 examples [00:02, 45905.07 examples/s]Generating train split: 102027 examples [00:02, 45834.88 examples/s]Generating train split: 111617 examples [00:02, 46285.48 examples/s]Generating train split: 116498 examples [00:02, 45971.94 examples/s]Generating train split: 121255 examples [00:02, 45807.08 examples/s]Generating train split: 130120 examples [00:02, 46196.21 examples/s]Generating train split: 136959 examples [00:03, 46087.62 examples/s]Generating train split: 143900 examples [00:03, 46211.81 examples/s]Generating train split: 150775 examples [00:03, 46599.49 examples/s]Generating train split: 155681 examples [00:03, 46875.12 examples/s]Generating train split: 162600 examples [00:03, 44998.14 examples/s]Generating train split: 169647 examples [00:03, 44510.96 examples/s]Generating train split: 176904 examples [00:03, 45994.08 examples/s]Generating train split: 183916 examples [00:04, 46214.33 examples/s]Generating train split: 191126 examples [00:04, 48143.58 examples/s]Generating train split: 197796 examples [00:04, 45910.72 examples/s]Generating train split: 204678 examples [00:04, 45101.49 examples/s]Generating train split: 211515 examples [00:04, 45662.60 examples/s]Generating train split: 220612 examples [00:04, 44624.71 examples/s]Generating train split: 227861 examples [00:05, 45456.69 examples/s]Generating train split: 236625 examples [00:05, 44878.33 examples/s]Generating train split: 245888 examples [00:05, 47164.48 examples/s]Generating train split: 252087 examples [00:05, 43617.04 examples/s]Generating train split: 261502 examples [00:05, 45568.98 examples/s]Generating train split: 270681 examples [00:05, 47496.31 examples/s]Generating train split: 277541 examples [00:06, 47539.12 examples/s]Generating train split: 286820 examples [00:06, 49935.94 examples/s]Generating train split: 295818 examples [00:06, 47178.60 examples/s]Generating train split: 302352 examples [00:06, 46433.31 examples/s]Generating train split: 307033 examples [00:06, 45175.73 examples/s]Generating train split: 316353 examples [00:06, 47306.99 examples/s]Generating train split: 322977 examples [00:07, 44594.66 examples/s]Generating train split: 331811 examples [00:07, 45506.05 examples/s]Generating train split: 340776 examples [00:07, 46472.54 examples/s]Generating train split: 345625 examples [00:07, 46210.43 examples/s]Generating train split: 352346 examples [00:07, 45556.34 examples/s]Generating train split: 359220 examples [00:07, 45407.88 examples/s]Generating train split: 363924 examples [00:07, 45367.04 examples/s]Generating train split: 373232 examples [00:08, 46490.06 examples/s]Generating train split: 382452 examples [00:08, 46620.35 examples/s]Generating train split: 387194 examples [00:08, 45744.56 examples/s]Generating train split: 391857 examples [00:08, 45350.97 examples/s]Generating train split: 398595 examples [00:08, 45565.80 examples/s]Generating train split: 408824 examples [00:08, 48399.14 examples/s]Generating train split: 417745 examples [00:09, 48976.44 examples/s]Generating train split: 424728 examples [00:09, 47621.31 examples/s]Generating train split: 431557 examples [00:09, 47329.67 examples/s]Generating train split: 436316 examples [00:09, 46070.04 examples/s]Generating train split: 443210 examples [00:09, 46140.88 examples/s]Generating train split: 450197 examples [00:09, 46427.16 examples/s]Generating train split: 459664 examples [00:10, 47109.27 examples/s]Generating train split: 466743 examples [00:10, 47340.28 examples/s]Generating train split: 473428 examples [00:10, 46787.88 examples/s]Generating train split: 480430 examples [00:10, 46952.98 examples/s]Generating train split: 487696 examples [00:10, 48057.79 examples/s]Generating train split: 494684 examples [00:10, 47513.37 examples/s]Generating train split: 499591 examples [00:10, 46935.23 examples/s]Generating train split: 506548 examples [00:11, 46599.24 examples/s]Generating train split: 515536 examples [00:11, 46634.64 examples/s]Generating train split: 520573 examples [00:11, 46470.16 examples/s]Generating train split: 527980 examples [00:11, 47976.50 examples/s]Generating train split: 532880 examples [00:11, 48115.19 examples/s]Generating train split: 542669 examples [00:11, 48519.21 examples/s]Generating train split: 549135 examples [00:11, 45797.02 examples/s]Generating train split: 553984 examples [00:12, 46049.61 examples/s]Generating train split: 563381 examples [00:12, 46802.25 examples/s]Generating train split: 568426 examples [00:12, 46885.51 examples/s]Generating train split: 577876 examples [00:12, 48117.02 examples/s]Generating train split: 584692 examples [00:12, 44679.88 examples/s]Generating train split: 592136 examples [00:12, 45761.23 examples/s]Generating train split: 601706 examples [00:13, 46800.95 examples/s]Generating train split: 610779 examples [00:13, 45769.99 examples/s]Generating train split: 617603 examples [00:13, 46019.90 examples/s]Generating train split: 626758 examples [00:13, 46281.29 examples/s]Generating train split: 633962 examples [00:13, 46700.56 examples/s]Generating train split: 638738 examples [00:13, 46614.37 examples/s]Generating train split: 645795 examples [00:13, 47064.25 examples/s]Generating train split: 654424 examples [00:14, 44798.73 examples/s]Generating train split: 659176 examples [00:14, 44439.65 examples/s]Generating train split: 668531 examples [00:14, 44938.13 examples/s]Generating train split: 673161 examples [00:14, 45081.44 examples/s]Generating train split: 682663 examples [00:14, 46514.66 examples/s]Generating train split: 691932 examples [00:15, 46841.36 examples/s]Generating train split: 701527 examples [00:15, 48519.99 examples/s]Generating train split: 708824 examples [00:15, 48620.66 examples/s]Generating train split: 718441 examples [00:15, 49579.10 examples/s]Generating train split: 724884 examples [00:15, 46888.57 examples/s]Generating train split: 729696 examples [00:15, 46113.24 examples/s]Generating train split: 738610 examples [00:15, 45599.06 examples/s]Generating train split: 748330 examples [00:16, 47057.99 examples/s]Generating train split: 753052 examples [00:16, 46708.08 examples/s]Generating train split: 759234 examples [00:16, 42437.64 examples/s]Generating train split: 768783 examples [00:16, 45173.07 examples/s]Generating train split: 775844 examples [00:16, 46034.37 examples/s]Generating train split: 784587 examples [00:17, 43579.38 examples/s]Generating train split: 791173 examples [00:17, 43674.98 examples/s]Generating train split: 800592 examples [00:17, 45206.68 examples/s]Generating train split: 805340 examples [00:17, 45062.44 examples/s]Generating train split: 810299 examples [00:17, 44517.87 examples/s]Generating train split: 817029 examples [00:17, 44326.62 examples/s]Generating train split: 823785 examples [00:17, 44284.37 examples/s]Generating train split: 833181 examples [00:18, 45709.30 examples/s]Generating train split: 837737 examples [00:18, 45377.95 examples/s]Generating train split: 844273 examples [00:18, 45114.56 examples/s]Generating train split: 850994 examples [00:18, 45478.97 examples/s]Generating train split: 855529 examples [00:18, 44568.16 examples/s]Generating train split: 855529 examples [00:18, 45981.60 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00000_of_00016.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00001_of_00016.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00002_of_00016.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00003_of_00016.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00004_of_00016.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00005_of_00016.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00006_of_00016.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00007_of_00016.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00008_of_00016.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00009_of_00016.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00010_of_00016.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00011_of_00016.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00012_of_00016.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00013_of_00016.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00014_of_00016.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00015_of_00016.arrow
Spawning 16 processes
Converting format of dataset (num_proc=16):   0%|          | 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00001_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00000_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00003_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00004_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00002_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00006_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00008_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00009_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00011_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00007_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00010_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00005_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00013_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00015_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00012_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00014_of_00016.arrow
Converting format of dataset (num_proc=16):   2%|▏         | 1000/50000 [00:00<00:12, 3890.64 examples/s]Converting format of dataset (num_proc=16):  88%|████████▊ | 43750/50000 [00:00<00:00, 148808.23 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 50000/50000 [00:00<00:00, 79607.04 examples/s] 
Concatenating 16 shards
Map:   0%|          | 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-ed6c79c94d5babf6.arrow
Map:   2%|▏         | 1000/50000 [00:00<00:05, 9482.08 examples/s]Map:   4%|▍         | 2210/50000 [00:00<00:04, 10980.82 examples/s]Map:   7%|▋         | 3446/50000 [00:00<00:04, 11602.01 examples/s]Map:   9%|▉         | 4704/50000 [00:00<00:03, 11962.72 examples/s]Map:  12%|█▏        | 5943/50000 [00:00<00:03, 12112.93 examples/s]Map:  15%|█▌        | 7727/50000 [00:00<00:03, 12013.40 examples/s]Map:  19%|█▉        | 9448/50000 [00:00<00:03, 11804.97 examples/s]Map:  21%|██▏       | 10695/50000 [00:00<00:03, 11950.52 examples/s]Map:  25%|██▍       | 12331/50000 [00:01<00:03, 11570.48 examples/s]Map:  27%|██▋       | 13714/50000 [00:01<00:03, 11857.59 examples/s]Map:  30%|██▉       | 14966/50000 [00:01<00:02, 12028.58 examples/s]Map:  33%|███▎      | 16703/50000 [00:01<00:02, 11525.73 examples/s]Map:  36%|███▌      | 17898/50000 [00:01<00:02, 11628.74 examples/s]Map:  39%|███▉      | 19710/50000 [00:01<00:02, 11543.89 examples/s]Map:  42%|████▏     | 20958/50000 [00:01<00:02, 11773.14 examples/s]Map:  45%|████▌     | 22694/50000 [00:01<00:02, 11628.44 examples/s]Map:  49%|████▊     | 24339/50000 [00:02<00:02, 11408.41 examples/s]Map:  51%|█████▏    | 25713/50000 [00:02<00:02, 11680.60 examples/s]Map:  54%|█████▍    | 26963/50000 [00:02<00:01, 11884.82 examples/s]Map:  57%|█████▋    | 28729/50000 [00:02<00:01, 11843.45 examples/s]Map:  61%|██████    | 30333/50000 [00:02<00:01, 11457.67 examples/s]Map:  63%|██████▎   | 31702/50000 [00:02<00:01, 11681.32 examples/s]Map:  67%|██████▋   | 33363/50000 [00:02<00:01, 11477.33 examples/s]Map:  69%|██████▉   | 34707/50000 [00:02<00:01, 11765.98 examples/s]Map:  72%|███████▏  | 35898/50000 [00:03<00:01, 11799.06 examples/s]Map:  75%|███████▌  | 37688/50000 [00:03<00:01, 11660.78 examples/s]Map:  78%|███████▊  | 38879/50000 [00:03<00:00, 11721.16 examples/s]Map:  81%|████████▏ | 40707/50000 [00:03<00:00, 11767.78 examples/s]Map:  84%|████████▍ | 41934/50000 [00:03<00:00, 11887.29 examples/s]Map:  87%|████████▋ | 43693/50000 [00:03<00:00, 11793.77 examples/s]Map:  90%|████████▉ | 44907/50000 [00:03<00:00, 11875.84 examples/s]Map:  93%|█████████▎| 46708/50000 [00:03<00:00, 11764.62 examples/s]Map:  96%|█████████▌| 47923/50000 [00:04<00:00, 11856.95 examples/s]Map:  99%|█████████▉| 49705/50000 [00:04<00:00, 11602.98 examples/s]Map: 100%|██████████| 50000/50000 [00:04<00:00, 11116.61 examples/s]
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
06/27/2024 23:38:43 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Using custom data configuration default-29b185349b8b5563
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00000_of_00016.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00001_of_00016.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00002_of_00016.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00003_of_00016.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00004_of_00016.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00005_of_00016.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00006_of_00016.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00007_of_00016.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00008_of_00016.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00009_of_00016.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00010_of_00016.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00011_of_00016.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00012_of_00016.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00013_of_00016.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00014_of_00016.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00015_of_00016.arrow
Spawning 16 processes
Converting format of dataset (num_proc=16):   0%|          | 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00000_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00001_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00003_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00004_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00002_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00006_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00005_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00007_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00008_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00015_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00011_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00009_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00010_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00013_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00012_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2_00014_of_00016.arrow
Converting format of dataset (num_proc=16):   2%|▏         | 1000/50000 [00:00<00:09, 4924.64 examples/s]Converting format of dataset (num_proc=16):  93%|█████████▎| 46500/50000 [00:00<00:00, 184788.92 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 50000/50000 [00:00<00:00, 98670.00 examples/s] 
Concatenating 16 shards
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-7eb4e0f55f4b2ce0.arrow
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00000_of_00016.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00001_of_00016.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00002_of_00016.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00003_of_00016.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00004_of_00016.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00005_of_00016.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00006_of_00016.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00007_of_00016.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00008_of_00016.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00009_of_00016.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00010_of_00016.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00011_of_00016.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00012_of_00016.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00013_of_00016.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00014_of_00016.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00015_of_00016.arrow
Spawning 16 processes
Running tokenizer on dataset (num_proc=16):   0%|          | 0/100000 [00:00<?, ? examples/s]06/27/2024 23:38:54 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
06/27/2024 23:38:54 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
06/27/2024 23:38:54 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00000_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   1%|          | 1000/100000 [00:09<15:20, 107.55 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00001_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   2%|▏         | 2000/100000 [00:11<08:38, 189.19 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00002_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   3%|▎         | 3000/100000 [00:14<06:23, 253.03 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:07,044 >> Token indices sequence length is longer than the specified maximum sequence length for this model (34999 > 32768). Running this sequence through the model will result in indexing errors
06/27/2024 23:39:07 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Running tokenizer on dataset (num_proc=16):   4%|▍         | 4000/100000 [00:14<04:09, 384.00 examples/s]06/27/2024 23:39:07 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
06/27/2024 23:39:08 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
[WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:09,426 >> Token indices sequence length is longer than the specified maximum sequence length for this model (38252 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00003_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   5%|▌         | 5000/100000 [00:17<04:00, 394.67 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:09,980 >> Token indices sequence length is longer than the specified maximum sequence length for this model (129191 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):   6%|▌         | 6000/100000 [00:17<02:55, 535.09 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:11,721 >> Token indices sequence length is longer than the specified maximum sequence length for this model (35848 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00004_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   7%|▋         | 7000/100000 [00:19<02:44, 564.68 examples/s]Running tokenizer on dataset (num_proc=16):   9%|▉         | 9000/100000 [00:20<01:40, 903.11 examples/s]Running tokenizer on dataset (num_proc=16):  10%|█         | 10000/100000 [00:22<02:06, 710.80 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:15,100 >> Token indices sequence length is longer than the specified maximum sequence length for this model (36232 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  11%|█         | 11000/100000 [00:22<01:34, 937.46 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00005_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  12%|█▏        | 12000/100000 [00:22<01:16, 1144.83 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 13000/100000 [00:24<01:31, 947.05 examples/s] Running tokenizer on dataset (num_proc=16):  14%|█▍        | 14000/100000 [00:24<01:09, 1236.13 examples/s]Running tokenizer on dataset (num_proc=16):  15%|█▌        | 15000/100000 [00:24<00:55, 1540.97 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:18,002 >> Token indices sequence length is longer than the specified maximum sequence length for this model (130268 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00006_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  16%|█▌        | 16000/100000 [00:25<01:01, 1365.83 examples/s]Running tokenizer on dataset (num_proc=16):  17%|█▋        | 17000/100000 [00:27<01:33, 884.88 examples/s] [WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:20,600 >> Token indices sequence length is longer than the specified maximum sequence length for this model (61819 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00007_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  18%|█▊        | 18000/100000 [00:28<01:17, 1054.12 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 19000/100000 [00:28<01:06, 1224.35 examples/s]Running tokenizer on dataset (num_proc=16):  20%|██        | 20000/100000 [00:28<00:48, 1644.49 examples/s]Running tokenizer on dataset (num_proc=16):  21%|██        | 21000/100000 [00:29<00:43, 1821.00 examples/s]Running tokenizer on dataset (num_proc=16):  22%|██▏       | 22000/100000 [00:29<00:35, 2204.59 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:23,485 >> Token indices sequence length is longer than the specified maximum sequence length for this model (36986 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00008_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  23%|██▎       | 23000/100000 [00:31<01:00, 1275.83 examples/s]Running tokenizer on dataset (num_proc=16):  24%|██▍       | 24000/100000 [00:31<00:44, 1707.11 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 25000/100000 [00:32<01:04, 1154.24 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:25,783 >> Token indices sequence length is longer than the specified maximum sequence length for this model (51131 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  26%|██▌       | 26000/100000 [00:33<00:53, 1385.52 examples/s]Running tokenizer on dataset (num_proc=16):  27%|██▋       | 27000/100000 [00:33<00:40, 1784.60 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:26,202 >> Token indices sequence length is longer than the specified maximum sequence length for this model (45420 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00009_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  28%|██▊       | 28000/100000 [00:33<00:37, 1917.74 examples/s]Running tokenizer on dataset (num_proc=16):  29%|██▉       | 29000/100000 [00:33<00:28, 2501.20 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███       | 31000/100000 [00:34<00:17, 3924.37 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 32000/100000 [00:34<00:16, 4047.97 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00010_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  33%|███▎      | 33250/100000 [00:36<00:43, 1545.79 examples/s]Running tokenizer on dataset (num_proc=16):  35%|███▌      | 35250/100000 [00:37<00:36, 1758.84 examples/s]Running tokenizer on dataset (num_proc=16):  36%|███▋      | 36250/100000 [00:37<00:32, 1965.81 examples/s]Running tokenizer on dataset (num_proc=16):  37%|███▋      | 37250/100000 [00:38<00:40, 1555.49 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:31,558 >> Token indices sequence length is longer than the specified maximum sequence length for this model (40115 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00011_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  38%|███▊      | 38250/100000 [00:38<00:37, 1653.92 examples/s]Running tokenizer on dataset (num_proc=16):  39%|███▉      | 39250/100000 [00:39<00:31, 1954.11 examples/s]Running tokenizer on dataset (num_proc=16):  41%|████▏     | 41250/100000 [00:39<00:22, 2583.05 examples/s]Running tokenizer on dataset (num_proc=16):  42%|████▏     | 42250/100000 [00:39<00:18, 3099.47 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:33,527 >> Token indices sequence length is longer than the specified maximum sequence length for this model (35336 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  44%|████▎     | 43500/100000 [00:40<00:25, 2210.05 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00012_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  44%|████▍     | 44500/100000 [00:41<00:24, 2289.21 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▌     | 45500/100000 [00:41<00:24, 2267.42 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▋     | 46500/100000 [00:42<00:24, 2178.61 examples/s]Running tokenizer on dataset (num_proc=16):  48%|████▊     | 47500/100000 [00:42<00:28, 1818.76 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00013_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  50%|████▉     | 49500/100000 [00:44<00:27, 1820.63 examples/s]Running tokenizer on dataset (num_proc=16):  51%|█████     | 50750/100000 [00:44<00:21, 2323.77 examples/s]Running tokenizer on dataset (num_proc=16):  52%|█████▏    | 51750/100000 [00:44<00:25, 1894.76 examples/s]Running tokenizer on dataset (num_proc=16):  53%|█████▎    | 52750/100000 [00:45<00:22, 2067.18 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▌    | 56000/100000 [00:45<00:11, 3798.91 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:39,375 >> Token indices sequence length is longer than the specified maximum sequence length for this model (43731 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00014_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 57000/100000 [00:46<00:19, 2230.75 examples/s]Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 58250/100000 [00:47<00:15, 2656.26 examples/s]Running tokenizer on dataset (num_proc=16):  60%|██████    | 60250/100000 [00:47<00:14, 2721.31 examples/s]Running tokenizer on dataset (num_proc=16):  61%|██████▏   | 61250/100000 [00:48<00:19, 1989.90 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-0bbf3976d2f8161e_00015_of_00016.arrow
[WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:42,301 >> Token indices sequence length is longer than the specified maximum sequence length for this model (42164 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 62250/100000 [00:49<00:21, 1766.88 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 63250/100000 [00:49<00:17, 2053.83 examples/s]Running tokenizer on dataset (num_proc=16):  64%|██████▍   | 64250/100000 [00:49<00:13, 2575.99 examples/s]Running tokenizer on dataset (num_proc=16):  65%|██████▌   | 65250/100000 [00:50<00:14, 2333.84 examples/s]Running tokenizer on dataset (num_proc=16):  66%|██████▋   | 66250/100000 [00:50<00:13, 2493.43 examples/s]Running tokenizer on dataset (num_proc=16):  68%|██████▊   | 68250/100000 [00:52<00:16, 1913.37 examples/s]Running tokenizer on dataset (num_proc=16):  70%|██████▉   | 69500/100000 [00:52<00:16, 1834.63 examples/s]Running tokenizer on dataset (num_proc=16):  70%|███████   | 70500/100000 [00:53<00:14, 2091.86 examples/s]Running tokenizer on dataset (num_proc=16):  72%|███████▏  | 71750/100000 [00:53<00:11, 2537.73 examples/s]Running tokenizer on dataset (num_proc=16):  73%|███████▎  | 72750/100000 [00:54<00:14, 1920.62 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▍  | 74750/100000 [00:55<00:13, 1905.25 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:48,321 >> Token indices sequence length is longer than the specified maximum sequence length for this model (32805 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  76%|███████▌  | 76000/100000 [00:55<00:10, 2275.88 examples/s]Running tokenizer on dataset (num_proc=16):  77%|███████▋  | 77000/100000 [00:55<00:09, 2366.82 examples/s]Running tokenizer on dataset (num_proc=16):  78%|███████▊  | 78000/100000 [00:56<00:09, 2327.52 examples/s]Running tokenizer on dataset (num_proc=16):  79%|███████▉  | 79000/100000 [00:58<00:18, 1111.27 examples/s]Running tokenizer on dataset (num_proc=16):  80%|████████  | 80000/100000 [00:58<00:14, 1395.68 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 81250/100000 [00:59<00:14, 1322.64 examples/s]Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 82250/100000 [01:00<00:10, 1672.58 examples/s]Running tokenizer on dataset (num_proc=16):  83%|████████▎ | 83250/100000 [01:00<00:10, 1667.11 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▍ | 84250/100000 [01:01<00:08, 1889.15 examples/s]Running tokenizer on dataset (num_proc=16):  85%|████████▌ | 85250/100000 [01:01<00:07, 1908.45 examples/s]Running tokenizer on dataset (num_proc=16):  86%|████████▋ | 86250/100000 [01:02<00:07, 1849.71 examples/s]Running tokenizer on dataset (num_proc=16):  86%|████████▋ | 86500/100000 [01:02<00:08, 1654.80 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 87500/100000 [01:03<00:11, 1067.41 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 88500/100000 [01:04<00:08, 1367.30 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:39:57,745 >> Token indices sequence length is longer than the specified maximum sequence length for this model (38175 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  90%|████████▉ | 89500/100000 [01:05<00:08, 1191.34 examples/s]Running tokenizer on dataset (num_proc=16):  90%|█████████ | 90500/100000 [01:06<00:08, 1167.35 examples/s]Running tokenizer on dataset (num_proc=16):  91%|█████████ | 90750/100000 [01:06<00:08, 1122.61 examples/s]Running tokenizer on dataset (num_proc=16):  92%|█████████▏| 91750/100000 [01:07<00:06, 1185.17 examples/s]Running tokenizer on dataset (num_proc=16):  93%|█████████▎| 93000/100000 [01:08<00:06, 1074.77 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 94000/100000 [01:09<00:04, 1248.73 examples/s]Running tokenizer on dataset (num_proc=16):  95%|█████████▌| 95000/100000 [01:09<00:03, 1587.57 examples/s]Running tokenizer on dataset (num_proc=16):  95%|█████████▌| 95250/100000 [01:10<00:05, 874.88 examples/s] Running tokenizer on dataset (num_proc=16):  96%|█████████▋| 96250/100000 [01:11<00:03, 1093.43 examples/s]Running tokenizer on dataset (num_proc=16):  97%|█████████▋| 97250/100000 [01:12<00:02, 980.53 examples/s] Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 97500/100000 [01:12<00:02, 978.70 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 98500/100000 [01:14<00:01, 812.67 examples/s]Running tokenizer on dataset (num_proc=16):  99%|█████████▉| 98750/100000 [01:16<00:02, 513.91 examples/s]Running tokenizer on dataset (num_proc=16): 100%|█████████▉| 99750/100000 [01:17<00:00, 575.83 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 100000/100000 [01:18<00:00, 488.59 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 100000/100000 [01:18<00:00, 1267.50 examples/s]
Concatenating 16 shards
input_ids:
[12093, 70290, 6859, 6484, 43, 890, 27134, 2448, 7395, 30400, 55, 549, 26209, 198, 6622, 198, 16578, 70290, 6859, 6484, 43, 890, 27134, 2448, 7395, 30400, 55, 198, 6622, 198, 151643, 7039, 33976, 432, 646, 387, 264, 2699, 312, 2015, 1388, 311, 3270, 458, 4549, 911, 279, 10990, 7070, 14429, 3840, 438, 1052, 374, 825, 304, 1449, 1614, 14418, 504, 882, 311, 882, 11, 323, 279, 3482, 374, 49485, 448, 1105, 304, 1449, 4128, 369, 5019, 879, 30997, 311, 1349, 1105, 13, 2055, 358, 686, 36355, 304, 11629, 1246, 358, 5798, 323, 23983, 279, 1614, 624, 34762, 1635, 4134, 16145, 6635, 311, 1281, 1549, 862, 10990, 7070, 14429, 31496, 438, 807, 1030, 264, 501, 3093, 315, 50270, 82, 429, 1410, 15551, 279, 12188, 504, 2155, 11067, 323, 25941, 323, 8450, 1281, 64255, 323, 803, 35201, 9666, 13, 2379, 3381, 429, 3432, 419, 501, 5440, 432, 1035, 387, 2664, 311, 8193, 1549, 279, 330, 1040, 1211, 1, 1091, 894, 1008, 25546, 6174, 323, 432, 4977, 429, 807, 1033, 1290, 438, 1449, 1463, 7073, 10788, 825, 52163, 269, 803, 624, 9485, 501, 50270, 82, 1033, 537, 279, 1172, 501, 3166, 304, 1493, 4119, 11, 807, 1030, 264, 738, 315, 13918, 7746, 2669, 18663, 504, 279, 8151, 1137, 5527, 311, 41740, 11, 1045, 6548, 36780, 9317, 5479, 323, 501, 97685, 624, 2121, 5135, 438, 358, 8930, 279, 3745, 358, 1030, 264, 1602, 2797, 2168, 315, 279, 3093, 315, 1614, 358, 4829, 311, 1281, 11, 358, 1030, 3884, 10077, 315, 24248, 304, 279, 6467, 11, 1602, 76873, 12645, 98732, 448, 25386, 424, 429, 95758, 2310, 7218, 76024, 624, 2461, 419, 2390, 358, 6635, 311, 990, 279, 5235, 18457, 53514, 22293, 738, 369, 279, 10990, 7070, 14429, 429, 374, 2167, 14452, 323, 18304, 13942, 624, 2132, 4436, 944, 16965, 438, 279, 5479, 4946, 1602, 1632, 323, 279, 11221, 525, 1602, 2797, 11, 2337, 279, 1882, 582, 686, 1172, 614, 311, 1896, 2453, 979, 11589, 279, 2632, 5479, 311, 5648, 14719, 1105, 476, 11785, 11, 775, 14421, 1105, 13, 1634, 847, 4522, 504, 279, 7167, 572, 311, 4009, 264, 12896, 448, 279, 23603, 86968, 18824, 24569, 23704, 700, 11, 358, 23983, 279, 1614, 448, 5938, 12258, 25685, 12463, 438, 358, 5798, 432, 311, 614, 419, 12463, 2331, 369, 279, 2937, 330, 35012, 18824, 1, 14762, 358, 1035, 3796, 389, 279, 86968, 13, 2055, 358, 5798, 279, 44909, 323, 279, 64386, 9380, 15663, 279, 305, 9118, 11, 22696, 11, 39932, 13569, 11, 4992, 13, 14576, 311, 6707, 6177, 279, 65739, 448, 279, 15625, 476, 279, 68348, 13, 151643, 641, 3213, 1635, 358, 614, 65806, 3807, 52269, 14697, 304, 847, 2205, 3082, 13, 20035, 537, 3884, 894, 32319, 15570, 4730, 5926, 358, 572, 18442, 311, 1490, 458, 52269, 16262, 825, 315, 847, 22791, 14697, 264, 5625, 315, 2849, 4134, 13, 60033, 358, 9099, 847, 8849, 6249, 14046, 311, 1490, 421, 279, 52269, 374, 264, 5792, 20181, 13, 3197, 358, 10067, 279, 21852, 419, 6556, 358, 572, 33972, 311, 1490, 358, 614, 264, 6716, 315, 32319, 15570, 4730, 304, 21682, 13, 358, 2776, 10282, 4297, 27031, 1431, 304, 279, 3900, 429]
inputs:
@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject
@end
@implementation PodsDummy_XCDLumberjackNSLogger_OSX
@end
<|endoftext|>Nowadays it can be a bit reiterative to write an article about the Panzer III history as there is one in every model magazine from time to time, and the web is saturated with them in every language for everyone who desires to read them. So I will concentrate in telling how I built and painted the model.
Several years ago Dragon decided to make again their Panzer III kits as they had a new kind of moulds that could inject the plastic from different sides and angles and thus make thinner and more delicate pieces. They thought that having this new technology it would be better to produce again the "classics" than any other newer ones and it seems that they were right as every modeller bought one…..or more.
These new moulds were not the only new thing in these models, they had a set of tracks links already separated from the sprues ready to assemble, some photoetched metal parts and new decals.
As soon as I opened the box I had a very clear image of the kind of model I wanted to make, I had seen lots of photographs in the books, very dusty machines cramped with equipage that resembled old moving vans.
For this project I decided to use the Blackdog resin accessories set for the Panzer III that is really suitable and fits perfectly.
It isn't complicated as the parts fit very well and the instructions are very clear, during the process we will only have to take care when handling the little parts to avoid breaking them or worst, loosing them. As my idea from the beginning was to represent a tank with the desert camouflage painting badly worn out, I painted the model with German Dark Grey colour as I built it to have this colour base for the later "soap painting" technique I would apply on the camouflage. So I built the chassis and the turret leaving aside the hatches, wheels, antenna rail, etc. mainly to easily paint the scratches with the brush or the sponge.<|endoftext|>In recent years I have erected several owl boxes in my local area. Having not seen any barn owls recently I was pleased to see an owl entering one of my nest boxes a couple of days ago. Yesterday I placed my trail camera nearby to see if the owl is a regular visitor. When I checked the footage this morning I was delighted to see I have a pair of barn owls in residence. I'm keeping everything crossed now in the hope that
Caching indices mapping at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-b4170678a8e12e38.arrow
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 202140
})
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): Qwen2ForCausalLM(
      (model): Qwen2Model(
        (embed_tokens): Embedding(151936, 2048)
        (layers): ModuleList(
          (0): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (1): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (2): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (3): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (4): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (5): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (6): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (7): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (8): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (9): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (10): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (11): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (12): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (13): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (14): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (15): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (16): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (17): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (18): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (19): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (20): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (21): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (22): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (23): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
        )
        (norm): Qwen2RMSNorm()
      )
      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
    )
  )
)
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:586] 2024-06-27 23:40:11,967 >> Using auto half precision backend
[2024-06-27 23:40:12,252] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 202140
})
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 202140
})
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 202140
})
[2024-06-27 23:40:17,121] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-06-27 23:40:17,122] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-06-27 23:40:17,123] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-27 23:40:17,125] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-06-27 23:40:17,125] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-06-27 23:40:17,125] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-06-27 23:40:17,125] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-06-27 23:40:17,125] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-06-27 23:40:17,125] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: False
[2024-06-27 23:40:17,125] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-06-27 23:40:17,433] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-06-27 23:40:17,434] [INFO] [utils.py:803:see_memory_usage] MA 5.38 GB         Max_MA 5.38 GB         CA 5.66 GB         Max_CA 6 GB 
[2024-06-27 23:40:17,434] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 13.6%
[2024-06-27 23:40:17,647] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-06-27 23:40:17,648] [INFO] [utils.py:803:see_memory_usage] MA 5.38 GB         Max_MA 5.38 GB         CA 5.66 GB         Max_CA 6 GB 
[2024-06-27 23:40:17,648] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 13.6%
[2024-06-27 23:40:17,648] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-06-27 23:40:17,858] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-06-27 23:40:17,859] [INFO] [utils.py:803:see_memory_usage] MA 5.38 GB         Max_MA 5.38 GB         CA 5.66 GB         Max_CA 6 GB 
[2024-06-27 23:40:17,859] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 29.51 GB, percent = 13.6%
[2024-06-27 23:40:17,860] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-06-27 23:40:17,860] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-06-27 23:40:17,860] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-06-27 23:40:17,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2024-06-27 23:40:17,862] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-06-27 23:40:17,862] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-27 23:40:17,862] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-27 23:40:17,862] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-06-27 23:40:17,862] [INFO] [config.py:978:print]   amp_params ................... False
[2024-06-27 23:40:17,862] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-27 23:40:17,862] [INFO] [config.py:978:print]   bfloat16_enabled ............. False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f45287ee770>
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   dump_state ................... False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   fp16_auto_cast ............... False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   fp16_enabled ................. True
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 4
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 65536
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   loss_scale ................... 0
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-06-27 23:40:17,863] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   pld_params ................... False
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   train_batch_size ............. 128
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  8
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   world_size ................... 4
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-27 23:40:17,864] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-06-27 23:40:17,864] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": false
    }
}
[INFO|trainer.py:1748] 2024-06-27 23:40:17,865 >> ***** Running training *****
[INFO|trainer.py:1749] 2024-06-27 23:40:17,865 >>   Num examples = 202,140
[INFO|trainer.py:1750] 2024-06-27 23:40:17,865 >>   Num Epochs = 1
[INFO|trainer.py:1751] 2024-06-27 23:40:17,865 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1754] 2024-06-27 23:40:17,865 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:1755] 2024-06-27 23:40:17,865 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1756] 2024-06-27 23:40:17,865 >>   Total optimization steps = 1,579
[INFO|trainer.py:1757] 2024-06-27 23:40:17,866 >>   Number of trainable parameters = 98,304
06/27/2024 23:40:17 - WARNING - llmtuner.extras.callbacks - Previous log file in this folder will be deleted.
  0%|          | 0/1579 [00:00<?, ?it/s]  0%|          | 1/1579 [00:05<2:11:36,  5.00s/it]  0%|          | 2/1579 [00:09<2:07:06,  4.84s/it]  0%|          | 3/1579 [00:14<2:05:38,  4.78s/it]  0%|          | 4/1579 [00:19<2:04:54,  4.76s/it]  0%|          | 5/1579 [00:23<2:04:38,  4.75s/it]  0%|          | 6/1579 [00:28<2:04:31,  4.75s/it]  0%|          | 7/1579 [00:33<2:04:21,  4.75s/it]  1%|          | 8/1579 [00:38<2:04:18,  4.75s/it]  1%|          | 9/1579 [00:42<2:04:16,  4.75s/it]  1%|          | 10/1579 [00:47<2:04:26,  4.76s/it]                                                   {'loss': 2.6834, 'learning_rate': 0.00019998020793251, 'epoch': 0.01}
  1%|          | 10/1579 [00:47<2:04:26,  4.76s/it]                                                   {'router_ce_loss': 0.7689183354377747, 'old_lang_expert0_score': '0.73 0.73 0.2 0.2 0.12 0.83 0.29 0.1 0.06 0.95 0.04 0.96 0.15 0.28 0.98 0.97 0.43 0.88 0.97 0.5 0.87 0.1 0.99 0.0', 'epoch': 0.01}
  1%|          | 10/1579 [00:48<2:04:26,  4.76s/it]  1%|          | 11/1579 [00:52<2:04:26,  4.76s/it]  1%|          | 12/1579 [00:57<2:04:11,  4.76s/it]  1%|          | 13/1579 [01:01<2:03:58,  4.75s/it]  1%|          | 14/1579 [01:06<2:03:58,  4.75s/it]  1%|          | 15/1579 [01:11<2:04:00,  4.76s/it]  1%|          | 16/1579 [01:16<2:03:49,  4.75s/it]  1%|          | 17/1579 [01:20<2:03:48,  4.76s/it]  1%|          | 18/1579 [01:25<2:03:59,  4.77s/it]  1%|          | 19/1579 [01:30<2:03:56,  4.77s/it]  1%|▏         | 20/1579 [01:35<2:03:51,  4.77s/it]                                                   {'loss': 2.6622, 'learning_rate': 0.00019992083956455858, 'epoch': 0.01}
  1%|▏         | 20/1579 [01:35<2:03:51,  4.77s/it]                                                   {'router_ce_loss': 0.7587435841560364, 'old_lang_expert0_score': '0.77 0.76 0.19 0.23 0.14 0.85 0.31 0.11 0.08 0.95 0.06 0.97 0.17 0.28 0.98 0.97 0.46 0.87 0.97 0.42 0.93 0.1 1.0 0.02', 'epoch': 0.01}
  1%|▏         | 20/1579 [01:35<2:03:51,  4.77s/it]  1%|▏         | 21/1579 [01:40<2:03:48,  4.77s/it]  1%|▏         | 22/1579 [01:44<2:03:46,  4.77s/it]  1%|▏         | 23/1579 [01:49<2:03:42,  4.77s/it]  2%|▏         | 24/1579 [01:54<2:03:38,  4.77s/it]  2%|▏         | 25/1579 [01:59<2:03:34,  4.77s/it]  2%|▏         | 26/1579 [02:03<2:03:26,  4.77s/it]  2%|▏         | 27/1579 [02:08<2:03:30,  4.77s/it]  2%|▏         | 28/1579 [02:13<2:03:28,  4.78s/it]  2%|▏         | 29/1579 [02:18<2:03:26,  4.78s/it]  2%|▏         | 30/1579 [02:23<2:03:21,  4.78s/it]                                                   {'loss': 2.6883, 'learning_rate': 0.00019982191839660072, 'epoch': 0.02}
  2%|▏         | 30/1579 [02:23<2:03:21,  4.78s/it]                                                   {'router_ce_loss': 0.7096717953681946, 'old_lang_expert0_score': '0.74 0.76 0.21 0.25 0.18 0.86 0.35 0.17 0.12 0.95 0.11 0.97 0.21 0.26 0.98 0.97 0.56 0.91 0.96 0.63 0.96 0.29 1.0 0.16', 'epoch': 0.02}
  2%|▏         | 30/1579 [02:23<2:03:21,  4.78s/it]  2%|▏         | 31/1579 [02:27<2:03:20,  4.78s/it]  2%|▏         | 32/1579 [02:33<2:06:36,  4.91s/it]  2%|▏         | 33/1579 [02:37<2:05:38,  4.88s/it]  2%|▏         | 34/1579 [02:42<2:04:53,  4.85s/it]  2%|▏         | 35/1579 [02:47<2:04:09,  4.82s/it]  2%|▏         | 36/1579 [02:52<2:03:48,  4.81s/it]  2%|▏         | 37/1579 [02:56<2:03:29,  4.81s/it]  2%|▏         | 38/1579 [03:01<2:03:17,  4.80s/it]  2%|▏         | 39/1579 [03:06<2:03:08,  4.80s/it]  3%|▎         | 40/1579 [03:11<2:02:57,  4.79s/it]                                                   {'loss': 2.646, 'learning_rate': 0.00019968348358572507, 'epoch': 0.03}
  3%|▎         | 40/1579 [03:11<2:02:57,  4.79s/it]                                                   {'router_ce_loss': 0.6821575164794922, 'old_lang_expert0_score': '0.77 0.78 0.21 0.27 0.21 0.88 0.33 0.19 0.17 0.94 0.17 0.97 0.21 0.32 0.98 0.97 0.64 0.92 0.97 0.65 0.96 0.36 0.99 0.31', 'epoch': 0.03}
  3%|▎         | 40/1579 [03:11<2:02:57,  4.79s/it]  3%|▎         | 41/1579 [03:16<2:02:58,  4.80s/it]  3%|▎         | 42/1579 [03:20<2:02:55,  4.80s/it]  3%|▎         | 43/1579 [03:25<2:02:50,  4.80s/it]  3%|▎         | 44/1579 [03:30<2:02:43,  4.80s/it]  3%|▎         | 45/1579 [03:35<2:02:33,  4.79s/it]  3%|▎         | 46/1579 [03:40<2:02:29,  4.79s/it]  3%|▎         | 47/1579 [03:44<2:02:26,  4.80s/it]  3%|▎         | 48/1579 [03:49<2:02:23,  4.80s/it]  3%|▎         | 49/1579 [03:54<2:02:18,  4.80s/it]  3%|▎         | 50/1579 [03:59<2:02:08,  4.79s/it]                                                   {'loss': 2.6556, 'learning_rate': 0.000199505589930154, 'epoch': 0.03}
  3%|▎         | 50/1579 [03:59<2:02:08,  4.79s/it]                                                   {'router_ce_loss': 0.7137913703918457, 'old_lang_expert0_score': '0.83 0.82 0.2 0.27 0.18 0.86 0.35 0.19 0.18 0.96 0.16 0.98 0.25 0.37 0.99 0.98 0.51 0.87 0.96 0.32 0.95 0.21 1.0 0.22', 'epoch': 0.03}
  3%|▎         | 50/1579 [03:59<2:02:08,  4.79s/it]  3%|▎         | 51/1579 [04:04<2:02:14,  4.80s/it]  3%|▎         | 52/1579 [04:08<2:02:08,  4.80s/it]  3%|▎         | 53/1579 [04:13<2:02:06,  4.80s/it]  3%|▎         | 54/1579 [04:18<2:02:00,  4.80s/it]  3%|▎         | 55/1579 [04:23<2:01:53,  4.80s/it]  4%|▎         | 56/1579 [04:28<2:01:49,  4.80s/it]  4%|▎         | 57/1579 [04:32<2:01:37,  4.79s/it]  4%|▎         | 58/1579 [04:37<2:01:39,  4.80s/it]  4%|▎         | 59/1579 [04:42<2:01:28,  4.79s/it]  4%|▍         | 60/1579 [04:47<2:01:34,  4.80s/it]                                                   {'loss': 2.6184, 'learning_rate': 0.00019928830784755227, 'epoch': 0.04}
  4%|▍         | 60/1579 [04:47<2:01:34,  4.80s/it]                                                   {'router_ce_loss': 0.6547027826309204, 'old_lang_expert0_score': '0.81 0.82 0.22 0.3 0.23 0.87 0.4 0.3 0.27 0.95 0.29 0.98 0.31 0.46 0.99 0.98 0.59 0.88 0.96 0.52 0.95 0.38 1.0 0.42', 'epoch': 0.04}
  4%|▍         | 60/1579 [04:47<2:01:34,  4.80s/it]  4%|▍         | 61/1579 [04:52<2:01:32,  4.80s/it]  4%|▍         | 62/1579 [04:56<2:01:27,  4.80s/it]  4%|▍         | 63/1579 [05:01<2:03:04,  4.87s/it]  4%|▍         | 64/1579 [05:06<2:03:57,  4.91s/it]  4%|▍         | 65/1579 [05:11<2:03:07,  4.88s/it]  4%|▍         | 66/1579 [05:16<2:02:29,  4.86s/it]  4%|▍         | 67/1579 [05:21<2:02:00,  4.84s/it]  4%|▍         | 68/1579 [05:26<2:01:38,  4.83s/it]  4%|▍         | 69/1579 [05:30<2:01:16,  4.82s/it]  4%|▍         | 70/1579 [05:35<2:01:04,  4.81s/it]                                                   {'loss': 2.6131, 'learning_rate': 0.00019903172334715272, 'epoch': 0.04}
  4%|▍         | 70/1579 [05:35<2:01:04,  4.81s/it]                                                   {'router_ce_loss': 0.6374384164810181, 'old_lang_expert0_score': '0.8 0.82 0.24 0.33 0.28 0.87 0.45 0.32 0.32 0.95 0.34 0.97 0.36 0.45 0.99 0.96 0.61 0.89 0.96 0.52 0.95 0.43 1.0 0.45', 'epoch': 0.04}
  4%|▍         | 70/1579 [05:36<2:01:04,  4.81s/it]  4%|▍         | 71/1579 [05:40<2:00:47,  4.81s/it]  5%|▍         | 72/1579 [05:45<2:00:41,  4.81s/it]  5%|▍         | 73/1579 [05:50<2:00:30,  4.80s/it]  5%|▍         | 74/1579 [05:54<2:00:27,  4.80s/it]  5%|▍         | 75/1579 [05:59<2:00:26,  4.80s/it]  5%|▍         | 76/1579 [06:04<2:00:20,  4.80s/it]  5%|▍         | 77/1579 [06:09<2:00:16,  4.80s/it]  5%|▍         | 78/1579 [06:14<2:00:07,  4.80s/it]  5%|▌         | 79/1579 [06:18<2:00:08,  4.81s/it]  5%|▌         | 80/1579 [06:23<2:00:04,  4.81s/it]                                                   {'loss': 2.5825, 'learning_rate': 0.00019873593799571036, 'epoch': 0.05}
  5%|▌         | 80/1579 [06:23<2:00:04,  4.81s/it]                                                   {'router_ce_loss': 0.6255115866661072, 'old_lang_expert0_score': '0.81 0.82 0.23 0.34 0.29 0.89 0.44 0.35 0.33 0.94 0.38 0.97 0.38 0.52 0.99 0.98 0.61 0.89 0.96 0.53 0.95 0.46 1.0 0.47', 'epoch': 0.05}
  5%|▌         | 80/1579 [06:24<2:00:04,  4.81s/it]  5%|▌         | 81/1579 [06:28<2:00:01,  4.81s/it]  5%|▌         | 82/1579 [06:33<1:59:54,  4.81s/it]  5%|▌         | 83/1579 [06:38<1:59:46,  4.80s/it]  5%|▌         | 84/1579 [06:42<1:59:39,  4.80s/it]  5%|▌         | 85/1579 [06:47<1:59:32,  4.80s/it]  5%|▌         | 86/1579 [06:52<1:59:26,  4.80s/it]  6%|▌         | 87/1579 [06:57<1:59:23,  4.80s/it]  6%|▌         | 88/1579 [07:02<1:59:19,  4.80s/it]  6%|▌         | 89/1579 [07:06<1:59:16,  4.80s/it]  6%|▌         | 90/1579 [07:11<1:59:18,  4.81s/it]                                                   {'loss': 2.6233, 'learning_rate': 0.00019840106887729792, 'epoch': 0.06}
  6%|▌         | 90/1579 [07:11<1:59:18,  4.81s/it]                                                   {'router_ce_loss': 0.627193033695221, 'old_lang_expert0_score': '0.82 0.81 0.24 0.36 0.31 0.87 0.47 0.37 0.36 0.95 0.4 0.97 0.39 0.51 0.99 0.96 0.57 0.88 0.96 0.51 0.94 0.45 1.0 0.46', 'epoch': 0.06}
  6%|▌         | 90/1579 [07:12<1:59:18,  4.81s/it]  6%|▌         | 91/1579 [07:16<1:59:17,  4.81s/it]  6%|▌         | 92/1579 [07:21<1:59:09,  4.81s/it]  6%|▌         | 93/1579 [07:26<1:59:06,  4.81s/it]  6%|▌         | 94/1579 [07:31<1:59:02,  4.81s/it]  6%|▌         | 95/1579 [07:36<2:00:19,  4.87s/it]  6%|▌         | 96/1579 [07:41<2:01:28,  4.91s/it]  6%|▌         | 97/1579 [07:45<2:00:29,  4.88s/it]  6%|▌         | 98/1579 [07:50<1:59:51,  4.86s/it]  6%|▋         | 99/1579 [07:55<1:59:22,  4.84s/it]  6%|▋         | 100/1579 [08:00<1:58:59,  4.83s/it]                                                    {'loss': 2.5962, 'learning_rate': 0.00019802724854695928, 'epoch': 0.06}
  6%|▋         | 100/1579 [08:00<1:58:59,  4.83s/it]                                                    {'router_ce_loss': 0.5635106563568115, 'old_lang_expert0_score': '0.8 0.82 0.27 0.41 0.38 0.9 0.54 0.48 0.49 0.94 0.5 0.97 0.49 0.62 0.99 0.98 0.71 0.92 0.97 0.64 0.96 0.6 1.0 0.62', 'epoch': 0.06}
  6%|▋         | 100/1579 [08:00<1:58:59,  4.83s/it]  6%|▋         | 101/1579 [08:05<1:58:48,  4.82s/it]  6%|▋         | 102/1579 [08:09<1:58:32,  4.82s/it]  7%|▋         | 103/1579 [08:14<1:58:20,  4.81s/it]  7%|▋         | 104/1579 [08:19<1:58:15,  4.81s/it]  7%|▋         | 105/1579 [08:24<1:57:57,  4.80s/it]  7%|▋         | 106/1579 [08:29<1:57:55,  4.80s/it]  7%|▋         | 107/1579 [08:33<1:57:46,  4.80s/it]  7%|▋         | 108/1579 [08:38<1:57:44,  4.80s/it]  7%|▋         | 109/1579 [08:43<1:57:43,  4.80s/it]  7%|▋         | 110/1579 [08:48<1:57:37,  4.80s/it]                                                    {'loss': 2.6254, 'learning_rate': 0.00019761462497823855, 'epoch': 0.07}
  7%|▋         | 110/1579 [08:48<1:57:37,  4.80s/it]                                                    {'router_ce_loss': 0.5391778349876404, 'old_lang_expert0_score': '0.77 0.83 0.28 0.43 0.41 0.91 0.58 0.55 0.51 0.95 0.53 0.97 0.55 0.64 0.98 0.97 0.77 0.94 0.97 0.74 0.95 0.64 1.0 0.63', 'epoch': 0.07}
  7%|▋         | 110/1579 [08:48<1:57:37,  4.80s/it]  7%|▋         | 111/1579 [08:53<1:57:32,  4.80s/it]  7%|▋         | 112/1579 [08:57<1:57:25,  4.80s/it]  7%|▋         | 113/1579 [09:02<1:57:19,  4.80s/it]  7%|▋         | 114/1579 [09:07<1:57:12,  4.80s/it]  7%|▋         | 115/1579 [09:12<1:57:08,  4.80s/it]  7%|▋         | 116/1579 [09:17<1:57:03,  4.80s/it]  7%|▋         | 117/1579 [09:21<1:57:07,  4.81s/it]  7%|▋         | 118/1579 [09:26<1:56:58,  4.80s/it]  8%|▊         | 119/1579 [09:31<1:56:49,  4.80s/it]  8%|▊         | 120/1579 [09:36<1:56:41,  4.80s/it]                                                    {'loss': 2.6028, 'learning_rate': 0.00019716336150460615, 'epoch': 0.08}
  8%|▊         | 120/1579 [09:36<1:56:41,  4.80s/it]                                                    {'router_ce_loss': 0.6028503775596619, 'old_lang_expert0_score': '0.84 0.84 0.26 0.4 0.34 0.89 0.5 0.43 0.41 0.96 0.42 0.98 0.46 0.58 0.99 0.98 0.64 0.9 0.96 0.52 0.94 0.47 1.0 0.49', 'epoch': 0.08}
  8%|▊         | 120/1579 [09:36<1:56:41,  4.80s/it]  8%|▊         | 121/1579 [09:41<1:56:37,  4.80s/it]  8%|▊         | 122/1579 [09:45<1:56:35,  4.80s/it]  8%|▊         | 123/1579 [09:50<1:56:31,  4.80s/it]  8%|▊         | 124/1579 [09:55<1:56:23,  4.80s/it]  8%|▊         | 125/1579 [10:00<1:56:18,  4.80s/it]  8%|▊         | 126/1579 [10:05<1:56:15,  4.80s/it][2024-06-27 23:50:27,945] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
  8%|▊         | 127/1579 [10:10<1:57:25,  4.85s/it]  8%|▊         | 128/1579 [10:15<1:58:30,  4.90s/it]  8%|▊         | 129/1579 [10:19<1:57:41,  4.87s/it]  8%|▊         | 130/1579 [10:24<1:57:12,  4.85s/it]                                                    {'loss': 2.6127, 'learning_rate': 0.00019672433458137637, 'epoch': 0.08}
  8%|▊         | 130/1579 [10:24<1:57:12,  4.85s/it]                                                    {'router_ce_loss': 0.6054051518440247, 'old_lang_expert0_score': '0.84 0.85 0.26 0.38 0.35 0.89 0.53 0.42 0.4 0.96 0.43 0.98 0.44 0.55 0.99 0.98 0.62 0.9 0.97 0.5 0.95 0.46 1.0 0.46', 'epoch': 0.08}
  8%|▊         | 130/1579 [10:25<1:57:12,  4.85s/it]  8%|▊         | 131/1579 [10:29<1:56:42,  4.84s/it]  8%|▊         | 132/1579 [10:34<1:56:26,  4.83s/it]  8%|▊         | 133/1579 [10:39<1:56:04,  4.82s/it]  8%|▊         | 134/1579 [10:43<1:55:49,  4.81s/it]  9%|▊         | 135/1579 [10:48<1:55:38,  4.81s/it]  9%|▊         | 136/1579 [10:53<1:55:29,  4.80s/it]  9%|▊         | 137/1579 [10:58<1:55:24,  4.80s/it]  9%|▊         | 138/1579 [11:03<1:55:17,  4.80s/it]  9%|▉         | 139/1579 [11:07<1:55:08,  4.80s/it]  9%|▉         | 140/1579 [11:12<1:55:06,  4.80s/it]                                                    {'loss': 2.5905, 'learning_rate': 0.00019620015999525738, 'epoch': 0.09}
  9%|▉         | 140/1579 [11:12<1:55:06,  4.80s/it]                                                    {'router_ce_loss': 0.6089259386062622, 'old_lang_expert0_score': '0.85 0.84 0.26 0.39 0.36 0.88 0.5 0.38 0.38 0.96 0.39 0.98 0.44 0.55 0.99 0.98 0.61 0.9 0.97 0.52 0.95 0.45 1.0 0.47', 'epoch': 0.09}
  9%|▉         | 140/1579 [11:13<1:55:06,  4.80s/it]  9%|▉         | 141/1579 [11:17<1:55:07,  4.80s/it]  9%|▉         | 142/1579 [11:22<1:55:02,  4.80s/it]  9%|▉         | 143/1579 [11:27<1:54:58,  4.80s/it]  9%|▉         | 144/1579 [11:31<1:54:59,  4.81s/it]  9%|▉         | 145/1579 [11:36<1:54:58,  4.81s/it]  9%|▉         | 146/1579 [11:41<1:54:58,  4.81s/it]  9%|▉         | 147/1579 [11:46<1:54:45,  4.81s/it]  9%|▉         | 148/1579 [11:51<1:54:39,  4.81s/it]  9%|▉         | 149/1579 [11:55<1:54:33,  4.81s/it]  9%|▉         | 150/1579 [12:00<1:54:28,  4.81s/it]                                                    {'loss': 2.6244, 'learning_rate': 0.00019563790540795482, 'epoch': 0.09}
  9%|▉         | 150/1579 [12:00<1:54:28,  4.81s/it]                                                    {'router_ce_loss': 0.5501785278320312, 'old_lang_expert0_score': '0.81 0.84 0.33 0.47 0.45 0.91 0.6 0.53 0.52 0.96 0.51 0.98 0.55 0.63 0.99 0.96 0.71 0.91 0.97 0.63 0.96 0.59 1.0 0.62', 'epoch': 0.09}
  9%|▉         | 150/1579 [12:01<1:54:28,  4.81s/it] 10%|▉         | 151/1579 [12:05<1:54:25,  4.81s/it] 10%|▉         | 152/1579 [12:10<1:54:15,  4.80s/it] 10%|▉         | 153/1579 [12:15<1:54:14,  4.81s/it] 10%|▉         | 154/1579 [12:19<1:54:00,  4.80s/it] 10%|▉         | 155/1579 [12:24<1:53:59,  4.80s/it] 10%|▉         | 156/1579 [12:29<1:53:48,  4.80s/it] 10%|▉         | 157/1579 [12:34<1:53:49,  4.80s/it] 10%|█         | 158/1579 [12:39<1:54:58,  4.85s/it] 10%|█         | 159/1579 [12:44<1:56:00,  4.90s/it] 10%|█         | 160/1579 [12:49<1:55:12,  4.87s/it]                                                    {'loss': 2.5839, 'learning_rate': 0.00019503779338308344, 'epoch': 0.1}
 10%|█         | 160/1579 [12:49<1:55:12,  4.87s/it]                                                    {'router_ce_loss': 0.5785830616950989, 'old_lang_expert0_score': '0.82 0.84 0.31 0.41 0.4 0.9 0.53 0.47 0.45 0.96 0.46 0.98 0.49 0.6 0.99 0.97 0.64 0.91 0.97 0.65 0.9 0.53 0.97 0.53', 'epoch': 0.1}
 10%|█         | 160/1579 [12:49<1:55:12,  4.87s/it] 10%|█         | 161/1579 [12:53<1:54:37,  4.85s/it] 10%|█         | 162/1579 [12:58<1:54:14,  4.84s/it] 10%|█         | 163/1579 [13:03<1:53:54,  4.83s/it] 10%|█         | 164/1579 [13:08<1:53:43,  4.82s/it] 10%|█         | 165/1579 [13:13<1:53:28,  4.81s/it] 11%|█         | 166/1579 [13:17<1:53:16,  4.81s/it] 11%|█         | 167/1579 [13:22<1:53:01,  4.80s/it] 11%|█         | 168/1579 [13:27<1:52:57,  4.80s/it] 11%|█         | 169/1579 [13:32<1:52:46,  4.80s/it] 11%|█         | 170/1579 [13:37<1:52:45,  4.80s/it]                                                    {'loss': 2.596, 'learning_rate': 0.00019440006146979722, 'epoch': 0.11}
 11%|█         | 170/1579 [13:37<1:52:45,  4.80s/it]                                                    {'router_ce_loss': 0.7057785987854004, 'old_lang_expert0_score': '0.91 0.88 0.21 0.31 0.23 0.88 0.4 0.24 0.24 0.98 0.23 0.99 0.28 0.45 0.99 0.98 0.45 0.83 0.96 0.22 0.94 0.15 1.0 0.15', 'epoch': 0.11}
 11%|█         | 170/1579 [13:37<1:52:45,  4.80s/it] 11%|█         | 171/1579 [13:41<1:52:36,  4.80s/it] 11%|█         | 172/1579 [13:46<1:52:41,  4.81s/it] 11%|█         | 173/1579 [13:51<1:52:38,  4.81s/it] 11%|█         | 174/1579 [13:56<1:52:35,  4.81s/it] 11%|█         | 175/1579 [14:01<1:52:27,  4.81s/it] 11%|█         | 176/1579 [14:06<1:52:25,  4.81s/it] 11%|█         | 177/1579 [14:10<1:52:21,  4.81s/it] 11%|█▏        | 178/1579 [14:15<1:52:14,  4.81s/it] 11%|█▏        | 179/1579 [14:20<1:52:08,  4.81s/it] 11%|█▏        | 180/1579 [14:25<1:51:55,  4.80s/it]                                                    {'loss': 2.5952, 'learning_rate': 0.00019372496210875753, 'epoch': 0.11}
 11%|█▏        | 180/1579 [14:25<1:51:55,  4.80s/it]                                                    {'router_ce_loss': 0.5657709240913391, 'old_lang_expert0_score': '0.83 0.85 0.32 0.45 0.4 0.91 0.56 0.47 0.47 0.97 0.47 0.98 0.52 0.63 0.99 0.97 0.69 0.9 0.97 0.62 0.95 0.55 1.0 0.55', 'epoch': 0.11}
 11%|█▏        | 180/1579 [14:25<1:51:55,  4.80s/it] 11%|█▏        | 181/1579 [14:30<1:51:53,  4.80s/it] 12%|█▏        | 182/1579 [14:34<1:51:44,  4.80s/it] 12%|█▏        | 183/1579 [14:39<1:51:39,  4.80s/it] 12%|█▏        | 184/1579 [14:44<1:51:32,  4.80s/it] 12%|█▏        | 185/1579 [14:49<1:51:29,  4.80s/it] 12%|█▏        | 186/1579 [14:54<1:51:28,  4.80s/it] 12%|█▏        | 187/1579 [14:58<1:51:25,  4.80s/it] 12%|█▏        | 188/1579 [15:03<1:51:22,  4.80s/it] 12%|█▏        | 189/1579 [15:08<1:52:22,  4.85s/it] 12%|█▏        | 190/1579 [15:13<1:53:40,  4.91s/it]                                                    {'loss': 2.5899, 'learning_rate': 0.00019301276253220668, 'epoch': 0.12}
 12%|█▏        | 190/1579 [15:13<1:53:40,  4.91s/it]                                                    {'router_ce_loss': 0.45251843333244324, 'old_lang_expert0_score': '0.79 0.84 0.43 0.58 0.58 0.93 0.7 0.7 0.7 0.95 0.69 0.97 0.72 0.77 0.99 0.98 0.88 0.96 0.98 0.87 0.98 0.84 1.0 0.86', 'epoch': 0.12}
 12%|█▏        | 190/1579 [15:14<1:53:40,  4.91s/it] 12%|█▏        | 191/1579 [15:18<1:52:55,  4.88s/it] 12%|█▏        | 192/1579 [15:23<1:52:11,  4.85s/it] 12%|█▏        | 193/1579 [15:28<1:51:38,  4.83s/it] 12%|█▏        | 194/1579 [15:32<1:51:13,  4.82s/it] 12%|█▏        | 195/1579 [15:37<1:51:02,  4.81s/it] 12%|█▏        | 196/1579 [15:42<1:50:50,  4.81s/it] 12%|█▏        | 197/1579 [15:47<1:50:40,  4.80s/it] 13%|█▎        | 198/1579 [15:52<1:50:34,  4.80s/it] 13%|█▎        | 199/1579 [15:56<1:50:29,  4.80s/it] 13%|█▎        | 200/1579 [16:01<1:50:29,  4.81s/it]                                                    {'loss': 2.5885, 'learning_rate': 0.0001922637446581864, 'epoch': 0.13}
 13%|█▎        | 200/1579 [16:01<1:50:29,  4.81s/it]                                                    {'router_ce_loss': 0.6710510849952698, 'old_lang_expert0_score': '0.91 0.88 0.24 0.35 0.28 0.89 0.46 0.31 0.3 0.98 0.29 0.99 0.34 0.47 0.99 0.98 0.49 0.86 0.97 0.3 0.95 0.26 1.0 0.26', 'epoch': 0.13}
 13%|█▎        | 200/1579 [16:02<1:50:29,  4.81s/it] 13%|█▎        | 201/1579 [16:06<1:50:28,  4.81s/it] 13%|█▎        | 202/1579 [16:11<1:50:16,  4.80s/it] 13%|█▎        | 203/1579 [16:16<1:50:08,  4.80s/it] 13%|█▎        | 204/1579 [16:20<1:50:01,  4.80s/it] 13%|█▎        | 205/1579 [16:25<1:49:55,  4.80s/it] 13%|█▎        | 206/1579 [16:30<1:49:48,  4.80s/it] 13%|█▎        | 207/1579 [16:35<1:49:38,  4.80s/it] 13%|█▎        | 208/1579 [16:40<1:49:31,  4.79s/it] 13%|█▎        | 209/1579 [16:44<1:49:24,  4.79s/it] 13%|█▎        | 210/1579 [16:49<1:49:22,  4.79s/it]                                                    {'loss': 2.5993, 'learning_rate': 0.00019147820497894297, 'epoch': 0.13}
 13%|█▎        | 210/1579 [16:49<1:49:22,  4.79s/it]                                                    {'router_ce_loss': 0.5941724181175232, 'old_lang_expert0_score': '0.86 0.84 0.31 0.44 0.4 0.89 0.55 0.43 0.42 0.97 0.43 0.98 0.5 0.59 0.99 0.97 0.62 0.9 0.97 0.52 0.95 0.48 1.0 0.48', 'epoch': 0.13}
 13%|█▎        | 210/1579 [16:50<1:49:22,  4.79s/it] 13%|█▎        | 211/1579 [16:54<1:49:21,  4.80s/it] 13%|█▎        | 212/1579 [16:59<1:49:19,  4.80s/it] 13%|█▎        | 213/1579 [17:04<1:49:15,  4.80s/it] 14%|█▎        | 214/1579 [17:08<1:49:17,  4.80s/it] 14%|█▎        | 215/1579 [17:13<1:49:20,  4.81s/it] 14%|█▎        | 216/1579 [17:18<1:49:12,  4.81s/it] 14%|█▎        | 217/1579 [17:23<1:49:07,  4.81s/it] 14%|█▍        | 218/1579 [17:28<1:48:56,  4.80s/it] 14%|█▍        | 219/1579 [17:32<1:48:53,  4.80s/it] 14%|█▍        | 220/1579 [17:37<1:50:24,  4.87s/it]                                                    {'loss': 2.6048, 'learning_rate': 0.0001906564544435633, 'epoch': 0.14}
 14%|█▍        | 220/1579 [17:37<1:50:24,  4.87s/it]                                                    {'router_ce_loss': 0.6564158797264099, 'old_lang_expert0_score': '0.9 0.87 0.25 0.38 0.31 0.87 0.5 0.34 0.33 0.98 0.32 0.99 0.37 0.51 0.99 0.99 0.52 0.87 0.97 0.33 0.96 0.28 1.0 0.27', 'epoch': 0.14}
 14%|█▍        | 220/1579 [17:38<1:50:24,  4.87s/it] 14%|█▍        | 221/1579 [17:42<1:49:50,  4.85s/it] 14%|█▍        | 222/1579 [17:47<1:50:59,  4.91s/it] 14%|█▍        | 223/1579 [17:52<1:50:11,  4.88s/it] 14%|█▍        | 224/1579 [17:57<1:49:36,  4.85s/it] 14%|█▍        | 225/1579 [18:02<1:49:13,  4.84s/it] 14%|█▍        | 226/1579 [18:06<1:48:50,  4.83s/it] 14%|█▍        | 227/1579 [18:11<1:48:38,  4.82s/it] 14%|█▍        | 228/1579 [18:16<1:48:27,  4.82s/it] 15%|█▍        | 229/1579 [18:21<1:48:15,  4.81s/it] 15%|█▍        | 230/1579 [18:26<1:48:02,  4.81s/it]                                                    {'loss': 2.5914, 'learning_rate': 0.00018979881833488857, 'epoch': 0.15}
 15%|█▍        | 230/1579 [18:26<1:48:02,  4.81s/it]                                                    {'router_ce_loss': 0.6802699565887451, 'old_lang_expert0_score': '0.9 0.87 0.24 0.35 0.28 0.88 0.45 0.27 0.24 0.98 0.27 0.99 0.33 0.47 0.99 0.97 0.49 0.84 0.96 0.31 0.94 0.26 1.0 0.26', 'epoch': 0.15}
 15%|█▍        | 230/1579 [18:26<1:48:02,  4.81s/it] 15%|█▍        | 231/1579 [18:30<1:47:57,  4.81s/it] 15%|█▍        | 232/1579 [18:35<1:47:49,  4.80s/it] 15%|█▍        | 233/1579 [18:40<1:47:46,  4.80s/it] 15%|█▍        | 234/1579 [18:45<1:47:42,  4.80s/it] 15%|█▍        | 235/1579 [18:50<1:47:36,  4.80s/it] 15%|█▍        | 236/1579 [18:54<1:47:31,  4.80s/it] 15%|█▌        | 237/1579 [18:59<1:47:24,  4.80s/it] 15%|█▌        | 238/1579 [19:04<1:47:20,  4.80s/it] 15%|█▌        | 239/1579 [19:09<1:47:16,  4.80s/it] 15%|█▌        | 240/1579 [19:14<1:47:13,  4.80s/it]                                                    {'loss': 2.5746, 'learning_rate': 0.00018890563614075366, 'epoch': 0.15}
 15%|█▌        | 240/1579 [19:14<1:47:13,  4.80s/it]                                                    {'router_ce_loss': 0.6421168446540833, 'old_lang_expert0_score': '0.89 0.88 0.28 0.37 0.32 0.89 0.47 0.34 0.33 0.98 0.33 0.99 0.39 0.53 0.99 0.98 0.56 0.87 0.97 0.42 0.94 0.37 1.0 0.33', 'epoch': 0.15}
 15%|█▌        | 240/1579 [19:14<1:47:13,  4.80s/it] 15%|█▌        | 241/1579 [19:18<1:47:07,  4.80s/it] 15%|█▌        | 242/1579 [19:23<1:47:05,  4.81s/it] 15%|█▌        | 243/1579 [19:28<1:46:58,  4.80s/it] 15%|█▌        | 244/1579 [19:33<1:46:52,  4.80s/it] 16%|█▌        | 245/1579 [19:38<1:46:48,  4.80s/it] 16%|█▌        | 246/1579 [19:43<1:46:47,  4.81s/it] 16%|█▌        | 247/1579 [19:47<1:46:45,  4.81s/it] 16%|█▌        | 248/1579 [19:52<1:46:36,  4.81s/it] 16%|█▌        | 249/1579 [19:57<1:46:31,  4.81s/it] 16%|█▌        | 250/1579 [20:02<1:46:18,  4.80s/it]                                                    {'loss': 2.5686, 'learning_rate': 0.00018797726141960385, 'epoch': 0.16}
 16%|█▌        | 250/1579 [20:02<1:46:18,  4.80s/it]                                                    {'router_ce_loss': 0.5408660769462585, 'old_lang_expert0_score': '0.85 0.86 0.39 0.49 0.49 0.92 0.59 0.54 0.55 0.97 0.52 0.98 0.57 0.66 0.99 0.98 0.72 0.92 0.98 0.63 0.95 0.58 1.0 0.6', 'epoch': 0.16}
 16%|█▌        | 250/1579 [20:02<1:46:18,  4.80s/it] 16%|█▌        | 251/1579 [20:07<1:46:17,  4.80s/it] 16%|█▌        | 252/1579 [20:12<1:47:38,  4.87s/it] 16%|█▌        | 253/1579 [20:17<1:48:36,  4.91s/it] 16%|█▌        | 254/1579 [20:21<1:47:45,  4.88s/it] 16%|█▌        | 255/1579 [20:26<1:47:06,  4.85s/it] 16%|█▌        | 256/1579 [20:31<1:46:39,  4.84s/it] 16%|█▋        | 257/1579 [20:36<1:46:19,  4.83s/it] 16%|█▋        | 258/1579 [20:41<1:46:05,  4.82s/it] 16%|█▋        | 259/1579 [20:45<1:45:54,  4.81s/it] 16%|█▋        | 260/1579 [20:50<1:45:40,  4.81s/it]                                                    {'loss': 2.5735, 'learning_rate': 0.00018701406166054198, 'epoch': 0.16}
 16%|█▋        | 260/1579 [20:50<1:45:40,  4.81s/it]                                                    {'router_ce_loss': 0.5339038968086243, 'old_lang_expert0_score': '0.86 0.87 0.4 0.53 0.5 0.91 0.63 0.57 0.56 0.97 0.55 0.99 0.58 0.69 0.99 0.98 0.73 0.91 0.98 0.63 0.96 0.59 1.0 0.61', 'epoch': 0.16}
 16%|█▋        | 260/1579 [20:51<1:45:40,  4.81s/it] 17%|█▋        | 261/1579 [20:55<1:45:38,  4.81s/it] 17%|█▋        | 262/1579 [21:00<1:45:29,  4.81s/it] 17%|█▋        | 263/1579 [21:05<1:45:18,  4.80s/it] 17%|█▋        | 264/1579 [21:09<1:45:13,  4.80s/it] 17%|█▋        | 265/1579 [21:14<1:45:08,  4.80s/it] 17%|█▋        | 266/1579 [21:19<1:45:02,  4.80s/it] 17%|█▋        | 267/1579 [21:24<1:44:55,  4.80s/it] 17%|█▋        | 268/1579 [21:29<1:44:49,  4.80s/it] 17%|█▋        | 269/1579 [21:33<1:44:44,  4.80s/it] 17%|█▋        | 270/1579 [21:38<1:44:43,  4.80s/it]                                                    {'loss': 2.5924, 'learning_rate': 0.00018601641813786074, 'epoch': 0.17}
 17%|█▋        | 270/1579 [21:38<1:44:43,  4.80s/it]                                                    {'router_ce_loss': 0.6243935227394104, 'old_lang_expert0_score': '0.89 0.87 0.31 0.42 0.37 0.9 0.5 0.39 0.39 0.98 0.37 0.98 0.4 0.56 0.99 0.96 0.59 0.87 0.96 0.45 0.94 0.37 1.0 0.37', 'epoch': 0.17}
 17%|█▋        | 270/1579 [21:39<1:44:43,  4.80s/it] 17%|█▋        | 271/1579 [21:43<1:44:41,  4.80s/it] 17%|█▋        | 272/1579 [21:48<1:44:36,  4.80s/it] 17%|█▋        | 273/1579 [21:53<1:44:30,  4.80s/it] 17%|█▋        | 274/1579 [21:57<1:44:27,  4.80s/it] 17%|█▋        | 275/1579 [22:02<1:44:22,  4.80s/it] 17%|█▋        | 276/1579 [22:07<1:44:20,  4.80s/it] 18%|█▊        | 277/1579 [22:12<1:44:15,  4.80s/it] 18%|█▊        | 278/1579 [22:17<1:44:05,  4.80s/it] 18%|█▊        | 279/1579 [22:21<1:43:59,  4.80s/it] 18%|█▊        | 280/1579 [22:26<1:43:50,  4.80s/it]                                                    {'loss': 2.601, 'learning_rate': 0.0001849847257601188, 'epoch': 0.18}
 18%|█▊        | 280/1579 [22:26<1:43:50,  4.80s/it]                                                    {'router_ce_loss': 0.5533851981163025, 'old_lang_expert0_score': '0.87 0.86 0.38 0.49 0.47 0.92 0.57 0.53 0.52 0.97 0.47 0.98 0.53 0.65 0.99 0.98 0.71 0.9 0.97 0.6 0.96 0.56 1.0 0.58', 'epoch': 0.18}
 18%|█▊        | 280/1579 [22:27<1:43:50,  4.80s/it] 18%|█▊        | 281/1579 [22:31<1:43:45,  4.80s/it] 18%|█▊        | 282/1579 [22:36<1:43:39,  4.79s/it] 18%|█▊        | 283/1579 [22:41<1:45:01,  4.86s/it] 18%|█▊        | 284/1579 [22:46<1:45:46,  4.90s/it] 18%|█▊        | 285/1579 [22:51<1:45:06,  4.87s/it] 18%|█▊        | 286/1579 [22:55<1:44:25,  4.85s/it] 18%|█▊        | 287/1579 [23:00<1:44:07,  4.84s/it] 18%|█▊        | 288/1579 [23:05<1:43:41,  4.82s/it] 18%|█▊        | 289/1579 [23:10<1:43:31,  4.82s/it] 18%|█▊        | 290/1579 [23:15<1:43:16,  4.81s/it]                                                    {'loss': 2.5626, 'learning_rate': 0.0001839193929138195, 'epoch': 0.18}
 18%|█▊        | 290/1579 [23:15<1:43:16,  4.81s/it]                                                    {'router_ce_loss': 0.6357449889183044, 'old_lang_expert0_score': '0.91 0.88 0.29 0.42 0.34 0.9 0.48 0.37 0.37 0.98 0.35 0.99 0.39 0.53 0.99 0.97 0.56 0.88 0.97 0.38 0.95 0.34 1.0 0.34', 'epoch': 0.18}
 18%|█▊        | 290/1579 [23:15<1:43:16,  4.81s/it] 18%|█▊        | 291/1579 [23:19<1:43:08,  4.80s/it] 18%|█▊        | 292/1579 [23:24<1:43:01,  4.80s/it] 19%|█▊        | 293/1579 [23:29<1:42:56,  4.80s/it] 19%|█▊        | 294/1579 [23:34<1:42:48,  4.80s/it] 19%|█▊        | 295/1579 [23:39<1:42:41,  4.80s/it] 19%|█▊        | 296/1579 [23:43<1:42:38,  4.80s/it] 19%|█▉        | 297/1579 [23:48<1:42:33,  4.80s/it] 19%|█▉        | 298/1579 [23:53<1:42:30,  4.80s/it] 19%|█▉        | 299/1579 [23:58<1:42:26,  4.80s/it] 19%|█▉        | 300/1579 [24:03<1:42:19,  4.80s/it]                                                    {'loss': 2.5844, 'learning_rate': 0.00018282084130175476, 'epoch': 0.19}
 19%|█▉        | 300/1579 [24:03<1:42:19,  4.80s/it]                                                    {'router_ce_loss': 0.5277031660079956, 'old_lang_expert0_score': '0.85 0.86 0.43 0.54 0.52 0.92 0.64 0.58 0.57 0.97 0.57 0.98 0.61 0.7 0.99 0.98 0.74 0.92 0.98 0.63 0.95 0.59 1.0 0.58', 'epoch': 0.19}
 19%|█▉        | 300/1579 [24:03<1:42:19,  4.80s/it] 19%|█▉        | 301/1579 [24:07<1:42:13,  4.80s/it] 19%|█▉        | 302/1579 [24:12<1:42:11,  4.80s/it] 19%|█▉        | 303/1579 [24:17<1:42:08,  4.80s/it] 19%|█▉        | 304/1579 [24:22<1:41:57,  4.80s/it] 19%|█▉        | 305/1579 [24:27<1:41:54,  4.80s/it] 19%|█▉        | 306/1579 [24:31<1:41:47,  4.80s/it] 19%|█▉        | 307/1579 [24:36<1:41:43,  4.80s/it] 20%|█▉        | 308/1579 [24:41<1:41:35,  4.80s/it] 20%|█▉        | 309/1579 [24:46<1:41:36,  4.80s/it] 20%|█▉        | 310/1579 [24:51<1:41:26,  4.80s/it]                                                    {'loss': 2.6256, 'learning_rate': 0.0001816895057760775, 'epoch': 0.2}
 20%|█▉        | 310/1579 [24:51<1:41:26,  4.80s/it]                                                    {'router_ce_loss': 0.48353925347328186, 'old_lang_expert0_score': '0.84 0.87 0.49 0.61 0.59 0.93 0.67 0.65 0.66 0.97 0.64 0.98 0.68 0.76 0.99 0.98 0.79 0.94 0.98 0.74 0.96 0.71 1.0 0.72', 'epoch': 0.2}
 20%|█▉        | 310/1579 [24:51<1:41:26,  4.80s/it] 20%|█▉        | 311/1579 [24:55<1:41:25,  4.80s/it] 20%|█▉        | 312/1579 [25:00<1:41:20,  4.80s/it] 20%|█▉        | 313/1579 [25:05<1:41:23,  4.81s/it] 20%|█▉        | 314/1579 [25:10<1:42:40,  4.87s/it] 20%|█▉        | 315/1579 [25:15<1:43:18,  4.90s/it] 20%|██        | 316/1579 [25:20<1:42:34,  4.87s/it] 20%|██        | 317/1579 [25:25<1:42:03,  4.85s/it] 20%|██        | 318/1579 [25:29<1:41:39,  4.84s/it] 20%|██        | 319/1579 [25:34<1:41:16,  4.82s/it] 20%|██        | 320/1579 [25:39<1:41:03,  4.82s/it]                                                    {'loss': 2.5672, 'learning_rate': 0.0001805258341661693, 'epoch': 0.2}
 20%|██        | 320/1579 [25:39<1:41:03,  4.82s/it]                                                    {'router_ce_loss': 0.6308156251907349, 'old_lang_expert0_score': '0.9 0.88 0.31 0.41 0.35 0.9 0.51 0.38 0.37 0.98 0.35 0.99 0.41 0.55 0.99 0.99 0.57 0.87 0.97 0.41 0.95 0.36 1.0 0.36', 'epoch': 0.2}
 20%|██        | 320/1579 [25:39<1:41:03,  4.82s/it] 20%|██        | 321/1579 [25:44<1:40:49,  4.81s/it] 20%|██        | 322/1579 [25:49<1:40:41,  4.81s/it] 20%|██        | 323/1579 [25:53<1:40:34,  4.80s/it] 21%|██        | 324/1579 [25:58<1:40:26,  4.80s/it] 21%|██        | 325/1579 [26:03<1:40:16,  4.80s/it] 21%|██        | 326/1579 [26:08<1:40:17,  4.80s/it] 21%|██        | 327/1579 [26:13<1:40:16,  4.81s/it] 21%|██        | 328/1579 [26:17<1:40:11,  4.81s/it] 21%|██        | 329/1579 [26:22<1:40:07,  4.81s/it] 21%|██        | 330/1579 [26:27<1:40:04,  4.81s/it]                                                    {'loss': 2.6106, 'learning_rate': 0.0001793302871013709, 'epoch': 0.21}
 21%|██        | 330/1579 [26:27<1:40:04,  4.81s/it]                                                    {'router_ce_loss': 0.5883308053016663, 'old_lang_expert0_score': '0.88 0.86 0.36 0.46 0.42 0.91 0.53 0.45 0.44 0.98 0.4 0.99 0.49 0.6 0.99 0.99 0.63 0.91 0.98 0.54 0.95 0.48 1.0 0.47', 'epoch': 0.21}
 21%|██        | 330/1579 [26:27<1:40:04,  4.81s/it] 21%|██        | 331/1579 [26:32<1:40:00,  4.81s/it] 21%|██        | 332/1579 [26:37<1:39:47,  4.80s/it] 21%|██        | 333/1579 [26:41<1:39:45,  4.80s/it] 21%|██        | 334/1579 [26:46<1:39:40,  4.80s/it] 21%|██        | 335/1579 [26:51<1:39:37,  4.80s/it] 21%|██▏       | 336/1579 [26:56<1:39:31,  4.80s/it] 21%|██▏       | 337/1579 [27:01<1:39:23,  4.80s/it] 21%|██▏       | 338/1579 [27:05<1:39:16,  4.80s/it] 21%|██▏       | 339/1579 [27:10<1:39:14,  4.80s/it] 22%|██▏       | 340/1579 [27:15<1:39:13,  4.80s/it]                                                    {'loss': 2.6303, 'learning_rate': 0.00017810333782864625, 'epoch': 0.22}
 22%|██▏       | 340/1579 [27:15<1:39:13,  4.80s/it]                                                    {'router_ce_loss': 0.5286937355995178, 'old_lang_expert0_score': '0.86 0.88 0.46 0.55 0.52 0.92 0.64 0.57 0.56 0.98 0.55 0.99 0.6 0.67 0.99 0.98 0.72 0.92 0.98 0.63 0.97 0.61 1.0 0.6', 'epoch': 0.22}
 22%|██▏       | 340/1579 [27:15<1:39:13,  4.80s/it] 22%|██▏       | 341/1579 [27:20<1:39:14,  4.81s/it] 22%|██▏       | 342/1579 [27:25<1:39:02,  4.80s/it] 22%|██▏       | 343/1579 [27:29<1:39:01,  4.81s/it] 22%|██▏       | 344/1579 [27:34<1:38:53,  4.80s/it] 22%|██▏       | 345/1579 [27:39<1:40:10,  4.87s/it] 22%|██▏       | 346/1579 [27:44<1:39:39,  4.85s/it] 22%|██▏       | 347/1579 [27:49<1:40:47,  4.91s/it] 22%|██▏       | 348/1579 [27:54<1:40:02,  4.88s/it] 22%|██▏       | 349/1579 [27:59<1:39:29,  4.85s/it] 22%|██▏       | 350/1579 [28:03<1:39:07,  4.84s/it]                                                    {'loss': 2.5763, 'learning_rate': 0.00017684547202525156, 'epoch': 0.22}
 22%|██▏       | 350/1579 [28:03<1:39:07,  4.84s/it]                                                    {'router_ce_loss': 0.6242802143096924, 'old_lang_expert0_score': '0.91 0.87 0.33 0.44 0.37 0.9 0.53 0.39 0.37 0.98 0.37 0.99 0.44 0.57 1.0 0.99 0.59 0.87 0.97 0.41 0.95 0.37 1.0 0.37', 'epoch': 0.22}
 22%|██▏       | 350/1579 [28:04<1:39:07,  4.84s/it] 22%|██▏       | 351/1579 [28:08<1:38:46,  4.83s/it] 22%|██▏       | 352/1579 [28:13<1:38:37,  4.82s/it] 22%|██▏       | 353/1579 [28:18<1:38:26,  4.82s/it] 22%|██▏       | 354/1579 [28:23<1:38:22,  4.82s/it] 22%|██▏       | 355/1579 [28:28<1:38:10,  4.81s/it] 23%|██▎       | 356/1579 [28:32<1:37:57,  4.81s/it] 23%|██▎       | 357/1579 [28:37<1:37:48,  4.80s/it] 23%|██▎       | 358/1579 [28:42<1:37:37,  4.80s/it] 23%|██▎       | 359/1579 [28:47<1:37:34,  4.80s/it] 23%|██▎       | 360/1579 [28:52<1:37:33,  4.80s/it]                                                    {'loss': 2.5923, 'learning_rate': 0.00017555718760648432, 'epoch': 0.23}
 23%|██▎       | 360/1579 [28:52<1:37:33,  4.80s/it]                                                    {'router_ce_loss': 0.5812205076217651, 'old_lang_expert0_score': '0.9 0.89 0.38 0.47 0.42 0.92 0.55 0.48 0.47 0.98 0.46 0.99 0.5 0.64 0.99 0.98 0.65 0.89 0.97 0.52 0.95 0.48 1.0 0.48', 'epoch': 0.23}
 23%|██▎       | 360/1579 [28:52<1:37:33,  4.80s/it] 23%|██▎       | 361/1579 [28:56<1:37:28,  4.80s/it] 23%|██▎       | 362/1579 [29:01<1:37:19,  4.80s/it] 23%|██▎       | 363/1579 [29:06<1:37:12,  4.80s/it] 23%|██▎       | 364/1579 [29:11<1:37:09,  4.80s/it] 23%|██▎       | 365/1579 [29:15<1:37:03,  4.80s/it] 23%|██▎       | 366/1579 [29:20<1:36:57,  4.80s/it] 23%|██▎       | 367/1579 [29:25<1:36:58,  4.80s/it] 23%|██▎       | 368/1579 [29:30<1:36:59,  4.81s/it] 23%|██▎       | 369/1579 [29:35<1:36:54,  4.81s/it] 23%|██▎       | 370/1579 [29:40<1:36:43,  4.80s/it]                                                    {'loss': 2.6055, 'learning_rate': 0.00017423899452858777, 'epoch': 0.23}
 23%|██▎       | 370/1579 [29:40<1:36:43,  4.80s/it]                                                    {'router_ce_loss': 0.6181288361549377, 'old_lang_expert0_score': '0.89 0.89 0.35 0.45 0.38 0.9 0.51 0.38 0.38 0.98 0.4 0.99 0.42 0.56 1.0 0.98 0.58 0.88 0.97 0.44 0.95 0.4 1.0 0.39', 'epoch': 0.23}
 23%|██▎       | 370/1579 [29:40<1:36:43,  4.80s/it] 23%|██▎       | 371/1579 [29:44<1:36:44,  4.80s/it] 24%|██▎       | 372/1579 [29:49<1:36:35,  4.80s/it] 24%|██▎       | 373/1579 [29:54<1:36:32,  4.80s/it] 24%|██▎       | 374/1579 [29:59<1:36:25,  4.80s/it] 24%|██▎       | 375/1579 [30:04<1:36:19,  4.80s/it] 24%|██▍       | 376/1579 [30:08<1:36:16,  4.80s/it] 24%|██▍       | 377/1579 [30:13<1:37:42,  4.88s/it] 24%|██▍       | 378/1579 [30:18<1:38:33,  4.92s/it] 24%|██▍       | 379/1579 [30:23<1:37:41,  4.88s/it] 24%|██▍       | 380/1579 [30:28<1:37:08,  4.86s/it]                                                    {'loss': 2.6041, 'learning_rate': 0.00017289141458688917, 'epoch': 0.24}
 24%|██▍       | 380/1579 [30:28<1:37:08,  4.86s/it]                                                    {'router_ce_loss': 0.6936145424842834, 'old_lang_expert0_score': '0.93 0.87 0.25 0.34 0.27 0.88 0.42 0.26 0.27 0.99 0.24 0.99 0.32 0.47 1.0 0.99 0.45 0.84 0.97 0.31 0.87 0.21 0.96 0.2', 'epoch': 0.24}
 24%|██▍       | 380/1579 [30:28<1:37:08,  4.86s/it] 24%|██▍       | 381/1579 [30:33<1:36:45,  4.85s/it] 24%|██▍       | 382/1579 [30:38<1:36:30,  4.84s/it] 24%|██▍       | 383/1579 [30:42<1:36:11,  4.83s/it] 24%|██▍       | 384/1579 [30:47<1:35:58,  4.82s/it] 24%|██▍       | 385/1579 [30:52<1:35:42,  4.81s/it] 24%|██▍       | 386/1579 [30:57<1:35:38,  4.81s/it] 25%|██▍       | 387/1579 [31:02<1:35:26,  4.80s/it] 25%|██▍       | 388/1579 [31:06<1:35:22,  4.80s/it] 25%|██▍       | 389/1579 [31:11<1:35:12,  4.80s/it] 25%|██▍       | 390/1579 [31:16<1:35:10,  4.80s/it]                                                    {'loss': 2.5988, 'learning_rate': 0.00017151498120925165, 'epoch': 0.25}
 25%|██▍       | 390/1579 [31:16<1:35:10,  4.80s/it]                                                    {'router_ce_loss': 0.6252633929252625, 'old_lang_expert0_score': '0.89 0.89 0.33 0.44 0.37 0.91 0.51 0.39 0.39 0.98 0.37 0.99 0.43 0.56 0.99 0.97 0.58 0.87 0.97 0.41 0.95 0.38 1.0 0.38', 'epoch': 0.25}
 25%|██▍       | 390/1579 [31:16<1:35:10,  4.80s/it] 25%|██▍       | 391/1579 [31:21<1:35:00,  4.80s/it] 25%|██▍       | 392/1579 [31:26<1:34:57,  4.80s/it] 25%|██▍       | 393/1579 [31:30<1:34:49,  4.80s/it] 25%|██▍       | 394/1579 [31:35<1:34:45,  4.80s/it] 25%|██▌       | 395/1579 [31:40<1:34:40,  4.80s/it] 25%|██▌       | 396/1579 [31:45<1:34:41,  4.80s/it] 25%|██▌       | 397/1579 [31:50<1:34:34,  4.80s/it] 25%|██▌       | 398/1579 [31:54<1:34:30,  4.80s/it] 25%|██▌       | 399/1579 [31:59<1:34:27,  4.80s/it] 25%|██▌       | 400/1579 [32:04<1:34:22,  4.80s/it]                                                    {'loss': 2.5987, 'learning_rate': 0.00017011023924492125, 'epoch': 0.25}
 25%|██▌       | 400/1579 [32:04<1:34:22,  4.80s/it]                                                    {'router_ce_loss': 0.5237529277801514, 'old_lang_expert0_score': '0.87 0.88 0.46 0.56 0.53 0.92 0.64 0.58 0.57 0.97 0.53 0.98 0.61 0.67 0.99 0.98 0.72 0.94 0.98 0.64 0.97 0.61 1.0 0.62', 'epoch': 0.25}
 25%|██▌       | 400/1579 [32:04<1:34:22,  4.80s/it] 25%|██▌       | 401/1579 [32:09<1:34:23,  4.81s/it] 25%|██▌       | 402/1579 [32:14<1:34:18,  4.81s/it] 26%|██▌       | 403/1579 [32:18<1:34:08,  4.80s/it] 26%|██▌       | 404/1579 [32:23<1:33:59,  4.80s/it] 26%|██▌       | 405/1579 [32:28<1:33:54,  4.80s/it] 26%|██▌       | 406/1579 [32:33<1:33:50,  4.80s/it] 26%|██▌       | 407/1579 [32:38<1:33:49,  4.80s/it] 26%|██▌       | 408/1579 [32:43<1:35:01,  4.87s/it] 26%|██▌       | 409/1579 [32:48<1:35:56,  4.92s/it] 26%|██▌       | 410/1579 [32:53<1:35:13,  4.89s/it]                                                    {'loss': 2.6127, 'learning_rate': 0.00016867774474885332, 'epoch': 0.26}
 26%|██▌       | 410/1579 [32:53<1:35:13,  4.89s/it]                                                    {'router_ce_loss': 0.5241629481315613, 'old_lang_expert0_score': '0.87 0.88 0.46 0.57 0.54 0.91 0.67 0.59 0.57 0.98 0.56 0.99 0.61 0.69 0.99 0.98 0.71 0.93 0.98 0.64 0.97 0.61 1.0 0.61', 'epoch': 0.26}
 26%|██▌       | 410/1579 [32:53<1:35:13,  4.89s/it] 26%|██▌       | 411/1579 [32:57<1:34:42,  4.86s/it] 26%|██▌       | 412/1579 [33:02<1:34:13,  4.84s/it] 26%|██▌       | 413/1579 [33:07<1:33:49,  4.83s/it] 26%|██▌       | 414/1579 [33:12<1:33:34,  4.82s/it] 26%|██▋       | 415/1579 [33:17<1:33:22,  4.81s/it] 26%|██▋       | 416/1579 [33:21<1:33:17,  4.81s/it] 26%|██▋       | 417/1579 [33:26<1:33:08,  4.81s/it] 26%|██▋       | 418/1579 [33:31<1:32:57,  4.80s/it] 27%|██▋       | 419/1579 [33:36<1:32:52,  4.80s/it] 27%|██▋       | 420/1579 [33:41<1:32:43,  4.80s/it]                                                    {'loss': 2.5991, 'learning_rate': 0.00016721806476160272, 'epoch': 0.27}
 27%|██▋       | 420/1579 [33:41<1:32:43,  4.80s/it]                                                    {'router_ce_loss': 0.6276912689208984, 'old_lang_expert0_score': '0.92 0.88 0.31 0.41 0.36 0.91 0.49 0.38 0.37 0.99 0.32 0.99 0.42 0.56 1.0 0.98 0.59 0.87 0.97 0.45 0.94 0.39 1.0 0.36', 'epoch': 0.27}
 27%|██▋       | 420/1579 [33:41<1:32:43,  4.80s/it] 27%|██▋       | 421/1579 [33:45<1:32:41,  4.80s/it] 27%|██▋       | 422/1579 [33:50<1:32:34,  4.80s/it] 27%|██▋       | 423/1579 [33:55<1:32:34,  4.80s/it] 27%|██▋       | 424/1579 [34:00<1:32:26,  4.80s/it] 27%|██▋       | 425/1579 [34:05<1:32:25,  4.81s/it] 27%|██▋       | 426/1579 [34:09<1:32:20,  4.81s/it] 27%|██▋       | 427/1579 [34:14<1:32:18,  4.81s/it] 27%|██▋       | 428/1579 [34:19<1:32:11,  4.81s/it] 27%|██▋       | 429/1579 [34:24<1:32:04,  4.80s/it] 27%|██▋       | 430/1579 [34:29<1:31:55,  4.80s/it]                                                    {'loss': 2.5827, 'learning_rate': 0.0001657317770848659, 'epoch': 0.27}
 27%|██▋       | 430/1579 [34:29<1:31:55,  4.80s/it]                                                    {'router_ce_loss': 0.5985155701637268, 'old_lang_expert0_score': '0.91 0.89 0.36 0.46 0.41 0.91 0.55 0.44 0.42 0.98 0.41 0.99 0.48 0.6 0.99 0.98 0.62 0.9 0.97 0.48 0.95 0.44 1.0 0.41', 'epoch': 0.27}
 27%|██▋       | 430/1579 [34:29<1:31:55,  4.80s/it] 27%|██▋       | 431/1579 [34:33<1:31:52,  4.80s/it] 27%|██▋       | 432/1579 [34:38<1:31:47,  4.80s/it] 27%|██▋       | 433/1579 [34:43<1:31:43,  4.80s/it] 27%|██▋       | 434/1579 [34:48<1:31:38,  4.80s/it] 28%|██▊       | 435/1579 [34:53<1:31:29,  4.80s/it] 28%|██▊       | 436/1579 [34:57<1:31:25,  4.80s/it] 28%|██▊       | 437/1579 [35:02<1:31:18,  4.80s/it] 28%|██▊       | 438/1579 [35:07<1:31:13,  4.80s/it] 28%|██▊       | 439/1579 [35:12<1:32:31,  4.87s/it] 28%|██▊       | 440/1579 [35:17<1:33:22,  4.92s/it]                                                    {'loss': 2.5857, 'learning_rate': 0.000164219470052763, 'epoch': 0.28}
 28%|██▊       | 440/1579 [35:17<1:33:22,  4.92s/it]                                                    {'router_ce_loss': 0.5890861749649048, 'old_lang_expert0_score': '0.92 0.89 0.37 0.46 0.42 0.92 0.55 0.43 0.41 0.99 0.39 0.99 0.47 0.6 1.0 0.98 0.65 0.89 0.98 0.53 0.96 0.48 1.0 0.46', 'epoch': 0.28}
 28%|██▊       | 440/1579 [35:17<1:33:22,  4.92s/it] 28%|██▊       | 441/1579 [35:22<1:32:41,  4.89s/it] 28%|██▊       | 442/1579 [35:27<1:32:10,  4.86s/it] 28%|██▊       | 443/1579 [35:31<1:31:47,  4.85s/it] 28%|██▊       | 444/1579 [35:36<1:31:27,  4.83s/it] 28%|██▊       | 445/1579 [35:41<1:31:13,  4.83s/it] 28%|██▊       | 446/1579 [35:46<1:30:59,  4.82s/it] 28%|██▊       | 447/1579 [35:51<1:30:47,  4.81s/it] 28%|██▊       | 448/1579 [35:55<1:30:38,  4.81s/it] 28%|██▊       | 449/1579 [36:00<1:30:27,  4.80s/it] 28%|██▊       | 450/1579 [36:05<1:30:21,  4.80s/it]                                                    {'loss': 2.6264, 'learning_rate': 0.00016268174229895093, 'epoch': 0.28}
 28%|██▊       | 450/1579 [36:05<1:30:21,  4.80s/it]                                                    {'router_ce_loss': 0.514337420463562, 'old_lang_expert0_score': '0.86 0.88 0.52 0.59 0.55 0.92 0.68 0.61 0.61 0.97 0.57 0.99 0.63 0.71 0.99 0.99 0.74 0.91 0.98 0.65 0.96 0.63 1.0 0.62', 'epoch': 0.28}
 28%|██▊       | 450/1579 [36:05<1:30:21,  4.80s/it] 29%|██▊       | 451/1579 [36:10<1:30:17,  4.80s/it] 29%|██▊       | 452/1579 [36:15<1:30:14,  4.80s/it] 29%|██▊       | 453/1579 [36:19<1:30:11,  4.81s/it] 29%|██▉       | 454/1579 [36:24<1:29:59,  4.80s/it] 29%|██▉       | 455/1579 [36:29<1:29:59,  4.80s/it] 29%|██▉       | 456/1579 [36:34<1:29:52,  4.80s/it] 29%|██▉       | 457/1579 [36:39<1:29:48,  4.80s/it] 29%|██▉       | 458/1579 [36:43<1:29:45,  4.80s/it] 29%|██▉       | 459/1579 [36:48<1:29:39,  4.80s/it] 29%|██▉       | 460/1579 [36:53<1:29:30,  4.80s/it]                                                    {'loss': 2.5909, 'learning_rate': 0.00016111920251965937, 'epoch': 0.29}
 29%|██▉       | 460/1579 [36:53<1:29:30,  4.80s/it]                                                    {'router_ce_loss': 0.7209239602088928, 'old_lang_expert0_score': '0.95 0.88 0.21 0.31 0.23 0.88 0.39 0.21 0.18 0.99 0.18 0.99 0.26 0.42 1.0 0.98 0.44 0.84 0.97 0.2 0.93 0.15 1.0 0.13', 'epoch': 0.29}
 29%|██▉       | 460/1579 [36:53<1:29:30,  4.80s/it] 29%|██▉       | 461/1579 [36:58<1:29:25,  4.80s/it] 29%|██▉       | 462/1579 [37:03<1:29:20,  4.80s/it] 29%|██▉       | 463/1579 [37:07<1:29:13,  4.80s/it] 29%|██▉       | 464/1579 [37:12<1:29:09,  4.80s/it] 29%|██▉       | 465/1579 [37:17<1:29:05,  4.80s/it] 30%|██▉       | 466/1579 [37:22<1:29:02,  4.80s/it] 30%|██▉       | 467/1579 [37:27<1:29:00,  4.80s/it] 30%|██▉       | 468/1579 [37:31<1:28:51,  4.80s/it] 30%|██▉       | 469/1579 [37:36<1:28:48,  4.80s/it] 30%|██▉       | 470/1579 [37:41<1:30:02,  4.87s/it]                                                    {'loss': 2.5883, 'learning_rate': 0.0001595324692327437, 'epoch': 0.3}
 30%|██▉       | 470/1579 [37:41<1:30:02,  4.87s/it]                                                    {'router_ce_loss': 0.6213327050209045, 'old_lang_expert0_score': '0.91 0.88 0.34 0.44 0.38 0.89 0.54 0.4 0.38 0.98 0.38 0.99 0.45 0.56 1.0 0.99 0.57 0.88 0.97 0.42 0.95 0.36 1.0 0.37', 'epoch': 0.3}
 30%|██▉       | 470/1579 [37:42<1:30:02,  4.87s/it] 30%|██▉       | 471/1579 [37:46<1:29:33,  4.85s/it] 30%|██▉       | 472/1579 [37:51<1:30:23,  4.90s/it] 30%|██▉       | 473/1579 [37:56<1:29:47,  4.87s/it] 30%|███       | 474/1579 [38:01<1:29:18,  4.85s/it] 30%|███       | 475/1579 [38:06<1:28:59,  4.84s/it] 30%|███       | 476/1579 [38:10<1:28:42,  4.83s/it] 30%|███       | 477/1579 [38:15<1:28:33,  4.82s/it] 30%|███       | 478/1579 [38:20<1:28:24,  4.82s/it] 30%|███       | 479/1579 [38:25<1:28:14,  4.81s/it] 30%|███       | 480/1579 [38:30<1:28:12,  4.82s/it]                                                    {'loss': 2.601, 'learning_rate': 0.00015792217053284998, 'epoch': 0.3}
 30%|███       | 480/1579 [38:30<1:28:12,  4.82s/it]                                                    {'router_ce_loss': 0.5736478567123413, 'old_lang_expert0_score': '0.89 0.89 0.43 0.51 0.46 0.91 0.6 0.49 0.47 0.98 0.46 0.99 0.52 0.63 0.99 0.98 0.66 0.89 0.97 0.52 0.95 0.49 1.0 0.49', 'epoch': 0.3}
 30%|███       | 480/1579 [38:30<1:28:12,  4.82s/it] 30%|███       | 481/1579 [38:34<1:28:03,  4.81s/it] 31%|███       | 482/1579 [38:39<1:27:55,  4.81s/it] 31%|███       | 483/1579 [38:44<1:27:45,  4.80s/it] 31%|███       | 484/1579 [38:49<1:27:38,  4.80s/it] 31%|███       | 485/1579 [38:54<1:27:33,  4.80s/it] 31%|███       | 486/1579 [38:58<1:27:29,  4.80s/it] 31%|███       | 487/1579 [39:03<1:27:23,  4.80s/it] 31%|███       | 488/1579 [39:08<1:27:20,  4.80s/it] 31%|███       | 489/1579 [39:13<1:27:15,  4.80s/it] 31%|███       | 490/1579 [39:18<1:27:09,  4.80s/it]                                                    {'loss': 2.5687, 'learning_rate': 0.00015628894384278914, 'epoch': 0.31}
 31%|███       | 490/1579 [39:18<1:27:09,  4.80s/it]                                                    {'router_ce_loss': 0.6204413771629333, 'old_lang_expert0_score': '0.94 0.89 0.36 0.44 0.39 0.91 0.52 0.39 0.38 0.99 0.37 0.99 0.42 0.57 1.0 0.98 0.59 0.88 0.97 0.41 0.95 0.37 1.0 0.37', 'epoch': 0.31}
 31%|███       | 490/1579 [39:18<1:27:09,  4.80s/it] 31%|███       | 491/1579 [39:22<1:27:04,  4.80s/it] 31%|███       | 492/1579 [39:27<1:27:00,  4.80s/it] 31%|███       | 493/1579 [39:32<1:26:58,  4.80s/it] 31%|███▏      | 494/1579 [39:37<1:26:56,  4.81s/it] 31%|███▏      | 495/1579 [39:42<1:26:46,  4.80s/it] 31%|███▏      | 496/1579 [39:46<1:26:40,  4.80s/it] 31%|███▏      | 497/1579 [39:51<1:26:31,  4.80s/it] 32%|███▏      | 498/1579 [39:56<1:26:32,  4.80s/it] 32%|███▏      | 499/1579 [40:01<1:26:27,  4.80s/it] 32%|███▏      | 500/1579 [40:06<1:26:24,  4.80s/it]                                                    {'loss': 2.5988, 'learning_rate': 0.0001546334356612187, 'epoch': 0.32}
 32%|███▏      | 500/1579 [40:06<1:26:24,  4.80s/it]                                                    {'router_ce_loss': 0.569343090057373, 'old_lang_expert0_score': '0.9 0.89 0.42 0.51 0.47 0.92 0.59 0.5 0.49 0.98 0.48 0.99 0.51 0.62 1.0 0.98 0.64 0.91 0.98 0.53 0.96 0.49 1.0 0.49', 'epoch': 0.32}
 32%|███▏      | 500/1579 [40:06<1:26:24,  4.80s/it] 32%|███▏      | 501/1579 [40:10<1:26:16,  4.80s/it] 32%|███▏      | 502/1579 [40:15<1:27:26,  4.87s/it] 32%|███▏      | 503/1579 [40:20<1:28:14,  4.92s/it] 32%|███▏      | 504/1579 [40:25<1:27:33,  4.89s/it] 32%|███▏      | 505/1579 [40:30<1:26:59,  4.86s/it] 32%|███▏      | 506/1579 [40:35<1:26:35,  4.84s/it] 32%|███▏      | 507/1579 [40:40<1:26:20,  4.83s/it] 32%|███▏      | 508/1579 [40:45<1:26:10,  4.83s/it] 32%|███▏      | 509/1579 [40:49<1:25:58,  4.82s/it] 32%|███▏      | 510/1579 [40:54<1:25:48,  4.82s/it]                                                    {'loss': 2.5889, 'learning_rate': 0.00015295630130673192, 'epoch': 0.32}
 32%|███▏      | 510/1579 [40:54<1:25:48,  4.82s/it]                                                    {'router_ce_loss': 0.619304358959198, 'old_lang_expert0_score': '0.92 0.89 0.36 0.44 0.39 0.91 0.51 0.41 0.4 0.98 0.38 0.99 0.44 0.57 0.99 0.96 0.59 0.87 0.97 0.42 0.95 0.39 1.0 0.37', 'epoch': 0.32}
 32%|███▏      | 510/1579 [40:55<1:25:48,  4.82s/it] 32%|███▏      | 511/1579 [40:59<1:25:40,  4.81s/it] 32%|███▏      | 512/1579 [41:04<1:25:30,  4.81s/it] 32%|███▏      | 513/1579 [41:09<1:25:27,  4.81s/it] 33%|███▎      | 514/1579 [41:13<1:25:19,  4.81s/it] 33%|███▎      | 515/1579 [41:18<1:25:15,  4.81s/it] 33%|███▎      | 516/1579 [41:23<1:25:08,  4.81s/it] 33%|███▎      | 517/1579 [41:28<1:25:03,  4.81s/it] 33%|███▎      | 518/1579 [41:33<1:24:54,  4.80s/it] 33%|███▎      | 519/1579 [41:37<1:24:50,  4.80s/it] 33%|███▎      | 520/1579 [41:42<1:24:42,  4.80s/it]                                                    {'loss': 2.5802, 'learning_rate': 0.00015125820465845537, 'epoch': 0.33}
 33%|███▎      | 520/1579 [41:42<1:24:42,  4.80s/it]                                                    {'router_ce_loss': 0.511816143989563, 'old_lang_expert0_score': '0.89 0.9 0.49 0.57 0.55 0.95 0.64 0.6 0.59 0.98 0.55 0.99 0.61 0.72 0.99 0.98 0.76 0.92 0.98 0.67 0.96 0.63 1.0 0.64', 'epoch': 0.33}
 33%|███▎      | 520/1579 [41:43<1:24:42,  4.80s/it] 33%|███▎      | 521/1579 [41:47<1:24:44,  4.81s/it] 33%|███▎      | 522/1579 [41:52<1:24:39,  4.81s/it] 33%|███▎      | 523/1579 [41:57<1:24:38,  4.81s/it] 33%|███▎      | 524/1579 [42:01<1:24:31,  4.81s/it] 33%|███▎      | 525/1579 [42:06<1:24:25,  4.81s/it] 33%|███▎      | 526/1579 [42:11<1:24:23,  4.81s/it] 33%|███▎      | 527/1579 [42:16<1:24:17,  4.81s/it] 33%|███▎      | 528/1579 [42:21<1:24:10,  4.80s/it] 34%|███▎      | 529/1579 [42:25<1:24:00,  4.80s/it] 34%|███▎      | 530/1579 [42:30<1:23:55,  4.80s/it]                                                    {'loss': 2.59, 'learning_rate': 0.0001495398178932585, 'epoch': 0.34}
 34%|███▎      | 530/1579 [42:30<1:23:55,  4.80s/it]                                                    {'router_ce_loss': 0.7167733311653137, 'old_lang_expert0_score': '0.96 0.89 0.22 0.32 0.23 0.89 0.41 0.22 0.2 0.99 0.19 0.99 0.26 0.42 1.0 0.98 0.44 0.84 0.96 0.2 0.93 0.15 1.0 0.13', 'epoch': 0.34}
 34%|███▎      | 530/1579 [42:31<1:23:55,  4.80s/it] 34%|███▎      | 531/1579 [42:35<1:23:47,  4.80s/it] 34%|███▎      | 532/1579 [42:40<1:23:42,  4.80s/it] 34%|███▍      | 533/1579 [42:45<1:24:45,  4.86s/it] 34%|███▍      | 534/1579 [42:50<1:25:31,  4.91s/it] 34%|███▍      | 535/1579 [42:55<1:24:56,  4.88s/it] 34%|███▍      | 536/1579 [42:59<1:24:24,  4.86s/it] 34%|███▍      | 537/1579 [43:04<1:24:04,  4.84s/it] 34%|███▍      | 538/1579 [43:09<1:23:45,  4.83s/it] 34%|███▍      | 539/1579 [43:14<1:23:31,  4.82s/it] 34%|███▍      | 540/1579 [43:19<1:23:20,  4.81s/it]                                                    {'loss': 2.6038, 'learning_rate': 0.00014780182121967785, 'epoch': 0.34}
 34%|███▍      | 540/1579 [43:19<1:23:20,  4.81s/it]                                                    {'router_ce_loss': 0.5052024722099304, 'old_lang_expert0_score': '0.87 0.9 0.5 0.61 0.58 0.94 0.67 0.62 0.6 0.98 0.58 0.99 0.64 0.71 0.99 0.97 0.76 0.93 0.98 0.68 0.96 0.66 1.0 0.65', 'epoch': 0.34}
 34%|███▍      | 540/1579 [43:19<1:23:20,  4.81s/it] 34%|███▍      | 541/1579 [43:23<1:23:15,  4.81s/it] 34%|███▍      | 542/1579 [43:28<1:23:03,  4.81s/it] 34%|███▍      | 543/1579 [43:33<1:22:57,  4.80s/it] 34%|███▍      | 544/1579 [43:38<1:22:50,  4.80s/it] 35%|███▍      | 545/1579 [43:43<1:22:46,  4.80s/it] 35%|███▍      | 546/1579 [43:47<1:22:39,  4.80s/it] 35%|███▍      | 547/1579 [43:52<1:22:34,  4.80s/it] 35%|███▍      | 548/1579 [43:57<1:22:27,  4.80s/it] 35%|███▍      | 549/1579 [44:02<1:22:27,  4.80s/it] 35%|███▍      | 550/1579 [44:07<1:22:24,  4.81s/it]                                                    {'loss': 2.5718, 'learning_rate': 0.00014604490260866268, 'epoch': 0.35}
 35%|███▍      | 550/1579 [44:07<1:22:24,  4.81s/it]                                                    {'router_ce_loss': 0.5217618346214294, 'old_lang_expert0_score': '0.89 0.9 0.49 0.59 0.55 0.93 0.67 0.59 0.57 0.98 0.57 0.99 0.62 0.71 0.99 0.98 0.74 0.92 0.98 0.63 0.97 0.6 1.0 0.6', 'epoch': 0.35}
 35%|███▍      | 550/1579 [44:07<1:22:24,  4.81s/it] 35%|███▍      | 551/1579 [44:11<1:22:25,  4.81s/it] 35%|███▍      | 552/1579 [44:16<1:22:16,  4.81s/it] 35%|███▌      | 553/1579 [44:21<1:22:11,  4.81s/it] 35%|███▌      | 554/1579 [44:26<1:22:03,  4.80s/it] 35%|███▌      | 555/1579 [44:31<1:22:00,  4.81s/it] 35%|███▌      | 556/1579 [44:36<1:21:55,  4.81s/it] 35%|███▌      | 557/1579 [44:40<1:21:47,  4.80s/it] 35%|███▌      | 558/1579 [44:45<1:21:43,  4.80s/it] 35%|███▌      | 559/1579 [44:50<1:21:35,  4.80s/it] 35%|███▌      | 560/1579 [44:55<1:21:31,  4.80s/it]                                                    {'loss': 2.6016, 'learning_rate': 0.00014426975752124748, 'epoch': 0.35}
 35%|███▌      | 560/1579 [44:55<1:21:31,  4.80s/it]                                                    {'router_ce_loss': 0.5677574276924133, 'old_lang_expert0_score': '0.91 0.9 0.44 0.54 0.48 0.92 0.63 0.51 0.5 0.98 0.47 0.99 0.54 0.62 1.0 0.99 0.63 0.91 0.97 0.52 0.96 0.49 1.0 0.49', 'epoch': 0.35}
 35%|███▌      | 560/1579 [44:55<1:21:31,  4.80s/it] 36%|███▌      | 561/1579 [45:00<1:21:31,  4.81s/it] 36%|███▌      | 562/1579 [45:04<1:21:27,  4.81s/it] 36%|███▌      | 563/1579 [45:09<1:21:24,  4.81s/it] 36%|███▌      | 564/1579 [45:14<1:22:15,  4.86s/it] 36%|███▌      | 565/1579 [45:19<1:22:57,  4.91s/it] 36%|███▌      | 566/1579 [45:24<1:22:15,  4.87s/it] 36%|███▌      | 567/1579 [45:29<1:21:49,  4.85s/it] 36%|███▌      | 568/1579 [45:34<1:21:26,  4.83s/it] 36%|███▌      | 569/1579 [45:38<1:21:12,  4.82s/it] 36%|███▌      | 570/1579 [45:43<1:21:03,  4.82s/it]                                                    {'loss': 2.5628, 'learning_rate': 0.00014247708863325968, 'epoch': 0.36}
 36%|███▌      | 570/1579 [45:43<1:21:03,  4.82s/it]                                                    {'router_ce_loss': 0.5766537189483643, 'old_lang_expert0_score': '0.91 0.9 0.42 0.5 0.45 0.92 0.59 0.48 0.46 0.98 0.46 0.99 0.52 0.62 0.99 0.98 0.65 0.9 0.98 0.53 0.95 0.49 1.0 0.49', 'epoch': 0.36}
 36%|███▌      | 570/1579 [45:44<1:21:03,  4.82s/it] 36%|███▌      | 571/1579 [45:48<1:20:52,  4.81s/it] 36%|███▌      | 572/1579 [45:53<1:20:42,  4.81s/it] 36%|███▋      | 573/1579 [45:58<1:20:41,  4.81s/it] 36%|███▋      | 574/1579 [46:02<1:20:31,  4.81s/it] 36%|███▋      | 575/1579 [46:07<1:20:25,  4.81s/it] 36%|███▋      | 576/1579 [46:12<1:20:19,  4.81s/it] 37%|███▋      | 577/1579 [46:17<1:20:16,  4.81s/it] 37%|███▋      | 578/1579 [46:22<1:20:13,  4.81s/it] 37%|███▋      | 579/1579 [46:26<1:20:10,  4.81s/it] 37%|███▋      | 580/1579 [46:31<1:20:01,  4.81s/it]                                                    {'loss': 2.6227, 'learning_rate': 0.0001406676055571717, 'epoch': 0.37}
 37%|███▋      | 580/1579 [46:31<1:20:01,  4.81s/it]                                                    {'router_ce_loss': 0.4765033423900604, 'old_lang_expert0_score': '0.87 0.9 0.56 0.63 0.62 0.94 0.73 0.67 0.65 0.97 0.64 0.99 0.7 0.75 0.99 0.98 0.79 0.94 0.98 0.74 0.97 0.71 1.0 0.71', 'epoch': 0.37}
 37%|███▋      | 580/1579 [46:32<1:20:01,  4.81s/it] 37%|███▋      | 581/1579 [46:36<1:19:54,  4.80s/it] 37%|███▋      | 582/1579 [46:41<1:19:50,  4.80s/it] 37%|███▋      | 583/1579 [46:46<1:19:44,  4.80s/it] 37%|███▋      | 584/1579 [46:50<1:19:40,  4.80s/it] 37%|███▋      | 585/1579 [46:55<1:19:29,  4.80s/it] 37%|███▋      | 586/1579 [47:00<1:19:23,  4.80s/it] 37%|███▋      | 587/1579 [47:05<1:19:13,  4.79s/it] 37%|███▋      | 588/1579 [47:10<1:19:08,  4.79s/it] 37%|███▋      | 589/1579 [47:14<1:19:01,  4.79s/it] 37%|███▋      | 590/1579 [47:19<1:18:57,  4.79s/it]                                                    {'loss': 2.571, 'learning_rate': 0.00013884202456120682, 'epoch': 0.37}
 37%|███▋      | 590/1579 [47:19<1:18:57,  4.79s/it]                                                    {'router_ce_loss': 0.520936906337738, 'old_lang_expert0_score': '0.89 0.9 0.5 0.58 0.55 0.92 0.68 0.59 0.59 0.98 0.56 0.99 0.61 0.69 0.99 0.99 0.72 0.92 0.99 0.63 0.97 0.6 1.0 0.6', 'epoch': 0.37}
 37%|███▋      | 590/1579 [47:20<1:18:57,  4.79s/it] 37%|███▋      | 591/1579 [47:24<1:19:00,  4.80s/it] 37%|███▋      | 592/1579 [47:29<1:19:01,  4.80s/it] 38%|███▊      | 593/1579 [47:34<1:19:01,  4.81s/it] 38%|███▊      | 594/1579 [47:38<1:18:52,  4.80s/it] 38%|███▊      | 595/1579 [47:43<1:19:44,  4.86s/it] 38%|███▊      | 596/1579 [47:48<1:19:21,  4.84s/it] 38%|███▊      | 597/1579 [47:53<1:20:12,  4.90s/it] 38%|███▊      | 598/1579 [47:58<1:19:33,  4.87s/it] 38%|███▊      | 599/1579 [48:03<1:19:10,  4.85s/it] 38%|███▊      | 600/1579 [48:08<1:18:48,  4.83s/it]                                                    {'loss': 2.5789, 'learning_rate': 0.00013700106828581064, 'epoch': 0.38}
 38%|███▊      | 600/1579 [48:08<1:18:48,  4.83s/it]                                                    {'router_ce_loss': 0.5680321455001831, 'old_lang_expert0_score': '0.9 0.9 0.46 0.54 0.5 0.93 0.61 0.45 0.46 0.98 0.46 0.99 0.5 0.62 1.0 0.98 0.67 0.89 0.97 0.52 0.96 0.47 1.0 0.48', 'epoch': 0.38}
 38%|███▊      | 600/1579 [48:08<1:18:48,  4.83s/it] 38%|███▊      | 601/1579 [48:12<1:18:36,  4.82s/it] 38%|███▊      | 602/1579 [48:17<1:18:22,  4.81s/it] 38%|███▊      | 603/1579 [48:22<1:18:15,  4.81s/it] 38%|███▊      | 604/1579 [48:27<1:18:07,  4.81s/it] 38%|███▊      | 605/1579 [48:32<1:18:04,  4.81s/it] 38%|███▊      | 606/1579 [48:36<1:17:56,  4.81s/it] 38%|███▊      | 607/1579 [48:41<1:17:53,  4.81s/it] 39%|███▊      | 608/1579 [48:46<1:17:47,  4.81s/it] 39%|███▊      | 609/1579 [48:51<1:17:39,  4.80s/it] 39%|███▊      | 610/1579 [48:56<1:17:38,  4.81s/it]                                                    {'loss': 2.5837, 'learning_rate': 0.00013514546545760015, 'epoch': 0.39}
 39%|███▊      | 610/1579 [48:56<1:17:38,  4.81s/it]                                                    {'router_ce_loss': 0.5719686150550842, 'old_lang_expert0_score': '0.92 0.91 0.43 0.5 0.46 0.92 0.57 0.49 0.47 0.98 0.45 0.99 0.52 0.64 0.99 0.98 0.67 0.9 0.98 0.52 0.96 0.48 1.0 0.48', 'epoch': 0.39}
 39%|███▊      | 610/1579 [48:56<1:17:38,  4.81s/it] 39%|███▊      | 611/1579 [49:00<1:17:32,  4.81s/it] 39%|███▉      | 612/1579 [49:05<1:17:28,  4.81s/it] 39%|███▉      | 613/1579 [49:10<1:17:18,  4.80s/it] 39%|███▉      | 614/1579 [49:15<1:17:15,  4.80s/it] 39%|███▉      | 615/1579 [49:20<1:17:06,  4.80s/it] 39%|███▉      | 616/1579 [49:24<1:17:05,  4.80s/it] 39%|███▉      | 617/1579 [49:29<1:16:58,  4.80s/it] 39%|███▉      | 618/1579 [49:34<1:16:58,  4.81s/it] 39%|███▉      | 619/1579 [49:39<1:16:53,  4.81s/it] 39%|███▉      | 620/1579 [49:44<1:16:51,  4.81s/it]                                                    {'loss': 2.5906, 'learning_rate': 0.0001332759506009035, 'epoch': 0.39}
 39%|███▉      | 620/1579 [49:44<1:16:51,  4.81s/it]                                                    {'router_ce_loss': 0.6634238958358765, 'old_lang_expert0_score': '0.94 0.9 0.3 0.39 0.31 0.91 0.47 0.31 0.31 0.99 0.31 0.99 0.36 0.51 0.99 0.98 0.54 0.84 0.97 0.31 0.93 0.27 1.0 0.26', 'epoch': 0.39}
 39%|███▉      | 620/1579 [49:44<1:16:51,  4.81s/it] 39%|███▉      | 621/1579 [49:48<1:16:44,  4.81s/it] 39%|███▉      | 622/1579 [49:53<1:16:37,  4.80s/it] 39%|███▉      | 623/1579 [49:58<1:16:29,  4.80s/it] 40%|███▉      | 624/1579 [50:03<1:16:28,  4.81s/it] 40%|███▉      | 625/1579 [50:08<1:16:19,  4.80s/it] 40%|███▉      | 626/1579 [50:12<1:16:19,  4.80s/it] 40%|███▉      | 627/1579 [50:17<1:17:08,  4.86s/it] 40%|███▉      | 628/1579 [50:23<1:17:55,  4.92s/it] 40%|███▉      | 629/1579 [50:27<1:17:16,  4.88s/it] 40%|███▉      | 630/1579 [50:32<1:16:49,  4.86s/it]                                                    {'loss': 2.6002, 'learning_rate': 0.00013139326374700513, 'epoch': 0.4}
 40%|███▉      | 630/1579 [50:32<1:16:49,  4.86s/it]                                                    {'router_ce_loss': 0.6477527022361755, 'old_lang_expert0_score': '0.93 0.89 0.31 0.39 0.33 0.91 0.45 0.34 0.33 0.99 0.33 0.99 0.39 0.55 0.99 0.98 0.57 0.86 0.97 0.37 0.94 0.32 1.0 0.3', 'epoch': 0.4}
 40%|███▉      | 630/1579 [50:33<1:16:49,  4.86s/it] 40%|███▉      | 631/1579 [50:37<1:16:29,  4.84s/it] 40%|████      | 632/1579 [50:42<1:16:12,  4.83s/it] 40%|████      | 633/1579 [50:47<1:16:01,  4.82s/it] 40%|████      | 634/1579 [50:51<1:15:50,  4.82s/it] 40%|████      | 635/1579 [50:56<1:15:39,  4.81s/it] 40%|████      | 636/1579 [51:01<1:15:30,  4.80s/it] 40%|████      | 637/1579 [51:06<1:15:24,  4.80s/it] 40%|████      | 638/1579 [51:11<1:15:21,  4.80s/it] 40%|████      | 639/1579 [51:15<1:15:14,  4.80s/it] 41%|████      | 640/1579 [51:20<1:15:11,  4.80s/it]                                                    {'loss': 2.5948, 'learning_rate': 0.00012949815014121047, 'epoch': 0.41}
 41%|████      | 640/1579 [51:20<1:15:11,  4.80s/it]                                                    {'router_ce_loss': 0.6732375621795654, 'old_lang_expert0_score': '0.95 0.89 0.28 0.36 0.29 0.9 0.42 0.3 0.29 0.99 0.27 0.99 0.33 0.49 1.0 0.98 0.52 0.86 0.97 0.3 0.93 0.26 1.0 0.23', 'epoch': 0.41}
 41%|████      | 640/1579 [51:21<1:15:11,  4.80s/it] 41%|████      | 641/1579 [51:25<1:15:04,  4.80s/it] 41%|████      | 642/1579 [51:30<1:15:00,  4.80s/it] 41%|████      | 643/1579 [51:35<1:14:52,  4.80s/it] 41%|████      | 644/1579 [51:39<1:14:49,  4.80s/it] 41%|████      | 645/1579 [51:44<1:14:43,  4.80s/it] 41%|████      | 646/1579 [51:49<1:14:39,  4.80s/it] 41%|████      | 647/1579 [51:54<1:14:34,  4.80s/it] 41%|████      | 648/1579 [51:59<1:14:34,  4.81s/it] 41%|████      | 649/1579 [52:03<1:14:32,  4.81s/it] 41%|████      | 650/1579 [52:08<1:14:25,  4.81s/it]                                                    {'loss': 2.6057, 'learning_rate': 0.0001275913599478473, 'epoch': 0.41}
 41%|████      | 650/1579 [52:08<1:14:25,  4.81s/it]                                                    {'router_ce_loss': 0.5695727467536926, 'old_lang_expert0_score': '0.9 0.89 0.42 0.52 0.47 0.92 0.6 0.48 0.49 0.98 0.48 0.99 0.53 0.64 0.99 0.98 0.66 0.89 0.98 0.53 0.96 0.49 1.0 0.49', 'epoch': 0.41}
 41%|████      | 650/1579 [52:09<1:14:25,  4.81s/it] 41%|████      | 651/1579 [52:13<1:14:21,  4.81s/it] 41%|████▏     | 652/1579 [52:18<1:14:17,  4.81s/it] 41%|████▏     | 653/1579 [52:23<1:14:12,  4.81s/it] 41%|████▏     | 654/1579 [52:27<1:14:05,  4.81s/it] 41%|████▏     | 655/1579 [52:32<1:13:58,  4.80s/it] 42%|████▏     | 656/1579 [52:37<1:13:51,  4.80s/it] 42%|████▏     | 657/1579 [52:42<1:13:45,  4.80s/it] 42%|████▏     | 658/1579 [52:47<1:14:26,  4.85s/it] 42%|████▏     | 659/1579 [52:52<1:15:08,  4.90s/it] 42%|████▏     | 660/1579 [52:57<1:14:33,  4.87s/it]                                                    {'loss': 2.5985, 'learning_rate': 0.00012567364795331953, 'epoch': 0.42}
 42%|████▏     | 660/1579 [52:57<1:14:33,  4.87s/it]                                                    {'router_ce_loss': 0.5736193060874939, 'old_lang_expert0_score': '0.92 0.9 0.42 0.48 0.45 0.92 0.6 0.5 0.48 0.98 0.46 0.99 0.52 0.64 1.0 0.98 0.66 0.9 0.98 0.52 0.96 0.48 1.0 0.47', 'epoch': 0.42}
 42%|████▏     | 660/1579 [52:57<1:14:33,  4.87s/it] 42%|████▏     | 661/1579 [53:01<1:14:14,  4.85s/it] 42%|████▏     | 662/1579 [53:06<1:13:56,  4.84s/it] 42%|████▏     | 663/1579 [53:11<1:13:39,  4.83s/it] 42%|████▏     | 664/1579 [53:16<1:13:26,  4.82s/it] 42%|████▏     | 665/1579 [53:21<1:13:18,  4.81s/it] 42%|████▏     | 666/1579 [53:25<1:13:11,  4.81s/it] 42%|████▏     | 667/1579 [53:30<1:13:04,  4.81s/it] 42%|████▏     | 668/1579 [53:35<1:12:56,  4.80s/it] 42%|████▏     | 669/1579 [53:40<1:12:50,  4.80s/it] 42%|████▏     | 670/1579 [53:45<1:12:46,  4.80s/it]                                                    {'loss': 2.601, 'learning_rate': 0.0001237457732673316, 'epoch': 0.42}
 42%|████▏     | 670/1579 [53:45<1:12:46,  4.80s/it]                                                    {'router_ce_loss': 0.6227496266365051, 'old_lang_expert0_score': '0.93 0.89 0.36 0.44 0.39 0.89 0.56 0.39 0.38 0.99 0.38 0.99 0.43 0.55 1.0 0.99 0.57 0.87 0.98 0.41 0.95 0.37 1.0 0.36', 'epoch': 0.42}
 42%|████▏     | 670/1579 [53:45<1:12:46,  4.80s/it] 42%|████▏     | 671/1579 [53:49<1:12:41,  4.80s/it] 43%|████▎     | 672/1579 [53:54<1:12:37,  4.80s/it] 43%|████▎     | 673/1579 [53:59<1:12:28,  4.80s/it] 43%|████▎     | 674/1579 [54:04<1:12:25,  4.80s/it] 43%|████▎     | 675/1579 [54:09<1:12:21,  4.80s/it] 43%|████▎     | 676/1579 [54:13<1:12:19,  4.81s/it] 43%|████▎     | 677/1579 [54:18<1:12:16,  4.81s/it] 43%|████▎     | 678/1579 [54:23<1:12:09,  4.80s/it] 43%|████▎     | 679/1579 [54:28<1:12:04,  4.80s/it] 43%|████▎     | 680/1579 [54:33<1:11:58,  4.80s/it]                                                    {'loss': 2.6081, 'learning_rate': 0.0001218084990224015, 'epoch': 0.43}
 43%|████▎     | 680/1579 [54:33<1:11:58,  4.80s/it]                                                    {'router_ce_loss': 0.5188189148902893, 'old_lang_expert0_score': '0.89 0.89 0.5 0.57 0.56 0.93 0.68 0.58 0.59 0.98 0.58 0.99 0.63 0.7 0.99 0.99 0.73 0.92 0.98 0.63 0.97 0.61 1.0 0.61', 'epoch': 0.43}
 43%|████▎     | 680/1579 [54:33<1:11:58,  4.80s/it] 43%|████▎     | 681/1579 [54:37<1:11:55,  4.81s/it] 43%|████▎     | 682/1579 [54:42<1:11:49,  4.80s/it] 43%|████▎     | 683/1579 [54:47<1:11:45,  4.80s/it] 43%|████▎     | 684/1579 [54:52<1:11:41,  4.81s/it] 43%|████▎     | 685/1579 [54:57<1:11:35,  4.80s/it] 43%|████▎     | 686/1579 [55:01<1:11:29,  4.80s/it] 44%|████▎     | 687/1579 [55:06<1:11:24,  4.80s/it] 44%|████▎     | 688/1579 [55:11<1:11:20,  4.80s/it] 44%|████▎     | 689/1579 [55:16<1:12:20,  4.88s/it] 44%|████▎     | 690/1579 [55:21<1:12:50,  4.92s/it]                                                    {'loss': 2.5631, 'learning_rate': 0.00011986259207178124, 'epoch': 0.44}
 44%|████▎     | 690/1579 [55:21<1:12:50,  4.92s/it]                                                    {'router_ce_loss': 0.6106149554252625, 'old_lang_expert0_score': '0.91 0.89 0.37 0.46 0.41 0.91 0.54 0.42 0.39 0.98 0.4 0.99 0.44 0.58 0.99 0.97 0.59 0.87 0.97 0.45 0.95 0.42 1.0 0.41', 'epoch': 0.44}
 44%|████▎     | 690/1579 [55:22<1:12:50,  4.92s/it] 44%|████▍     | 691/1579 [55:26<1:12:15,  4.88s/it] 44%|████▍     | 692/1579 [55:31<1:11:46,  4.85s/it] 44%|████▍     | 693/1579 [55:36<1:11:25,  4.84s/it] 44%|████▍     | 694/1579 [55:40<1:11:10,  4.83s/it] 44%|████▍     | 695/1579 [55:45<1:11:01,  4.82s/it] 44%|████▍     | 696/1579 [55:50<1:10:49,  4.81s/it] 44%|████▍     | 697/1579 [55:55<1:10:40,  4.81s/it] 44%|████▍     | 698/1579 [56:00<1:10:36,  4.81s/it] 44%|████▍     | 699/1579 [56:04<1:10:30,  4.81s/it] 44%|████▍     | 700/1579 [56:09<1:10:24,  4.81s/it]                                                    {'loss': 2.5777, 'learning_rate': 0.00011790882268590473, 'epoch': 0.44}
 44%|████▍     | 700/1579 [56:09<1:10:24,  4.81s/it]                                                    {'router_ce_loss': 0.5745452046394348, 'old_lang_expert0_score': '0.91 0.89 0.43 0.51 0.46 0.91 0.6 0.49 0.47 0.98 0.47 0.99 0.54 0.63 1.0 0.99 0.65 0.89 0.98 0.52 0.95 0.49 1.0 0.49', 'epoch': 0.44}
 44%|████▍     | 700/1579 [56:10<1:10:24,  4.81s/it] 44%|████▍     | 701/1579 [56:14<1:10:20,  4.81s/it] 44%|████▍     | 702/1579 [56:19<1:10:15,  4.81s/it] 45%|████▍     | 703/1579 [56:24<1:10:12,  4.81s/it] 45%|████▍     | 704/1579 [56:28<1:10:07,  4.81s/it] 45%|████▍     | 705/1579 [56:33<1:09:56,  4.80s/it] 45%|████▍     | 706/1579 [56:38<1:09:52,  4.80s/it] 45%|████▍     | 707/1579 [56:43<1:09:45,  4.80s/it] 45%|████▍     | 708/1579 [56:48<1:09:41,  4.80s/it] 45%|████▍     | 709/1579 [56:52<1:09:32,  4.80s/it] 45%|████▍     | 710/1579 [56:57<1:09:27,  4.80s/it]                                                    {'loss': 2.6232, 'learning_rate': 0.0001159479642474829, 'epoch': 0.45}
 45%|████▍     | 710/1579 [56:57<1:09:27,  4.80s/it]                                                    {'router_ce_loss': 0.5792756080627441, 'old_lang_expert0_score': '0.9 0.89 0.43 0.51 0.46 0.92 0.58 0.46 0.45 0.98 0.45 0.99 0.51 0.62 1.0 0.98 0.64 0.89 0.98 0.53 0.93 0.48 0.98 0.47', 'epoch': 0.45}
 45%|████▍     | 710/1579 [56:58<1:09:27,  4.80s/it] 45%|████▌     | 711/1579 [57:02<1:09:23,  4.80s/it] 45%|████▌     | 712/1579 [57:07<1:09:20,  4.80s/it] 45%|████▌     | 713/1579 [57:12<1:09:10,  4.79s/it] 45%|████▌     | 714/1579 [57:16<1:09:07,  4.79s/it] 45%|████▌     | 715/1579 [57:21<1:09:01,  4.79s/it] 45%|████▌     | 716/1579 [57:26<1:09:01,  4.80s/it] 45%|████▌     | 717/1579 [57:31<1:08:58,  4.80s/it] 45%|████▌     | 718/1579 [57:36<1:09:00,  4.81s/it] 46%|████▌     | 719/1579 [57:40<1:08:53,  4.81s/it] 46%|████▌     | 720/1579 [57:45<1:09:42,  4.87s/it]                                                    {'loss': 2.5828, 'learning_rate': 0.00011398079294536676, 'epoch': 0.46}
 46%|████▌     | 720/1579 [57:45<1:09:42,  4.87s/it]                                                    {'router_ce_loss': 0.5080836415290833, 'old_lang_expert0_score': '0.9 0.9 0.54 0.61 0.58 0.93 0.67 0.61 0.61 0.98 0.61 0.99 0.64 0.71 0.99 0.99 0.73 0.94 0.98 0.65 0.97 0.62 1.0 0.63', 'epoch': 0.46}
 46%|████▌     | 720/1579 [57:46<1:09:42,  4.87s/it] 46%|████▌     | 721/1579 [57:50<1:09:20,  4.85s/it] 46%|████▌     | 722/1579 [57:55<1:10:00,  4.90s/it] 46%|████▌     | 723/1579 [58:00<1:09:28,  4.87s/it] 46%|████▌     | 724/1579 [58:05<1:09:00,  4.84s/it] 46%|████▌     | 725/1579 [58:10<1:08:43,  4.83s/it] 46%|████▌     | 726/1579 [58:14<1:08:31,  4.82s/it] 46%|████▌     | 727/1579 [58:19<1:08:17,  4.81s/it] 46%|████▌     | 728/1579 [58:24<1:08:05,  4.80s/it] 46%|████▌     | 729/1579 [58:29<1:07:59,  4.80s/it] 46%|████▌     | 730/1579 [58:34<1:07:56,  4.80s/it]                                                    {'loss': 2.5591, 'learning_rate': 0.00011200808746729984, 'epoch': 0.46}
 46%|████▌     | 730/1579 [58:34<1:07:56,  4.80s/it]                                                    {'router_ce_loss': 0.5183344483375549, 'old_lang_expert0_score': '0.9 0.9 0.54 0.6 0.56 0.92 0.68 0.6 0.58 0.98 0.58 0.99 0.62 0.71 0.99 0.99 0.73 0.92 0.98 0.63 0.97 0.6 1.0 0.61', 'epoch': 0.46}
 46%|████▌     | 730/1579 [58:34<1:07:56,  4.80s/it] 46%|████▋     | 731/1579 [58:38<1:07:55,  4.81s/it] 46%|████▋     | 732/1579 [58:43<1:07:49,  4.80s/it] 46%|████▋     | 733/1579 [58:48<1:07:44,  4.80s/it] 46%|████▋     | 734/1579 [58:53<1:07:37,  4.80s/it] 47%|████▋     | 735/1579 [58:58<1:07:35,  4.81s/it] 47%|████▋     | 736/1579 [59:02<1:07:30,  4.81s/it] 47%|████▋     | 737/1579 [59:07<1:07:24,  4.80s/it] 47%|████▋     | 738/1579 [59:12<1:07:20,  4.80s/it] 47%|████▋     | 739/1579 [59:17<1:07:14,  4.80s/it] 47%|████▋     | 740/1579 [59:22<1:07:09,  4.80s/it]                                                    {'loss': 2.5932, 'learning_rate': 0.00011003062869168137, 'epoch': 0.47}
 47%|████▋     | 740/1579 [59:22<1:07:09,  4.80s/it]                                                    {'router_ce_loss': 0.5169064998626709, 'old_lang_expert0_score': '0.89 0.9 0.54 0.6 0.57 0.93 0.67 0.6 0.58 0.98 0.58 0.99 0.61 0.7 0.99 0.98 0.73 0.91 0.98 0.64 0.96 0.6 1.0 0.61', 'epoch': 0.47}
 47%|████▋     | 740/1579 [59:22<1:07:09,  4.80s/it] 47%|████▋     | 741/1579 [59:26<1:07:00,  4.80s/it] 47%|████▋     | 742/1579 [59:31<1:06:56,  4.80s/it] 47%|████▋     | 743/1579 [59:36<1:06:51,  4.80s/it] 47%|████▋     | 744/1579 [59:41<1:06:49,  4.80s/it] 47%|████▋     | 745/1579 [59:46<1:06:46,  4.80s/it] 47%|████▋     | 746/1579 [59:50<1:06:43,  4.81s/it] 47%|████▋     | 747/1579 [59:55<1:06:39,  4.81s/it] 47%|████▋     | 748/1579 [1:00:00<1:06:32,  4.80s/it] 47%|████▋     | 749/1579 [1:00:05<1:06:27,  4.80s/it] 47%|████▋     | 750/1579 [1:00:10<1:06:26,  4.81s/it]                                                      {'loss': 2.6054, 'learning_rate': 0.00010804919937846221, 'epoch': 0.47}
 47%|████▋     | 750/1579 [1:00:10<1:06:26,  4.81s/it]                                                      {'router_ce_loss': 0.5125424265861511, 'old_lang_expert0_score': '0.9 0.89 0.54 0.62 0.58 0.93 0.71 0.6 0.58 0.98 0.58 0.99 0.63 0.69 1.0 0.98 0.73 0.92 0.98 0.64 0.97 0.62 1.0 0.61', 'epoch': 0.47}
 47%|████▋     | 750/1579 [1:00:10<1:06:26,  4.81s/it] 48%|████▊     | 751/1579 [1:00:14<1:06:23,  4.81s/it] 48%|████▊     | 752/1579 [1:00:19<1:07:02,  4.86s/it] 48%|████▊     | 753/1579 [1:00:24<1:07:39,  4.91s/it] 48%|████▊     | 754/1579 [1:00:29<1:07:05,  4.88s/it] 48%|████▊     | 755/1579 [1:00:34<1:06:38,  4.85s/it] 48%|████▊     | 756/1579 [1:00:39<1:06:22,  4.84s/it] 48%|████▊     | 757/1579 [1:00:44<1:06:10,  4.83s/it] 48%|████▊     | 758/1579 [1:00:48<1:06:01,  4.82s/it] 48%|████▊     | 759/1579 [1:00:53<1:05:52,  4.82s/it] 48%|████▊     | 760/1579 [1:00:58<1:05:45,  4.82s/it]                                                      {'loss': 2.5833, 'learning_rate': 0.00010606458385929626, 'epoch': 0.48}
 48%|████▊     | 760/1579 [1:00:58<1:05:45,  4.82s/it]                                                      {'router_ce_loss': 0.7236072421073914, 'old_lang_expert0_score': '0.96 0.89 0.21 0.31 0.22 0.89 0.4 0.2 0.19 0.99 0.18 1.0 0.26 0.44 1.0 0.98 0.45 0.82 0.96 0.19 0.93 0.14 1.0 0.13', 'epoch': 0.48}
 48%|████▊     | 760/1579 [1:00:58<1:05:45,  4.82s/it] 48%|████▊     | 761/1579 [1:01:03<1:05:36,  4.81s/it] 48%|████▊     | 762/1579 [1:01:08<1:05:31,  4.81s/it] 48%|████▊     | 763/1579 [1:01:13<1:05:22,  4.81s/it] 48%|████▊     | 764/1579 [1:01:17<1:05:16,  4.81s/it] 48%|████▊     | 765/1579 [1:01:22<1:05:13,  4.81s/it] 49%|████▊     | 766/1579 [1:01:27<1:05:04,  4.80s/it] 49%|████▊     | 767/1579 [1:01:32<1:04:58,  4.80s/it] 49%|████▊     | 768/1579 [1:01:37<1:04:53,  4.80s/it] 49%|████▊     | 769/1579 [1:01:41<1:04:48,  4.80s/it] 49%|████▉     | 770/1579 [1:01:46<1:04:41,  4.80s/it]                                                      {'loss': 2.58, 'learning_rate': 0.0001040775677270695, 'epoch': 0.49}
 49%|████▉     | 770/1579 [1:01:46<1:04:41,  4.80s/it]                                                      {'router_ce_loss': 0.4147452116012573, 'old_lang_expert0_score': '0.89 0.91 0.68 0.73 0.73 0.96 0.79 0.79 0.77 0.97 0.74 0.99 0.79 0.84 0.99 0.98 0.89 0.97 0.99 0.87 0.98 0.85 1.0 0.84', 'epoch': 0.49}
 49%|████▉     | 770/1579 [1:01:46<1:04:41,  4.80s/it] 49%|████▉     | 771/1579 [1:01:51<1:04:41,  4.80s/it] 49%|████▉     | 772/1579 [1:01:56<1:04:36,  4.80s/it] 49%|████▉     | 773/1579 [1:02:01<1:04:36,  4.81s/it] 49%|████▉     | 774/1579 [1:02:05<1:04:28,  4.81s/it] 49%|████▉     | 775/1579 [1:02:10<1:04:22,  4.80s/it] 49%|████▉     | 776/1579 [1:02:15<1:04:15,  4.80s/it] 49%|████▉     | 777/1579 [1:02:20<1:04:11,  4.80s/it] 49%|████▉     | 778/1579 [1:02:25<1:04:06,  4.80s/it] 49%|████▉     | 779/1579 [1:02:29<1:04:03,  4.80s/it] 49%|████▉     | 780/1579 [1:02:34<1:03:57,  4.80s/it]                                                      {'loss': 2.5541, 'learning_rate': 0.0001020889375249297, 'epoch': 0.49}
 49%|████▉     | 780/1579 [1:02:34<1:03:57,  4.80s/it]                                                      {'router_ce_loss': 0.48206400871276855, 'old_lang_expert0_score': '0.88 0.91 0.6 0.66 0.61 0.94 0.74 0.66 0.66 0.98 0.66 0.99 0.7 0.75 0.99 0.99 0.76 0.94 0.98 0.69 0.97 0.67 1.0 0.66', 'epoch': 0.49}
 49%|████▉     | 780/1579 [1:02:35<1:03:57,  4.80s/it] 49%|████▉     | 781/1579 [1:02:39<1:03:52,  4.80s/it] 50%|████▉     | 782/1579 [1:02:44<1:03:50,  4.81s/it] 50%|████▉     | 783/1579 [1:02:49<1:04:29,  4.86s/it] 50%|████▉     | 784/1579 [1:02:54<1:05:03,  4.91s/it] 50%|████▉     | 785/1579 [1:02:59<1:04:31,  4.88s/it] 50%|████▉     | 786/1579 [1:03:03<1:04:07,  4.85s/it] 50%|████▉     | 787/1579 [1:03:08<1:03:52,  4.84s/it] 50%|████▉     | 788/1579 [1:03:13<1:03:41,  4.83s/it] 50%|████▉     | 789/1579 [1:03:18<1:03:29,  4.82s/it] 50%|█████     | 790/1579 [1:03:23<1:03:20,  4.82s/it]                                                      {'loss': 2.5895, 'learning_rate': 0.00010009948043494041, 'epoch': 0.5}
 50%|█████     | 790/1579 [1:03:23<1:03:20,  4.82s/it]                                                      {'router_ce_loss': 0.5992417931556702, 'old_lang_expert0_score': '0.93 0.9 0.43 0.49 0.44 0.92 0.55 0.43 0.42 0.99 0.39 0.99 0.44 0.59 1.0 0.98 0.63 0.87 0.97 0.45 0.95 0.43 1.0 0.41', 'epoch': 0.5}
 50%|█████     | 790/1579 [1:03:23<1:03:20,  4.82s/it] 50%|█████     | 791/1579 [1:03:27<1:03:14,  4.81s/it] 50%|█████     | 792/1579 [1:03:32<1:03:03,  4.81s/it] 50%|█████     | 793/1579 [1:03:37<1:02:58,  4.81s/it] 50%|█████     | 794/1579 [1:03:42<1:02:51,  4.80s/it] 50%|█████     | 795/1579 [1:03:47<1:02:45,  4.80s/it] 50%|█████     | 796/1579 [1:03:51<1:02:38,  4.80s/it] 50%|█████     | 797/1579 [1:03:56<1:02:32,  4.80s/it] 51%|█████     | 798/1579 [1:04:01<1:02:29,  4.80s/it] 51%|█████     | 799/1579 [1:04:06<1:02:25,  4.80s/it] 51%|█████     | 800/1579 [1:04:11<1:02:24,  4.81s/it]                                                      {'loss': 2.5887, 'learning_rate': 9.810998396648147e-05, 'epoch': 0.51}
 51%|█████     | 800/1579 [1:04:11<1:02:24,  4.81s/it]                                                      {'router_ce_loss': 0.5048328042030334, 'old_lang_expert0_score': '0.89 0.9 0.54 0.62 0.59 0.94 0.7 0.62 0.61 0.98 0.58 0.99 0.65 0.7 0.99 0.98 0.74 0.93 0.98 0.67 0.97 0.64 1.0 0.64', 'epoch': 0.51}
 51%|█████     | 800/1579 [1:04:11<1:02:24,  4.81s/it] 51%|█████     | 801/1579 [1:04:15<1:02:18,  4.81s/it] 51%|█████     | 802/1579 [1:04:20<1:02:13,  4.81s/it] 51%|█████     | 803/1579 [1:04:25<1:02:07,  4.80s/it] 51%|█████     | 804/1579 [1:04:30<1:02:00,  4.80s/it] 51%|█████     | 805/1579 [1:04:35<1:01:54,  4.80s/it] 51%|█████     | 806/1579 [1:04:39<1:01:49,  4.80s/it] 51%|█████     | 807/1579 [1:04:44<1:01:44,  4.80s/it] 51%|█████     | 808/1579 [1:04:49<1:01:39,  4.80s/it] 51%|█████     | 809/1579 [1:04:54<1:01:34,  4.80s/it] 51%|█████▏    | 810/1579 [1:04:59<1:01:30,  4.80s/it]                                                      {'loss': 2.5649, 'learning_rate': 9.612123564452037e-05, 'epoch': 0.51}
 51%|█████▏    | 810/1579 [1:04:59<1:01:30,  4.80s/it]                                                      {'router_ce_loss': 0.519813597202301, 'old_lang_expert0_score': '0.91 0.91 0.52 0.56 0.55 0.94 0.67 0.59 0.56 0.99 0.56 0.99 0.59 0.69 1.0 0.99 0.74 0.91 0.99 0.64 0.97 0.61 1.0 0.61', 'epoch': 0.51}
 51%|█████▏    | 810/1579 [1:04:59<1:01:30,  4.80s/it] 51%|█████▏    | 811/1579 [1:05:03<1:01:24,  4.80s/it] 51%|█████▏    | 812/1579 [1:05:08<1:01:19,  4.80s/it] 51%|█████▏    | 813/1579 [1:05:13<1:01:14,  4.80s/it] 52%|█████▏    | 814/1579 [1:05:18<1:01:54,  4.86s/it] 52%|█████▏    | 815/1579 [1:05:23<1:02:32,  4.91s/it] 52%|█████▏    | 816/1579 [1:05:28<1:02:03,  4.88s/it] 52%|█████▏    | 817/1579 [1:05:33<1:01:40,  4.86s/it] 52%|█████▏    | 818/1579 [1:05:37<1:01:21,  4.84s/it] 52%|█████▏    | 819/1579 [1:05:42<1:01:11,  4.83s/it] 52%|█████▏    | 820/1579 [1:05:47<1:01:02,  4.83s/it]                                                      {'loss': 2.5927, 'learning_rate': 9.413402269787734e-05, 'epoch': 0.52}
 52%|█████▏    | 820/1579 [1:05:47<1:01:02,  4.83s/it]                                                      {'router_ce_loss': 0.61702960729599, 'old_lang_expert0_score': '0.91 0.89 0.38 0.46 0.4 0.91 0.55 0.39 0.4 0.99 0.37 0.99 0.44 0.54 1.0 0.99 0.57 0.9 0.98 0.41 0.96 0.38 1.0 0.38', 'epoch': 0.52}
 52%|█████▏    | 820/1579 [1:05:47<1:01:02,  4.83s/it] 52%|█████▏    | 821/1579 [1:05:52<1:00:55,  4.82s/it] 52%|█████▏    | 822/1579 [1:05:57<1:00:42,  4.81s/it] 52%|█████▏    | 823/1579 [1:06:01<1:00:34,  4.81s/it] 52%|█████▏    | 824/1579 [1:06:06<1:00:24,  4.80s/it] 52%|█████▏    | 825/1579 [1:06:11<1:00:20,  4.80s/it] 52%|█████▏    | 826/1579 [1:06:16<1:00:15,  4.80s/it] 52%|█████▏    | 827/1579 [1:06:21<1:00:10,  4.80s/it] 52%|█████▏    | 828/1579 [1:06:25<1:00:07,  4.80s/it] 53%|█████▎    | 829/1579 [1:06:30<1:00:04,  4.81s/it] 53%|█████▎    | 830/1579 [1:06:35<59:58,  4.80s/it]                                                      {'loss': 2.6025, 'learning_rate': 9.214913174760743e-05, 'epoch': 0.53}
 53%|█████▎    | 830/1579 [1:06:35<59:58,  4.80s/it]                                                    {'router_ce_loss': 0.5536433458328247, 'old_lang_expert0_score': '0.92 0.91 0.49 0.57 0.52 0.92 0.64 0.52 0.52 0.98 0.5 0.99 0.55 0.66 1.0 0.99 0.67 0.9 0.98 0.53 0.96 0.51 1.0 0.5', 'epoch': 0.53}
 53%|█████▎    | 830/1579 [1:06:35<59:58,  4.80s/it] 53%|█████▎    | 831/1579 [1:06:40<59:53,  4.80s/it] 53%|█████▎    | 832/1579 [1:06:45<59:46,  4.80s/it] 53%|█████▎    | 833/1579 [1:06:49<59:42,  4.80s/it] 53%|█████▎    | 834/1579 [1:06:54<59:38,  4.80s/it] 53%|█████▎    | 835/1579 [1:06:59<59:31,  4.80s/it] 53%|█████▎    | 836/1579 [1:07:04<59:29,  4.80s/it] 53%|█████▎    | 837/1579 [1:07:09<59:22,  4.80s/it] 53%|█████▎    | 838/1579 [1:07:13<59:16,  4.80s/it] 53%|█████▎    | 839/1579 [1:07:18<59:09,  4.80s/it] 53%|█████▎    | 840/1579 [1:07:23<59:04,  4.80s/it]                                                    {'loss': 2.5711, 'learning_rate': 9.016734849562372e-05, 'epoch': 0.53}
 53%|█████▎    | 840/1579 [1:07:23<59:04,  4.80s/it]                                                    {'router_ce_loss': 0.49877920746803284, 'old_lang_expert0_score': '0.89 0.9 0.54 0.61 0.59 0.94 0.69 0.64 0.62 0.98 0.62 0.99 0.67 0.74 0.99 0.99 0.77 0.93 0.98 0.67 0.97 0.65 1.0 0.64', 'epoch': 0.53}
 53%|█████▎    | 840/1579 [1:07:23<59:04,  4.80s/it] 53%|█████▎    | 841/1579 [1:07:28<59:02,  4.80s/it] 53%|█████▎    | 842/1579 [1:07:33<58:59,  4.80s/it] 53%|█████▎    | 843/1579 [1:07:37<58:54,  4.80s/it] 53%|█████▎    | 844/1579 [1:07:42<58:52,  4.81s/it] 54%|█████▎    | 845/1579 [1:07:47<59:36,  4.87s/it] 54%|█████▎    | 846/1579 [1:07:52<59:13,  4.85s/it] 54%|█████▎    | 847/1579 [1:07:57<59:49,  4.90s/it] 54%|█████▎    | 848/1579 [1:08:02<59:23,  4.87s/it] 54%|█████▍    | 849/1579 [1:08:07<58:59,  4.85s/it] 54%|█████▍    | 850/1579 [1:08:12<58:46,  4.84s/it]                                                    {'loss': 2.5958, 'learning_rate': 8.818945741368357e-05, 'epoch': 0.54}
 54%|█████▍    | 850/1579 [1:08:12<58:46,  4.84s/it]                                                    {'router_ce_loss': 0.49912524223327637, 'old_lang_expert0_score': '0.9 0.88 0.59 0.65 0.63 0.95 0.69 0.62 0.62 0.98 0.58 0.99 0.68 0.73 1.0 0.99 0.72 0.94 0.98 0.75 0.86 0.63 0.92 0.63', 'epoch': 0.54}
 54%|█████▍    | 850/1579 [1:08:12<58:46,  4.84s/it] 54%|█████▍    | 851/1579 [1:08:16<58:33,  4.83s/it] 54%|█████▍    | 852/1579 [1:08:21<58:24,  4.82s/it] 54%|█████▍    | 853/1579 [1:08:26<58:14,  4.81s/it] 54%|█████▍    | 854/1579 [1:08:31<58:07,  4.81s/it] 54%|█████▍    | 855/1579 [1:08:36<57:59,  4.81s/it] 54%|█████▍    | 856/1579 [1:08:40<57:54,  4.81s/it] 54%|█████▍    | 857/1579 [1:08:45<57:49,  4.80s/it] 54%|█████▍    | 858/1579 [1:08:50<57:46,  4.81s/it] 54%|█████▍    | 859/1579 [1:08:55<57:39,  4.81s/it] 54%|█████▍    | 860/1579 [1:09:00<57:33,  4.80s/it]                                                    {'loss': 2.5996, 'learning_rate': 8.62162414328627e-05, 'epoch': 0.54}
 54%|█████▍    | 860/1579 [1:09:00<57:33,  4.80s/it]                                                    {'router_ce_loss': 0.602681040763855, 'old_lang_expert0_score': '0.94 0.91 0.41 0.49 0.42 0.91 0.56 0.42 0.4 0.99 0.4 0.99 0.44 0.55 1.0 0.98 0.59 0.9 0.98 0.46 0.96 0.42 1.0 0.4', 'epoch': 0.54}
 54%|█████▍    | 860/1579 [1:09:00<57:33,  4.80s/it] 55%|█████▍    | 861/1579 [1:09:04<57:29,  4.80s/it] 55%|█████▍    | 862/1579 [1:09:09<57:23,  4.80s/it] 55%|█████▍    | 863/1579 [1:09:14<57:17,  4.80s/it] 55%|█████▍    | 864/1579 [1:09:19<57:12,  4.80s/it] 55%|█████▍    | 865/1579 [1:09:24<57:09,  4.80s/it] 55%|█████▍    | 866/1579 [1:09:28<57:03,  4.80s/it] 55%|█████▍    | 867/1579 [1:09:33<57:00,  4.80s/it] 55%|█████▍    | 868/1579 [1:09:38<56:53,  4.80s/it] 55%|█████▌    | 869/1579 [1:09:43<56:49,  4.80s/it] 55%|█████▌    | 870/1579 [1:09:48<56:43,  4.80s/it]                                                    {'loss': 2.5836, 'learning_rate': 8.424848163363834e-05, 'epoch': 0.55}
 55%|█████▌    | 870/1579 [1:09:48<56:43,  4.80s/it]                                                    {'router_ce_loss': 0.5715855956077576, 'old_lang_expert0_score': '0.94 0.91 0.46 0.51 0.45 0.92 0.59 0.5 0.49 0.99 0.44 0.99 0.51 0.62 1.0 0.98 0.64 0.89 0.98 0.53 0.95 0.49 1.0 0.48', 'epoch': 0.55}
 55%|█████▌    | 870/1579 [1:09:48<56:43,  4.80s/it] 55%|█████▌    | 871/1579 [1:09:52<56:39,  4.80s/it] 55%|█████▌    | 872/1579 [1:09:57<56:36,  4.80s/it] 55%|█████▌    | 873/1579 [1:10:02<56:31,  4.80s/it] 55%|█████▌    | 874/1579 [1:10:07<56:26,  4.80s/it] 55%|█████▌    | 875/1579 [1:10:12<56:21,  4.80s/it] 55%|█████▌    | 876/1579 [1:10:16<56:18,  4.81s/it] 56%|█████▌    | 877/1579 [1:10:21<56:53,  4.86s/it] 56%|█████▌    | 878/1579 [1:10:26<57:28,  4.92s/it] 56%|█████▌    | 879/1579 [1:10:31<56:56,  4.88s/it] 56%|█████▌    | 880/1579 [1:10:36<56:34,  4.86s/it]                                                    {'loss': 2.6024, 'learning_rate': 8.228695693670557e-05, 'epoch': 0.56}
 56%|█████▌    | 880/1579 [1:10:36<56:34,  4.86s/it]                                                    {'router_ce_loss': 0.4593653976917267, 'old_lang_expert0_score': '0.89 0.91 0.6 0.67 0.65 0.95 0.73 0.71 0.67 0.98 0.69 0.99 0.74 0.78 0.99 0.98 0.82 0.95 0.99 0.77 0.98 0.76 1.0 0.7', 'epoch': 0.56}
 56%|█████▌    | 880/1579 [1:10:36<56:34,  4.86s/it] 56%|█████▌    | 881/1579 [1:10:41<56:19,  4.84s/it] 56%|█████▌    | 882/1579 [1:10:46<56:07,  4.83s/it] 56%|█████▌    | 883/1579 [1:10:50<55:58,  4.82s/it] 56%|█████▌    | 884/1579 [1:10:55<55:48,  4.82s/it] 56%|█████▌    | 885/1579 [1:11:00<55:41,  4.81s/it] 56%|█████▌    | 886/1579 [1:11:05<55:35,  4.81s/it] 56%|█████▌    | 887/1579 [1:11:10<55:26,  4.81s/it] 56%|█████▌    | 888/1579 [1:11:15<55:23,  4.81s/it] 56%|█████▋    | 889/1579 [1:11:19<55:16,  4.81s/it] 56%|█████▋    | 890/1579 [1:11:24<55:09,  4.80s/it]                                                    {'loss': 2.5908, 'learning_rate': 8.0332443794648e-05, 'epoch': 0.56}
 56%|█████▋    | 890/1579 [1:11:24<55:09,  4.80s/it]                                                    {'router_ce_loss': 0.5195382833480835, 'old_lang_expert0_score': '0.89 0.89 0.51 0.59 0.57 0.94 0.67 0.56 0.57 0.98 0.58 0.99 0.63 0.7 0.99 0.98 0.7 0.94 0.98 0.63 0.96 0.6 1.0 0.61', 'epoch': 0.56}
 56%|█████▋    | 890/1579 [1:11:25<55:09,  4.80s/it] 56%|█████▋    | 891/1579 [1:11:29<55:02,  4.80s/it] 56%|█████▋    | 892/1579 [1:11:34<54:55,  4.80s/it] 57%|█████▋    | 893/1579 [1:11:38<54:47,  4.79s/it] 57%|█████▋    | 894/1579 [1:11:43<54:44,  4.79s/it] 57%|█████▋    | 895/1579 [1:11:48<54:38,  4.79s/it] 57%|█████▋    | 896/1579 [1:11:53<54:34,  4.79s/it] 57%|█████▋    | 897/1579 [1:11:58<54:29,  4.79s/it] 57%|█████▋    | 898/1579 [1:12:02<54:27,  4.80s/it] 57%|█████▋    | 899/1579 [1:12:07<54:25,  4.80s/it] 57%|█████▋    | 900/1579 [1:12:12<54:23,  4.81s/it]                                                    {'loss': 2.5902, 'learning_rate': 7.838571588458608e-05, 'epoch': 0.57}
 57%|█████▋    | 900/1579 [1:12:12<54:23,  4.81s/it]                                                    {'router_ce_loss': 0.5627737641334534, 'old_lang_expert0_score': '0.92 0.9 0.46 0.54 0.49 0.92 0.61 0.5 0.5 0.98 0.5 0.99 0.54 0.63 0.99 0.99 0.66 0.91 0.98 0.53 0.97 0.51 1.0 0.5', 'epoch': 0.57}
 57%|█████▋    | 900/1579 [1:12:12<54:23,  4.81s/it] 57%|█████▋    | 901/1579 [1:12:17<54:18,  4.81s/it] 57%|█████▋    | 902/1579 [1:12:22<54:12,  4.80s/it] 57%|█████▋    | 903/1579 [1:12:27<54:09,  4.81s/it] 57%|█████▋    | 904/1579 [1:12:31<54:04,  4.81s/it] 57%|█████▋    | 905/1579 [1:12:36<54:00,  4.81s/it] 57%|█████▋    | 906/1579 [1:12:41<53:51,  4.80s/it] 57%|█████▋    | 907/1579 [1:12:46<53:48,  4.80s/it] 58%|█████▊    | 908/1579 [1:12:51<54:17,  4.85s/it] 58%|█████▊    | 909/1579 [1:12:56<54:49,  4.91s/it] 58%|█████▊    | 910/1579 [1:13:01<54:22,  4.88s/it]                                                    {'loss': 2.6085, 'learning_rate': 7.644754380192335e-05, 'epoch': 0.58}
 58%|█████▊    | 910/1579 [1:13:01<54:22,  4.88s/it]                                                    {'router_ce_loss': 0.6654090285301208, 'old_lang_expert0_score': '0.94 0.9 0.3 0.39 0.32 0.9 0.46 0.31 0.3 0.99 0.29 1.0 0.35 0.5 1.0 0.98 0.53 0.86 0.97 0.31 0.94 0.27 1.0 0.26', 'epoch': 0.58}
 58%|█████▊    | 910/1579 [1:13:01<54:22,  4.88s/it] 58%|█████▊    | 911/1579 [1:13:05<54:02,  4.85s/it] 58%|█████▊    | 912/1579 [1:13:10<53:50,  4.84s/it] 58%|█████▊    | 913/1579 [1:13:15<53:38,  4.83s/it] 58%|█████▊    | 914/1579 [1:13:20<53:29,  4.83s/it] 58%|█████▊    | 915/1579 [1:13:25<53:18,  4.82s/it] 58%|█████▊    | 916/1579 [1:13:29<53:10,  4.81s/it] 58%|█████▊    | 917/1579 [1:13:34<53:02,  4.81s/it] 58%|█████▊    | 918/1579 [1:13:39<52:54,  4.80s/it] 58%|█████▊    | 919/1579 [1:13:44<52:49,  4.80s/it] 58%|█████▊    | 920/1579 [1:13:49<52:43,  4.80s/it]                                                    {'loss': 2.5799, 'learning_rate': 7.451869475531324e-05, 'epoch': 0.58}
 58%|█████▊    | 920/1579 [1:13:49<52:43,  4.80s/it]                                                    {'router_ce_loss': 0.5685344934463501, 'old_lang_expert0_score': '0.94 0.92 0.45 0.52 0.48 0.93 0.57 0.48 0.47 0.99 0.45 0.99 0.52 0.63 1.0 0.99 0.67 0.91 0.98 0.52 0.95 0.5 1.0 0.49', 'epoch': 0.58}
 58%|█████▊    | 920/1579 [1:13:49<52:43,  4.80s/it] 58%|█████▊    | 921/1579 [1:13:53<52:38,  4.80s/it] 58%|█████▊    | 922/1579 [1:13:58<52:32,  4.80s/it] 58%|█████▊    | 923/1579 [1:14:03<52:27,  4.80s/it] 59%|█████▊    | 924/1579 [1:14:08<52:20,  4.79s/it] 59%|█████▊    | 925/1579 [1:14:13<52:17,  4.80s/it] 59%|█████▊    | 926/1579 [1:14:17<52:14,  4.80s/it] 59%|█████▊    | 927/1579 [1:14:22<52:11,  4.80s/it] 59%|█████▉    | 928/1579 [1:14:27<52:11,  4.81s/it] 59%|█████▉    | 929/1579 [1:14:32<52:01,  4.80s/it] 59%|█████▉    | 930/1579 [1:14:37<51:56,  4.80s/it]                                                    {'loss': 2.6017, 'learning_rate': 7.259993226296581e-05, 'epoch': 0.59}
 59%|█████▉    | 930/1579 [1:14:37<51:56,  4.80s/it]                                                    {'router_ce_loss': 0.6090524792671204, 'old_lang_expert0_score': '0.93 0.89 0.41 0.47 0.41 0.92 0.55 0.42 0.41 0.98 0.4 0.99 0.46 0.57 0.99 0.98 0.6 0.87 0.98 0.43 0.95 0.4 1.0 0.39', 'epoch': 0.59}
 59%|█████▉    | 930/1579 [1:14:37<51:56,  4.80s/it] 59%|█████▉    | 931/1579 [1:14:41<51:50,  4.80s/it] 59%|█████▉    | 932/1579 [1:14:46<51:47,  4.80s/it] 59%|█████▉    | 933/1579 [1:14:51<51:41,  4.80s/it] 59%|█████▉    | 934/1579 [1:14:56<51:36,  4.80s/it] 59%|█████▉    | 935/1579 [1:15:01<51:30,  4.80s/it] 59%|█████▉    | 936/1579 [1:15:05<51:26,  4.80s/it] 59%|█████▉    | 937/1579 [1:15:10<51:21,  4.80s/it] 59%|█████▉    | 938/1579 [1:15:15<51:17,  4.80s/it] 59%|█████▉    | 939/1579 [1:15:20<51:48,  4.86s/it] 60%|█████▉    | 940/1579 [1:15:25<52:16,  4.91s/it]                                                    {'loss': 2.583, 'learning_rate': 7.06920158504161e-05, 'epoch': 0.6}
 60%|█████▉    | 940/1579 [1:15:25<52:16,  4.91s/it]                                                    {'router_ce_loss': 0.6220381855964661, 'old_lang_expert0_score': '0.92 0.88 0.39 0.48 0.41 0.89 0.56 0.4 0.38 0.98 0.37 0.99 0.44 0.54 1.0 0.98 0.55 0.89 0.97 0.41 0.96 0.38 1.0 0.37', 'epoch': 0.6}
 60%|█████▉    | 940/1579 [1:15:25<52:16,  4.91s/it] 60%|█████▉    | 941/1579 [1:15:30<51:51,  4.88s/it] 60%|█████▉    | 942/1579 [1:15:35<51:29,  4.85s/it] 60%|█████▉    | 943/1579 [1:15:39<51:15,  4.84s/it] 60%|█████▉    | 944/1579 [1:15:44<51:01,  4.82s/it] 60%|█████▉    | 945/1579 [1:15:49<50:56,  4.82s/it] 60%|█████▉    | 946/1579 [1:15:54<50:44,  4.81s/it] 60%|█████▉    | 947/1579 [1:15:59<50:38,  4.81s/it] 60%|██████    | 948/1579 [1:16:03<50:31,  4.80s/it] 60%|██████    | 949/1579 [1:16:08<50:25,  4.80s/it] 60%|██████    | 950/1579 [1:16:13<50:19,  4.80s/it]                                                    {'loss': 2.6096, 'learning_rate': 6.879570074987204e-05, 'epoch': 0.6}
 60%|██████    | 950/1579 [1:16:13<50:19,  4.80s/it]                                                    {'router_ce_loss': 0.47600722312927246, 'old_lang_expert0_score': '0.91 0.92 0.58 0.62 0.59 0.95 0.71 0.67 0.67 0.98 0.67 0.99 0.7 0.77 0.99 0.99 0.81 0.94 0.98 0.72 0.96 0.72 1.0 0.67', 'epoch': 0.6}
 60%|██████    | 950/1579 [1:16:13<50:19,  4.80s/it] 60%|██████    | 951/1579 [1:16:18<50:13,  4.80s/it] 60%|██████    | 952/1579 [1:16:23<50:08,  4.80s/it] 60%|██████    | 953/1579 [1:16:27<50:03,  4.80s/it] 60%|██████    | 954/1579 [1:16:32<49:59,  4.80s/it] 60%|██████    | 955/1579 [1:16:37<49:57,  4.80s/it] 61%|██████    | 956/1579 [1:16:42<49:51,  4.80s/it] 61%|██████    | 957/1579 [1:16:47<49:47,  4.80s/it] 61%|██████    | 958/1579 [1:16:51<49:43,  4.80s/it] 61%|██████    | 959/1579 [1:16:56<49:38,  4.80s/it] 61%|██████    | 960/1579 [1:17:01<49:33,  4.80s/it]                                                    {'loss': 2.5944, 'learning_rate': 6.69117376012628e-05, 'epoch': 0.61}
 61%|██████    | 960/1579 [1:17:01<49:33,  4.80s/it]                                                    {'router_ce_loss': 0.5613331198692322, 'old_lang_expert0_score': '0.92 0.91 0.48 0.54 0.5 0.93 0.6 0.51 0.5 0.99 0.49 0.99 0.55 0.65 1.0 0.99 0.67 0.9 0.98 0.54 0.96 0.5 1.0 0.5', 'epoch': 0.61}
 61%|██████    | 960/1579 [1:17:01<49:33,  4.80s/it] 61%|██████    | 961/1579 [1:17:06<49:30,  4.81s/it] 61%|██████    | 962/1579 [1:17:11<49:21,  4.80s/it] 61%|██████    | 963/1579 [1:17:15<49:14,  4.80s/it] 61%|██████    | 964/1579 [1:17:20<49:08,  4.79s/it] 61%|██████    | 965/1579 [1:17:25<49:03,  4.79s/it] 61%|██████    | 966/1579 [1:17:30<48:57,  4.79s/it] 61%|██████    | 967/1579 [1:17:35<48:53,  4.79s/it] 61%|██████▏   | 968/1579 [1:17:39<48:47,  4.79s/it] 61%|██████▏   | 969/1579 [1:17:44<48:45,  4.80s/it] 61%|██████▏   | 970/1579 [1:17:49<49:25,  4.87s/it]                                                    {'loss': 2.5827, 'learning_rate': 6.504087215510397e-05, 'epoch': 0.61}
 61%|██████▏   | 970/1579 [1:17:49<49:25,  4.87s/it]                                                    {'router_ce_loss': 0.6186007261276245, 'old_lang_expert0_score': '0.94 0.9 0.38 0.47 0.4 0.91 0.55 0.4 0.38 0.99 0.38 0.99 0.44 0.56 1.0 0.98 0.57 0.88 0.98 0.41 0.96 0.37 1.0 0.37', 'epoch': 0.61}
 61%|██████▏   | 970/1579 [1:17:50<49:25,  4.87s/it] 61%|██████▏   | 971/1579 [1:17:54<49:07,  4.85s/it] 62%|██████▏   | 972/1579 [1:17:59<49:37,  4.91s/it] 62%|██████▏   | 973/1579 [1:18:04<49:14,  4.88s/it] 62%|██████▏   | 974/1579 [1:18:09<48:57,  4.86s/it] 62%|██████▏   | 975/1579 [1:18:13<48:43,  4.84s/it] 62%|██████▏   | 976/1579 [1:18:18<48:29,  4.82s/it] 62%|██████▏   | 977/1579 [1:18:23<48:19,  4.82s/it] 62%|██████▏   | 978/1579 [1:18:28<48:12,  4.81s/it] 62%|██████▏   | 979/1579 [1:18:33<48:04,  4.81s/it] 62%|██████▏   | 980/1579 [1:18:37<47:58,  4.81s/it]                                                    {'loss': 2.6071, 'learning_rate': 6.318384497729917e-05, 'epoch': 0.62}
 62%|██████▏   | 980/1579 [1:18:37<47:58,  4.81s/it]                                                    {'router_ce_loss': 0.5046442747116089, 'old_lang_expert0_score': '0.91 0.91 0.56 0.62 0.58 0.95 0.67 0.62 0.62 0.98 0.59 0.99 0.62 0.73 0.99 0.99 0.76 0.93 0.98 0.67 0.96 0.63 1.0 0.62', 'epoch': 0.62}
 62%|██████▏   | 980/1579 [1:18:38<47:58,  4.81s/it] 62%|██████▏   | 981/1579 [1:18:42<47:54,  4.81s/it] 62%|██████▏   | 982/1579 [1:18:47<47:48,  4.80s/it] 62%|██████▏   | 983/1579 [1:18:52<47:41,  4.80s/it] 62%|██████▏   | 984/1579 [1:18:57<47:36,  4.80s/it] 62%|██████▏   | 985/1579 [1:19:01<47:28,  4.79s/it] 62%|██████▏   | 986/1579 [1:19:06<47:23,  4.80s/it] 63%|██████▎   | 987/1579 [1:19:11<47:18,  4.79s/it] 63%|██████▎   | 988/1579 [1:19:16<47:17,  4.80s/it] 63%|██████▎   | 989/1579 [1:19:21<47:11,  4.80s/it] 63%|██████▎   | 990/1579 [1:19:25<47:08,  4.80s/it]                                                    {'loss': 2.5837, 'learning_rate': 6.1341391155993e-05, 'epoch': 0.63}
 63%|██████▎   | 990/1579 [1:19:25<47:08,  4.80s/it]                                                    {'router_ce_loss': 0.5188234448432922, 'old_lang_expert0_score': '0.91 0.9 0.52 0.58 0.55 0.94 0.68 0.6 0.57 0.98 0.56 0.99 0.62 0.72 1.0 0.99 0.75 0.91 0.98 0.64 0.96 0.61 1.0 0.61', 'epoch': 0.63}
 63%|██████▎   | 990/1579 [1:19:26<47:08,  4.80s/it] 63%|██████▎   | 991/1579 [1:19:30<47:05,  4.81s/it] 63%|██████▎   | 992/1579 [1:19:35<46:58,  4.80s/it] 63%|██████▎   | 993/1579 [1:19:40<46:55,  4.80s/it] 63%|██████▎   | 994/1579 [1:19:45<46:50,  4.80s/it] 63%|██████▎   | 995/1579 [1:19:49<46:46,  4.81s/it] 63%|██████▎   | 996/1579 [1:19:54<46:41,  4.80s/it] 63%|██████▎   | 997/1579 [1:19:59<46:38,  4.81s/it] 63%|██████▎   | 998/1579 [1:20:04<46:31,  4.80s/it] 63%|██████▎   | 999/1579 [1:20:09<46:26,  4.80s/it] 63%|██████▎   | 1000/1579 [1:20:13<46:19,  4.80s/it]                                                     {'loss': 2.5863, 'learning_rate': 5.951424001059306e-05, 'epoch': 0.63}
 63%|██████▎   | 1000/1579 [1:20:13<46:19,  4.80s/it]                                                     {'router_ce_loss': 0.6041104793548584, 'old_lang_expert0_score': '0.92 0.89 0.4 0.5 0.43 0.91 0.56 0.44 0.42 0.99 0.42 0.99 0.47 0.58 1.0 0.98 0.6 0.88 0.97 0.43 0.96 0.38 1.0 0.38', 'epoch': 0.63}
 63%|██████▎   | 1000/1579 [1:20:14<46:19,  4.80s/it] 63%|██████▎   | 1001/1579 [1:20:18<46:16,  4.80s/it] 63%|██████▎   | 1002/1579 [1:20:23<46:52,  4.88s/it] 64%|██████▎   | 1003/1579 [1:20:28<47:14,  4.92s/it] 64%|██████▎   | 1004/1579 [1:20:33<46:48,  4.88s/it] 64%|██████▎   | 1005/1579 [1:20:38<46:28,  4.86s/it] 64%|██████▎   | 1006/1579 [1:20:43<46:12,  4.84s/it] 64%|██████▍   | 1007/1579 [1:20:48<46:02,  4.83s/it] 64%|██████▍   | 1008/1579 [1:20:52<45:50,  4.82s/it] 64%|██████▍   | 1009/1579 [1:20:57<45:43,  4.81s/it] 64%|██████▍   | 1010/1579 [1:21:02<45:37,  4.81s/it]                                                     {'loss': 2.5923, 'learning_rate': 5.770311480307503e-05, 'epoch': 0.64}
 64%|██████▍   | 1010/1579 [1:21:02<45:37,  4.81s/it]                                                     {'router_ce_loss': 0.5637350678443909, 'old_lang_expert0_score': '0.92 0.9 0.46 0.53 0.48 0.93 0.61 0.51 0.49 0.98 0.48 0.99 0.54 0.66 1.0 0.99 0.69 0.9 0.98 0.52 0.96 0.49 1.0 0.49', 'epoch': 0.64}
 64%|██████▍   | 1010/1579 [1:21:02<45:37,  4.81s/it] 64%|██████▍   | 1011/1579 [1:21:07<45:32,  4.81s/it] 64%|██████▍   | 1012/1579 [1:21:12<45:26,  4.81s/it] 64%|██████▍   | 1013/1579 [1:21:16<45:20,  4.81s/it] 64%|██████▍   | 1014/1579 [1:21:21<45:14,  4.80s/it] 64%|██████▍   | 1015/1579 [1:21:26<45:09,  4.80s/it] 64%|██████▍   | 1016/1579 [1:21:31<45:06,  4.81s/it] 64%|██████▍   | 1017/1579 [1:21:36<45:01,  4.81s/it] 64%|██████▍   | 1018/1579 [1:21:40<44:53,  4.80s/it] 65%|██████▍   | 1019/1579 [1:21:45<44:48,  4.80s/it] 65%|██████▍   | 1020/1579 [1:21:50<44:41,  4.80s/it]                                                     {'loss': 2.5982, 'learning_rate': 5.590873245168575e-05, 'epoch': 0.65}
 65%|██████▍   | 1020/1579 [1:21:50<44:41,  4.80s/it]                                                     {'router_ce_loss': 0.6159676909446716, 'old_lang_expert0_score': '0.91 0.89 0.39 0.48 0.42 0.91 0.56 0.4 0.39 0.99 0.38 0.99 0.46 0.57 0.99 0.99 0.57 0.86 0.97 0.41 0.95 0.38 1.0 0.38', 'epoch': 0.65}
 65%|██████▍   | 1020/1579 [1:21:50<44:41,  4.80s/it] 65%|██████▍   | 1021/1579 [1:21:55<44:37,  4.80s/it] 65%|██████▍   | 1022/1579 [1:22:00<44:29,  4.79s/it] 65%|██████▍   | 1023/1579 [1:22:04<44:25,  4.79s/it] 65%|██████▍   | 1024/1579 [1:22:09<44:20,  4.79s/it] 65%|██████▍   | 1025/1579 [1:22:14<44:18,  4.80s/it] 65%|██████▍   | 1026/1579 [1:22:19<44:15,  4.80s/it] 65%|██████▌   | 1027/1579 [1:22:24<44:08,  4.80s/it] 65%|██████▌   | 1028/1579 [1:22:28<44:05,  4.80s/it] 65%|██████▌   | 1029/1579 [1:22:33<44:00,  4.80s/it] 65%|██████▌   | 1030/1579 [1:22:38<43:56,  4.80s/it]                                                     {'loss': 2.5576, 'learning_rate': 5.4131803247157184e-05, 'epoch': 0.65}
 65%|██████▌   | 1030/1579 [1:22:38<43:56,  4.80s/it]                                                     {'router_ce_loss': 0.6181983351707458, 'old_lang_expert0_score': '0.93 0.9 0.4 0.46 0.41 0.92 0.51 0.39 0.39 0.99 0.38 0.99 0.44 0.56 1.0 0.98 0.59 0.88 0.97 0.41 0.95 0.37 1.0 0.37', 'epoch': 0.65}
 65%|██████▌   | 1030/1579 [1:22:38<43:56,  4.80s/it] 65%|██████▌   | 1031/1579 [1:22:43<43:51,  4.80s/it] 65%|██████▌   | 1032/1579 [1:22:48<43:43,  4.80s/it] 65%|██████▌   | 1033/1579 [1:22:53<44:12,  4.86s/it] 65%|██████▌   | 1034/1579 [1:22:58<44:31,  4.90s/it] 66%|██████▌   | 1035/1579 [1:23:02<44:07,  4.87s/it] 66%|██████▌   | 1036/1579 [1:23:07<43:51,  4.85s/it] 66%|██████▌   | 1037/1579 [1:23:12<43:36,  4.83s/it] 66%|██████▌   | 1038/1579 [1:23:17<43:31,  4.83s/it] 66%|██████▌   | 1039/1579 [1:23:22<43:22,  4.82s/it] 66%|██████▌   | 1040/1579 [1:23:26<43:14,  4.81s/it]                                                     {'loss': 2.5866, 'learning_rate': 5.237303057154419e-05, 'epoch': 0.66}
 66%|██████▌   | 1040/1579 [1:23:26<43:14,  4.81s/it]                                                     {'router_ce_loss': 0.570193350315094, 'old_lang_expert0_score': '0.92 0.9 0.46 0.52 0.49 0.93 0.61 0.5 0.49 0.98 0.46 0.99 0.52 0.63 1.0 0.98 0.65 0.9 0.97 0.53 0.95 0.48 1.0 0.48', 'epoch': 0.66}
 66%|██████▌   | 1040/1579 [1:23:27<43:14,  4.81s/it] 66%|██████▌   | 1041/1579 [1:23:31<43:08,  4.81s/it] 66%|██████▌   | 1042/1579 [1:23:36<43:01,  4.81s/it] 66%|██████▌   | 1043/1579 [1:23:41<42:57,  4.81s/it] 66%|██████▌   | 1044/1579 [1:23:46<42:51,  4.81s/it] 66%|██████▌   | 1045/1579 [1:23:50<42:45,  4.80s/it] 66%|██████▌   | 1046/1579 [1:23:55<42:40,  4.80s/it] 66%|██████▋   | 1047/1579 [1:24:00<42:35,  4.80s/it] 66%|██████▋   | 1048/1579 [1:24:05<42:28,  4.80s/it] 66%|██████▋   | 1049/1579 [1:24:10<42:23,  4.80s/it] 66%|██████▋   | 1050/1579 [1:24:14<42:18,  4.80s/it]                                                     {'loss': 2.584, 'learning_rate': 5.0633110619796676e-05, 'epoch': 0.66}
 66%|██████▋   | 1050/1579 [1:24:14<42:18,  4.80s/it]                                                     {'router_ce_loss': 0.5441363453865051, 'old_lang_expert0_score': '0.91 0.89 0.5 0.56 0.53 0.93 0.63 0.55 0.55 0.99 0.54 0.99 0.58 0.67 0.99 0.98 0.68 0.91 0.98 0.57 0.96 0.54 1.0 0.53', 'epoch': 0.66}
 66%|██████▋   | 1050/1579 [1:24:15<42:18,  4.80s/it] 67%|██████▋   | 1051/1579 [1:24:19<42:13,  4.80s/it] 67%|██████▋   | 1052/1579 [1:24:24<42:10,  4.80s/it] 67%|██████▋   | 1053/1579 [1:24:29<42:03,  4.80s/it] 67%|██████▋   | 1054/1579 [1:24:34<41:59,  4.80s/it] 67%|██████▋   | 1055/1579 [1:24:38<41:53,  4.80s/it] 67%|██████▋   | 1056/1579 [1:24:43<41:48,  4.80s/it] 67%|██████▋   | 1057/1579 [1:24:48<41:43,  4.80s/it] 67%|██████▋   | 1058/1579 [1:24:53<41:38,  4.80s/it] 67%|██████▋   | 1059/1579 [1:24:57<41:34,  4.80s/it] 67%|██████▋   | 1060/1579 [1:25:02<41:28,  4.79s/it]                                                     {'loss': 2.5984, 'learning_rate': 4.891273212417692e-05, 'epoch': 0.67}
 67%|██████▋   | 1060/1579 [1:25:02<41:28,  4.79s/it]                                                     {'router_ce_loss': 0.5274107456207275, 'old_lang_expert0_score': '0.92 0.9 0.51 0.58 0.55 0.93 0.65 0.55 0.54 0.98 0.52 0.99 0.59 0.69 1.0 0.99 0.73 0.92 0.99 0.63 0.96 0.6 1.0 0.6', 'epoch': 0.67}
 67%|██████▋   | 1060/1579 [1:25:03<41:28,  4.79s/it] 67%|██████▋   | 1061/1579 [1:25:07<41:25,  4.80s/it] 67%|██████▋   | 1062/1579 [1:25:12<41:20,  4.80s/it] 67%|██████▋   | 1063/1579 [1:25:17<41:16,  4.80s/it] 67%|██████▋   | 1064/1579 [1:25:22<41:46,  4.87s/it] 67%|██████▋   | 1065/1579 [1:25:27<42:06,  4.92s/it] 68%|██████▊   | 1066/1579 [1:25:32<41:43,  4.88s/it] 68%|██████▊   | 1067/1579 [1:25:36<41:27,  4.86s/it] 68%|██████▊   | 1068/1579 [1:25:41<41:14,  4.84s/it] 68%|██████▊   | 1069/1579 [1:25:46<41:03,  4.83s/it] 68%|██████▊   | 1070/1579 [1:25:51<40:53,  4.82s/it]                                                     {'loss': 2.5913, 'learning_rate': 4.7212576081630724e-05, 'epoch': 0.68}
 68%|██████▊   | 1070/1579 [1:25:51<40:53,  4.82s/it]                                                     {'router_ce_loss': 0.6177675127983093, 'old_lang_expert0_score': '0.94 0.9 0.39 0.47 0.4 0.91 0.53 0.4 0.39 0.99 0.37 0.99 0.44 0.56 1.0 0.98 0.58 0.89 0.98 0.42 0.94 0.39 1.0 0.38', 'epoch': 0.68}
 68%|██████▊   | 1070/1579 [1:25:51<40:53,  4.82s/it] 68%|██████▊   | 1071/1579 [1:25:56<40:48,  4.82s/it] 68%|██████▊   | 1072/1579 [1:26:00<40:38,  4.81s/it] 68%|██████▊   | 1073/1579 [1:26:05<40:34,  4.81s/it] 68%|██████▊   | 1074/1579 [1:26:10<40:25,  4.80s/it] 68%|██████▊   | 1075/1579 [1:26:15<40:21,  4.80s/it] 68%|██████▊   | 1076/1579 [1:26:20<40:14,  4.80s/it] 68%|██████▊   | 1077/1579 [1:26:24<40:10,  4.80s/it] 68%|██████▊   | 1078/1579 [1:26:29<40:03,  4.80s/it] 68%|██████▊   | 1079/1579 [1:26:34<39:58,  4.80s/it] 68%|██████▊   | 1080/1579 [1:26:39<39:53,  4.80s/it]                                                     {'loss': 2.5616, 'learning_rate': 4.553331548422086e-05, 'epoch': 0.68}
 68%|██████▊   | 1080/1579 [1:26:39<39:53,  4.80s/it]                                                     {'router_ce_loss': 0.5028306245803833, 'old_lang_expert0_score': '0.9 0.91 0.55 0.61 0.58 0.94 0.7 0.63 0.61 0.98 0.6 0.99 0.66 0.74 0.99 0.98 0.77 0.92 0.98 0.66 0.95 0.63 1.0 0.63', 'epoch': 0.68}
 68%|██████▊   | 1080/1579 [1:26:39<39:53,  4.80s/it] 68%|██████▊   | 1081/1579 [1:26:44<39:51,  4.80s/it] 69%|██████▊   | 1082/1579 [1:26:48<39:45,  4.80s/it] 69%|██████▊   | 1083/1579 [1:26:53<39:41,  4.80s/it] 69%|██████▊   | 1084/1579 [1:26:58<39:36,  4.80s/it] 69%|██████▊   | 1085/1579 [1:27:03<39:32,  4.80s/it] 69%|██████▉   | 1086/1579 [1:27:08<39:27,  4.80s/it] 69%|██████▉   | 1087/1579 [1:27:12<39:22,  4.80s/it] 69%|██████▉   | 1088/1579 [1:27:17<39:17,  4.80s/it] 69%|██████▉   | 1089/1579 [1:27:22<39:11,  4.80s/it] 69%|██████▉   | 1090/1579 [1:27:27<39:06,  4.80s/it]                                                     {'loss': 2.5838, 'learning_rate': 4.387561505272891e-05, 'epoch': 0.69}
 69%|██████▉   | 1090/1579 [1:27:27<39:06,  4.80s/it]                                                     {'router_ce_loss': 0.5614131093025208, 'old_lang_expert0_score': '0.92 0.91 0.46 0.54 0.5 0.92 0.62 0.49 0.49 0.98 0.49 0.99 0.54 0.65 1.0 0.99 0.67 0.9 0.99 0.53 0.96 0.5 1.0 0.49', 'epoch': 0.69}
 69%|██████▉   | 1090/1579 [1:27:27<39:06,  4.80s/it] 69%|██████▉   | 1091/1579 [1:27:32<39:01,  4.80s/it] 69%|██████▉   | 1092/1579 [1:27:36<38:59,  4.80s/it] 69%|██████▉   | 1093/1579 [1:27:41<38:52,  4.80s/it] 69%|██████▉   | 1094/1579 [1:27:46<38:50,  4.81s/it] 69%|██████▉   | 1095/1579 [1:27:51<39:16,  4.87s/it] 69%|██████▉   | 1096/1579 [1:27:56<39:00,  4.85s/it] 69%|██████▉   | 1097/1579 [1:28:01<39:19,  4.90s/it] 70%|██████▉   | 1098/1579 [1:28:06<39:02,  4.87s/it] 70%|██████▉   | 1099/1579 [1:28:10<38:48,  4.85s/it] 70%|██████▉   | 1100/1579 [1:28:15<38:35,  4.83s/it]                                                     {'loss': 2.6053, 'learning_rate': 4.224013097353126e-05, 'epoch': 0.7}
 70%|██████▉   | 1100/1579 [1:28:15<38:35,  4.83s/it]                                                     {'router_ce_loss': 0.5655924677848816, 'old_lang_expert0_score': '0.91 0.91 0.47 0.54 0.49 0.93 0.62 0.5 0.5 0.98 0.48 0.99 0.52 0.63 1.0 0.98 0.65 0.9 0.98 0.53 0.96 0.5 1.0 0.5', 'epoch': 0.7}
 70%|██████▉   | 1100/1579 [1:28:16<38:35,  4.83s/it] 70%|██████▉   | 1101/1579 [1:28:20<38:26,  4.82s/it] 70%|██████▉   | 1102/1579 [1:28:25<38:17,  4.82s/it] 70%|██████▉   | 1103/1579 [1:28:30<38:10,  4.81s/it] 70%|██████▉   | 1104/1579 [1:28:34<38:02,  4.81s/it] 70%|██████▉   | 1105/1579 [1:28:39<37:57,  4.80s/it] 70%|███████   | 1106/1579 [1:28:44<37:51,  4.80s/it] 70%|███████   | 1107/1579 [1:28:49<37:47,  4.80s/it] 70%|███████   | 1108/1579 [1:28:54<37:41,  4.80s/it] 70%|███████   | 1109/1579 [1:28:58<37:38,  4.81s/it] 70%|███████   | 1110/1579 [1:29:03<37:33,  4.80s/it]                                                     {'loss': 2.5661, 'learning_rate': 4.0627510638853314e-05, 'epoch': 0.7}
 70%|███████   | 1110/1579 [1:29:03<37:33,  4.80s/it]                                                     {'router_ce_loss': 0.48016178607940674, 'old_lang_expert0_score': '0.92 0.9 0.6 0.68 0.65 0.95 0.7 0.67 0.66 0.98 0.62 0.99 0.69 0.75 0.99 0.98 0.77 0.94 0.99 0.73 0.93 0.68 0.97 0.66', 'epoch': 0.7}
 70%|███████   | 1110/1579 [1:29:04<37:33,  4.80s/it] 70%|███████   | 1111/1579 [1:29:08<37:28,  4.81s/it] 70%|███████   | 1112/1579 [1:29:13<37:22,  4.80s/it] 70%|███████   | 1113/1579 [1:29:18<37:18,  4.80s/it] 71%|███████   | 1114/1579 [1:29:22<37:13,  4.80s/it] 71%|███████   | 1115/1579 [1:29:27<37:07,  4.80s/it] 71%|███████   | 1116/1579 [1:29:32<37:02,  4.80s/it] 71%|███████   | 1117/1579 [1:29:37<36:58,  4.80s/it] 71%|███████   | 1118/1579 [1:29:42<36:52,  4.80s/it] 71%|███████   | 1119/1579 [1:29:46<36:48,  4.80s/it] 71%|███████   | 1120/1579 [1:29:51<36:43,  4.80s/it]                                                     {'loss': 2.5787, 'learning_rate': 3.9038392390505154e-05, 'epoch': 0.71}
 71%|███████   | 1120/1579 [1:29:51<36:43,  4.80s/it]                                                     {'router_ce_loss': 0.5411377549171448, 'old_lang_expert0_score': '0.92 0.9 0.51 0.56 0.53 0.93 0.65 0.55 0.54 0.98 0.53 0.99 0.57 0.69 1.0 0.99 0.71 0.91 0.98 0.58 0.96 0.54 1.0 0.54', 'epoch': 0.71}
 71%|███████   | 1120/1579 [1:29:52<36:43,  4.80s/it] 71%|███████   | 1121/1579 [1:29:56<36:37,  4.80s/it] 71%|███████   | 1122/1579 [1:30:01<36:35,  4.80s/it] 71%|███████   | 1123/1579 [1:30:06<36:31,  4.81s/it] 71%|███████   | 1124/1579 [1:30:10<36:24,  4.80s/it] 71%|███████   | 1125/1579 [1:30:15<36:21,  4.80s/it] 71%|███████▏  | 1126/1579 [1:30:20<36:14,  4.80s/it] 71%|███████▏  | 1127/1579 [1:30:25<36:40,  4.87s/it] 71%|███████▏  | 1128/1579 [1:30:30<36:58,  4.92s/it] 72%|███████▏  | 1129/1579 [1:30:35<36:38,  4.89s/it] 72%|███████▏  | 1130/1579 [1:30:40<36:20,  4.86s/it]                                                     {'loss': 2.5785, 'learning_rate': 3.747340526719908e-05, 'epoch': 0.72}
 72%|███████▏  | 1130/1579 [1:30:40<36:20,  4.86s/it]                                                     {'router_ce_loss': 0.6174154877662659, 'old_lang_expert0_score': '0.94 0.91 0.38 0.45 0.39 0.91 0.55 0.41 0.39 0.99 0.38 0.99 0.45 0.59 1.0 0.99 0.6 0.86 0.98 0.42 0.94 0.38 1.0 0.37', 'epoch': 0.72}
 72%|███████▏  | 1130/1579 [1:30:40<36:20,  4.86s/it] 72%|███████▏  | 1131/1579 [1:30:45<36:08,  4.84s/it] 72%|███████▏  | 1132/1579 [1:30:49<35:59,  4.83s/it] 72%|███████▏  | 1133/1579 [1:30:54<35:51,  4.82s/it] 72%|███████▏  | 1134/1579 [1:30:59<35:44,  4.82s/it] 72%|███████▏  | 1135/1579 [1:31:04<35:36,  4.81s/it] 72%|███████▏  | 1136/1579 [1:31:09<35:32,  4.81s/it] 72%|███████▏  | 1137/1579 [1:31:13<35:26,  4.81s/it] 72%|███████▏  | 1138/1579 [1:31:18<35:20,  4.81s/it] 72%|███████▏  | 1139/1579 [1:31:23<35:15,  4.81s/it] 72%|███████▏  | 1140/1579 [1:31:28<35:10,  4.81s/it]                                                     {'loss': 2.6009, 'learning_rate': 3.593316875555056e-05, 'epoch': 0.72}
 72%|███████▏  | 1140/1579 [1:31:28<35:10,  4.81s/it]                                                     {'router_ce_loss': 0.6169927716255188, 'old_lang_expert0_score': '0.93 0.9 0.41 0.46 0.4 0.91 0.53 0.4 0.39 0.99 0.39 0.99 0.44 0.6 1.0 0.99 0.61 0.85 0.97 0.42 0.94 0.38 1.0 0.37', 'epoch': 0.72}
 72%|███████▏  | 1140/1579 [1:31:28<35:10,  4.81s/it] 72%|███████▏  | 1141/1579 [1:31:33<35:04,  4.81s/it] 72%|███████▏  | 1142/1579 [1:31:37<34:59,  4.80s/it] 72%|███████▏  | 1143/1579 [1:31:42<34:54,  4.80s/it] 72%|███████▏  | 1144/1579 [1:31:47<34:48,  4.80s/it] 73%|███████▎  | 1145/1579 [1:31:52<34:42,  4.80s/it] 73%|███████▎  | 1146/1579 [1:31:57<34:38,  4.80s/it] 73%|███████▎  | 1147/1579 [1:32:01<34:33,  4.80s/it] 73%|███████▎  | 1148/1579 [1:32:06<34:28,  4.80s/it] 73%|███████▎  | 1149/1579 [1:32:11<34:23,  4.80s/it] 73%|███████▎  | 1150/1579 [1:32:16<34:21,  4.80s/it]                                                     {'loss': 2.5696, 'learning_rate': 3.4418292544859256e-05, 'epoch': 0.73}
 73%|███████▎  | 1150/1579 [1:32:16<34:21,  4.80s/it]                                                     {'router_ce_loss': 0.6860665678977966, 'old_lang_expert0_score': '0.96 0.9 0.24 0.33 0.25 0.91 0.42 0.24 0.23 0.99 0.22 0.99 0.29 0.48 1.0 0.98 0.52 0.85 0.97 0.32 0.94 0.26 1.0 0.25', 'epoch': 0.73}
 73%|███████▎  | 1150/1579 [1:32:16<34:21,  4.80s/it] 73%|███████▎  | 1151/1579 [1:32:21<34:16,  4.80s/it] 73%|███████▎  | 1152/1579 [1:32:25<34:11,  4.80s/it] 73%|███████▎  | 1153/1579 [1:32:30<34:06,  4.80s/it] 73%|███████▎  | 1154/1579 [1:32:35<34:02,  4.81s/it] 73%|███████▎  | 1155/1579 [1:32:40<33:57,  4.81s/it] 73%|███████▎  | 1156/1579 [1:32:45<33:52,  4.81s/it] 73%|███████▎  | 1157/1579 [1:32:49<33:46,  4.80s/it] 73%|███████▎  | 1158/1579 [1:32:54<34:06,  4.86s/it] 73%|███████▎  | 1159/1579 [1:32:59<34:21,  4.91s/it] 73%|███████▎  | 1160/1579 [1:33:04<34:01,  4.87s/it]                                                     {'loss': 2.5894, 'learning_rate': 3.292937628576926e-05, 'epoch': 0.73}
 73%|███████▎  | 1160/1579 [1:33:04<34:01,  4.87s/it]                                                     {'router_ce_loss': 0.6650436520576477, 'old_lang_expert0_score': '0.95 0.91 0.32 0.4 0.32 0.9 0.49 0.31 0.3 0.99 0.29 0.99 0.36 0.5 1.0 0.99 0.51 0.86 0.97 0.31 0.94 0.27 1.0 0.26', 'epoch': 0.73}
 73%|███████▎  | 1160/1579 [1:33:05<34:01,  4.87s/it] 74%|███████▎  | 1161/1579 [1:33:09<33:47,  4.85s/it] 74%|███████▎  | 1162/1579 [1:33:14<33:35,  4.83s/it] 74%|███████▎  | 1163/1579 [1:33:19<33:27,  4.83s/it] 74%|███████▎  | 1164/1579 [1:33:23<33:18,  4.82s/it] 74%|███████▍  | 1165/1579 [1:33:28<33:13,  4.82s/it] 74%|███████▍  | 1166/1579 [1:33:33<33:07,  4.81s/it] 74%|███████▍  | 1167/1579 [1:33:38<33:01,  4.81s/it] 74%|███████▍  | 1168/1579 [1:33:43<32:56,  4.81s/it] 74%|███████▍  | 1169/1579 [1:33:47<32:50,  4.81s/it] 74%|███████▍  | 1170/1579 [1:33:52<32:46,  4.81s/it]                                                     {'loss': 2.5824, 'learning_rate': 3.146700935290226e-05, 'epoch': 0.74}
 74%|███████▍  | 1170/1579 [1:33:52<32:46,  4.81s/it]                                                     {'router_ce_loss': 0.6786307692527771, 'old_lang_expert0_score': '0.95 0.89 0.29 0.37 0.3 0.9 0.45 0.29 0.26 0.99 0.26 1.0 0.33 0.48 1.0 0.99 0.51 0.84 0.97 0.3 0.94 0.26 1.0 0.25', 'epoch': 0.74}
 74%|███████▍  | 1170/1579 [1:33:53<32:46,  4.81s/it] 74%|███████▍  | 1171/1579 [1:33:57<32:39,  4.80s/it] 74%|███████▍  | 1172/1579 [1:34:02<32:34,  4.80s/it] 74%|███████▍  | 1173/1579 [1:34:07<32:28,  4.80s/it] 74%|███████▍  | 1174/1579 [1:34:11<32:24,  4.80s/it] 74%|███████▍  | 1175/1579 [1:34:16<32:18,  4.80s/it] 74%|███████▍  | 1176/1579 [1:34:21<32:13,  4.80s/it] 75%|███████▍  | 1177/1579 [1:34:26<32:09,  4.80s/it] 75%|███████▍  | 1178/1579 [1:34:31<32:05,  4.80s/it] 75%|███████▍  | 1179/1579 [1:34:35<32:01,  4.80s/it] 75%|███████▍  | 1180/1579 [1:34:40<31:55,  4.80s/it]                                                     {'loss': 2.5859, 'learning_rate': 3.0031770611558886e-05, 'epoch': 0.75}
 75%|███████▍  | 1180/1579 [1:34:40<31:55,  4.80s/it]                                                     {'router_ce_loss': 0.5746209025382996, 'old_lang_expert0_score': '0.93 0.9 0.44 0.51 0.47 0.93 0.58 0.48 0.46 0.98 0.43 0.99 0.5 0.62 1.0 0.99 0.65 0.9 0.98 0.52 0.96 0.49 1.0 0.48', 'epoch': 0.75}
 75%|███████▍  | 1180/1579 [1:34:41<31:55,  4.80s/it] 75%|███████▍  | 1181/1579 [1:34:45<31:51,  4.80s/it] 75%|███████▍  | 1182/1579 [1:34:50<31:45,  4.80s/it] 75%|███████▍  | 1183/1579 [1:34:55<31:41,  4.80s/it] 75%|███████▍  | 1184/1579 [1:34:59<31:37,  4.80s/it] 75%|███████▌  | 1185/1579 [1:35:04<31:31,  4.80s/it] 75%|███████▌  | 1186/1579 [1:35:09<31:25,  4.80s/it] 75%|███████▌  | 1187/1579 [1:35:14<31:21,  4.80s/it] 75%|███████▌  | 1188/1579 [1:35:19<31:16,  4.80s/it] 75%|███████▌  | 1189/1579 [1:35:24<31:37,  4.87s/it] 75%|███████▌  | 1190/1579 [1:35:29<31:47,  4.90s/it]                                                     {'loss': 2.6178, 'learning_rate': 2.8624228188579783e-05, 'epoch': 0.75}
 75%|███████▌  | 1190/1579 [1:35:29<31:47,  4.90s/it]                                                     {'router_ce_loss': 0.6707716584205627, 'old_lang_expert0_score': '0.94 0.89 0.32 0.39 0.32 0.89 0.49 0.3 0.29 0.99 0.28 1.0 0.36 0.5 1.0 0.99 0.51 0.84 0.97 0.3 0.94 0.26 1.0 0.25', 'epoch': 0.75}
 75%|███████▌  | 1190/1579 [1:35:29<31:47,  4.90s/it] 75%|███████▌  | 1191/1579 [1:35:33<31:31,  4.87s/it] 75%|███████▌  | 1192/1579 [1:35:38<31:17,  4.85s/it] 76%|███████▌  | 1193/1579 [1:35:43<31:07,  4.84s/it] 76%|███████▌  | 1194/1579 [1:35:48<30:57,  4.83s/it] 76%|███████▌  | 1195/1579 [1:35:53<30:50,  4.82s/it] 76%|███████▌  | 1196/1579 [1:35:57<30:43,  4.81s/it] 76%|███████▌  | 1197/1579 [1:36:02<30:37,  4.81s/it] 76%|███████▌  | 1198/1579 [1:36:07<30:31,  4.81s/it] 76%|███████▌  | 1199/1579 [1:36:12<30:26,  4.81s/it] 76%|███████▌  | 1200/1579 [1:36:17<30:21,  4.81s/it]                                                     {'loss': 2.6493, 'learning_rate': 2.7244939247457624e-05, 'epoch': 0.76}
 76%|███████▌  | 1200/1579 [1:36:17<30:21,  4.81s/it]                                                     {'router_ce_loss': 0.5153821110725403, 'old_lang_expert0_score': '0.93 0.91 0.54 0.6 0.57 0.93 0.67 0.6 0.58 0.98 0.57 0.99 0.62 0.71 0.99 0.98 0.75 0.91 0.98 0.63 0.97 0.6 1.0 0.6', 'epoch': 0.76}
 76%|███████▌  | 1200/1579 [1:36:17<30:21,  4.81s/it] 76%|███████▌  | 1201/1579 [1:36:21<30:15,  4.80s/it] 76%|███████▌  | 1202/1579 [1:36:26<30:10,  4.80s/it] 76%|███████▌  | 1203/1579 [1:36:31<30:04,  4.80s/it] 76%|███████▋  | 1204/1579 [1:36:36<30:01,  4.80s/it] 76%|███████▋  | 1205/1579 [1:36:41<29:54,  4.80s/it] 76%|███████▋  | 1206/1579 [1:36:45<29:50,  4.80s/it] 76%|███████▋  | 1207/1579 [1:36:50<29:45,  4.80s/it] 77%|███████▋  | 1208/1579 [1:36:55<29:40,  4.80s/it] 77%|███████▋  | 1209/1579 [1:37:00<29:36,  4.80s/it] 77%|███████▋  | 1210/1579 [1:37:05<29:31,  4.80s/it]                                                     {'loss': 2.5782, 'learning_rate': 2.5894449767788554e-05, 'epoch': 0.77}
 77%|███████▋  | 1210/1579 [1:37:05<29:31,  4.80s/it]                                                     {'router_ce_loss': 0.49924126267433167, 'old_lang_expert0_score': '0.91 0.91 0.56 0.63 0.6 0.93 0.71 0.64 0.63 0.98 0.61 0.99 0.67 0.73 1.0 0.99 0.75 0.94 0.99 0.65 0.97 0.64 1.0 0.62', 'epoch': 0.77}
 77%|███████▋  | 1210/1579 [1:37:05<29:31,  4.80s/it] 77%|███████▋  | 1211/1579 [1:37:09<29:29,  4.81s/it] 77%|███████▋  | 1212/1579 [1:37:14<29:24,  4.81s/it] 77%|███████▋  | 1213/1579 [1:37:19<29:17,  4.80s/it] 77%|███████▋  | 1214/1579 [1:37:24<29:14,  4.81s/it] 77%|███████▋  | 1215/1579 [1:37:29<29:08,  4.80s/it] 77%|███████▋  | 1216/1579 [1:37:33<29:04,  4.80s/it] 77%|███████▋  | 1217/1579 [1:37:38<28:59,  4.81s/it] 77%|███████▋  | 1218/1579 [1:37:43<28:54,  4.81s/it] 77%|███████▋  | 1219/1579 [1:37:48<28:50,  4.81s/it] 77%|███████▋  | 1220/1579 [1:37:53<29:10,  4.88s/it]                                                     {'loss': 2.5773, 'learning_rate': 2.457329432915112e-05, 'epoch': 0.77}
 77%|███████▋  | 1220/1579 [1:37:53<29:10,  4.88s/it]                                                     {'router_ce_loss': 0.6177947521209717, 'old_lang_expert0_score': '0.94 0.9 0.38 0.46 0.41 0.92 0.53 0.4 0.38 0.99 0.38 1.0 0.45 0.57 1.0 0.99 0.58 0.88 0.98 0.42 0.95 0.38 1.0 0.37', 'epoch': 0.77}
 77%|███████▋  | 1220/1579 [1:37:53<29:10,  4.88s/it] 77%|███████▋  | 1221/1579 [1:37:58<28:58,  4.86s/it] 77%|███████▋  | 1222/1579 [1:38:03<29:13,  4.91s/it] 77%|███████▋  | 1223/1579 [1:38:08<28:54,  4.87s/it] 78%|███████▊  | 1224/1579 [1:38:12<28:43,  4.85s/it] 78%|███████▊  | 1225/1579 [1:38:17<28:34,  4.84s/it] 78%|███████▊  | 1226/1579 [1:38:22<28:24,  4.83s/it] 78%|███████▊  | 1227/1579 [1:38:27<28:15,  4.82s/it] 78%|███████▊  | 1228/1579 [1:38:32<28:08,  4.81s/it] 78%|███████▊  | 1229/1579 [1:38:36<28:02,  4.81s/it] 78%|███████▊  | 1230/1579 [1:38:41<27:57,  4.81s/it]                                                     {'loss': 2.5957, 'learning_rate': 2.3281995899497454e-05, 'epoch': 0.78}
 78%|███████▊  | 1230/1579 [1:38:41<27:57,  4.81s/it]                                                     {'router_ce_loss': 0.41390636563301086, 'old_lang_expert0_score': '0.88 0.91 0.71 0.75 0.75 0.96 0.82 0.79 0.76 0.98 0.74 0.99 0.79 0.83 0.99 0.99 0.87 0.98 0.99 0.86 0.99 0.85 1.0 0.85', 'epoch': 0.78}
 78%|███████▊  | 1230/1579 [1:38:42<27:57,  4.81s/it] 78%|███████▊  | 1231/1579 [1:38:46<27:52,  4.81s/it] 78%|███████▊  | 1232/1579 [1:38:51<27:46,  4.80s/it] 78%|███████▊  | 1233/1579 [1:38:56<27:42,  4.80s/it] 78%|███████▊  | 1234/1579 [1:39:00<27:37,  4.80s/it] 78%|███████▊  | 1235/1579 [1:39:05<27:32,  4.80s/it] 78%|███████▊  | 1236/1579 [1:39:10<27:27,  4.80s/it] 78%|███████▊  | 1237/1579 [1:39:15<27:21,  4.80s/it] 78%|███████▊  | 1238/1579 [1:39:20<27:17,  4.80s/it] 78%|███████▊  | 1239/1579 [1:39:24<27:11,  4.80s/it] 79%|███████▊  | 1240/1579 [1:39:29<27:07,  4.80s/it]                                                     {'loss': 2.5911, 'learning_rate': 2.202106562814098e-05, 'epoch': 0.79}
 79%|███████▊  | 1240/1579 [1:39:29<27:07,  4.80s/it]                                                     {'router_ce_loss': 0.3987813889980316, 'old_lang_expert0_score': '0.88 0.92 0.71 0.77 0.77 0.97 0.82 0.82 0.82 0.98 0.78 0.99 0.82 0.85 0.99 0.98 0.9 0.97 0.99 0.89 0.99 0.89 1.0 0.88', 'epoch': 0.79}
 79%|███████▊  | 1240/1579 [1:39:30<27:07,  4.80s/it] 79%|███████▊  | 1241/1579 [1:39:34<27:04,  4.80s/it] 79%|███████▊  | 1242/1579 [1:39:39<26:58,  4.80s/it] 79%|███████▊  | 1243/1579 [1:39:44<26:54,  4.80s/it] 79%|███████▉  | 1244/1579 [1:39:48<26:48,  4.80s/it] 79%|███████▉  | 1245/1579 [1:39:53<26:43,  4.80s/it] 79%|███████▉  | 1246/1579 [1:39:58<26:38,  4.80s/it] 79%|███████▉  | 1247/1579 [1:40:03<26:33,  4.80s/it] 79%|███████▉  | 1248/1579 [1:40:08<26:29,  4.80s/it] 79%|███████▉  | 1249/1579 [1:40:12<26:27,  4.81s/it] 79%|███████▉  | 1250/1579 [1:40:17<26:20,  4.80s/it]                                                     {'loss': 2.6044, 'learning_rate': 2.0791002643422263e-05, 'epoch': 0.79}
 79%|███████▉  | 1250/1579 [1:40:17<26:20,  4.80s/it]                                                     {'router_ce_loss': 0.7237469553947449, 'old_lang_expert0_score': '0.97 0.9 0.21 0.3 0.21 0.9 0.37 0.2 0.18 0.99 0.18 1.0 0.24 0.42 1.0 0.98 0.42 0.83 0.97 0.21 0.93 0.15 1.0 0.14', 'epoch': 0.79}
 79%|███████▉  | 1250/1579 [1:40:18<26:20,  4.80s/it] 79%|███████▉  | 1251/1579 [1:40:22<26:15,  4.80s/it] 79%|███████▉  | 1252/1579 [1:40:27<26:31,  4.87s/it] 79%|███████▉  | 1253/1579 [1:40:32<26:43,  4.92s/it] 79%|███████▉  | 1254/1579 [1:40:37<26:27,  4.89s/it] 79%|███████▉  | 1255/1579 [1:40:42<26:14,  4.86s/it] 80%|███████▉  | 1256/1579 [1:40:47<26:03,  4.84s/it] 80%|███████▉  | 1257/1579 [1:40:51<25:53,  4.83s/it] 80%|███████▉  | 1258/1579 [1:40:56<25:46,  4.82s/it] 80%|███████▉  | 1259/1579 [1:41:01<25:39,  4.81s/it] 80%|███████▉  | 1260/1579 [1:41:06<25:32,  4.81s/it]                                                     {'loss': 2.574, 'learning_rate': 1.959229385513356e-05, 'epoch': 0.8}
 80%|███████▉  | 1260/1579 [1:41:06<25:32,  4.81s/it]                                                     {'router_ce_loss': 0.6168740391731262, 'old_lang_expert0_score': '0.93 0.9 0.38 0.45 0.39 0.92 0.52 0.4 0.38 0.98 0.38 0.99 0.44 0.58 1.0 0.99 0.59 0.88 0.98 0.43 0.95 0.38 1.0 0.37', 'epoch': 0.8}
 80%|███████▉  | 1260/1579 [1:41:06<25:32,  4.81s/it] 80%|███████▉  | 1261/1579 [1:41:11<25:28,  4.81s/it] 80%|███████▉  | 1262/1579 [1:41:15<25:22,  4.80s/it] 80%|███████▉  | 1263/1579 [1:41:20<25:18,  4.80s/it] 80%|████████  | 1264/1579 [1:41:25<25:13,  4.80s/it] 80%|████████  | 1265/1579 [1:41:30<25:08,  4.80s/it] 80%|████████  | 1266/1579 [1:41:35<25:03,  4.80s/it] 80%|████████  | 1267/1579 [1:41:39<24:59,  4.81s/it] 80%|████████  | 1268/1579 [1:41:44<24:53,  4.80s/it] 80%|████████  | 1269/1579 [1:41:49<24:48,  4.80s/it] 80%|████████  | 1270/1579 [1:41:54<24:43,  4.80s/it]                                                     {'loss': 2.5878, 'learning_rate': 1.8425413761779586e-05, 'epoch': 0.8}
 80%|████████  | 1270/1579 [1:41:54<24:43,  4.80s/it]                                                     {'router_ce_loss': 0.6120409965515137, 'old_lang_expert0_score': '0.94 0.91 0.38 0.47 0.41 0.92 0.56 0.41 0.39 0.99 0.39 0.99 0.46 0.59 1.0 0.99 0.6 0.88 0.98 0.41 0.95 0.38 1.0 0.36', 'epoch': 0.8}
 80%|████████  | 1270/1579 [1:41:54<24:43,  4.80s/it] 80%|████████  | 1271/1579 [1:41:59<24:38,  4.80s/it] 81%|████████  | 1272/1579 [1:42:03<24:33,  4.80s/it] 81%|████████  | 1273/1579 [1:42:08<24:28,  4.80s/it] 81%|████████  | 1274/1579 [1:42:13<24:23,  4.80s/it] 81%|████████  | 1275/1579 [1:42:18<24:19,  4.80s/it] 81%|████████  | 1276/1579 [1:42:23<24:16,  4.81s/it] 81%|████████  | 1277/1579 [1:42:27<24:12,  4.81s/it] 81%|████████  | 1278/1579 [1:42:32<24:06,  4.81s/it] 81%|████████  | 1279/1579 [1:42:37<24:01,  4.81s/it] 81%|████████  | 1280/1579 [1:42:42<23:57,  4.81s/it]                                                     {'loss': 2.5917, 'learning_rate': 1.729082426275158e-05, 'epoch': 0.81}
 81%|████████  | 1280/1579 [1:42:42<23:57,  4.81s/it]                                                     {'router_ce_loss': 0.5703368782997131, 'old_lang_expert0_score': '0.92 0.91 0.46 0.52 0.48 0.92 0.62 0.5 0.48 0.98 0.47 0.99 0.53 0.62 0.99 0.97 0.64 0.89 0.98 0.53 0.96 0.49 1.0 0.49', 'epoch': 0.81}
 81%|████████  | 1280/1579 [1:42:42<23:57,  4.81s/it] 81%|████████  | 1281/1579 [1:42:47<23:51,  4.80s/it] 81%|████████  | 1282/1579 [1:42:51<23:47,  4.81s/it] 81%|████████▏ | 1283/1579 [1:42:56<23:57,  4.86s/it] 81%|████████▏ | 1284/1579 [1:43:01<24:09,  4.91s/it] 81%|████████▏ | 1285/1579 [1:43:06<23:56,  4.88s/it] 81%|████████▏ | 1286/1579 [1:43:11<23:43,  4.86s/it] 82%|████████▏ | 1287/1579 [1:43:16<23:33,  4.84s/it] 82%|████████▏ | 1288/1579 [1:43:21<23:24,  4.83s/it] 82%|████████▏ | 1289/1579 [1:43:25<23:18,  4.82s/it] 82%|████████▏ | 1290/1579 [1:43:30<23:13,  4.82s/it]                                                     {'loss': 2.5813, 'learning_rate': 1.6188974475488317e-05, 'epoch': 0.82}
 82%|████████▏ | 1290/1579 [1:43:30<23:13,  4.82s/it]                                                     {'router_ce_loss': 0.4666636884212494, 'old_lang_expert0_score': '0.9 0.91 0.63 0.68 0.67 0.95 0.75 0.68 0.66 0.98 0.65 0.99 0.72 0.75 1.0 0.99 0.79 0.94 0.99 0.74 0.98 0.72 1.0 0.72', 'epoch': 0.82}
 82%|████████▏ | 1290/1579 [1:43:31<23:13,  4.82s/it] 82%|████████▏ | 1291/1579 [1:43:35<23:07,  4.82s/it] 82%|████████▏ | 1292/1579 [1:43:40<23:00,  4.81s/it] 82%|████████▏ | 1293/1579 [1:43:45<22:55,  4.81s/it] 82%|████████▏ | 1294/1579 [1:43:49<22:49,  4.81s/it] 82%|████████▏ | 1295/1579 [1:43:54<22:44,  4.80s/it] 82%|████████▏ | 1296/1579 [1:43:59<22:39,  4.80s/it] 82%|████████▏ | 1297/1579 [1:44:04<22:34,  4.80s/it] 82%|████████▏ | 1298/1579 [1:44:09<22:29,  4.80s/it] 82%|████████▏ | 1299/1579 [1:44:13<22:24,  4.80s/it] 82%|████████▏ | 1300/1579 [1:44:18<22:19,  4.80s/it]                                                     {'loss': 2.5895, 'learning_rate': 1.5120300557696887e-05, 'epoch': 0.82}
 82%|████████▏ | 1300/1579 [1:44:18<22:19,  4.80s/it]                                                     {'router_ce_loss': 0.6594144105911255, 'old_lang_expert0_score': '0.95 0.9 0.32 0.4 0.33 0.91 0.5 0.33 0.32 0.99 0.29 1.0 0.36 0.53 1.0 0.99 0.54 0.86 0.98 0.31 0.95 0.28 1.0 0.26', 'epoch': 0.82}
 82%|████████▏ | 1300/1579 [1:44:19<22:19,  4.80s/it] 82%|████████▏ | 1301/1579 [1:44:23<22:15,  4.80s/it] 82%|████████▏ | 1302/1579 [1:44:28<22:11,  4.81s/it] 83%|████████▎ | 1303/1579 [1:44:33<22:06,  4.81s/it] 83%|████████▎ | 1304/1579 [1:44:37<22:02,  4.81s/it] 83%|████████▎ | 1305/1579 [1:44:42<21:56,  4.81s/it] 83%|████████▎ | 1306/1579 [1:44:47<21:51,  4.81s/it] 83%|████████▎ | 1307/1579 [1:44:52<21:46,  4.80s/it] 83%|████████▎ | 1308/1579 [1:44:57<21:41,  4.80s/it] 83%|████████▎ | 1309/1579 [1:45:01<21:36,  4.80s/it] 83%|████████▎ | 1310/1579 [1:45:06<21:31,  4.80s/it]                                                     {'loss': 2.5667, 'learning_rate': 1.408522553470335e-05, 'epoch': 0.83}
 83%|████████▎ | 1310/1579 [1:45:06<21:31,  4.80s/it]                                                     {'router_ce_loss': 0.5687828660011292, 'old_lang_expert0_score': '0.92 0.9 0.46 0.54 0.5 0.93 0.59 0.47 0.46 0.99 0.44 0.99 0.51 0.62 1.0 0.99 0.66 0.9 0.98 0.54 0.97 0.5 1.0 0.49', 'epoch': 0.83}
 83%|████████▎ | 1310/1579 [1:45:07<21:31,  4.80s/it] 83%|████████▎ | 1311/1579 [1:45:11<21:27,  4.80s/it] 83%|████████▎ | 1312/1579 [1:45:16<21:22,  4.80s/it] 83%|████████▎ | 1313/1579 [1:45:21<21:17,  4.80s/it] 83%|████████▎ | 1314/1579 [1:45:26<21:27,  4.86s/it] 83%|████████▎ | 1315/1579 [1:45:31<21:36,  4.91s/it] 83%|████████▎ | 1316/1579 [1:45:36<21:22,  4.88s/it] 83%|████████▎ | 1317/1579 [1:45:40<21:12,  4.86s/it] 83%|████████▎ | 1318/1579 [1:45:45<21:03,  4.84s/it] 84%|████████▎ | 1319/1579 [1:45:50<20:57,  4.83s/it] 84%|████████▎ | 1320/1579 [1:45:55<20:49,  4.82s/it]                                                     {'loss': 2.5663, 'learning_rate': 1.3084159132001972e-05, 'epoch': 0.84}
 84%|████████▎ | 1320/1579 [1:45:55<20:49,  4.82s/it]                                                     {'router_ce_loss': 0.6687929630279541, 'old_lang_expert0_score': '0.95 0.9 0.31 0.39 0.32 0.9 0.45 0.31 0.3 0.99 0.28 0.99 0.35 0.51 1.0 0.98 0.54 0.85 0.97 0.32 0.93 0.27 1.0 0.26', 'epoch': 0.84}
 84%|████████▎ | 1320/1579 [1:45:55<20:49,  4.82s/it] 84%|████████▎ | 1321/1579 [1:46:00<20:43,  4.82s/it] 84%|████████▎ | 1322/1579 [1:46:04<20:37,  4.81s/it] 84%|████████▍ | 1323/1579 [1:46:09<20:31,  4.81s/it] 84%|████████▍ | 1324/1579 [1:46:14<20:26,  4.81s/it] 84%|████████▍ | 1325/1579 [1:46:19<20:20,  4.80s/it] 84%|████████▍ | 1326/1579 [1:46:24<20:14,  4.80s/it] 84%|████████▍ | 1327/1579 [1:46:28<20:09,  4.80s/it] 84%|████████▍ | 1328/1579 [1:46:33<20:05,  4.80s/it] 84%|████████▍ | 1329/1579 [1:46:38<19:59,  4.80s/it] 84%|████████▍ | 1330/1579 [1:46:43<19:55,  4.80s/it]                                                     {'loss': 2.568, 'learning_rate': 1.2117497613068861e-05, 'epoch': 0.84}
 84%|████████▍ | 1330/1579 [1:46:43<19:55,  4.80s/it]                                                     {'router_ce_loss': 0.6631176471710205, 'old_lang_expert0_score': '0.94 0.9 0.32 0.4 0.33 0.9 0.49 0.32 0.29 0.99 0.29 0.99 0.35 0.51 1.0 0.99 0.52 0.86 0.98 0.31 0.95 0.27 1.0 0.24', 'epoch': 0.84}
 84%|████████▍ | 1330/1579 [1:46:43<19:55,  4.80s/it] 84%|████████▍ | 1331/1579 [1:46:48<19:50,  4.80s/it] 84%|████████▍ | 1332/1579 [1:46:52<19:46,  4.80s/it] 84%|████████▍ | 1333/1579 [1:46:57<19:41,  4.80s/it] 84%|████████▍ | 1334/1579 [1:47:02<19:36,  4.80s/it] 85%|████████▍ | 1335/1579 [1:47:07<19:30,  4.80s/it] 85%|████████▍ | 1336/1579 [1:47:12<19:26,  4.80s/it] 85%|████████▍ | 1337/1579 [1:47:16<19:21,  4.80s/it] 85%|████████▍ | 1338/1579 [1:47:21<19:17,  4.80s/it] 85%|████████▍ | 1339/1579 [1:47:26<19:11,  4.80s/it] 85%|████████▍ | 1340/1579 [1:47:31<19:07,  4.80s/it]                                                     {'loss': 2.5969, 'learning_rate': 1.118562362250447e-05, 'epoch': 0.85}
 85%|████████▍ | 1340/1579 [1:47:31<19:07,  4.80s/it]                                                     {'router_ce_loss': 0.5663328766822815, 'old_lang_expert0_score': '0.93 0.9 0.47 0.52 0.48 0.92 0.62 0.51 0.49 0.99 0.48 0.99 0.55 0.65 0.99 0.98 0.66 0.9 0.98 0.52 0.95 0.49 1.0 0.48', 'epoch': 0.85}
 85%|████████▍ | 1340/1579 [1:47:31<19:07,  4.80s/it] 85%|████████▍ | 1341/1579 [1:47:36<19:02,  4.80s/it] 85%|████████▍ | 1342/1579 [1:47:40<18:58,  4.80s/it] 85%|████████▌ | 1343/1579 [1:47:45<18:52,  4.80s/it] 85%|████████▌ | 1344/1579 [1:47:50<18:48,  4.80s/it] 85%|████████▌ | 1345/1579 [1:47:55<18:55,  4.85s/it] 85%|████████▌ | 1346/1579 [1:48:00<18:47,  4.84s/it] 85%|████████▌ | 1347/1579 [1:48:05<18:56,  4.90s/it] 85%|████████▌ | 1348/1579 [1:48:10<18:44,  4.87s/it] 85%|████████▌ | 1349/1579 [1:48:14<18:34,  4.85s/it] 85%|████████▌ | 1350/1579 [1:48:19<18:27,  4.83s/it]                                                     {'loss': 2.5895, 'learning_rate': 1.0288906034567036e-05, 'epoch': 0.85}
 85%|████████▌ | 1350/1579 [1:48:19<18:27,  4.83s/it]                                                     {'router_ce_loss': 0.5277156829833984, 'old_lang_expert0_score': '0.92 0.89 0.52 0.57 0.56 0.92 0.65 0.56 0.54 0.99 0.49 0.99 0.61 0.69 0.99 0.99 0.73 0.92 0.99 0.64 0.96 0.61 1.0 0.61', 'epoch': 0.85}
 85%|████████▌ | 1350/1579 [1:48:20<18:27,  4.83s/it] 86%|████████▌ | 1351/1579 [1:48:24<18:20,  4.83s/it] 86%|████████▌ | 1352/1579 [1:48:29<18:13,  4.82s/it] 86%|████████▌ | 1353/1579 [1:48:34<18:07,  4.81s/it] 86%|████████▌ | 1354/1579 [1:48:38<18:01,  4.80s/it] 86%|████████▌ | 1355/1579 [1:48:43<17:56,  4.80s/it] 86%|████████▌ | 1356/1579 [1:48:48<17:49,  4.80s/it] 86%|████████▌ | 1357/1579 [1:48:53<17:45,  4.80s/it] 86%|████████▌ | 1358/1579 [1:48:58<17:40,  4.80s/it] 86%|████████▌ | 1359/1579 [1:49:02<17:36,  4.80s/it] 86%|████████▌ | 1360/1579 [1:49:07<17:31,  4.80s/it]                                                     {'loss': 2.58, 'learning_rate': 9.427699807156986e-06, 'epoch': 0.86}
 86%|████████▌ | 1360/1579 [1:49:07<17:31,  4.80s/it]                                                     {'router_ce_loss': 0.564835250377655, 'old_lang_expert0_score': '0.93 0.91 0.48 0.54 0.49 0.92 0.63 0.49 0.48 0.99 0.48 0.99 0.53 0.62 1.0 0.98 0.65 0.91 0.98 0.52 0.97 0.5 1.0 0.5', 'epoch': 0.86}
 86%|████████▌ | 1360/1579 [1:49:08<17:31,  4.80s/it] 86%|████████▌ | 1361/1579 [1:49:12<17:27,  4.81s/it] 86%|████████▋ | 1362/1579 [1:49:17<17:22,  4.80s/it] 86%|████████▋ | 1363/1579 [1:49:22<17:17,  4.80s/it] 86%|████████▋ | 1364/1579 [1:49:26<17:12,  4.80s/it] 86%|████████▋ | 1365/1579 [1:49:31<17:08,  4.80s/it] 87%|████████▋ | 1366/1579 [1:49:36<17:03,  4.80s/it] 87%|████████▋ | 1367/1579 [1:49:41<16:58,  4.80s/it] 87%|████████▋ | 1368/1579 [1:49:46<16:52,  4.80s/it] 87%|████████▋ | 1369/1579 [1:49:50<16:47,  4.80s/it] 87%|████████▋ | 1370/1579 [1:49:55<16:42,  4.80s/it]                                                     {'loss': 2.5817, 'learning_rate': 8.6023458413098e-06, 'epoch': 0.87}
 87%|████████▋ | 1370/1579 [1:49:55<16:42,  4.80s/it]                                                     {'router_ce_loss': 0.5806127190589905, 'old_lang_expert0_score': '0.93 0.91 0.45 0.51 0.45 0.92 0.6 0.47 0.46 0.99 0.44 0.99 0.51 0.58 0.99 0.98 0.61 0.9 0.98 0.51 0.95 0.48 1.0 0.48', 'epoch': 0.87}
 87%|████████▋ | 1370/1579 [1:49:56<16:42,  4.80s/it] 87%|████████▋ | 1371/1579 [1:50:00<16:38,  4.80s/it] 87%|████████▋ | 1372/1579 [1:50:05<16:34,  4.80s/it] 87%|████████▋ | 1373/1579 [1:50:10<16:28,  4.80s/it] 87%|████████▋ | 1374/1579 [1:50:14<16:24,  4.80s/it] 87%|████████▋ | 1375/1579 [1:50:19<16:20,  4.80s/it] 87%|████████▋ | 1376/1579 [1:50:24<16:14,  4.80s/it] 87%|████████▋ | 1377/1579 [1:50:29<16:20,  4.85s/it] 87%|████████▋ | 1378/1579 [1:50:34<16:26,  4.91s/it] 87%|████████▋ | 1379/1579 [1:50:39<16:15,  4.88s/it] 87%|████████▋ | 1380/1579 [1:50:44<16:05,  4.85s/it]                                                     {'loss': 2.588, 'learning_rate': 7.813170846253448e-06, 'epoch': 0.87}
 87%|████████▋ | 1380/1579 [1:50:44<16:05,  4.85s/it]                                                     {'router_ce_loss': 0.5212946534156799, 'old_lang_expert0_score': '0.93 0.91 0.54 0.59 0.56 0.94 0.63 0.57 0.57 0.98 0.55 0.99 0.59 0.68 1.0 0.99 0.72 0.92 0.98 0.65 0.94 0.6 0.99 0.57', 'epoch': 0.87}
 87%|████████▋ | 1380/1579 [1:50:44<16:05,  4.85s/it] 87%|████████▋ | 1381/1579 [1:50:48<15:58,  4.84s/it] 88%|████████▊ | 1382/1579 [1:50:53<15:50,  4.82s/it] 88%|████████▊ | 1383/1579 [1:50:58<15:44,  4.82s/it] 88%|████████▊ | 1384/1579 [1:51:03<15:38,  4.81s/it] 88%|████████▊ | 1385/1579 [1:51:08<15:33,  4.81s/it] 88%|████████▊ | 1386/1579 [1:51:12<15:27,  4.81s/it] 88%|████████▊ | 1387/1579 [1:51:17<15:23,  4.81s/it] 88%|████████▊ | 1388/1579 [1:51:22<15:19,  4.81s/it] 88%|████████▊ | 1389/1579 [1:51:27<15:14,  4.81s/it] 88%|████████▊ | 1390/1579 [1:51:32<15:09,  4.81s/it]                                                     {'loss': 2.594, 'learning_rate': 7.060487210083133e-06, 'epoch': 0.88}
 88%|████████▊ | 1390/1579 [1:51:32<15:09,  4.81s/it]                                                     {'router_ce_loss': 0.4262298047542572, 'old_lang_expert0_score': '0.91 0.92 0.68 0.71 0.72 0.96 0.77 0.76 0.72 0.98 0.69 0.99 0.77 0.82 1.0 0.99 0.87 0.96 0.99 0.86 0.98 0.83 1.0 0.83', 'epoch': 0.88}
 88%|████████▊ | 1390/1579 [1:51:32<15:09,  4.81s/it] 88%|████████▊ | 1391/1579 [1:51:36<15:04,  4.81s/it] 88%|████████▊ | 1392/1579 [1:51:41<14:59,  4.81s/it] 88%|████████▊ | 1393/1579 [1:51:46<14:54,  4.81s/it] 88%|████████▊ | 1394/1579 [1:51:51<14:49,  4.81s/it] 88%|████████▊ | 1395/1579 [1:51:56<14:44,  4.81s/it] 88%|████████▊ | 1396/1579 [1:52:01<14:39,  4.80s/it] 88%|████████▊ | 1397/1579 [1:52:05<14:34,  4.80s/it] 89%|████████▊ | 1398/1579 [1:52:10<14:29,  4.80s/it] 89%|████████▊ | 1399/1579 [1:52:15<14:25,  4.81s/it] 89%|████████▊ | 1400/1579 [1:52:20<14:20,  4.81s/it]                                                     {'loss': 2.5825, 'learning_rate': 6.344592876105426e-06, 'epoch': 0.89}
 89%|████████▊ | 1400/1579 [1:52:20<14:20,  4.81s/it]                                                     {'router_ce_loss': 0.45775970816612244, 'old_lang_expert0_score': '0.91 0.92 0.63 0.7 0.69 0.95 0.77 0.72 0.69 0.98 0.68 0.99 0.74 0.78 0.99 0.99 0.81 0.94 0.99 0.75 0.97 0.74 1.0 0.72', 'epoch': 0.89}
 89%|████████▊ | 1400/1579 [1:52:20<14:20,  4.81s/it] 89%|████████▊ | 1401/1579 [1:52:25<14:16,  4.81s/it] 89%|████████▉ | 1402/1579 [1:52:29<14:11,  4.81s/it] 89%|████████▉ | 1403/1579 [1:52:34<14:06,  4.81s/it] 89%|████████▉ | 1404/1579 [1:52:39<14:00,  4.80s/it] 89%|████████▉ | 1405/1579 [1:52:44<13:55,  4.80s/it] 89%|████████▉ | 1406/1579 [1:52:49<13:51,  4.80s/it] 89%|████████▉ | 1407/1579 [1:52:53<13:45,  4.80s/it] 89%|████████▉ | 1408/1579 [1:52:58<13:49,  4.85s/it] 89%|████████▉ | 1409/1579 [1:53:03<13:54,  4.91s/it] 89%|████████▉ | 1410/1579 [1:53:08<13:44,  4.88s/it]                                                     {'loss': 2.598, 'learning_rate': 5.665771224899763e-06, 'epoch': 0.89}
 89%|████████▉ | 1410/1579 [1:53:08<13:44,  4.88s/it]                                                     {'router_ce_loss': 0.5210744738578796, 'old_lang_expert0_score': '0.94 0.92 0.52 0.59 0.56 0.93 0.66 0.58 0.56 0.98 0.55 0.99 0.61 0.73 1.0 0.99 0.74 0.91 0.98 0.62 0.97 0.58 1.0 0.56', 'epoch': 0.89}
 89%|████████▉ | 1410/1579 [1:53:09<13:44,  4.88s/it] 89%|████████▉ | 1411/1579 [1:53:13<13:35,  4.85s/it] 89%|████████▉ | 1412/1579 [1:53:18<13:27,  4.84s/it] 89%|████████▉ | 1413/1579 [1:53:23<13:20,  4.82s/it] 90%|████████▉ | 1414/1579 [1:53:27<13:14,  4.82s/it] 90%|████████▉ | 1415/1579 [1:53:32<13:09,  4.81s/it] 90%|████████▉ | 1416/1579 [1:53:37<13:04,  4.81s/it] 90%|████████▉ | 1417/1579 [1:53:42<12:59,  4.81s/it] 90%|████████▉ | 1418/1579 [1:53:47<12:53,  4.81s/it] 90%|████████▉ | 1419/1579 [1:53:51<12:48,  4.81s/it] 90%|████████▉ | 1420/1579 [1:53:56<12:43,  4.80s/it]                                                     {'loss': 2.5655, 'learning_rate': 5.02429096214484e-06, 'epoch': 0.9}
 90%|████████▉ | 1420/1579 [1:53:56<12:43,  4.80s/it]                                                     {'router_ce_loss': 0.5508705973625183, 'old_lang_expert0_score': '0.93 0.91 0.48 0.55 0.51 0.93 0.62 0.54 0.53 0.99 0.51 0.99 0.55 0.65 1.0 0.98 0.67 0.92 0.98 0.55 0.97 0.53 1.0 0.51', 'epoch': 0.9}
 90%|████████▉ | 1420/1579 [1:53:57<12:43,  4.80s/it] 90%|████████▉ | 1421/1579 [1:54:01<12:38,  4.80s/it] 90%|█████████ | 1422/1579 [1:54:06<12:34,  4.80s/it] 90%|█████████ | 1423/1579 [1:54:11<12:28,  4.80s/it] 90%|█████████ | 1424/1579 [1:54:15<12:23,  4.80s/it] 90%|█████████ | 1425/1579 [1:54:20<12:19,  4.80s/it] 90%|█████████ | 1426/1579 [1:54:25<12:14,  4.80s/it] 90%|█████████ | 1427/1579 [1:54:30<12:09,  4.80s/it] 90%|█████████ | 1428/1579 [1:54:35<12:05,  4.80s/it] 91%|█████████ | 1429/1579 [1:54:39<12:00,  4.80s/it] 91%|█████████ | 1430/1579 [1:54:44<11:56,  4.81s/it]                                                     {'loss': 2.5961, 'learning_rate': 4.420406012253731e-06, 'epoch': 0.91}
 91%|█████████ | 1430/1579 [1:54:44<11:56,  4.81s/it]                                                     {'router_ce_loss': 0.3504759967327118, 'old_lang_expert0_score': '0.86 0.91 0.82 0.85 0.87 0.97 0.91 0.93 0.92 0.97 0.89 0.99 0.94 0.94 0.99 0.99 0.97 0.99 0.99 0.98 0.99 0.98 1.0 0.98', 'epoch': 0.91}
 91%|█████████ | 1430/1579 [1:54:45<11:56,  4.81s/it] 91%|█████████ | 1431/1579 [1:54:49<11:51,  4.81s/it] 91%|█████████ | 1432/1579 [1:54:54<11:46,  4.81s/it] 91%|█████████ | 1433/1579 [1:54:59<11:41,  4.80s/it] 91%|█████████ | 1434/1579 [1:55:03<11:36,  4.81s/it] 91%|█████████ | 1435/1579 [1:55:08<11:31,  4.81s/it] 91%|█████████ | 1436/1579 [1:55:13<11:26,  4.80s/it] 91%|█████████ | 1437/1579 [1:55:18<11:22,  4.80s/it] 91%|█████████ | 1438/1579 [1:55:23<11:17,  4.80s/it] 91%|█████████ | 1439/1579 [1:55:28<11:22,  4.87s/it] 91%|█████████ | 1440/1579 [1:55:33<11:24,  4.92s/it]                                                     {'loss': 2.5829, 'learning_rate': 3.854355417860167e-06, 'epoch': 0.91}
 91%|█████████ | 1440/1579 [1:55:33<11:24,  4.92s/it]                                                     {'router_ce_loss': 0.5721952319145203, 'old_lang_expert0_score': '0.93 0.91 0.48 0.53 0.49 0.93 0.61 0.46 0.46 0.98 0.46 0.99 0.5 0.59 1.0 0.99 0.62 0.9 0.98 0.51 0.96 0.49 1.0 0.49', 'epoch': 0.91}
 91%|█████████ | 1440/1579 [1:55:33<11:24,  4.92s/it] 91%|█████████▏| 1441/1579 [1:55:38<11:14,  4.89s/it] 91%|█████████▏| 1442/1579 [1:55:42<11:05,  4.86s/it] 91%|█████████▏| 1443/1579 [1:55:47<10:59,  4.85s/it] 91%|█████████▏| 1444/1579 [1:55:52<10:52,  4.83s/it] 92%|█████████▏| 1445/1579 [1:55:57<10:46,  4.83s/it] 92%|█████████▏| 1446/1579 [1:56:02<10:40,  4.82s/it] 92%|█████████▏| 1447/1579 [1:56:06<10:35,  4.81s/it] 92%|█████████▏| 1448/1579 [1:56:11<10:29,  4.81s/it] 92%|█████████▏| 1449/1579 [1:56:16<10:25,  4.81s/it] 92%|█████████▏| 1450/1579 [1:56:21<10:19,  4.81s/it]                                                     {'loss': 2.5792, 'learning_rate': 3.3263632451954296e-06, 'epoch': 0.92}
 92%|█████████▏| 1450/1579 [1:56:21<10:19,  4.81s/it]                                                     {'router_ce_loss': 0.6158655285835266, 'old_lang_expert0_score': '0.93 0.91 0.4 0.47 0.41 0.92 0.53 0.41 0.39 0.99 0.38 0.99 0.43 0.56 1.0 0.97 0.59 0.88 0.98 0.42 0.95 0.4 1.0 0.38', 'epoch': 0.92}
 92%|█████████▏| 1450/1579 [1:56:21<10:19,  4.81s/it] 92%|█████████▏| 1451/1579 [1:56:26<10:15,  4.81s/it] 92%|█████████▏| 1452/1579 [1:56:30<10:09,  4.80s/it] 92%|█████████▏| 1453/1579 [1:56:35<10:04,  4.80s/it] 92%|█████████▏| 1454/1579 [1:56:40<09:59,  4.80s/it] 92%|█████████▏| 1455/1579 [1:56:45<09:54,  4.80s/it] 92%|█████████▏| 1456/1579 [1:56:50<09:50,  4.80s/it] 92%|█████████▏| 1457/1579 [1:56:54<09:45,  4.80s/it] 92%|█████████▏| 1458/1579 [1:56:59<09:40,  4.80s/it] 92%|█████████▏| 1459/1579 [1:57:04<09:36,  4.81s/it] 92%|█████████▏| 1460/1579 [1:57:09<09:31,  4.80s/it]                                                     {'loss': 2.5924, 'learning_rate': 2.836638495393862e-06, 'epoch': 0.92}
 92%|█████████▏| 1460/1579 [1:57:09<09:31,  4.80s/it]                                                     {'router_ce_loss': 0.5589900612831116, 'old_lang_expert0_score': '0.93 0.91 0.47 0.53 0.48 0.93 0.61 0.52 0.51 0.99 0.49 0.99 0.54 0.67 1.0 0.99 0.69 0.89 0.98 0.53 0.95 0.5 1.0 0.5', 'epoch': 0.92}
 92%|█████████▏| 1460/1579 [1:57:09<09:31,  4.80s/it] 93%|█████████▎| 1461/1579 [1:57:14<09:26,  4.80s/it] 93%|█████████▎| 1462/1579 [1:57:18<09:21,  4.80s/it] 93%|█████████▎| 1463/1579 [1:57:23<09:17,  4.80s/it] 93%|█████████▎| 1464/1579 [1:57:28<09:12,  4.80s/it] 93%|█████████▎| 1465/1579 [1:57:33<09:07,  4.80s/it] 93%|█████████▎| 1466/1579 [1:57:38<09:02,  4.80s/it] 93%|█████████▎| 1467/1579 [1:57:42<08:57,  4.80s/it] 93%|█████████▎| 1468/1579 [1:57:47<08:52,  4.80s/it] 93%|█████████▎| 1469/1579 [1:57:52<08:48,  4.80s/it] 93%|█████████▎| 1470/1579 [1:57:57<08:50,  4.86s/it]                                                     {'loss': 2.5754, 'learning_rate': 2.38537502176146e-06, 'epoch': 0.93}
 93%|█████████▎| 1470/1579 [1:57:57<08:50,  4.86s/it]                                                     {'router_ce_loss': 0.5697362422943115, 'old_lang_expert0_score': '0.93 0.9 0.47 0.53 0.48 0.93 0.61 0.49 0.49 0.99 0.48 0.99 0.53 0.62 1.0 0.98 0.65 0.9 0.98 0.51 0.95 0.49 1.0 0.48', 'epoch': 0.93}
 93%|█████████▎| 1470/1579 [1:57:57<08:50,  4.86s/it] 93%|█████████▎| 1471/1579 [1:58:02<08:43,  4.85s/it] 93%|█████████▎| 1472/1579 [1:58:07<08:44,  4.90s/it] 93%|█████████▎| 1473/1579 [1:58:12<08:35,  4.87s/it] 93%|█████████▎| 1474/1579 [1:58:16<08:28,  4.85s/it] 93%|█████████▎| 1475/1579 [1:58:21<08:22,  4.83s/it] 93%|█████████▎| 1476/1579 [1:58:26<08:16,  4.82s/it] 94%|█████████▎| 1477/1579 [1:58:31<08:11,  4.81s/it] 94%|█████████▎| 1478/1579 [1:58:36<08:05,  4.81s/it] 94%|█████████▎| 1479/1579 [1:58:40<08:00,  4.80s/it] 94%|█████████▎| 1480/1579 [1:58:45<07:55,  4.80s/it]                                                     {'loss': 2.5683, 'learning_rate': 1.9727514530407333e-06, 'epoch': 0.94}
 94%|█████████▎| 1480/1579 [1:58:45<07:55,  4.80s/it]                                                     {'router_ce_loss': 0.5667529106140137, 'old_lang_expert0_score': '0.91 0.9 0.48 0.54 0.49 0.92 0.61 0.49 0.48 0.98 0.48 0.99 0.53 0.63 0.99 0.97 0.66 0.89 0.98 0.52 0.97 0.49 1.0 0.49', 'epoch': 0.94}
 94%|█████████▎| 1480/1579 [1:58:46<07:55,  4.80s/it] 94%|█████████▍| 1481/1579 [1:58:50<07:50,  4.80s/it] 94%|█████████▍| 1482/1579 [1:58:55<07:46,  4.81s/it] 94%|█████████▍| 1483/1579 [1:59:00<07:41,  4.80s/it] 94%|█████████▍| 1484/1579 [1:59:04<07:36,  4.80s/it] 94%|█████████▍| 1485/1579 [1:59:09<07:31,  4.81s/it] 94%|█████████▍| 1486/1579 [1:59:14<07:26,  4.81s/it] 94%|█████████▍| 1487/1579 [1:59:19<07:22,  4.81s/it] 94%|█████████▍| 1488/1579 [1:59:24<07:16,  4.80s/it] 94%|█████████▍| 1489/1579 [1:59:28<07:12,  4.80s/it] 94%|█████████▍| 1490/1579 [1:59:33<07:07,  4.80s/it]                                                     {'loss': 2.5926, 'learning_rate': 1.5989311227020854e-06, 'epoch': 0.94}
 94%|█████████▍| 1490/1579 [1:59:33<07:07,  4.80s/it]                                                     {'router_ce_loss': 0.515513002872467, 'old_lang_expert0_score': '0.92 0.91 0.55 0.61 0.58 0.93 0.68 0.6 0.58 0.98 0.58 0.99 0.62 0.7 0.99 0.99 0.73 0.92 0.98 0.63 0.96 0.61 1.0 0.6', 'epoch': 0.94}
 94%|█████████▍| 1490/1579 [1:59:34<07:07,  4.80s/it] 94%|█████████▍| 1491/1579 [1:59:38<07:02,  4.80s/it] 94%|█████████▍| 1492/1579 [1:59:43<06:57,  4.80s/it] 95%|█████████▍| 1493/1579 [1:59:48<06:52,  4.80s/it] 95%|█████████▍| 1494/1579 [1:59:52<06:47,  4.80s/it] 95%|█████████▍| 1495/1579 [1:59:57<06:43,  4.80s/it] 95%|█████████▍| 1496/1579 [2:00:02<06:38,  4.80s/it] 95%|█████████▍| 1497/1579 [2:00:07<06:33,  4.80s/it] 95%|█████████▍| 1498/1579 [2:00:12<06:29,  4.80s/it] 95%|█████████▍| 1499/1579 [2:00:16<06:24,  4.80s/it] 95%|█████████▍| 1500/1579 [2:00:21<06:19,  4.80s/it]                                                     {'loss': 2.5714, 'learning_rate': 1.2640620042896568e-06, 'epoch': 0.95}
 95%|█████████▍| 1500/1579 [2:00:21<06:19,  4.80s/it]                                                     {'router_ce_loss': 0.5634846687316895, 'old_lang_expert0_score': '0.92 0.91 0.48 0.54 0.51 0.93 0.59 0.5 0.49 0.98 0.46 0.99 0.53 0.64 1.0 0.99 0.67 0.89 0.98 0.54 0.95 0.51 1.0 0.49', 'epoch': 0.95}
 95%|█████████▍| 1500/1579 [2:00:22<06:19,  4.80s/it] 95%|█████████▌| 1501/1579 [2:00:26<06:14,  4.81s/it] 95%|█████████▌| 1502/1579 [2:00:31<06:14,  4.86s/it] 95%|█████████▌| 1503/1579 [2:00:36<06:12,  4.91s/it] 95%|█████████▌| 1504/1579 [2:00:41<06:05,  4.87s/it] 95%|█████████▌| 1505/1579 [2:00:46<05:58,  4.85s/it] 95%|█████████▌| 1506/1579 [2:00:50<05:53,  4.84s/it] 95%|█████████▌| 1507/1579 [2:00:55<05:47,  4.83s/it] 96%|█████████▌| 1508/1579 [2:01:00<05:42,  4.82s/it] 96%|█████████▌| 1509/1579 [2:01:05<05:37,  4.82s/it] 96%|█████████▌| 1510/1579 [2:01:10<05:31,  4.81s/it]                                                     {'loss': 2.5828, 'learning_rate': 9.682766528472887e-07, 'epoch': 0.96}
 96%|█████████▌| 1510/1579 [2:01:10<05:31,  4.81s/it]                                                     {'router_ce_loss': 0.6133632063865662, 'old_lang_expert0_score': '0.93 0.89 0.39 0.47 0.41 0.92 0.54 0.41 0.4 0.99 0.38 0.99 0.45 0.58 1.0 0.97 0.6 0.88 0.98 0.42 0.95 0.39 1.0 0.38', 'epoch': 0.96}
 96%|█████████▌| 1510/1579 [2:01:10<05:31,  4.81s/it] 96%|█████████▌| 1511/1579 [2:01:14<05:26,  4.81s/it] 96%|█████████▌| 1512/1579 [2:01:19<05:22,  4.81s/it] 96%|█████████▌| 1513/1579 [2:01:24<05:17,  4.81s/it] 96%|█████████▌| 1514/1579 [2:01:29<05:12,  4.81s/it] 96%|█████████▌| 1515/1579 [2:01:34<05:08,  4.81s/it] 96%|█████████▌| 1516/1579 [2:01:39<05:02,  4.81s/it] 96%|█████████▌| 1517/1579 [2:01:43<04:58,  4.81s/it] 96%|█████████▌| 1518/1579 [2:01:48<04:53,  4.81s/it] 96%|█████████▌| 1519/1579 [2:01:53<04:48,  4.81s/it] 96%|█████████▋| 1520/1579 [2:01:58<04:43,  4.80s/it]                                                     {'loss': 2.5559, 'learning_rate': 7.116921524477405e-07, 'epoch': 0.96}
 96%|█████████▋| 1520/1579 [2:01:58<04:43,  4.80s/it]                                                     {'router_ce_loss': 0.5581321120262146, 'old_lang_expert0_score': '0.93 0.9 0.5 0.57 0.51 0.92 0.65 0.53 0.52 0.98 0.49 0.99 0.56 0.63 1.0 0.99 0.64 0.91 0.98 0.53 0.97 0.51 1.0 0.5', 'epoch': 0.96}
 96%|█████████▋| 1520/1579 [2:01:58<04:43,  4.80s/it] 96%|█████████▋| 1521/1579 [2:02:03<04:38,  4.80s/it] 96%|█████████▋| 1522/1579 [2:02:07<04:33,  4.80s/it] 96%|█████████▋| 1523/1579 [2:02:12<04:28,  4.80s/it] 97%|█████████▋| 1524/1579 [2:02:17<04:24,  4.80s/it] 97%|█████████▋| 1525/1579 [2:02:22<04:18,  4.80s/it] 97%|█████████▋| 1526/1579 [2:02:27<04:14,  4.80s/it] 97%|█████████▋| 1527/1579 [2:02:31<04:09,  4.80s/it] 97%|█████████▋| 1528/1579 [2:02:36<04:05,  4.80s/it] 97%|█████████▋| 1529/1579 [2:02:41<04:00,  4.80s/it] 97%|█████████▋| 1530/1579 [2:02:46<03:55,  4.80s/it]                                                     {'loss': 2.631, 'learning_rate': 4.944100698460075e-07, 'epoch': 0.97}
 97%|█████████▋| 1530/1579 [2:02:46<03:55,  4.80s/it]                                                     {'router_ce_loss': 0.5116782784461975, 'old_lang_expert0_score': '0.91 0.92 0.55 0.6 0.56 0.94 0.67 0.61 0.6 0.98 0.59 0.99 0.63 0.73 0.99 0.99 0.76 0.91 0.98 0.64 0.96 0.62 1.0 0.61', 'epoch': 0.97}
 97%|█████████▋| 1530/1579 [2:02:46<03:55,  4.80s/it] 97%|█████████▋| 1531/1579 [2:02:51<03:50,  4.80s/it] 97%|█████████▋| 1532/1579 [2:02:55<03:45,  4.80s/it] 97%|█████████▋| 1533/1579 [2:03:00<03:44,  4.87s/it] 97%|█████████▋| 1534/1579 [2:03:05<03:41,  4.92s/it] 97%|█████████▋| 1535/1579 [2:03:10<03:34,  4.88s/it] 97%|█████████▋| 1536/1579 [2:03:15<03:29,  4.86s/it] 97%|█████████▋| 1537/1579 [2:03:20<03:23,  4.85s/it] 97%|█████████▋| 1538/1579 [2:03:25<03:18,  4.83s/it] 97%|█████████▋| 1539/1579 [2:03:29<03:12,  4.82s/it] 98%|█████████▊| 1540/1579 [2:03:34<03:07,  4.81s/it]                                                     {'loss': 2.6255, 'learning_rate': 3.165164142749366e-07, 'epoch': 0.98}
 98%|█████████▊| 1540/1579 [2:03:34<03:07,  4.81s/it]                                                     {'router_ce_loss': 0.4662982225418091, 'old_lang_expert0_score': '0.91 0.92 0.62 0.66 0.66 0.95 0.74 0.69 0.68 0.98 0.66 0.99 0.7 0.79 1.0 0.98 0.81 0.94 0.99 0.74 0.97 0.72 1.0 0.72', 'epoch': 0.98}
 98%|█████████▊| 1540/1579 [2:03:35<03:07,  4.81s/it] 98%|█████████▊| 1541/1579 [2:03:39<03:02,  4.81s/it] 98%|█████████▊| 1542/1579 [2:03:44<02:58,  4.81s/it] 98%|█████████▊| 1543/1579 [2:03:49<02:53,  4.81s/it] 98%|█████████▊| 1544/1579 [2:03:53<02:48,  4.81s/it] 98%|█████████▊| 1545/1579 [2:03:58<02:43,  4.81s/it] 98%|█████████▊| 1546/1579 [2:04:03<02:38,  4.81s/it] 98%|█████████▊| 1547/1579 [2:04:08<02:33,  4.81s/it] 98%|█████████▊| 1548/1579 [2:04:13<02:28,  4.80s/it] 98%|█████████▊| 1549/1579 [2:04:17<02:24,  4.80s/it] 98%|█████████▊| 1550/1579 [2:04:22<02:19,  4.80s/it]                                                     {'loss': 2.6083, 'learning_rate': 1.780816033992827e-07, 'epoch': 0.98}
 98%|█████████▊| 1550/1579 [2:04:22<02:19,  4.80s/it]                                                     {'router_ce_loss': 0.621002733707428, 'old_lang_expert0_score': '0.94 0.9 0.37 0.44 0.39 0.91 0.53 0.4 0.37 0.99 0.36 1.0 0.45 0.57 1.0 0.99 0.6 0.88 0.98 0.42 0.94 0.38 1.0 0.37', 'epoch': 0.98}
 98%|█████████▊| 1550/1579 [2:04:23<02:19,  4.80s/it] 98%|█████████▊| 1551/1579 [2:04:27<02:14,  4.80s/it] 98%|█████████▊| 1552/1579 [2:04:32<02:09,  4.80s/it] 98%|█████████▊| 1553/1579 [2:04:37<02:04,  4.80s/it] 98%|█████████▊| 1554/1579 [2:04:41<02:00,  4.80s/it] 98%|█████████▊| 1555/1579 [2:04:46<01:55,  4.81s/it] 99%|█████████▊| 1556/1579 [2:04:51<01:50,  4.80s/it] 99%|█████████▊| 1557/1579 [2:04:56<01:45,  4.80s/it] 99%|█████████▊| 1558/1579 [2:05:01<01:40,  4.80s/it] 99%|█████████▊| 1559/1579 [2:05:05<01:35,  4.80s/it] 99%|█████████▉| 1560/1579 [2:05:10<01:31,  4.80s/it]                                                     {'loss': 2.615, 'learning_rate': 7.91604354414277e-08, 'epoch': 0.99}
 99%|█████████▉| 1560/1579 [2:05:10<01:31,  4.80s/it]                                                     {'router_ce_loss': 0.5167276263237, 'old_lang_expert0_score': '0.91 0.91 0.55 0.6 0.57 0.95 0.65 0.58 0.57 0.98 0.56 0.99 0.63 0.7 0.99 0.98 0.74 0.92 0.99 0.63 0.96 0.61 1.0 0.61', 'epoch': 0.99}
 99%|█████████▉| 1560/1579 [2:05:11<01:31,  4.80s/it] 99%|█████████▉| 1561/1579 [2:05:15<01:26,  4.80s/it] 99%|█████████▉| 1562/1579 [2:05:20<01:21,  4.80s/it] 99%|█████████▉| 1563/1579 [2:05:25<01:16,  4.80s/it] 99%|█████████▉| 1564/1579 [2:05:30<01:12,  4.86s/it] 99%|█████████▉| 1565/1579 [2:05:35<01:08,  4.92s/it] 99%|█████████▉| 1566/1579 [2:05:40<01:03,  4.88s/it] 99%|█████████▉| 1567/1579 [2:05:44<00:58,  4.85s/it] 99%|█████████▉| 1568/1579 [2:05:49<00:53,  4.84s/it] 99%|█████████▉| 1569/1579 [2:05:54<00:48,  4.83s/it] 99%|█████████▉| 1570/1579 [2:05:59<00:43,  4.83s/it]                                                     {'loss': 2.6093, 'learning_rate': 1.979206749003204e-08, 'epoch': 0.99}
 99%|█████████▉| 1570/1579 [2:05:59<00:43,  4.83s/it]                                                     {'router_ce_loss': 0.5107857584953308, 'old_lang_expert0_score': '0.92 0.9 0.55 0.63 0.59 0.93 0.71 0.61 0.6 0.98 0.58 0.99 0.64 0.7 1.0 0.99 0.72 0.93 0.99 0.64 0.97 0.62 1.0 0.62', 'epoch': 0.99}
 99%|█████████▉| 1570/1579 [2:05:59<00:43,  4.83s/it] 99%|█████████▉| 1571/1579 [2:06:04<00:38,  4.82s/it]100%|█████████▉| 1572/1579 [2:06:08<00:33,  4.81s/it]100%|█████████▉| 1573/1579 [2:06:13<00:28,  4.81s/it]100%|█████████▉| 1574/1579 [2:06:18<00:24,  4.81s/it]100%|█████████▉| 1575/1579 [2:06:23<00:19,  4.81s/it]100%|█████████▉| 1576/1579 [2:06:28<00:14,  4.81s/it]100%|█████████▉| 1577/1579 [2:06:32<00:09,  4.80s/it]100%|█████████▉| 1578/1579 [2:06:37<00:04,  4.80s/it]100%|██████████| 1579/1579 [2:06:42<00:00,  4.80s/it][INFO|trainer.py:1989] 2024-06-28 01:47:00,330 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 7602.4642, 'train_samples_per_second': 26.589, 'train_steps_per_second': 0.208, 'train_loss': 2.5935698201793747, 'epoch': 1.0}
100%|██████████| 1579/1579 [2:06:42<00:00,  4.80s/it]100%|██████████| 1579/1579 [2:06:42<00:00,  4.81s/it]
[INFO|trainer.py:2981] 2024-06-28 01:47:04,336 >> Saving model checkpoint to /home/nfs04/wangzj/checkpoints/moe/test
/home/nfs02/wangzj/aliyun/temp_data/peft/src/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/nfs02/wangzj/models/Qwen1.5-1.8B - will assume that the vocabulary was not modified.
  warnings.warn(
[2024-06-28 01:47:06,832] [INFO] [launch.py:347:main] Process 19442 exits successfully.
[2024-06-28 01:47:06,832] [INFO] [launch.py:347:main] Process 19444 exits successfully.
[2024-06-28 01:47:06,832] [INFO] [launch.py:347:main] Process 19443 exits successfully.
[INFO|tokenization_utils_base.py:2435] 2024-06-28 01:47:19,384 >> tokenizer config file saved in /home/nfs04/wangzj/checkpoints/moe/test/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-06-28 01:47:19,386 >> Special tokens file saved in /home/nfs04/wangzj/checkpoints/moe/test/special_tokens_map.json
[INFO|tokenization_utils_base.py:2495] 2024-06-28 01:47:19,387 >> added tokens file saved in /home/nfs04/wangzj/checkpoints/moe/test/added_tokens.json
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     2.5936
  train_runtime            = 2:06:42.46
  train_samples_per_second =     26.589
  train_steps_per_second   =      0.208
Figure saved: /home/nfs04/wangzj/checkpoints/moe/test/training_loss.png
06/28/2024 01:47:20 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-06-28 01:47:20,421 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-06-28 01:47:21,848] [INFO] [launch.py:347:main] Process 19441 exits successfully.
