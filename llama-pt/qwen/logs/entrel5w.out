[2024-07-11 11:27:06,941] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-11 11:27:09,954] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-11 11:27:10,220] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/alimoe/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/nfs04/wangzj/models/Qwen1.5-1.8B --adapter_name_or_path /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01 --train_only_router --ce_loss_coef 0.1 --do_train --dataset slimpajam_1b,tr_1b,el_1b --max_samples 50000 --preprocessing_num_workers 16 --cutoff_len 512 --finetuning_type moe --output_dir /home/nfs04/wangzj/checkpoints/moe/test --overwrite_output_dir --per_device_train_batch_size 16 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 100 --save_steps 10000 --save_only_model --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-07-11 11:27:12,034] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-11 11:27:14,489] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1, 2, 3]}
[2024-07-11 11:27:14,489] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=3, node_rank=0
[2024-07-11 11:27:14,489] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2]})
[2024-07-11 11:27:14,489] [INFO] [launch.py:163:main] dist_world_size=3
[2024-07-11 11:27:14,489] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1,2,3
[2024-07-11 11:27:20,860] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-11 11:27:20,862] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-11 11:27:20,864] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-11 11:27:28,563] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-11 11:27:28,567] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-11 11:27:28,567] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-11 11:27:28,571] [INFO] [comm.py:637:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
07/11/2024 11:27:29 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/11/2024 11:27:29 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul11_11-27-28_Titan_RTX,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
07/11/2024 11:27:29 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/11/2024 11:27:29 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul11_11-27-28_Titan_RTX,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
07/11/2024 11:27:29 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/11/2024 11:27:29 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul11_11-27-28_Titan_RTX,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-07-11 11:27:29,656 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2027] 2024-07-11 11:27:29,656 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2027] 2024-07-11 11:27:29,656 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-07-11 11:27:29,656 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-07-11 11:27:29,656 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-07-11 11:27:29,656 >> loading file tokenizer.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-07-11 11:27:29,966 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:727] 2024-07-11 11:27:29,967 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/config.json
[INFO|configuration_utils.py:792] 2024-07-11 11:27:29,969 >> Model config Qwen2Config {
  "_name_or_path": "/home/nfs04/wangzj/models/Qwen1.5-1.8B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3334] 2024-07-11 11:27:30,069 >> loading weights file /home/nfs04/wangzj/models/Qwen1.5-1.8B/model.safetensors
[INFO|modeling_utils.py:1459] 2024-07-11 11:27:30,083 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:827] 2024-07-11 11:27:30,086 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}

[INFO|modeling_utils.py:4070] 2024-07-11 11:27:32,559 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4078] 2024-07-11 11:27:32,559 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/nfs04/wangzj/models/Qwen1.5-1.8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-07-11 11:27:32,562 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/generation_config.json
[INFO|configuration_utils.py:827] 2024-07-11 11:27:32,562 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "max_new_tokens": 2048
}

07/11/2024 11:27:32 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/11/2024 11:27:32 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/11/2024 11:27:32 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
07/11/2024 11:27:32 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/11/2024 11:27:32 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/11/2024 11:27:32 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
07/11/2024 11:27:32 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/11/2024 11:27:32 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/11/2024 11:27:32 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
07/11/2024 11:27:36 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.1)
07/11/2024 11:27:36 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/11/2024 11:27:36 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/11/2024 11:27:36 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/11/2024 11:27:37 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.1)
07/11/2024 11:27:37 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/11/2024 11:27:37 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/11/2024 11:27:37 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/11/2024 11:27:37 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.1)
07/11/2024 11:27:37 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/11/2024 11:27:37 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/11/2024 11:27:37 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/11/2024 11:27:44 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Using custom data configuration default-74e4f6a24fa3d738
Loading Dataset Infos from /home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00000_of_00016.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00001_of_00016.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00002_of_00016.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00003_of_00016.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00004_of_00016.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00005_of_00016.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00006_of_00016.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00007_of_00016.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00008_of_00016.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00009_of_00016.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00010_of_00016.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00011_of_00016.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00012_of_00016.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00013_of_00016.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00014_of_00016.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00015_of_00016.arrow
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_*_of_00016.arrow
Concatenating 16 shards
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-ed6c79c94d5babf6.arrow
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
07/11/2024 11:29:11 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/tr_part1b_00000.jsonl.
Using custom data configuration default-b3680afa0f50076c
Loading Dataset Infos from /home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00000_of_00016.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00001_of_00016.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00002_of_00016.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00003_of_00016.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00004_of_00016.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00005_of_00016.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00006_of_00016.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00007_of_00016.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00008_of_00016.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00009_of_00016.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00010_of_00016.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00011_of_00016.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00012_of_00016.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00013_of_00016.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00014_of_00016.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_00015_of_00016.arrow
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3a4b4dd10c80abee_*_of_00016.arrow
Concatenating 16 shards
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b3680afa0f50076c/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3a7e7bd5307f4b7.arrow
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
07/11/2024 11:30:35 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/el_part1b_00000.jsonl.
Using custom data configuration default-aceb257b54d43d7e
Loading Dataset Infos from /home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00000_of_00016.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00001_of_00016.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00002_of_00016.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00003_of_00016.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00004_of_00016.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00005_of_00016.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00006_of_00016.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00007_of_00016.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00008_of_00016.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00009_of_00016.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00010_of_00016.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00011_of_00016.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00012_of_00016.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00013_of_00016.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00014_of_00016.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_00015_of_00016.arrow
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-39ee6679ec7fa273_*_of_00016.arrow
Concatenating 16 shards
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-aceb257b54d43d7e/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-66037b825b01b373.arrow
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00000_of_00016.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00001_of_00016.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00002_of_00016.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00003_of_00016.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00004_of_00016.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00005_of_00016.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00006_of_00016.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00007_of_00016.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00008_of_00016.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00009_of_00016.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00010_of_00016.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00011_of_00016.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00012_of_00016.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00013_of_00016.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00014_of_00016.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_00015_of_00016.arrow
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6d89bec793e30a51_*_of_00016.arrow
Concatenating 16 shards
input_ids:
[12093, 70290, 6859, 6484, 43, 890, 27134, 2448, 7395, 30400, 55, 549, 26209, 198, 6622, 198, 16578, 70290, 6859, 6484, 43, 890, 27134, 2448, 7395, 30400, 55, 198, 6622, 198, 151643, 7039, 33976, 432, 646, 387, 264, 2699, 312, 2015, 1388, 311, 3270, 458, 4549, 911, 279, 10990, 7070, 14429, 3840, 438, 1052, 374, 825, 304, 1449, 1614, 14418, 504, 882, 311, 882, 11, 323, 279, 3482, 374, 49485, 448, 1105, 304, 1449, 4128, 369, 5019, 879, 30997, 311, 1349, 1105, 13, 2055, 358, 686, 36355, 304, 11629, 1246, 358, 5798, 323, 23983, 279, 1614, 624, 34762, 1635, 4134, 16145, 6635, 311, 1281, 1549, 862, 10990, 7070, 14429, 31496, 438, 807, 1030, 264, 501, 3093, 315, 50270, 82, 429, 1410, 15551, 279, 12188, 504, 2155, 11067, 323, 25941, 323, 8450, 1281, 64255, 323, 803, 35201, 9666, 13, 2379, 3381, 429, 3432, 419, 501, 5440, 432, 1035, 387, 2664, 311, 8193, 1549, 279, 330, 1040, 1211, 1, 1091, 894, 1008, 25546, 6174, 323, 432, 4977, 429, 807, 1033, 1290, 438, 1449, 1463, 7073, 10788, 825, 52163, 269, 803, 624, 9485, 501, 50270, 82, 1033, 537, 279, 1172, 501, 3166, 304, 1493, 4119, 11, 807, 1030, 264, 738, 315, 13918, 7746, 2669, 18663, 504, 279, 8151, 1137, 5527, 311, 41740, 11, 1045, 6548, 36780, 9317, 5479, 323, 501, 97685, 624, 2121, 5135, 438, 358, 8930, 279, 3745, 358, 1030, 264, 1602, 2797, 2168, 315, 279, 3093, 315, 1614, 358, 4829, 311, 1281, 11, 358, 1030, 3884, 10077, 315, 24248, 304, 279, 6467, 11, 1602, 76873, 12645, 98732, 448, 25386, 424, 429, 95758, 2310, 7218, 76024, 624, 2461, 419, 2390, 358, 6635, 311, 990, 279, 5235, 18457, 53514, 22293, 738, 369, 279, 10990, 7070, 14429, 429, 374, 2167, 14452, 323, 18304, 13942, 624, 2132, 4436, 944, 16965, 438, 279, 5479, 4946, 1602, 1632, 323, 279, 11221, 525, 1602, 2797, 11, 2337, 279, 1882, 582, 686, 1172, 614, 311, 1896, 2453, 979, 11589, 279, 2632, 5479, 311, 5648, 14719, 1105, 476, 11785, 11, 775, 14421, 1105, 13, 1634, 847, 4522, 504, 279, 7167, 572, 311, 4009, 264, 12896, 448, 279, 23603, 86968, 18824, 24569, 23704, 700, 11, 358, 23983, 279, 1614, 448, 5938, 12258, 25685, 12463, 438, 358, 5798, 432, 311, 614, 419, 12463, 2331, 369, 279, 2937, 330, 35012, 18824, 1, 14762, 358, 1035, 3796, 389, 279, 86968, 13, 2055, 358, 5798, 279, 44909, 323, 279, 64386, 9380, 15663, 279, 305, 9118, 11, 22696, 11, 39932, 13569, 11, 4992, 13, 14576, 311, 6707, 6177, 279, 65739, 448, 279, 15625, 476, 279, 68348, 13, 151643, 641, 3213, 1635, 358, 614, 65806, 3807, 52269, 14697, 304, 847, 2205, 3082, 13, 20035, 537, 3884, 894, 32319, 15570, 4730, 5926, 358, 572, 18442, 311, 1490, 458, 52269, 16262, 825, 315, 847, 22791, 14697, 264, 5625, 315, 2849, 4134, 13, 60033, 358, 9099, 847, 8849, 6249, 14046, 311, 1490, 421, 279, 52269, 374, 264, 5792, 20181, 13, 3197, 358, 10067, 279, 21852, 419, 6556, 358, 572, 33972, 311, 1490, 358, 614, 264, 6716, 315, 32319, 15570, 4730, 304, 21682, 13, 358, 2776, 10282, 4297, 27031, 1431, 304, 279, 3900, 429]
inputs:
@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject
@end
@implementation PodsDummy_XCDLumberjackNSLogger_OSX
@end
<|endoftext|>Nowadays it can be a bit reiterative to write an article about the Panzer III history as there is one in every model magazine from time to time, and the web is saturated with them in every language for everyone who desires to read them. So I will concentrate in telling how I built and painted the model.
Several years ago Dragon decided to make again their Panzer III kits as they had a new kind of moulds that could inject the plastic from different sides and angles and thus make thinner and more delicate pieces. They thought that having this new technology it would be better to produce again the "classics" than any other newer ones and it seems that they were right as every modeller bought oneâ€¦..or more.
These new moulds were not the only new thing in these models, they had a set of tracks links already separated from the sprues ready to assemble, some photoetched metal parts and new decals.
As soon as I opened the box I had a very clear image of the kind of model I wanted to make, I had seen lots of photographs in the books, very dusty machines cramped with equipage that resembled old moving vans.
For this project I decided to use the Blackdog resin accessories set for the Panzer III that is really suitable and fits perfectly.
It isn't complicated as the parts fit very well and the instructions are very clear, during the process we will only have to take care when handling the little parts to avoid breaking them or worst, loosing them. As my idea from the beginning was to represent a tank with the desert camouflage painting badly worn out, I painted the model with German Dark Grey colour as I built it to have this colour base for the later "soap painting" technique I would apply on the camouflage. So I built the chassis and the turret leaving aside the hatches, wheels, antenna rail, etc. mainly to easily paint the scratches with the brush or the sponge.<|endoftext|>In recent years I have erected several owl boxes in my local area. Having not seen any barn owls recently I was pleased to see an owl entering one of my nest boxes a couple of days ago. Yesterday I placed my trail camera nearby to see if the owl is a regular visitor. When I checked the footage this morning I was delighted to see I have a pair of barn owls in residence. I'm keeping everything crossed now in the hope that
07/11/2024 11:32:05 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
07/11/2024 11:32:06 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
07/11/2024 11:33:32 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/tr_part1b_00000.jsonl.
07/11/2024 11:33:32 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/tr_part1b_00000.jsonl.
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
07/11/2024 11:34:56 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/el_part1b_00000.jsonl.
07/11/2024 11:34:57 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/el_part1b_00000.jsonl.
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Loading cached shuffled indices for dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a89db6bda84d9ad5.arrow
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 433163
})
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): Qwen2ForCausalLM(
      (model): Qwen2Model(
        (embed_tokens): Embedding(151936, 2048)
        (layers): ModuleList(
          (0): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (1): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (2): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (3): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (4): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (5): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (6): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (7): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (8): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (9): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (10): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (11): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (12): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (13): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (14): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (15): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (16): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (17): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (18): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (19): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (20): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (21): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (22): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (23): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
        )
        (norm): Qwen2RMSNorm()
      )
      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
    )
  )
)
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:586] 2024-07-11 11:36:17,572 >> Using auto half precision backend
[2024-07-11 11:36:17,928] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 433163
})
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 433163
})
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
[2024-07-11 11:36:26,594] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-11 11:36:26,597] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-07-11 11:36:26,597] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-07-11 11:36:26,600] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-07-11 11:36:26,601] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-07-11 11:36:26,601] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-07-11 11:36:26,601] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-07-11 11:36:26,601] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-07-11 11:36:26,601] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: False
[2024-07-11 11:36:26,601] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-07-11 11:36:27,723] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-07-11 11:36:27,724] [INFO] [utils.py:803:see_memory_usage] MA 8.47 GB         Max_MA 8.47 GB         CA 8.76 GB         Max_CA 9 GB 
[2024-07-11 11:36:27,725] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 32.49 GB, percent = 12.9%
[2024-07-11 11:36:27,913] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-07-11 11:36:27,914] [INFO] [utils.py:803:see_memory_usage] MA 8.47 GB         Max_MA 8.47 GB         CA 8.76 GB         Max_CA 9 GB 
[2024-07-11 11:36:27,914] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 32.49 GB, percent = 12.9%
[2024-07-11 11:36:27,914] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-07-11 11:36:28,085] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-07-11 11:36:28,086] [INFO] [utils.py:803:see_memory_usage] MA 8.47 GB         Max_MA 8.47 GB         CA 8.76 GB         Max_CA 9 GB 
[2024-07-11 11:36:28,086] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 32.49 GB, percent = 12.9%
[2024-07-11 11:36:28,087] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-07-11 11:36:28,087] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-07-11 11:36:28,087] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-07-11 11:36:28,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2024-07-11 11:36:28,089] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-07-11 11:36:28,089] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-11 11:36:28,089] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-11 11:36:28,089] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-07-11 11:36:28,089] [INFO] [config.py:978:print]   amp_params ................... False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   bfloat16_enabled ............. True
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9b0c6431f0>
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   dump_state ................... False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... None
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   fp16_auto_cast ............... None
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   fp16_enabled ................. False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 4
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-07-11 11:36:28,090] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 1
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   loss_scale ................... 1.0
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   pld_params ................... False
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   train_batch_size ............. 192
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  16
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   world_size ................... 3
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-11 11:36:28,091] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-07-11 11:36:28,092] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 192, 
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1748] 2024-07-11 11:36:28,092 >> ***** Running training *****
[INFO|trainer.py:1749] 2024-07-11 11:36:28,092 >>   Num examples = 433,163
[INFO|trainer.py:1750] 2024-07-11 11:36:28,092 >>   Num Epochs = 1
[INFO|trainer.py:1751] 2024-07-11 11:36:28,092 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1754] 2024-07-11 11:36:28,092 >>   Total train batch size (w. parallel, distributed & accumulation) = 192
[INFO|trainer.py:1755] 2024-07-11 11:36:28,092 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1756] 2024-07-11 11:36:28,092 >>   Total optimization steps = 2,256
[INFO|trainer.py:1757] 2024-07-11 11:36:28,094 >>   Number of trainable parameters = 196,608
07/11/2024 11:36:28 - WARNING - llmtuner.extras.callbacks - Previous log file in this folder will be deleted.
  0%|          | 0/2256 [00:00<?, ?it/s]  0%|          | 1/2256 [00:07<4:34:45,  7.31s/it]  0%|          | 2/2256 [00:13<4:16:36,  6.83s/it]  0%|          | 3/2256 [00:20<4:10:53,  6.68s/it]  0%|          | 4/2256 [00:26<4:08:09,  6.61s/it]  0%|          | 5/2256 [00:33<4:06:30,  6.57s/it]  0%|          | 6/2256 [00:39<4:05:33,  6.55s/it]  0%|          | 7/2256 [00:46<4:04:50,  6.53s/it]  0%|          | 8/2256 [00:52<4:04:22,  6.52s/it]  0%|          | 9/2256 [00:59<4:04:34,  6.53s/it]  0%|          | 10/2256 [01:05<4:04:22,  6.53s/it]                                                   {'loss': 1.994, 'learning_rate': 4.999757604501921e-05, 'epoch': 0.0}
  0%|          | 10/2256 [01:05<4:04:22,  6.53s/it]                                                   {'router_ce_loss': 1.127155065536499, 'old_lang_expert0_score': '0.22 0.03 0.08 0.23 0.75 0.75 0.25 0.79 0.37 0.63 0.24 0.83 0.28 0.75 0.87 0.53 0.75 0.66 0.76 0.8 0.68 0.78 0.69 0.87', 'epoch': 0.0}
  0%|          | 10/2256 [01:06<4:04:22,  6.53s/it]  0%|          | 11/2256 [01:12<4:04:39,  6.54s/it]  1%|          | 12/2256 [01:18<4:04:33,  6.54s/it]  1%|          | 13/2256 [01:25<4:05:00,  6.55s/it]  1%|          | 14/2256 [01:32<4:04:58,  6.56s/it]  1%|          | 15/2256 [01:38<4:04:50,  6.56s/it]  1%|          | 16/2256 [01:45<4:04:45,  6.56s/it]  1%|          | 17/2256 [01:51<4:04:30,  6.55s/it]  1%|          | 18/2256 [01:58<4:10:15,  6.71s/it]  1%|          | 19/2256 [02:05<4:08:20,  6.66s/it]  1%|          | 20/2256 [02:11<4:06:56,  6.63s/it]                                                   {'loss': 1.8081, 'learning_rate': 4.9990304650121456e-05, 'epoch': 0.01}
  1%|          | 20/2256 [02:11<4:06:56,  6.63s/it]                                                   {'router_ce_loss': 1.0839954614639282, 'old_lang_expert0_score': '0.24 0.03 0.05 0.2 0.79 0.81 0.25 0.88 0.47 0.72 0.19 0.82 0.19 0.82 0.91 0.65 0.81 0.76 0.86 0.82 0.78 0.87 0.83 0.94', 'epoch': 0.01}
  1%|          | 20/2256 [02:12<4:06:56,  6.63s/it]  1%|          | 21/2256 [02:18<4:06:19,  6.61s/it]  1%|          | 22/2256 [02:25<4:05:42,  6.60s/it]  1%|          | 23/2256 [02:31<4:05:19,  6.59s/it]  1%|          | 24/2256 [02:38<4:04:52,  6.58s/it]  1%|          | 25/2256 [02:44<4:04:43,  6.58s/it]  1%|          | 26/2256 [02:51<4:04:50,  6.59s/it]  1%|          | 27/2256 [02:58<4:04:40,  6.59s/it]  1%|          | 28/2256 [03:04<4:04:56,  6.60s/it]  1%|â–         | 29/2256 [03:11<4:04:53,  6.60s/it]  1%|â–         | 30/2256 [03:17<4:04:39,  6.59s/it]                                                   {'loss': 1.8157, 'learning_rate': 4.9978187225349436e-05, 'epoch': 0.01}
  1%|â–         | 30/2256 [03:17<4:04:39,  6.59s/it]                                                   {'router_ce_loss': 1.0766997337341309, 'old_lang_expert0_score': '0.23 0.03 0.06 0.19 0.77 0.8 0.29 0.88 0.47 0.78 0.28 0.89 0.28 0.8 0.93 0.6 0.83 0.75 0.84 0.83 0.81 0.83 0.82 0.93', 'epoch': 0.01}
  1%|â–         | 30/2256 [03:18<4:04:39,  6.59s/it]  1%|â–         | 31/2256 [03:24<4:04:34,  6.60s/it]  1%|â–         | 32/2256 [03:31<4:04:44,  6.60s/it]  1%|â–         | 33/2256 [03:37<4:04:23,  6.60s/it]  2%|â–         | 34/2256 [03:44<4:04:17,  6.60s/it]  2%|â–         | 35/2256 [03:51<4:07:08,  6.68s/it]  2%|â–         | 36/2256 [03:57<4:09:16,  6.74s/it]  2%|â–         | 37/2256 [04:04<4:07:25,  6.69s/it]  2%|â–         | 38/2256 [04:11<4:06:03,  6.66s/it]  2%|â–         | 39/2256 [04:17<4:05:03,  6.63s/it]  2%|â–         | 40/2256 [04:24<4:04:02,  6.61s/it]                                                   {'loss': 1.877, 'learning_rate': 4.9961226120470545e-05, 'epoch': 0.02}
  2%|â–         | 40/2256 [04:24<4:04:02,  6.61s/it]                                                   {'router_ce_loss': 1.0995489358901978, 'old_lang_expert0_score': '0.24 0.04 0.06 0.15 0.75 0.74 0.26 0.85 0.42 0.71 0.25 0.86 0.26 0.79 0.88 0.53 0.75 0.75 0.82 0.83 0.76 0.83 0.86 0.92', 'epoch': 0.02}
  2%|â–         | 40/2256 [04:24<4:04:02,  6.61s/it]  2%|â–         | 41/2256 [04:30<4:03:31,  6.60s/it]  2%|â–         | 42/2256 [04:37<4:03:21,  6.60s/it]  2%|â–         | 43/2256 [04:43<4:03:10,  6.59s/it]  2%|â–         | 44/2256 [04:50<4:02:48,  6.59s/it]  2%|â–         | 45/2256 [04:57<4:02:37,  6.58s/it]  2%|â–         | 46/2256 [05:03<4:02:32,  6.58s/it]  2%|â–         | 47/2256 [05:10<4:02:31,  6.59s/it]  2%|â–         | 48/2256 [05:16<4:02:27,  6.59s/it]  2%|â–         | 49/2256 [05:23<4:02:55,  6.60s/it]  2%|â–         | 50/2256 [05:30<4:02:47,  6.60s/it]                                                   {'loss': 1.962, 'learning_rate': 4.9939424624521137e-05, 'epoch': 0.02}
  2%|â–         | 50/2256 [05:30<4:02:47,  6.60s/it]                                                   {'router_ce_loss': 1.0809569358825684, 'old_lang_expert0_score': '0.22 0.04 0.06 0.22 0.77 0.78 0.26 0.85 0.47 0.74 0.19 0.89 0.24 0.81 0.89 0.64 0.82 0.8 0.87 0.85 0.8 0.82 0.89 0.93', 'epoch': 0.02}
  2%|â–         | 50/2256 [05:30<4:02:47,  6.60s/it]  2%|â–         | 51/2256 [05:36<4:02:34,  6.60s/it]  2%|â–         | 52/2256 [05:43<4:02:13,  6.59s/it]  2%|â–         | 53/2256 [05:50<4:04:29,  6.66s/it]  2%|â–         | 54/2256 [05:56<4:03:31,  6.64s/it]  2%|â–         | 55/2256 [06:03<4:02:56,  6.62s/it]  2%|â–         | 56/2256 [06:09<4:02:21,  6.61s/it]  3%|â–Ž         | 57/2256 [06:16<4:01:50,  6.60s/it]  3%|â–Ž         | 58/2256 [06:23<4:01:44,  6.60s/it]  3%|â–Ž         | 59/2256 [06:29<4:01:28,  6.59s/it]  3%|â–Ž         | 60/2256 [06:36<4:00:59,  6.58s/it]                                                   {'loss': 1.7919, 'learning_rate': 4.991278696516879e-05, 'epoch': 0.03}
  3%|â–Ž         | 60/2256 [06:36<4:00:59,  6.58s/it]                                                   {'router_ce_loss': 1.083133339881897, 'old_lang_expert0_score': '0.23 0.03 0.07 0.18 0.72 0.73 0.18 0.84 0.43 0.8 0.22 0.9 0.23 0.86 0.93 0.63 0.85 0.82 0.85 0.86 0.82 0.86 0.87 0.93', 'epoch': 0.03}
  3%|â–Ž         | 60/2256 [06:36<4:00:59,  6.58s/it]  3%|â–Ž         | 61/2256 [06:42<4:00:54,  6.59s/it]  3%|â–Ž         | 62/2256 [06:49<4:00:47,  6.59s/it]  3%|â–Ž         | 63/2256 [06:55<4:00:38,  6.58s/it]  3%|â–Ž         | 64/2256 [07:02<4:00:24,  6.58s/it]  3%|â–Ž         | 65/2256 [07:09<4:00:13,  6.58s/it]  3%|â–Ž         | 66/2256 [07:15<3:59:59,  6.58s/it]  3%|â–Ž         | 67/2256 [07:22<4:00:03,  6.58s/it]  3%|â–Ž         | 68/2256 [07:28<3:59:37,  6.57s/it]  3%|â–Ž         | 69/2256 [07:35<3:59:36,  6.57s/it]  3%|â–Ž         | 70/2256 [07:41<3:59:39,  6.58s/it]                                                   {'loss': 1.9375, 'learning_rate': 4.988131830789248e-05, 'epoch': 0.03}
  3%|â–Ž         | 70/2256 [07:41<3:59:39,  6.58s/it]                                                   {'router_ce_loss': 1.0807342529296875, 'old_lang_expert0_score': '0.24 0.04 0.04 0.22 0.79 0.78 0.24 0.85 0.41 0.79 0.11 0.89 0.16 0.87 0.94 0.78 0.91 0.78 0.79 0.87 0.82 0.83 0.9 0.94', 'epoch': 0.03}
  3%|â–Ž         | 70/2256 [07:42<3:59:39,  6.58s/it]  3%|â–Ž         | 71/2256 [07:49<4:04:55,  6.73s/it]  3%|â–Ž         | 72/2256 [07:55<4:03:31,  6.69s/it]  3%|â–Ž         | 73/2256 [08:02<4:02:28,  6.66s/it]  3%|â–Ž         | 74/2256 [08:08<4:01:29,  6.64s/it]  3%|â–Ž         | 75/2256 [08:15<4:00:46,  6.62s/it]  3%|â–Ž         | 76/2256 [08:22<4:00:13,  6.61s/it]  3%|â–Ž         | 77/2256 [08:28<4:00:09,  6.61s/it]  3%|â–Ž         | 78/2256 [08:35<3:59:47,  6.61s/it]  4%|â–Ž         | 79/2256 [08:41<3:59:46,  6.61s/it]  4%|â–Ž         | 80/2256 [08:48<3:59:37,  6.61s/it]                                                   {'loss': 1.822, 'learning_rate': 4.9845024754980876e-05, 'epoch': 0.04}
  4%|â–Ž         | 80/2256 [08:48<3:59:37,  6.61s/it]                                                   {'router_ce_loss': 1.0527584552764893, 'old_lang_expert0_score': '0.25 0.03 0.05 0.17 0.8 0.83 0.25 0.88 0.53 0.81 0.17 0.9 0.23 0.87 0.95 0.77 0.9 0.83 0.9 0.9 0.89 0.88 0.93 0.95', 'epoch': 0.04}
  4%|â–Ž         | 80/2256 [08:48<3:59:37,  6.61s/it]  4%|â–Ž         | 81/2256 [08:55<3:59:37,  6.61s/it]  4%|â–Ž         | 82/2256 [09:01<3:59:24,  6.61s/it]  4%|â–Ž         | 83/2256 [09:08<3:59:06,  6.60s/it]  4%|â–Ž         | 84/2256 [09:14<3:58:33,  6.59s/it]  4%|â–         | 85/2256 [09:21<3:58:17,  6.59s/it]  4%|â–         | 86/2256 [09:27<3:57:57,  6.58s/it]  4%|â–         | 87/2256 [09:34<3:58:03,  6.59s/it]  4%|â–         | 88/2256 [09:41<4:00:07,  6.65s/it]  4%|â–         | 89/2256 [09:48<4:01:21,  6.68s/it]  4%|â–         | 90/2256 [09:54<4:00:20,  6.66s/it]                                                   {'loss': 1.8448, 'learning_rate': 4.980391334434905e-05, 'epoch': 0.04}
  4%|â–         | 90/2256 [09:54<4:00:20,  6.66s/it]                                                   {'router_ce_loss': 1.046015977859497, 'old_lang_expert0_score': '0.23 0.05 0.09 0.21 0.81 0.84 0.22 0.88 0.57 0.81 0.25 0.92 0.3 0.9 0.94 0.74 0.88 0.84 0.89 0.86 0.82 0.84 0.93 0.96', 'epoch': 0.04}
  4%|â–         | 90/2256 [09:55<4:00:20,  6.66s/it]  4%|â–         | 91/2256 [10:01<3:59:35,  6.64s/it]  4%|â–         | 92/2256 [10:07<3:58:57,  6.63s/it]  4%|â–         | 93/2256 [10:14<3:58:34,  6.62s/it]  4%|â–         | 94/2256 [10:21<3:57:51,  6.60s/it]  4%|â–         | 95/2256 [10:27<3:57:48,  6.60s/it]  4%|â–         | 96/2256 [10:34<3:57:25,  6.59s/it]  4%|â–         | 97/2256 [10:40<3:57:09,  6.59s/it]  4%|â–         | 98/2256 [10:47<3:57:03,  6.59s/it]  4%|â–         | 99/2256 [10:54<3:57:09,  6.60s/it]  4%|â–         | 100/2256 [11:00<3:56:56,  6.59s/it]                                                    {'loss': 1.7568, 'learning_rate': 4.975799204817369e-05, 'epoch': 0.04}
  4%|â–         | 100/2256 [11:00<3:56:56,  6.59s/it]                                                    {'router_ce_loss': 1.0278522968292236, 'old_lang_expert0_score': '0.24 0.04 0.06 0.23 0.84 0.86 0.25 0.93 0.58 0.83 0.17 0.91 0.18 0.93 0.97 0.86 0.92 0.91 0.94 0.94 0.91 0.92 0.95 0.96', 'epoch': 0.04}
  4%|â–         | 100/2256 [11:01<3:56:56,  6.59s/it]  4%|â–         | 101/2256 [11:07<3:56:58,  6.60s/it]  5%|â–         | 102/2256 [11:13<3:56:39,  6.59s/it]  5%|â–         | 103/2256 [11:20<3:56:04,  6.58s/it]  5%|â–         | 104/2256 [11:26<3:56:05,  6.58s/it]  5%|â–         | 105/2256 [11:33<3:56:07,  6.59s/it]  5%|â–         | 106/2256 [11:40<3:58:22,  6.65s/it]  5%|â–         | 107/2256 [11:46<3:57:37,  6.63s/it]  5%|â–         | 108/2256 [11:53<3:57:01,  6.62s/it]  5%|â–         | 109/2256 [12:00<3:56:30,  6.61s/it]  5%|â–         | 110/2256 [12:06<3:56:25,  6.61s/it]                                                    {'loss': 1.7752, 'learning_rate': 4.9707269771347165e-05, 'epoch': 0.05}
  5%|â–         | 110/2256 [12:06<3:56:25,  6.61s/it]                                                    {'router_ce_loss': 1.0366908311843872, 'old_lang_expert0_score': '0.25 0.03 0.06 0.34 0.86 0.89 0.27 0.9 0.47 0.82 0.11 0.86 0.1 0.9 0.95 0.86 0.91 0.88 0.95 0.93 0.92 0.92 0.96 0.97', 'epoch': 0.05}
  5%|â–         | 110/2256 [12:07<3:56:25,  6.61s/it]  5%|â–         | 111/2256 [12:13<3:56:09,  6.61s/it]  5%|â–         | 112/2256 [12:19<3:56:17,  6.61s/it]  5%|â–Œ         | 113/2256 [12:26<3:55:59,  6.61s/it]  5%|â–Œ         | 114/2256 [12:33<3:55:42,  6.60s/it]  5%|â–Œ         | 115/2256 [12:39<3:55:27,  6.60s/it]  5%|â–Œ         | 116/2256 [12:46<3:55:28,  6.60s/it]  5%|â–Œ         | 117/2256 [12:52<3:55:04,  6.59s/it]  5%|â–Œ         | 118/2256 [12:59<3:54:49,  6.59s/it]  5%|â–Œ         | 119/2256 [13:06<3:54:39,  6.59s/it]  5%|â–Œ         | 120/2256 [13:12<3:54:25,  6.59s/it]                                                    {'loss': 1.7465, 'learning_rate': 4.9651756349750716e-05, 'epoch': 0.05}
  5%|â–Œ         | 120/2256 [13:12<3:54:25,  6.59s/it]                                                    {'router_ce_loss': 1.025457739830017, 'old_lang_expert0_score': '0.24 0.03 0.08 0.24 0.82 0.86 0.29 0.91 0.61 0.87 0.24 0.91 0.22 0.91 0.96 0.83 0.93 0.92 0.94 0.91 0.9 0.89 0.93 0.96', 'epoch': 0.05}
  5%|â–Œ         | 120/2256 [13:13<3:54:25,  6.59s/it]  5%|â–Œ         | 121/2256 [13:19<3:54:36,  6.59s/it]  5%|â–Œ         | 122/2256 [13:25<3:54:21,  6.59s/it]  5%|â–Œ         | 123/2256 [13:32<3:54:08,  6.59s/it]  5%|â–Œ         | 124/2256 [13:39<3:56:17,  6.65s/it]  6%|â–Œ         | 125/2256 [13:46<3:57:56,  6.70s/it]  6%|â–Œ         | 126/2256 [13:52<3:56:44,  6.67s/it]  6%|â–Œ         | 127/2256 [13:59<3:55:38,  6.64s/it]  6%|â–Œ         | 128/2256 [14:05<3:56:16,  6.66s/it]  6%|â–Œ         | 129/2256 [14:12<3:55:11,  6.63s/it]  6%|â–Œ         | 130/2256 [14:19<3:54:34,  6.62s/it]                                                    {'loss': 1.8487, 'learning_rate': 4.9591462548347125e-05, 'epoch': 0.06}
  6%|â–Œ         | 130/2256 [14:19<3:54:34,  6.62s/it]                                                    {'router_ce_loss': 1.043466329574585, 'old_lang_expert0_score': '0.24 0.03 0.05 0.2 0.82 0.84 0.16 0.88 0.6 0.85 0.13 0.9 0.18 0.91 0.95 0.86 0.91 0.88 0.94 0.93 0.9 0.92 0.96 0.98', 'epoch': 0.06}
  6%|â–Œ         | 130/2256 [14:19<3:54:34,  6.62s/it]  6%|â–Œ         | 131/2256 [14:25<3:54:17,  6.62s/it]  6%|â–Œ         | 132/2256 [14:32<3:54:11,  6.62s/it]  6%|â–Œ         | 133/2256 [14:38<3:53:46,  6.61s/it]  6%|â–Œ         | 134/2256 [14:45<3:53:29,  6.60s/it]  6%|â–Œ         | 135/2256 [14:52<3:53:16,  6.60s/it]  6%|â–Œ         | 136/2256 [14:58<3:53:06,  6.60s/it]  6%|â–Œ         | 137/2256 [15:05<3:52:49,  6.59s/it]  6%|â–Œ         | 138/2256 [15:11<3:52:36,  6.59s/it]  6%|â–Œ         | 139/2256 [15:18<3:52:43,  6.60s/it]  6%|â–Œ         | 140/2256 [15:25<3:52:24,  6.59s/it]                                                    {'loss': 1.722, 'learning_rate': 4.9526400059093214e-05, 'epoch': 0.06}
  6%|â–Œ         | 140/2256 [15:25<3:52:24,  6.59s/it]                                                    {'router_ce_loss': 1.0357391834259033, 'old_lang_expert0_score': '0.24 0.03 0.05 0.22 0.84 0.86 0.21 0.9 0.61 0.83 0.1 0.91 0.16 0.91 0.96 0.89 0.93 0.91 0.93 0.93 0.92 0.93 0.95 0.97', 'epoch': 0.06}
  6%|â–Œ         | 140/2256 [15:25<3:52:24,  6.59s/it]  6%|â–‹         | 141/2256 [15:31<3:52:01,  6.58s/it]  6%|â–‹         | 142/2256 [15:38<3:54:00,  6.64s/it]  6%|â–‹         | 143/2256 [15:45<3:55:21,  6.68s/it]  6%|â–‹         | 144/2256 [15:51<3:54:09,  6.65s/it]  6%|â–‹         | 145/2256 [15:58<3:53:34,  6.64s/it]  6%|â–‹         | 146/2256 [16:04<3:53:14,  6.63s/it]  7%|â–‹         | 147/2256 [16:11<3:52:45,  6.62s/it]  7%|â–‹         | 148/2256 [16:18<3:52:31,  6.62s/it]  7%|â–‹         | 149/2256 [16:24<3:52:26,  6.62s/it]  7%|â–‹         | 150/2256 [16:31<3:52:12,  6.62s/it]                                                    {'loss': 1.8277, 'learning_rate': 4.9456581498672574e-05, 'epoch': 0.07}
  7%|â–‹         | 150/2256 [16:31<3:52:12,  6.62s/it]                                                    {'router_ce_loss': 1.0346226692199707, 'old_lang_expert0_score': '0.24 0.03 0.05 0.22 0.84 0.84 0.2 0.94 0.68 0.85 0.14 0.92 0.21 0.91 0.95 0.82 0.92 0.88 0.92 0.94 0.91 0.91 0.95 0.97', 'epoch': 0.07}
  7%|â–‹         | 150/2256 [16:31<3:52:12,  6.62s/it]  7%|â–‹         | 151/2256 [16:37<3:51:59,  6.61s/it]  7%|â–‹         | 152/2256 [16:44<3:51:35,  6.60s/it]  7%|â–‹         | 153/2256 [16:51<3:51:26,  6.60s/it]  7%|â–‹         | 154/2256 [16:57<3:51:21,  6.60s/it]  7%|â–‹         | 155/2256 [17:04<3:51:02,  6.60s/it]  7%|â–‹         | 156/2256 [17:10<3:51:17,  6.61s/it]  7%|â–‹         | 157/2256 [17:17<3:51:16,  6.61s/it]  7%|â–‹         | 158/2256 [17:24<3:51:02,  6.61s/it]  7%|â–‹         | 159/2256 [17:30<3:50:52,  6.61s/it]  7%|â–‹         | 160/2256 [17:37<3:53:24,  6.68s/it]                                                    {'loss': 1.8213, 'learning_rate': 4.938202040604898e-05, 'epoch': 0.07}
  7%|â–‹         | 160/2256 [17:37<3:53:24,  6.68s/it]                                                    {'router_ce_loss': 1.0376132726669312, 'old_lang_expert0_score': '0.24 0.03 0.06 0.21 0.83 0.84 0.19 0.9 0.59 0.85 0.14 0.91 0.19 0.9 0.95 0.84 0.92 0.91 0.94 0.94 0.93 0.92 0.95 0.97', 'epoch': 0.07}
  7%|â–‹         | 160/2256 [17:38<3:53:24,  6.68s/it]  7%|â–‹         | 161/2256 [17:44<3:52:39,  6.66s/it]  7%|â–‹         | 162/2256 [17:50<3:51:54,  6.65s/it]  7%|â–‹         | 163/2256 [17:57<3:51:20,  6.63s/it]  7%|â–‹         | 164/2256 [18:04<3:51:05,  6.63s/it]  7%|â–‹         | 165/2256 [18:10<3:50:22,  6.61s/it]  7%|â–‹         | 166/2256 [18:17<3:50:05,  6.61s/it]  7%|â–‹         | 167/2256 [18:23<3:49:41,  6.60s/it]  7%|â–‹         | 168/2256 [18:30<3:48:57,  6.58s/it]  7%|â–‹         | 169/2256 [18:36<3:48:48,  6.58s/it]  8%|â–Š         | 170/2256 [18:43<3:48:42,  6.58s/it]                                                    {'loss': 1.7973, 'learning_rate': 4.930273123984098e-05, 'epoch': 0.08}
  8%|â–Š         | 170/2256 [18:43<3:48:42,  6.58s/it]                                                    {'router_ce_loss': 1.0498770475387573, 'old_lang_expert0_score': '0.22 0.02 0.04 0.14 0.74 0.83 0.14 0.88 0.66 0.86 0.24 0.93 0.37 0.89 0.97 0.82 0.84 0.85 0.94 0.92 0.9 0.8 0.93 0.94', 'epoch': 0.08}
  8%|â–Š         | 170/2256 [18:44<3:48:42,  6.58s/it]  8%|â–Š         | 171/2256 [18:50<3:49:15,  6.60s/it]  8%|â–Š         | 172/2256 [18:56<3:49:06,  6.60s/it]  8%|â–Š         | 173/2256 [19:03<3:48:52,  6.59s/it]  8%|â–Š         | 174/2256 [19:09<3:48:47,  6.59s/it]  8%|â–Š         | 175/2256 [19:16<3:48:43,  6.59s/it]  8%|â–Š         | 176/2256 [19:23<3:48:24,  6.59s/it]  8%|â–Š         | 177/2256 [19:29<3:48:19,  6.59s/it]  8%|â–Š         | 178/2256 [19:36<3:53:01,  6.73s/it]  8%|â–Š         | 179/2256 [19:43<3:51:43,  6.69s/it]  8%|â–Š         | 180/2256 [19:49<3:50:35,  6.66s/it]                                                    {'loss': 1.7911, 'learning_rate': 4.9218729375518135e-05, 'epoch': 0.08}
  8%|â–Š         | 180/2256 [19:49<3:50:35,  6.66s/it]                                                    {'router_ce_loss': 1.0241036415100098, 'old_lang_expert0_score': '0.2 0.02 0.06 0.21 0.71 0.88 0.27 0.91 0.78 0.92 0.27 0.97 0.41 0.91 0.97 0.8 0.87 0.83 0.93 0.88 0.93 0.88 0.97 0.98', 'epoch': 0.08}
  8%|â–Š         | 180/2256 [19:50<3:50:35,  6.66s/it]  8%|â–Š         | 181/2256 [19:56<3:49:59,  6.65s/it]  8%|â–Š         | 182/2256 [20:03<3:49:10,  6.63s/it]  8%|â–Š         | 183/2256 [20:09<3:48:41,  6.62s/it]  8%|â–Š         | 184/2256 [20:16<3:48:09,  6.61s/it]  8%|â–Š         | 185/2256 [20:22<3:47:45,  6.60s/it]  8%|â–Š         | 186/2256 [20:29<3:47:43,  6.60s/it]  8%|â–Š         | 187/2256 [20:36<3:47:29,  6.60s/it]  8%|â–Š         | 188/2256 [20:42<3:47:19,  6.60s/it]  8%|â–Š         | 189/2256 [20:49<3:47:17,  6.60s/it]  8%|â–Š         | 190/2256 [20:55<3:47:11,  6.60s/it]                                                    {'loss': 1.8836, 'learning_rate': 4.9130031102419426e-05, 'epoch': 0.08}
  8%|â–Š         | 190/2256 [20:55<3:47:11,  6.60s/it]                                                    {'router_ce_loss': 1.0462125539779663, 'old_lang_expert0_score': '0.25 0.03 0.04 0.16 0.77 0.83 0.17 0.87 0.61 0.84 0.15 0.91 0.17 0.91 0.96 0.81 0.91 0.91 0.92 0.93 0.93 0.94 0.94 0.97', 'epoch': 0.08}
  8%|â–Š         | 190/2256 [20:56<3:47:11,  6.60s/it]  8%|â–Š         | 191/2256 [21:02<3:46:53,  6.59s/it]  9%|â–Š         | 192/2256 [21:09<3:46:32,  6.59s/it]  9%|â–Š         | 193/2256 [21:15<3:46:26,  6.59s/it]  9%|â–Š         | 194/2256 [21:22<3:46:18,  6.59s/it]  9%|â–Š         | 195/2256 [21:29<3:48:12,  6.64s/it]  9%|â–Š         | 196/2256 [21:35<3:49:54,  6.70s/it]  9%|â–Š         | 197/2256 [21:42<3:48:29,  6.66s/it]  9%|â–‰         | 198/2256 [21:49<3:47:47,  6.64s/it]  9%|â–‰         | 199/2256 [21:55<3:47:01,  6.62s/it]  9%|â–‰         | 200/2256 [22:02<3:46:37,  6.61s/it]                                                    {'loss': 1.7048, 'learning_rate': 4.903665362059453e-05, 'epoch': 0.09}
  9%|â–‰         | 200/2256 [22:02<3:46:37,  6.61s/it]                                                    {'router_ce_loss': 1.0221909284591675, 'old_lang_expert0_score': '0.24 0.04 0.06 0.27 0.84 0.91 0.26 0.94 0.66 0.85 0.12 0.93 0.14 0.94 0.95 0.89 0.9 0.91 0.95 0.94 0.93 0.94 0.97 0.98', 'epoch': 0.09}
  9%|â–‰         | 200/2256 [22:02<3:46:37,  6.61s/it]  9%|â–‰         | 201/2256 [22:08<3:46:21,  6.61s/it]  9%|â–‰         | 202/2256 [22:15<3:46:04,  6.60s/it]  9%|â–‰         | 203/2256 [22:21<3:45:45,  6.60s/it]  9%|â–‰         | 204/2256 [22:28<3:45:33,  6.60s/it]  9%|â–‰         | 205/2256 [22:35<3:45:15,  6.59s/it]  9%|â–‰         | 206/2256 [22:41<3:45:02,  6.59s/it]  9%|â–‰         | 207/2256 [22:48<3:44:57,  6.59s/it]  9%|â–‰         | 208/2256 [22:54<3:44:47,  6.59s/it]  9%|â–‰         | 209/2256 [23:01<3:44:36,  6.58s/it]  9%|â–‰         | 210/2256 [23:08<3:44:32,  6.58s/it]                                                    {'loss': 1.8532, 'learning_rate': 4.893861503746841e-05, 'epoch': 0.09}
  9%|â–‰         | 210/2256 [23:08<3:44:32,  6.58s/it]                                                    {'router_ce_loss': 1.0465245246887207, 'old_lang_expert0_score': '0.23 0.03 0.05 0.15 0.78 0.82 0.19 0.9 0.66 0.84 0.2 0.91 0.28 0.88 0.94 0.8 0.88 0.83 0.93 0.9 0.9 0.87 0.96 0.97', 'epoch': 0.09}
  9%|â–‰         | 210/2256 [23:08<3:44:32,  6.58s/it]  9%|â–‰         | 211/2256 [23:14<3:44:35,  6.59s/it]  9%|â–‰         | 212/2256 [23:21<3:44:36,  6.59s/it]  9%|â–‰         | 213/2256 [23:28<3:46:54,  6.66s/it]  9%|â–‰         | 214/2256 [23:34<3:45:56,  6.64s/it] 10%|â–‰         | 215/2256 [23:41<3:45:12,  6.62s/it] 10%|â–‰         | 216/2256 [23:47<3:44:38,  6.61s/it] 10%|â–‰         | 217/2256 [23:54<3:44:14,  6.60s/it] 10%|â–‰         | 218/2256 [24:00<3:44:06,  6.60s/it] 10%|â–‰         | 219/2256 [24:07<3:43:51,  6.59s/it] 10%|â–‰         | 220/2256 [24:14<3:43:36,  6.59s/it]                                                    {'loss': 1.7517, 'learning_rate': 4.8835934364330024e-05, 'epoch': 0.1}
 10%|â–‰         | 220/2256 [24:14<3:43:36,  6.59s/it]                                                    {'router_ce_loss': 1.0173146724700928, 'old_lang_expert0_score': '0.26 0.02 0.03 0.22 0.89 0.88 0.21 0.94 0.79 0.89 0.12 0.91 0.17 0.92 0.97 0.91 0.95 0.92 0.97 0.96 0.93 0.95 0.99 0.98', 'epoch': 0.1}
 10%|â–‰         | 220/2256 [24:14<3:43:36,  6.59s/it] 10%|â–‰         | 221/2256 [24:20<3:43:29,  6.59s/it] 10%|â–‰         | 222/2256 [24:27<3:43:57,  6.61s/it] 10%|â–‰         | 223/2256 [24:33<3:43:43,  6.60s/it] 10%|â–‰         | 224/2256 [24:40<3:43:23,  6.60s/it] 10%|â–‰         | 225/2256 [24:47<3:43:05,  6.59s/it] 10%|â–ˆ         | 226/2256 [24:53<3:43:00,  6.59s/it] 10%|â–ˆ         | 227/2256 [25:00<3:42:46,  6.59s/it] 10%|â–ˆ         | 228/2256 [25:06<3:42:36,  6.59s/it] 10%|â–ˆ         | 229/2256 [25:13<3:42:35,  6.59s/it] 10%|â–ˆ         | 230/2256 [25:20<3:42:32,  6.59s/it]                                                    {'loss': 1.7417, 'learning_rate': 4.8728631512645696e-05, 'epoch': 0.1}
 10%|â–ˆ         | 230/2256 [25:20<3:42:32,  6.59s/it]                                                    {'router_ce_loss': 1.0167831182479858, 'old_lang_expert0_score': '0.24 0.04 0.06 0.23 0.84 0.85 0.23 0.92 0.72 0.91 0.2 0.94 0.25 0.93 0.97 0.88 0.91 0.91 0.94 0.93 0.92 0.93 0.95 0.97', 'epoch': 0.1}
 10%|â–ˆ         | 230/2256 [25:20<3:42:32,  6.59s/it] 10%|â–ˆ         | 231/2256 [25:27<3:46:45,  6.72s/it] 10%|â–ˆ         | 232/2256 [25:33<3:45:35,  6.69s/it] 10%|â–ˆ         | 233/2256 [25:40<3:44:23,  6.66s/it] 10%|â–ˆ         | 234/2256 [25:46<3:43:32,  6.63s/it] 10%|â–ˆ         | 235/2256 [25:53<3:42:59,  6.62s/it] 10%|â–ˆ         | 236/2256 [26:00<3:42:24,  6.61s/it] 11%|â–ˆ         | 237/2256 [26:06<3:42:09,  6.60s/it] 11%|â–ˆ         | 238/2256 [26:13<3:41:47,  6.59s/it] 11%|â–ˆ         | 239/2256 [26:19<3:41:31,  6.59s/it] 11%|â–ˆ         | 240/2256 [26:26<3:41:17,  6.59s/it]                                                    {'loss': 1.7566, 'learning_rate': 4.861672729019797e-05, 'epoch': 0.11}
 11%|â–ˆ         | 240/2256 [26:26<3:41:17,  6.59s/it]                                                    {'router_ce_loss': 1.0431187152862549, 'old_lang_expert0_score': '0.23 0.03 0.07 0.16 0.77 0.75 0.22 0.88 0.71 0.88 0.21 0.92 0.34 0.9 0.95 0.79 0.86 0.87 0.92 0.91 0.87 0.89 0.96 0.97', 'epoch': 0.11}
 11%|â–ˆ         | 240/2256 [26:26<3:41:17,  6.59s/it] 11%|â–ˆ         | 241/2256 [26:32<3:41:04,  6.58s/it] 11%|â–ˆ         | 242/2256 [26:39<3:40:58,  6.58s/it] 11%|â–ˆ         | 243/2256 [26:46<3:40:56,  6.59s/it] 11%|â–ˆ         | 244/2256 [26:52<3:40:48,  6.58s/it] 11%|â–ˆ         | 245/2256 [26:59<3:40:32,  6.58s/it] 11%|â–ˆ         | 246/2256 [27:05<3:40:36,  6.59s/it] 11%|â–ˆ         | 247/2256 [27:12<3:40:24,  6.58s/it] 11%|â–ˆ         | 248/2256 [27:19<3:42:28,  6.65s/it] 11%|â–ˆ         | 249/2256 [27:25<3:43:33,  6.68s/it] 11%|â–ˆ         | 250/2256 [27:32<3:42:13,  6.65s/it]                                                    {'loss': 1.8191, 'learning_rate': 4.850024339705064e-05, 'epoch': 0.11}
 11%|â–ˆ         | 250/2256 [27:32<3:42:13,  6.65s/it]                                                    {'router_ce_loss': 1.0217139720916748, 'old_lang_expert0_score': '0.24 0.02 0.06 0.22 0.85 0.86 0.23 0.91 0.73 0.86 0.18 0.92 0.24 0.91 0.97 0.88 0.92 0.91 0.94 0.93 0.93 0.94 0.97 0.98', 'epoch': 0.11}
 11%|â–ˆ         | 250/2256 [27:33<3:42:13,  6.65s/it] 11%|â–ˆ         | 251/2256 [27:39<3:41:21,  6.62s/it] 11%|â–ˆ         | 252/2256 [27:45<3:40:55,  6.61s/it] 11%|â–ˆ         | 253/2256 [27:52<3:40:28,  6.60s/it] 11%|â–ˆâ–        | 254/2256 [27:58<3:39:54,  6.59s/it] 11%|â–ˆâ–        | 255/2256 [28:05<3:39:31,  6.58s/it] 11%|â–ˆâ–        | 256/2256 [28:11<3:39:20,  6.58s/it] 11%|â–ˆâ–        | 257/2256 [28:18<3:39:07,  6.58s/it] 11%|â–ˆâ–        | 258/2256 [28:25<3:38:44,  6.57s/it] 11%|â–ˆâ–        | 259/2256 [28:31<3:38:46,  6.57s/it] 12%|â–ˆâ–        | 260/2256 [28:38<3:38:27,  6.57s/it]                                                    {'loss': 1.7646, 'learning_rate': 4.837920242134074e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 260/2256 [28:38<3:38:27,  6.57s/it]                                                    {'router_ce_loss': 1.0152682065963745, 'old_lang_expert0_score': '0.23 0.03 0.07 0.25 0.81 0.84 0.3 0.93 0.73 0.88 0.25 0.92 0.31 0.92 0.95 0.85 0.92 0.9 0.93 0.9 0.91 0.91 0.97 0.97', 'epoch': 0.12}
 12%|â–ˆâ–        | 260/2256 [28:38<3:38:27,  6.57s/it] 12%|â–ˆâ–        | 261/2256 [28:44<3:38:17,  6.57s/it] 12%|â–ˆâ–        | 262/2256 [28:51<3:38:38,  6.58s/it] 12%|â–ˆâ–        | 263/2256 [28:58<3:38:31,  6.58s/it] 12%|â–ˆâ–        | 264/2256 [29:04<3:38:13,  6.57s/it] 12%|â–ˆâ–        | 265/2256 [29:11<3:38:08,  6.57s/it] 12%|â–ˆâ–        | 266/2256 [29:17<3:40:15,  6.64s/it] 12%|â–ˆâ–        | 267/2256 [29:24<3:39:16,  6.61s/it] 12%|â–ˆâ–        | 268/2256 [29:31<3:38:39,  6.60s/it] 12%|â–ˆâ–        | 269/2256 [29:37<3:38:21,  6.59s/it] 12%|â–ˆâ–        | 270/2256 [29:44<3:37:58,  6.59s/it]                                                    {'loss': 1.8041, 'learning_rate': 4.825362783489835e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 270/2256 [29:44<3:37:58,  6.59s/it]                                                    {'router_ce_loss': 1.0316182374954224, 'old_lang_expert0_score': '0.24 0.02 0.05 0.18 0.8 0.82 0.21 0.87 0.69 0.87 0.13 0.91 0.2 0.93 0.97 0.9 0.94 0.92 0.95 0.93 0.93 0.93 0.98 0.98', 'epoch': 0.12}
 12%|â–ˆâ–        | 270/2256 [29:44<3:37:58,  6.59s/it] 12%|â–ˆâ–        | 271/2256 [29:50<3:37:41,  6.58s/it] 12%|â–ˆâ–        | 272/2256 [29:57<3:37:35,  6.58s/it] 12%|â–ˆâ–        | 273/2256 [30:03<3:37:12,  6.57s/it] 12%|â–ˆâ–        | 274/2256 [30:10<3:37:09,  6.57s/it] 12%|â–ˆâ–        | 275/2256 [30:17<3:36:59,  6.57s/it] 12%|â–ˆâ–        | 276/2256 [30:23<3:36:49,  6.57s/it] 12%|â–ˆâ–        | 277/2256 [30:30<3:36:47,  6.57s/it] 12%|â–ˆâ–        | 278/2256 [30:36<3:36:41,  6.57s/it] 12%|â–ˆâ–        | 279/2256 [30:43<3:36:35,  6.57s/it] 12%|â–ˆâ–        | 280/2256 [30:49<3:36:27,  6.57s/it]                                                    {'loss': 1.7865, 'learning_rate': 4.8123543988695005e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 280/2256 [30:49<3:36:27,  6.57s/it]                                                    {'router_ce_loss': 1.0213117599487305, 'old_lang_expert0_score': '0.24 0.03 0.05 0.2 0.84 0.87 0.19 0.92 0.76 0.87 0.14 0.92 0.2 0.92 0.95 0.91 0.93 0.92 0.96 0.94 0.95 0.95 0.97 0.97', 'epoch': 0.12}
 12%|â–ˆâ–        | 280/2256 [30:50<3:36:27,  6.57s/it] 12%|â–ˆâ–        | 281/2256 [30:56<3:36:25,  6.57s/it] 12%|â–ˆâ–Ž        | 282/2256 [31:03<3:36:26,  6.58s/it] 13%|â–ˆâ–Ž        | 283/2256 [31:09<3:36:21,  6.58s/it] 13%|â–ˆâ–Ž        | 284/2256 [31:16<3:40:13,  6.70s/it] 13%|â–ˆâ–Ž        | 285/2256 [31:23<3:39:00,  6.67s/it] 13%|â–ˆâ–Ž        | 286/2256 [31:29<3:37:54,  6.64s/it] 13%|â–ˆâ–Ž        | 287/2256 [31:36<3:37:02,  6.61s/it] 13%|â–ˆâ–Ž        | 288/2256 [31:42<3:36:19,  6.60s/it] 13%|â–ˆâ–Ž        | 289/2256 [31:49<3:35:40,  6.58s/it] 13%|â–ˆâ–Ž        | 290/2256 [31:56<3:35:29,  6.58s/it]                                                    {'loss': 1.7585, 'learning_rate': 4.798897610812167e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 290/2256 [31:56<3:35:29,  6.58s/it]                                                    {'router_ce_loss': 1.0206354856491089, 'old_lang_expert0_score': '0.23 0.03 0.06 0.23 0.83 0.84 0.21 0.91 0.74 0.88 0.14 0.93 0.23 0.94 0.97 0.89 0.95 0.93 0.96 0.95 0.94 0.93 0.96 0.97', 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 290/2256 [31:56<3:35:29,  6.58s/it] 13%|â–ˆâ–Ž        | 291/2256 [32:02<3:35:25,  6.58s/it] 13%|â–ˆâ–Ž        | 292/2256 [32:09<3:35:17,  6.58s/it] 13%|â–ˆâ–Ž        | 293/2256 [32:15<3:35:12,  6.58s/it] 13%|â–ˆâ–Ž        | 294/2256 [32:22<3:34:55,  6.57s/it] 13%|â–ˆâ–Ž        | 295/2256 [32:28<3:34:31,  6.56s/it] 13%|â–ˆâ–Ž        | 296/2256 [32:35<3:34:29,  6.57s/it] 13%|â–ˆâ–Ž        | 297/2256 [32:42<3:34:29,  6.57s/it] 13%|â–ˆâ–Ž        | 298/2256 [32:48<3:34:13,  6.56s/it] 13%|â–ˆâ–Ž        | 299/2256 [32:55<3:34:12,  6.57s/it] 13%|â–ˆâ–Ž        | 300/2256 [33:01<3:34:23,  6.58s/it]                                                    {'loss': 1.7959, 'learning_rate': 4.784995028809707e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 300/2256 [33:01<3:34:23,  6.58s/it]                                                    {'router_ce_loss': 1.0208388566970825, 'old_lang_expert0_score': '0.24 0.03 0.05 0.21 0.82 0.86 0.23 0.91 0.77 0.9 0.15 0.92 0.23 0.93 0.96 0.89 0.93 0.92 0.95 0.95 0.94 0.94 0.97 0.98', 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 300/2256 [33:02<3:34:23,  6.58s/it] 13%|â–ˆâ–Ž        | 301/2256 [33:08<3:36:37,  6.65s/it] 13%|â–ˆâ–Ž        | 302/2256 [33:15<3:37:50,  6.69s/it] 13%|â–ˆâ–Ž        | 303/2256 [33:21<3:36:27,  6.65s/it] 13%|â–ˆâ–Ž        | 304/2256 [33:28<3:35:30,  6.62s/it] 14%|â–ˆâ–Ž        | 305/2256 [33:35<3:34:42,  6.60s/it] 14%|â–ˆâ–Ž        | 306/2256 [33:41<3:34:07,  6.59s/it] 14%|â–ˆâ–Ž        | 307/2256 [33:48<3:33:54,  6.58s/it] 14%|â–ˆâ–Ž        | 308/2256 [33:54<3:33:35,  6.58s/it] 14%|â–ˆâ–Ž        | 309/2256 [34:01<3:33:10,  6.57s/it] 14%|â–ˆâ–Ž        | 310/2256 [34:07<3:33:06,  6.57s/it]                                                    {'loss': 1.8004, 'learning_rate': 4.7706493488007545e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 310/2256 [34:07<3:33:06,  6.57s/it]                                                    {'router_ce_loss': 0.9884727001190186, 'old_lang_expert0_score': '0.24 0.03 0.06 0.23 0.89 0.94 0.28 0.96 0.89 0.94 0.2 0.96 0.26 0.97 0.98 0.95 0.96 0.95 0.97 0.96 0.97 0.97 0.98 0.98', 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 310/2256 [34:08<3:33:06,  6.57s/it] 14%|â–ˆâ–        | 311/2256 [34:14<3:32:53,  6.57s/it] 14%|â–ˆâ–        | 312/2256 [34:20<3:32:43,  6.57s/it] 14%|â–ˆâ–        | 313/2256 [34:27<3:32:39,  6.57s/it] 14%|â–ˆâ–        | 314/2256 [34:34<3:32:24,  6.56s/it] 14%|â–ˆâ–        | 315/2256 [34:40<3:32:14,  6.56s/it] 14%|â–ˆâ–        | 316/2256 [34:47<3:32:09,  6.56s/it] 14%|â–ˆâ–        | 317/2256 [34:53<3:32:05,  6.56s/it] 14%|â–ˆâ–        | 318/2256 [35:00<3:32:11,  6.57s/it] 14%|â–ˆâ–        | 319/2256 [35:07<3:34:29,  6.64s/it] 14%|â–ˆâ–        | 320/2256 [35:13<3:33:37,  6.62s/it]                                                    {'loss': 1.8199, 'learning_rate': 4.755863352647909e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 320/2256 [35:13<3:33:37,  6.62s/it]                                                    {'router_ce_loss': 1.019231915473938, 'old_lang_expert0_score': '0.25 0.06 0.07 0.21 0.81 0.83 0.22 0.91 0.76 0.9 0.16 0.93 0.23 0.94 0.96 0.91 0.92 0.92 0.93 0.93 0.91 0.94 0.97 0.98', 'epoch': 0.14}
 14%|â–ˆâ–        | 320/2256 [35:14<3:33:37,  6.62s/it] 14%|â–ˆâ–        | 321/2256 [35:20<3:33:06,  6.61s/it] 14%|â–ˆâ–        | 322/2256 [35:26<3:32:39,  6.60s/it] 14%|â–ˆâ–        | 323/2256 [35:33<3:32:25,  6.59s/it] 14%|â–ˆâ–        | 324/2256 [35:40<3:32:06,  6.59s/it] 14%|â–ˆâ–        | 325/2256 [35:46<3:31:58,  6.59s/it] 14%|â–ˆâ–        | 326/2256 [35:53<3:31:31,  6.58s/it] 14%|â–ˆâ–        | 327/2256 [35:59<3:31:13,  6.57s/it] 15%|â–ˆâ–        | 328/2256 [36:06<3:30:57,  6.57s/it] 15%|â–ˆâ–        | 329/2256 [36:12<3:30:58,  6.57s/it] 15%|â–ˆâ–        | 330/2256 [36:19<3:30:51,  6.57s/it]                                                    {'loss': 1.8224, 'learning_rate': 4.740639907598293e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 330/2256 [36:19<3:30:51,  6.57s/it]                                                    {'router_ce_loss': 0.9943665862083435, 'old_lang_expert0_score': '0.25 0.03 0.05 0.27 0.88 0.91 0.27 0.94 0.83 0.92 0.16 0.95 0.28 0.96 0.98 0.95 0.95 0.96 0.97 0.96 0.97 0.97 0.98 0.99', 'epoch': 0.15}
 15%|â–ˆâ–        | 330/2256 [36:19<3:30:51,  6.57s/it] 15%|â–ˆâ–        | 331/2256 [36:25<3:30:39,  6.57s/it] 15%|â–ˆâ–        | 332/2256 [36:32<3:30:29,  6.56s/it] 15%|â–ˆâ–        | 333/2256 [36:39<3:30:09,  6.56s/it] 15%|â–ˆâ–        | 334/2256 [36:45<3:30:20,  6.57s/it] 15%|â–ˆâ–        | 335/2256 [36:52<3:30:20,  6.57s/it] 15%|â–ˆâ–        | 336/2256 [36:58<3:30:22,  6.57s/it] 15%|â–ˆâ–        | 337/2256 [37:05<3:32:07,  6.63s/it] 15%|â–ˆâ–        | 338/2256 [37:12<3:33:18,  6.67s/it] 15%|â–ˆâ–Œ        | 339/2256 [37:18<3:32:01,  6.64s/it] 15%|â–ˆâ–Œ        | 340/2256 [37:25<3:31:15,  6.62s/it]                                                    {'loss': 1.8028, 'learning_rate': 4.724981965727542e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 340/2256 [37:25<3:31:15,  6.62s/it]                                                    {'router_ce_loss': 1.0153419971466064, 'old_lang_expert0_score': '0.24 0.03 0.05 0.22 0.84 0.88 0.22 0.94 0.79 0.9 0.17 0.93 0.25 0.93 0.96 0.88 0.94 0.91 0.96 0.94 0.93 0.94 0.97 0.98', 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 340/2256 [37:26<3:31:15,  6.62s/it] 15%|â–ˆâ–Œ        | 341/2256 [37:32<3:30:52,  6.61s/it] 15%|â–ˆâ–Œ        | 342/2256 [37:38<3:30:24,  6.60s/it] 15%|â–ˆâ–Œ        | 343/2256 [37:45<3:30:02,  6.59s/it] 15%|â–ˆâ–Œ        | 344/2256 [37:51<3:29:52,  6.59s/it] 15%|â–ˆâ–Œ        | 345/2256 [37:58<3:29:45,  6.59s/it] 15%|â–ˆâ–Œ        | 346/2256 [38:04<3:29:27,  6.58s/it] 15%|â–ˆâ–Œ        | 347/2256 [38:11<3:29:14,  6.58s/it] 15%|â–ˆâ–Œ        | 348/2256 [38:18<3:28:58,  6.57s/it] 15%|â–ˆâ–Œ        | 349/2256 [38:24<3:28:53,  6.57s/it] 16%|â–ˆâ–Œ        | 350/2256 [38:31<3:28:34,  6.57s/it]                                                    {'loss': 1.7867, 'learning_rate': 4.708892563367351e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 350/2256 [38:31<3:28:34,  6.57s/it]                                                    {'router_ce_loss': 1.0043251514434814, 'old_lang_expert0_score': '0.24 0.05 0.07 0.29 0.85 0.89 0.26 0.93 0.78 0.92 0.17 0.94 0.24 0.94 0.97 0.91 0.93 0.95 0.96 0.96 0.92 0.96 0.97 0.98', 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 350/2256 [38:31<3:28:34,  6.57s/it] 16%|â–ˆâ–Œ        | 351/2256 [38:37<3:28:28,  6.57s/it] 16%|â–ˆâ–Œ        | 352/2256 [38:44<3:28:33,  6.57s/it] 16%|â–ˆâ–Œ        | 353/2256 [38:50<3:28:31,  6.57s/it] 16%|â–ˆâ–Œ        | 354/2256 [38:57<3:28:02,  6.56s/it] 16%|â–ˆâ–Œ        | 355/2256 [39:04<3:30:13,  6.64s/it] 16%|â–ˆâ–Œ        | 356/2256 [39:11<3:31:29,  6.68s/it] 16%|â–ˆâ–Œ        | 357/2256 [39:17<3:30:15,  6.64s/it] 16%|â–ˆâ–Œ        | 358/2256 [39:24<3:29:27,  6.62s/it] 16%|â–ˆâ–Œ        | 359/2256 [39:30<3:28:57,  6.61s/it] 16%|â–ˆâ–Œ        | 360/2256 [39:37<3:28:30,  6.60s/it]                                                    {'loss': 1.7464, 'learning_rate': 4.692374820516679e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 360/2256 [39:37<3:28:30,  6.60s/it]                                                    {'router_ce_loss': 1.007871389389038, 'old_lang_expert0_score': '0.24 0.03 0.07 0.24 0.83 0.87 0.29 0.9 0.78 0.88 0.22 0.93 0.34 0.92 0.95 0.9 0.92 0.91 0.95 0.95 0.94 0.94 0.97 0.98', 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 360/2256 [39:37<3:28:30,  6.60s/it] 16%|â–ˆâ–Œ        | 361/2256 [39:43<3:28:11,  6.59s/it] 16%|â–ˆâ–Œ        | 362/2256 [39:50<3:27:43,  6.58s/it] 16%|â–ˆâ–Œ        | 363/2256 [39:57<3:27:23,  6.57s/it] 16%|â–ˆâ–Œ        | 364/2256 [40:03<3:27:10,  6.57s/it] 16%|â–ˆâ–Œ        | 365/2256 [40:10<3:27:13,  6.58s/it] 16%|â–ˆâ–Œ        | 366/2256 [40:16<3:27:02,  6.57s/it] 16%|â–ˆâ–‹        | 367/2256 [40:23<3:26:39,  6.56s/it] 16%|â–ˆâ–‹        | 368/2256 [40:29<3:26:40,  6.57s/it] 16%|â–ˆâ–‹        | 369/2256 [40:36<3:26:42,  6.57s/it] 16%|â–ˆâ–‹        | 370/2256 [40:43<3:26:33,  6.57s/it]                                                    {'loss': 1.8357, 'learning_rate': 4.675431940236731e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 370/2256 [40:43<3:26:33,  6.57s/it]                                                    {'router_ce_loss': 0.9949169158935547, 'old_lang_expert0_score': '0.25 0.03 0.06 0.25 0.88 0.92 0.26 0.95 0.82 0.92 0.15 0.95 0.26 0.95 0.97 0.94 0.95 0.96 0.97 0.97 0.98 0.97 0.98 0.99', 'epoch': 0.16}
 16%|â–ˆâ–‹        | 370/2256 [40:43<3:26:33,  6.57s/it] 16%|â–ˆâ–‹        | 371/2256 [40:49<3:26:30,  6.57s/it] 16%|â–ˆâ–‹        | 372/2256 [40:56<3:26:30,  6.58s/it] 17%|â–ˆâ–‹        | 373/2256 [41:02<3:28:24,  6.64s/it] 17%|â–ˆâ–‹        | 374/2256 [41:09<3:27:32,  6.62s/it] 17%|â–ˆâ–‹        | 375/2256 [41:16<3:26:52,  6.60s/it] 17%|â–ˆâ–‹        | 376/2256 [41:22<3:26:41,  6.60s/it] 17%|â–ˆâ–‹        | 377/2256 [41:29<3:26:18,  6.59s/it] 17%|â–ˆâ–‹        | 378/2256 [41:35<3:25:57,  6.58s/it] 17%|â–ˆâ–‹        | 379/2256 [41:42<3:25:35,  6.57s/it] 17%|â–ˆâ–‹        | 380/2256 [41:48<3:25:26,  6.57s/it]                                                    {'loss': 1.853, 'learning_rate': 4.65806720802983e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 380/2256 [41:48<3:25:26,  6.57s/it]                                                    {'router_ce_loss': 0.9884063005447388, 'old_lang_expert0_score': '0.26 0.04 0.06 0.24 0.86 0.91 0.3 0.96 0.83 0.94 0.2 0.96 0.26 0.97 0.99 0.96 0.97 0.97 0.98 0.97 0.96 0.97 0.99 0.99', 'epoch': 0.17}
 17%|â–ˆâ–‹        | 380/2256 [41:49<3:25:26,  6.57s/it] 17%|â–ˆâ–‹        | 381/2256 [41:55<3:25:20,  6.57s/it] 17%|â–ˆâ–‹        | 382/2256 [42:02<3:25:18,  6.57s/it] 17%|â–ˆâ–‹        | 383/2256 [42:08<3:25:04,  6.57s/it] 17%|â–ˆâ–‹        | 384/2256 [42:15<3:25:04,  6.57s/it] 17%|â–ˆâ–‹        | 385/2256 [42:21<3:24:53,  6.57s/it] 17%|â–ˆâ–‹        | 386/2256 [42:28<3:24:59,  6.58s/it] 17%|â–ˆâ–‹        | 387/2256 [42:34<3:24:44,  6.57s/it] 17%|â–ˆâ–‹        | 388/2256 [42:41<3:24:46,  6.58s/it] 17%|â–ˆâ–‹        | 389/2256 [42:48<3:24:24,  6.57s/it] 17%|â–ˆâ–‹        | 390/2256 [42:54<3:24:13,  6.57s/it]                                                    {'loss': 1.7698, 'learning_rate': 4.640283991202306e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 390/2256 [42:54<3:24:13,  6.57s/it]                                                    {'router_ce_loss': 1.0011886358261108, 'old_lang_expert0_score': '0.26 0.03 0.05 0.24 0.87 0.89 0.26 0.93 0.82 0.91 0.2 0.93 0.28 0.92 0.97 0.92 0.94 0.94 0.96 0.95 0.96 0.97 0.98 0.98', 'epoch': 0.17}
 17%|â–ˆâ–‹        | 390/2256 [42:55<3:24:13,  6.57s/it] 17%|â–ˆâ–‹        | 391/2256 [43:01<3:28:17,  6.70s/it] 17%|â–ˆâ–‹        | 392/2256 [43:08<3:26:57,  6.66s/it] 17%|â–ˆâ–‹        | 393/2256 [43:14<3:25:56,  6.63s/it] 17%|â–ˆâ–‹        | 394/2256 [43:21<3:25:14,  6.61s/it] 18%|â–ˆâ–Š        | 395/2256 [43:27<3:24:28,  6.59s/it] 18%|â–ˆâ–Š        | 396/2256 [43:34<3:24:07,  6.58s/it] 18%|â–ˆâ–Š        | 397/2256 [43:41<3:23:57,  6.58s/it] 18%|â–ˆâ–Š        | 398/2256 [43:47<3:23:37,  6.58s/it] 18%|â–ˆâ–Š        | 399/2256 [43:54<3:23:23,  6.57s/it] 18%|â–ˆâ–Š        | 400/2256 [44:00<3:23:13,  6.57s/it]                                                    {'loss': 1.7135, 'learning_rate': 4.622085738211518e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 400/2256 [44:00<3:23:13,  6.57s/it]                                                    {'router_ce_loss': 1.0089726448059082, 'old_lang_expert0_score': '0.24 0.02 0.07 0.22 0.8 0.85 0.23 0.92 0.81 0.9 0.21 0.95 0.37 0.93 0.97 0.9 0.93 0.93 0.95 0.96 0.94 0.93 0.97 0.98', 'epoch': 0.18}
 18%|â–ˆâ–Š        | 400/2256 [44:01<3:23:13,  6.57s/it] 18%|â–ˆâ–Š        | 401/2256 [44:07<3:23:05,  6.57s/it] 18%|â–ˆâ–Š        | 402/2256 [44:13<3:22:52,  6.57s/it] 18%|â–ˆâ–Š        | 403/2256 [44:20<3:22:46,  6.57s/it] 18%|â–ˆâ–Š        | 404/2256 [44:26<3:22:39,  6.57s/it] 18%|â–ˆâ–Š        | 405/2256 [44:33<3:22:22,  6.56s/it] 18%|â–ˆâ–Š        | 406/2256 [44:40<3:22:17,  6.56s/it] 18%|â–ˆâ–Š        | 407/2256 [44:46<3:22:15,  6.56s/it] 18%|â–ˆâ–Š        | 408/2256 [44:53<3:24:21,  6.63s/it] 18%|â–ˆâ–Š        | 409/2256 [45:00<3:25:15,  6.67s/it] 18%|â–ˆâ–Š        | 410/2256 [45:06<3:24:21,  6.64s/it]                                                    {'loss': 1.7714, 'learning_rate': 4.603475977997147e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 410/2256 [45:06<3:24:21,  6.64s/it]                                                    {'router_ce_loss': 0.9912523627281189, 'old_lang_expert0_score': '0.27 0.04 0.06 0.19 0.8 0.86 0.28 0.94 0.84 0.92 0.34 0.95 0.49 0.94 0.96 0.91 0.93 0.92 0.95 0.95 0.95 0.93 0.98 0.97', 'epoch': 0.18}
 18%|â–ˆâ–Š        | 410/2256 [45:07<3:24:21,  6.64s/it] 18%|â–ˆâ–Š        | 411/2256 [45:13<3:23:26,  6.62s/it] 18%|â–ˆâ–Š        | 412/2256 [45:19<3:22:41,  6.60s/it] 18%|â–ˆâ–Š        | 413/2256 [45:26<3:22:19,  6.59s/it] 18%|â–ˆâ–Š        | 414/2256 [45:33<3:22:05,  6.58s/it] 18%|â–ˆâ–Š        | 415/2256 [45:39<3:21:56,  6.58s/it] 18%|â–ˆâ–Š        | 416/2256 [45:46<3:21:42,  6.58s/it] 18%|â–ˆâ–Š        | 417/2256 [45:52<3:21:26,  6.57s/it] 19%|â–ˆâ–Š        | 418/2256 [45:59<3:21:18,  6.57s/it] 19%|â–ˆâ–Š        | 419/2256 [46:05<3:20:59,  6.56s/it] 19%|â–ˆâ–Š        | 420/2256 [46:12<3:20:55,  6.57s/it]                                                    {'loss': 1.7466, 'learning_rate': 4.584458319296868e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 420/2256 [46:12<3:20:55,  6.57s/it]                                                    {'router_ce_loss': 0.9876816272735596, 'old_lang_expert0_score': '0.23 0.03 0.04 0.23 0.89 0.93 0.27 0.95 0.86 0.93 0.23 0.96 0.36 0.97 0.97 0.94 0.95 0.96 0.98 0.96 0.97 0.96 0.98 0.99', 'epoch': 0.19}
 19%|â–ˆâ–Š        | 420/2256 [46:12<3:20:55,  6.57s/it] 19%|â–ˆâ–Š        | 421/2256 [46:18<3:20:48,  6.57s/it] 19%|â–ˆâ–Š        | 422/2256 [46:25<3:20:38,  6.56s/it] 19%|â–ˆâ–‰        | 423/2256 [46:32<3:20:29,  6.56s/it] 19%|â–ˆâ–‰        | 424/2256 [46:38<3:20:25,  6.56s/it] 19%|â–ˆâ–‰        | 425/2256 [46:45<3:20:20,  6.57s/it] 19%|â–ˆâ–‰        | 426/2256 [46:52<3:22:15,  6.63s/it] 19%|â–ˆâ–‰        | 427/2256 [46:58<3:21:26,  6.61s/it] 19%|â–ˆâ–‰        | 428/2256 [47:05<3:20:51,  6.59s/it] 19%|â–ˆâ–‰        | 429/2256 [47:11<3:20:24,  6.58s/it] 19%|â–ˆâ–‰        | 430/2256 [47:18<3:20:05,  6.57s/it]                                                    {'loss': 1.7259, 'learning_rate': 4.5650364499465636e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 430/2256 [47:18<3:20:05,  6.57s/it]                                                    {'router_ce_loss': 0.9932553768157959, 'old_lang_expert0_score': '0.25 0.03 0.05 0.24 0.87 0.89 0.25 0.94 0.87 0.93 0.18 0.96 0.35 0.95 0.97 0.94 0.95 0.95 0.97 0.96 0.96 0.96 0.99 0.99', 'epoch': 0.19}
 19%|â–ˆâ–‰        | 430/2256 [47:18<3:20:05,  6.57s/it] 19%|â–ˆâ–‰        | 431/2256 [47:24<3:19:42,  6.57s/it] 19%|â–ˆâ–‰        | 432/2256 [47:31<3:19:34,  6.57s/it] 19%|â–ˆâ–‰        | 433/2256 [47:37<3:19:27,  6.56s/it] 19%|â–ˆâ–‰        | 434/2256 [47:44<3:19:18,  6.56s/it] 19%|â–ˆâ–‰        | 435/2256 [47:51<3:19:12,  6.56s/it] 19%|â–ˆâ–‰        | 436/2256 [47:57<3:19:05,  6.56s/it] 19%|â–ˆâ–‰        | 437/2256 [48:04<3:18:59,  6.56s/it] 19%|â–ˆâ–‰        | 438/2256 [48:10<3:18:50,  6.56s/it] 19%|â–ˆâ–‰        | 439/2256 [48:17<3:18:48,  6.57s/it] 20%|â–ˆâ–‰        | 440/2256 [48:23<3:18:36,  6.56s/it]                                                    {'loss': 1.7617, 'learning_rate': 4.54521413616519e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 440/2256 [48:23<3:18:36,  6.56s/it]                                                    {'router_ce_loss': 0.9922430515289307, 'old_lang_expert0_score': '0.24 0.03 0.07 0.27 0.86 0.91 0.28 0.94 0.85 0.93 0.24 0.94 0.32 0.93 0.97 0.92 0.95 0.94 0.96 0.96 0.97 0.96 0.98 0.99', 'epoch': 0.2}
 20%|â–ˆâ–‰        | 440/2256 [48:24<3:18:36,  6.56s/it] 20%|â–ˆâ–‰        | 441/2256 [48:30<3:18:43,  6.57s/it] 20%|â–ˆâ–‰        | 442/2256 [48:37<3:18:40,  6.57s/it] 20%|â–ˆâ–‰        | 443/2256 [48:43<3:18:37,  6.57s/it] 20%|â–ˆâ–‰        | 444/2256 [48:50<3:22:32,  6.71s/it] 20%|â–ˆâ–‰        | 445/2256 [48:57<3:21:11,  6.67s/it] 20%|â–ˆâ–‰        | 446/2256 [49:03<3:20:03,  6.63s/it] 20%|â–ˆâ–‰        | 447/2256 [49:10<3:19:10,  6.61s/it] 20%|â–ˆâ–‰        | 448/2256 [49:16<3:18:43,  6.59s/it] 20%|â–ˆâ–‰        | 449/2256 [49:23<3:18:22,  6.59s/it] 20%|â–ˆâ–‰        | 450/2256 [49:29<3:17:56,  6.58s/it]                                                    {'loss': 1.7838, 'learning_rate': 4.5249952218244453e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 450/2256 [49:29<3:17:56,  6.58s/it]                                                    {'router_ce_loss': 1.0072640180587769, 'old_lang_expert0_score': '0.25 0.02 0.04 0.21 0.86 0.88 0.22 0.94 0.84 0.92 0.16 0.94 0.24 0.93 0.96 0.92 0.93 0.94 0.97 0.95 0.97 0.97 0.98 0.99', 'epoch': 0.2}
 20%|â–ˆâ–‰        | 450/2256 [49:30<3:17:56,  6.58s/it] 20%|â–ˆâ–‰        | 451/2256 [49:36<3:17:51,  6.58s/it] 20%|â–ˆâ–ˆ        | 452/2256 [49:43<3:17:42,  6.58s/it] 20%|â–ˆâ–ˆ        | 453/2256 [49:49<3:17:33,  6.57s/it] 20%|â–ˆâ–ˆ        | 454/2256 [49:56<3:17:25,  6.57s/it] 20%|â–ˆâ–ˆ        | 455/2256 [50:02<3:17:22,  6.58s/it] 20%|â–ˆâ–ˆ        | 456/2256 [50:09<3:17:10,  6.57s/it] 20%|â–ˆâ–ˆ        | 457/2256 [50:16<3:17:05,  6.57s/it] 20%|â–ˆâ–ˆ        | 458/2256 [50:22<3:16:59,  6.57s/it] 20%|â–ˆâ–ˆ        | 459/2256 [50:29<3:16:59,  6.58s/it] 20%|â–ˆâ–ˆ        | 460/2256 [50:35<3:16:35,  6.57s/it]                                                    {'loss': 1.9857, 'learning_rate': 4.5043836277033776e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 460/2256 [50:35<3:16:35,  6.57s/it]                                                    {'router_ce_loss': 0.9987496733665466, 'old_lang_expert0_score': '0.24 0.02 0.07 0.26 0.83 0.85 0.29 0.93 0.83 0.89 0.32 0.93 0.44 0.91 0.95 0.87 0.91 0.9 0.94 0.93 0.94 0.95 0.97 0.98', 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 460/2256 [50:36<3:16:35,  6.57s/it] 20%|â–ˆâ–ˆ        | 461/2256 [50:42<3:18:23,  6.63s/it] 20%|â–ˆâ–ˆ        | 462/2256 [50:49<3:19:33,  6.67s/it] 21%|â–ˆâ–ˆ        | 463/2256 [50:55<3:18:29,  6.64s/it] 21%|â–ˆâ–ˆ        | 464/2256 [51:02<3:17:38,  6.62s/it] 21%|â–ˆâ–ˆ        | 465/2256 [51:08<3:17:11,  6.61s/it] 21%|â–ˆâ–ˆ        | 466/2256 [51:15<3:16:36,  6.59s/it] 21%|â–ˆâ–ˆ        | 467/2256 [51:22<3:16:17,  6.58s/it] 21%|â–ˆâ–ˆ        | 468/2256 [51:28<3:16:07,  6.58s/it] 21%|â–ˆâ–ˆ        | 469/2256 [51:35<3:15:42,  6.57s/it] 21%|â–ˆâ–ˆ        | 470/2256 [51:41<3:15:36,  6.57s/it]                                                    {'loss': 1.7789, 'learning_rate': 4.4833833507280884e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 470/2256 [51:41<3:15:36,  6.57s/it]                                                    {'router_ce_loss': 1.022929310798645, 'old_lang_expert0_score': '0.27 0.03 0.02 0.11 0.77 0.74 0.21 0.88 0.78 0.89 0.24 0.93 0.4 0.92 0.96 0.91 0.9 0.91 0.95 0.94 0.93 0.93 0.98 0.97', 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 470/2256 [51:42<3:15:36,  6.57s/it] 21%|â–ˆâ–ˆ        | 471/2256 [51:48<3:15:19,  6.57s/it] 21%|â–ˆâ–ˆ        | 472/2256 [51:54<3:15:10,  6.56s/it] 21%|â–ˆâ–ˆ        | 473/2256 [52:01<3:15:07,  6.57s/it] 21%|â–ˆâ–ˆ        | 474/2256 [52:08<3:15:03,  6.57s/it] 21%|â–ˆâ–ˆ        | 475/2256 [52:14<3:14:48,  6.56s/it] 21%|â–ˆâ–ˆ        | 476/2256 [52:21<3:14:46,  6.57s/it] 21%|â–ˆâ–ˆ        | 477/2256 [52:27<3:14:38,  6.56s/it] 21%|â–ˆâ–ˆ        | 478/2256 [52:34<3:14:37,  6.57s/it] 21%|â–ˆâ–ˆ        | 479/2256 [52:41<3:16:31,  6.64s/it] 21%|â–ˆâ–ˆâ–       | 480/2256 [52:47<3:16:01,  6.62s/it]                                                    {'loss': 1.7644, 'learning_rate': 4.4619984631966524e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 480/2256 [52:47<3:16:01,  6.62s/it]                                                    {'router_ce_loss': 0.9982028603553772, 'old_lang_expert0_score': '0.22 0.02 0.03 0.22 0.87 0.87 0.28 0.94 0.86 0.94 0.27 0.93 0.42 0.94 0.97 0.88 0.92 0.93 0.96 0.94 0.95 0.94 0.98 0.98', 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 480/2256 [52:48<3:16:01,  6.62s/it] 21%|â–ˆâ–ˆâ–       | 481/2256 [52:54<3:15:22,  6.60s/it] 21%|â–ˆâ–ˆâ–       | 482/2256 [53:00<3:14:55,  6.59s/it] 21%|â–ˆâ–ˆâ–       | 483/2256 [53:07<3:14:36,  6.59s/it] 21%|â–ˆâ–ˆâ–       | 484/2256 [53:13<3:14:20,  6.58s/it] 21%|â–ˆâ–ˆâ–       | 485/2256 [53:20<3:14:15,  6.58s/it] 22%|â–ˆâ–ˆâ–       | 486/2256 [53:27<3:14:08,  6.58s/it] 22%|â–ˆâ–ˆâ–       | 487/2256 [53:33<3:13:45,  6.57s/it] 22%|â–ˆâ–ˆâ–       | 488/2256 [53:40<3:13:32,  6.57s/it] 22%|â–ˆâ–ˆâ–       | 489/2256 [53:46<3:13:27,  6.57s/it] 22%|â–ˆâ–ˆâ–       | 490/2256 [53:53<3:13:09,  6.56s/it]                                                    {'loss': 1.7417, 'learning_rate': 4.4402331119894435e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 490/2256 [53:53<3:13:09,  6.56s/it]                                                    {'router_ce_loss': 0.9909898638725281, 'old_lang_expert0_score': '0.23 0.03 0.06 0.33 0.84 0.9 0.36 0.94 0.86 0.92 0.24 0.93 0.28 0.94 0.96 0.87 0.94 0.95 0.96 0.97 0.97 0.97 0.98 0.99', 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 490/2256 [53:53<3:13:09,  6.56s/it] 22%|â–ˆâ–ˆâ–       | 491/2256 [53:59<3:13:08,  6.57s/it] 22%|â–ˆâ–ˆâ–       | 492/2256 [54:06<3:13:01,  6.57s/it] 22%|â–ˆâ–ˆâ–       | 493/2256 [54:13<3:12:48,  6.56s/it] 22%|â–ˆâ–ˆâ–       | 494/2256 [54:19<3:12:53,  6.57s/it] 22%|â–ˆâ–ˆâ–       | 495/2256 [54:26<3:12:39,  6.56s/it] 22%|â–ˆâ–ˆâ–       | 496/2256 [54:32<3:12:39,  6.57s/it] 22%|â–ˆâ–ˆâ–       | 497/2256 [54:39<3:16:17,  6.70s/it] 22%|â–ˆâ–ˆâ–       | 498/2256 [54:46<3:15:00,  6.66s/it] 22%|â–ˆâ–ˆâ–       | 499/2256 [54:52<3:14:09,  6.63s/it] 22%|â–ˆâ–ˆâ–       | 500/2256 [54:59<3:13:18,  6.61s/it]                                                    {'loss': 1.8172, 'learning_rate': 4.4180915177649775e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 500/2256 [54:59<3:13:18,  6.61s/it]                                                    {'router_ce_loss': 0.9877811074256897, 'old_lang_expert0_score': '0.25 0.03 0.05 0.25 0.87 0.9 0.31 0.94 0.85 0.93 0.21 0.96 0.36 0.95 0.98 0.94 0.96 0.96 0.98 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 500/2256 [54:59<3:13:18,  6.61s/it] 22%|â–ˆâ–ˆâ–       | 501/2256 [55:05<3:12:54,  6.59s/it] 22%|â–ˆâ–ˆâ–       | 502/2256 [55:12<3:12:29,  6.58s/it] 22%|â–ˆâ–ˆâ–       | 503/2256 [55:19<3:12:15,  6.58s/it] 22%|â–ˆâ–ˆâ–       | 504/2256 [55:25<3:11:46,  6.57s/it] 22%|â–ˆâ–ˆâ–       | 505/2256 [55:32<3:11:37,  6.57s/it] 22%|â–ˆâ–ˆâ–       | 506/2256 [55:38<3:11:31,  6.57s/it] 22%|â–ˆâ–ˆâ–       | 507/2256 [55:45<3:11:24,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 508/2256 [55:51<3:11:25,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 509/2256 [55:58<3:11:24,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 510/2256 [56:05<3:11:14,  6.57s/it]                                                    {'loss': 1.7185, 'learning_rate': 4.395577974141464e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 510/2256 [56:05<3:11:14,  6.57s/it]                                                    {'router_ce_loss': 1.0003341436386108, 'old_lang_expert0_score': '0.26 0.03 0.06 0.2 0.85 0.85 0.3 0.94 0.81 0.92 0.23 0.94 0.36 0.93 0.97 0.9 0.92 0.91 0.96 0.94 0.96 0.95 0.98 0.98', 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 510/2256 [56:05<3:11:14,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 511/2256 [56:11<3:11:06,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 512/2256 [56:18<3:10:53,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 513/2256 [56:24<3:10:48,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 514/2256 [56:31<3:12:33,  6.63s/it] 23%|â–ˆâ–ˆâ–Ž       | 515/2256 [56:38<3:13:33,  6.67s/it] 23%|â–ˆâ–ˆâ–Ž       | 516/2256 [56:44<3:12:21,  6.63s/it] 23%|â–ˆâ–ˆâ–Ž       | 517/2256 [56:51<3:11:30,  6.61s/it] 23%|â–ˆâ–ˆâ–Ž       | 518/2256 [56:58<3:11:12,  6.60s/it] 23%|â–ˆâ–ˆâ–Ž       | 519/2256 [57:04<3:10:48,  6.59s/it] 23%|â–ˆâ–ˆâ–Ž       | 520/2256 [57:11<3:10:37,  6.59s/it]                                                    {'loss': 1.7874, 'learning_rate': 4.372696846864197e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 520/2256 [57:11<3:10:37,  6.59s/it]                                                    {'router_ce_loss': 0.9879800081253052, 'old_lang_expert0_score': '0.26 0.05 0.04 0.18 0.84 0.86 0.26 0.95 0.84 0.94 0.32 0.96 0.48 0.94 0.97 0.93 0.93 0.92 0.97 0.94 0.95 0.96 0.99 0.98', 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 520/2256 [57:11<3:10:37,  6.59s/it] 23%|â–ˆâ–ˆâ–Ž       | 521/2256 [57:17<3:10:10,  6.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 522/2256 [57:24<3:10:06,  6.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 523/2256 [57:30<3:09:58,  6.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 524/2256 [57:37<3:09:39,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 525/2256 [57:44<3:09:37,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 526/2256 [57:50<3:09:23,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 527/2256 [57:57<3:09:21,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 528/2256 [58:03<3:09:21,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 529/2256 [58:10<3:08:58,  6.57s/it] 23%|â–ˆâ–ˆâ–Ž       | 530/2256 [58:16<3:09:05,  6.57s/it]                                                    {'loss': 1.7807, 'learning_rate': 4.349452572958974e-05, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 530/2256 [58:16<3:09:05,  6.57s/it]                                                    {'router_ce_loss': 0.9934040904045105, 'old_lang_expert0_score': '0.24 0.02 0.03 0.16 0.89 0.91 0.2 0.96 0.83 0.93 0.19 0.96 0.41 0.95 0.97 0.96 0.96 0.94 0.98 0.97 0.97 0.98 0.99 0.99', 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 530/2256 [58:17<3:09:05,  6.57s/it] 24%|â–ˆâ–ˆâ–Ž       | 531/2256 [58:23<3:08:45,  6.57s/it] 24%|â–ˆâ–ˆâ–Ž       | 532/2256 [58:30<3:10:42,  6.64s/it] 24%|â–ˆâ–ˆâ–Ž       | 533/2256 [58:36<3:10:01,  6.62s/it] 24%|â–ˆâ–ˆâ–Ž       | 534/2256 [58:43<3:09:23,  6.60s/it] 24%|â–ˆâ–ˆâ–Ž       | 535/2256 [58:49<3:08:55,  6.59s/it] 24%|â–ˆâ–ˆâ–       | 536/2256 [58:56<3:08:44,  6.58s/it] 24%|â–ˆâ–ˆâ–       | 537/2256 [59:03<3:08:35,  6.58s/it] 24%|â–ˆâ–ˆâ–       | 538/2256 [59:09<3:08:11,  6.57s/it] 24%|â–ˆâ–ˆâ–       | 539/2256 [59:16<3:08:10,  6.58s/it] 24%|â–ˆâ–ˆâ–       | 540/2256 [59:22<3:08:03,  6.58s/it]                                                    {'loss': 1.8484, 'learning_rate': 4.3258496598716736e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 540/2256 [59:22<3:08:03,  6.58s/it]                                                    {'router_ce_loss': 0.9843623638153076, 'old_lang_expert0_score': '0.25 0.03 0.06 0.24 0.85 0.91 0.31 0.95 0.86 0.94 0.28 0.96 0.4 0.95 0.98 0.91 0.95 0.94 0.95 0.97 0.95 0.96 0.98 0.99', 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 540/2256 [59:23<3:08:03,  6.58s/it] 24%|â–ˆâ–ˆâ–       | 541/2256 [59:29<3:07:49,  6.57s/it] 24%|â–ˆâ–ˆâ–       | 542/2256 [59:35<3:07:28,  6.56s/it] 24%|â–ˆâ–ˆâ–       | 543/2256 [59:42<3:07:22,  6.56s/it] 24%|â–ˆâ–ˆâ–       | 544/2256 [59:48<3:07:17,  6.56s/it] 24%|â–ˆâ–ˆâ–       | 545/2256 [59:55<3:07:15,  6.57s/it] 24%|â–ˆâ–ˆâ–       | 546/2256 [1:00:02<3:06:56,  6.56s/it] 24%|â–ˆâ–ˆâ–       | 547/2256 [1:00:08<3:06:55,  6.56s/it] 24%|â–ˆâ–ˆâ–       | 548/2256 [1:00:15<3:06:51,  6.56s/it] 24%|â–ˆâ–ˆâ–       | 549/2256 [1:00:21<3:06:44,  6.56s/it] 24%|â–ˆâ–ˆâ–       | 550/2256 [1:00:28<3:08:24,  6.63s/it]                                                      {'loss': 1.7538, 'learning_rate': 4.301892684594195e-05, 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 550/2256 [1:00:28<3:08:24,  6.63s/it]                                                      {'router_ce_loss': 0.9765437245368958, 'old_lang_expert0_score': '0.22 0.05 0.09 0.2 0.8 0.84 0.29 0.96 0.81 0.93 0.45 0.98 0.76 0.97 0.98 0.91 0.94 0.93 0.95 0.91 0.92 0.88 0.96 0.98', 'epoch': 0.24}
 24%|â–ˆâ–ˆâ–       | 550/2256 [1:00:29<3:08:24,  6.63s/it] 24%|â–ˆâ–ˆâ–       | 551/2256 [1:00:35<3:09:34,  6.67s/it] 24%|â–ˆâ–ˆâ–       | 552/2256 [1:00:41<3:08:30,  6.64s/it] 25%|â–ˆâ–ˆâ–       | 553/2256 [1:00:48<3:07:41,  6.61s/it] 25%|â–ˆâ–ˆâ–       | 554/2256 [1:00:55<3:07:17,  6.60s/it] 25%|â–ˆâ–ˆâ–       | 555/2256 [1:01:01<3:06:54,  6.59s/it] 25%|â–ˆâ–ˆâ–       | 556/2256 [1:01:08<3:06:39,  6.59s/it] 25%|â–ˆâ–ˆâ–       | 557/2256 [1:01:14<3:06:23,  6.58s/it] 25%|â–ˆâ–ˆâ–       | 558/2256 [1:01:21<3:05:55,  6.57s/it] 25%|â–ˆâ–ˆâ–       | 559/2256 [1:01:27<3:05:41,  6.57s/it] 25%|â–ˆâ–ˆâ–       | 560/2256 [1:01:34<3:05:33,  6.56s/it]                                                      {'loss': 1.8496, 'learning_rate': 4.2775862927769025e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 560/2256 [1:01:34<3:05:33,  6.56s/it]                                                      {'router_ce_loss': 0.9872972965240479, 'old_lang_expert0_score': '0.24 0.03 0.06 0.22 0.87 0.86 0.29 0.93 0.84 0.93 0.32 0.95 0.52 0.94 0.97 0.91 0.95 0.94 0.96 0.96 0.94 0.94 0.98 0.99', 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–       | 560/2256 [1:01:34<3:05:33,  6.56s/it] 25%|â–ˆâ–ˆâ–       | 561/2256 [1:01:40<3:05:32,  6.57s/it] 25%|â–ˆâ–ˆâ–       | 562/2256 [1:01:47<3:05:20,  6.56s/it] 25%|â–ˆâ–ˆâ–       | 563/2256 [1:01:54<3:05:11,  6.56s/it] 25%|â–ˆâ–ˆâ–Œ       | 564/2256 [1:02:00<3:05:01,  6.56s/it] 25%|â–ˆâ–ˆâ–Œ       | 565/2256 [1:02:07<3:05:03,  6.57s/it] 25%|â–ˆâ–ˆâ–Œ       | 566/2256 [1:02:13<3:05:02,  6.57s/it] 25%|â–ˆâ–ˆâ–Œ       | 567/2256 [1:02:20<3:04:59,  6.57s/it] 25%|â–ˆâ–ˆâ–Œ       | 568/2256 [1:02:27<3:06:38,  6.63s/it] 25%|â–ˆâ–ˆâ–Œ       | 569/2256 [1:02:33<3:07:21,  6.66s/it] 25%|â–ˆâ–ˆâ–Œ       | 570/2256 [1:02:40<3:06:25,  6.63s/it]                                                      {'loss': 1.8573, 'learning_rate': 4.2529351978277564e-05, 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 570/2256 [1:02:40<3:06:25,  6.63s/it]                                                      {'router_ce_loss': 0.9814370274543762, 'old_lang_expert0_score': '0.25 0.02 0.05 0.25 0.88 0.9 0.29 0.95 0.89 0.93 0.3 0.95 0.45 0.94 0.97 0.94 0.95 0.96 0.97 0.95 0.96 0.95 0.98 0.99', 'epoch': 0.25}
 25%|â–ˆâ–ˆâ–Œ       | 570/2256 [1:02:40<3:06:25,  6.63s/it] 25%|â–ˆâ–ˆâ–Œ       | 571/2256 [1:02:47<3:05:37,  6.61s/it] 25%|â–ˆâ–ˆâ–Œ       | 572/2256 [1:02:53<3:05:08,  6.60s/it] 25%|â–ˆâ–ˆâ–Œ       | 573/2256 [1:03:00<3:04:40,  6.58s/it] 25%|â–ˆâ–ˆâ–Œ       | 574/2256 [1:03:06<3:04:27,  6.58s/it] 25%|â–ˆâ–ˆâ–Œ       | 575/2256 [1:03:13<3:04:18,  6.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 576/2256 [1:03:19<3:03:53,  6.57s/it] 26%|â–ˆâ–ˆâ–Œ       | 577/2256 [1:03:26<3:03:41,  6.56s/it] 26%|â–ˆâ–ˆâ–Œ       | 578/2256 [1:03:32<3:03:38,  6.57s/it] 26%|â–ˆâ–ˆâ–Œ       | 579/2256 [1:03:39<3:03:30,  6.57s/it] 26%|â–ˆâ–ˆâ–Œ       | 580/2256 [1:03:46<3:03:20,  6.56s/it]                                                      {'loss': 1.7259, 'learning_rate': 4.227944179998308e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 580/2256 [1:03:46<3:03:20,  6.56s/it]                                                      {'router_ce_loss': 0.9913715720176697, 'old_lang_expert0_score': '0.23 0.02 0.04 0.19 0.84 0.83 0.29 0.92 0.85 0.92 0.3 0.95 0.53 0.95 0.98 0.92 0.96 0.95 0.93 0.96 0.95 0.94 0.99 0.98', 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 580/2256 [1:03:46<3:03:20,  6.56s/it] 26%|â–ˆâ–ˆâ–Œ       | 581/2256 [1:03:52<3:03:16,  6.56s/it] 26%|â–ˆâ–ˆâ–Œ       | 582/2256 [1:03:59<3:03:05,  6.56s/it] 26%|â–ˆâ–ˆâ–Œ       | 583/2256 [1:04:05<3:02:59,  6.56s/it] 26%|â–ˆâ–ˆâ–Œ       | 584/2256 [1:04:12<3:02:54,  6.56s/it] 26%|â–ˆâ–ˆâ–Œ       | 585/2256 [1:04:18<3:02:41,  6.56s/it] 26%|â–ˆâ–ˆâ–Œ       | 586/2256 [1:04:25<3:04:25,  6.63s/it] 26%|â–ˆâ–ˆâ–Œ       | 587/2256 [1:04:32<3:03:40,  6.60s/it] 26%|â–ˆâ–ˆâ–Œ       | 588/2256 [1:04:38<3:03:15,  6.59s/it] 26%|â–ˆâ–ˆâ–Œ       | 589/2256 [1:04:45<3:02:51,  6.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 590/2256 [1:04:51<3:02:34,  6.58s/it]                                                      {'loss': 1.7875, 'learning_rate': 4.2026180854567285e-05, 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 590/2256 [1:04:51<3:02:34,  6.58s/it]                                                      {'router_ce_loss': 0.9773866534233093, 'old_lang_expert0_score': '0.24 0.02 0.05 0.22 0.87 0.87 0.31 0.94 0.89 0.94 0.29 0.97 0.58 0.96 0.97 0.94 0.96 0.96 0.97 0.97 0.96 0.97 0.99 0.99', 'epoch': 0.26}
 26%|â–ˆâ–ˆâ–Œ       | 590/2256 [1:04:52<3:02:34,  6.58s/it] 26%|â–ˆâ–ˆâ–Œ       | 591/2256 [1:04:58<3:02:13,  6.57s/it] 26%|â–ˆâ–ˆâ–Œ       | 592/2256 [1:05:05<3:01:57,  6.56s/it] 26%|â–ˆâ–ˆâ–‹       | 593/2256 [1:05:11<3:01:52,  6.56s/it] 26%|â–ˆâ–ˆâ–‹       | 594/2256 [1:05:18<3:01:45,  6.56s/it] 26%|â–ˆâ–ˆâ–‹       | 595/2256 [1:05:24<3:01:43,  6.56s/it] 26%|â–ˆâ–ˆâ–‹       | 596/2256 [1:05:31<3:01:29,  6.56s/it] 26%|â–ˆâ–ˆâ–‹       | 597/2256 [1:05:37<3:01:23,  6.56s/it] 27%|â–ˆâ–ˆâ–‹       | 598/2256 [1:05:44<3:01:13,  6.56s/it] 27%|â–ˆâ–ˆâ–‹       | 599/2256 [1:05:50<3:01:12,  6.56s/it] 27%|â–ˆâ–ˆâ–‹       | 600/2256 [1:05:57<3:01:11,  6.56s/it]                                                      {'loss': 1.805, 'learning_rate': 4.176961825348059e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 600/2256 [1:05:57<3:01:11,  6.56s/it]                                                      {'router_ce_loss': 0.9815790057182312, 'old_lang_expert0_score': '0.25 0.02 0.05 0.27 0.89 0.9 0.32 0.92 0.86 0.93 0.25 0.95 0.46 0.94 0.97 0.93 0.94 0.95 0.97 0.97 0.97 0.98 0.98 0.99', 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 600/2256 [1:05:58<3:01:11,  6.56s/it] 27%|â–ˆâ–ˆâ–‹       | 601/2256 [1:06:04<3:01:05,  6.57s/it] 27%|â–ˆâ–ˆâ–‹       | 602/2256 [1:06:10<3:00:53,  6.56s/it] 27%|â–ˆâ–ˆâ–‹       | 603/2256 [1:06:17<3:00:47,  6.56s/it] 27%|â–ˆâ–ˆâ–‹       | 604/2256 [1:06:24<3:04:12,  6.69s/it] 27%|â–ˆâ–ˆâ–‹       | 605/2256 [1:06:30<3:02:57,  6.65s/it] 27%|â–ˆâ–ˆâ–‹       | 606/2256 [1:06:37<3:02:11,  6.62s/it] 27%|â–ˆâ–ˆâ–‹       | 607/2256 [1:06:43<3:01:34,  6.61s/it] 27%|â–ˆâ–ˆâ–‹       | 608/2256 [1:06:50<3:00:55,  6.59s/it] 27%|â–ˆâ–ˆâ–‹       | 609/2256 [1:06:56<3:00:36,  6.58s/it] 27%|â–ˆâ–ˆâ–‹       | 610/2256 [1:07:03<3:00:18,  6.57s/it]                                                      {'loss': 1.8224, 'learning_rate': 4.1509803748418575e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 610/2256 [1:07:03<3:00:18,  6.57s/it]                                                      {'router_ce_loss': 0.9750193357467651, 'old_lang_expert0_score': '0.24 0.02 0.05 0.28 0.9 0.92 0.33 0.94 0.89 0.94 0.33 0.94 0.52 0.95 0.97 0.93 0.95 0.95 0.96 0.96 0.96 0.96 0.98 0.98', 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 610/2256 [1:07:04<3:00:18,  6.57s/it] 27%|â–ˆâ–ˆâ–‹       | 611/2256 [1:07:10<3:00:05,  6.57s/it] 27%|â–ˆâ–ˆâ–‹       | 612/2256 [1:07:16<2:59:57,  6.57s/it] 27%|â–ˆâ–ˆâ–‹       | 613/2256 [1:07:23<2:59:55,  6.57s/it] 27%|â–ˆâ–ˆâ–‹       | 614/2256 [1:07:29<2:59:42,  6.57s/it] 27%|â–ˆâ–ˆâ–‹       | 615/2256 [1:07:36<2:59:24,  6.56s/it] 27%|â–ˆâ–ˆâ–‹       | 616/2256 [1:07:42<2:59:26,  6.56s/it] 27%|â–ˆâ–ˆâ–‹       | 617/2256 [1:07:49<2:59:11,  6.56s/it] 27%|â–ˆâ–ˆâ–‹       | 618/2256 [1:07:56<2:59:02,  6.56s/it] 27%|â–ˆâ–ˆâ–‹       | 619/2256 [1:08:02<2:58:56,  6.56s/it] 27%|â–ˆâ–ˆâ–‹       | 620/2256 [1:08:09<2:58:55,  6.56s/it]                                                      {'loss': 1.8103, 'learning_rate': 4.124678772167432e-05, 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 620/2256 [1:08:09<2:58:55,  6.56s/it]                                                      {'router_ce_loss': 0.9622304439544678, 'old_lang_expert0_score': '0.24 0.03 0.07 0.29 0.86 0.92 0.42 0.95 0.91 0.95 0.37 0.97 0.56 0.96 0.97 0.93 0.95 0.96 0.97 0.97 0.97 0.97 0.98 0.99', 'epoch': 0.27}
 27%|â–ˆâ–ˆâ–‹       | 620/2256 [1:08:09<2:58:55,  6.56s/it] 28%|â–ˆâ–ˆâ–Š       | 621/2256 [1:08:15<3:00:29,  6.62s/it] 28%|â–ˆâ–ˆâ–Š       | 622/2256 [1:08:22<3:01:31,  6.67s/it] 28%|â–ˆâ–ˆâ–Š       | 623/2256 [1:08:29<3:00:32,  6.63s/it] 28%|â–ˆâ–ˆâ–Š       | 624/2256 [1:08:35<2:59:53,  6.61s/it] 28%|â–ˆâ–ˆâ–Š       | 625/2256 [1:08:42<2:59:12,  6.59s/it] 28%|â–ˆâ–ˆâ–Š       | 626/2256 [1:08:48<2:58:45,  6.58s/it] 28%|â–ˆâ–ˆâ–Š       | 627/2256 [1:08:55<2:58:36,  6.58s/it] 28%|â–ˆâ–ˆâ–Š       | 628/2256 [1:09:02<2:58:29,  6.58s/it] 28%|â–ˆâ–ˆâ–Š       | 629/2256 [1:09:08<2:58:11,  6.57s/it] 28%|â–ˆâ–ˆâ–Š       | 630/2256 [1:09:15<2:58:04,  6.57s/it]                                                      {'loss': 1.8142, 'learning_rate': 4.0980621176368485e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 630/2256 [1:09:15<2:58:04,  6.57s/it]                                                      {'router_ce_loss': 0.9985315203666687, 'old_lang_expert0_score': '0.24 0.02 0.07 0.26 0.81 0.86 0.29 0.87 0.81 0.91 0.29 0.92 0.45 0.92 0.97 0.91 0.92 0.93 0.95 0.96 0.94 0.93 0.97 0.98', 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 630/2256 [1:09:15<2:58:04,  6.57s/it] 28%|â–ˆâ–ˆâ–Š       | 631/2256 [1:09:21<2:58:05,  6.58s/it] 28%|â–ˆâ–ˆâ–Š       | 632/2256 [1:09:28<2:57:48,  6.57s/it] 28%|â–ˆâ–ˆâ–Š       | 633/2256 [1:09:34<2:57:25,  6.56s/it] 28%|â–ˆâ–ˆâ–Š       | 634/2256 [1:09:41<2:57:22,  6.56s/it] 28%|â–ˆâ–ˆâ–Š       | 635/2256 [1:09:47<2:57:15,  6.56s/it] 28%|â–ˆâ–ˆâ–Š       | 636/2256 [1:09:54<2:57:05,  6.56s/it] 28%|â–ˆâ–ˆâ–Š       | 637/2256 [1:10:01<2:57:00,  6.56s/it] 28%|â–ˆâ–ˆâ–Š       | 638/2256 [1:10:07<2:56:53,  6.56s/it] 28%|â–ˆâ–ˆâ–Š       | 639/2256 [1:10:14<2:58:30,  6.62s/it] 28%|â–ˆâ–ˆâ–Š       | 640/2256 [1:10:20<2:57:59,  6.61s/it]                                                      {'loss': 1.7484, 'learning_rate': 4.071135572655892e-05, 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 640/2256 [1:10:20<2:57:59,  6.61s/it]                                                      {'router_ce_loss': 0.9838281273841858, 'old_lang_expert0_score': '0.23 0.02 0.07 0.29 0.82 0.87 0.34 0.93 0.83 0.89 0.34 0.95 0.56 0.93 0.95 0.92 0.93 0.95 0.95 0.96 0.93 0.96 0.98 0.98', 'epoch': 0.28}
 28%|â–ˆâ–ˆâ–Š       | 640/2256 [1:10:21<2:57:59,  6.61s/it] 28%|â–ˆâ–ˆâ–Š       | 641/2256 [1:10:27<2:57:33,  6.60s/it] 28%|â–ˆâ–ˆâ–Š       | 642/2256 [1:10:34<2:57:14,  6.59s/it] 29%|â–ˆâ–ˆâ–Š       | 643/2256 [1:10:40<2:56:52,  6.58s/it] 29%|â–ˆâ–ˆâ–Š       | 644/2256 [1:10:47<2:56:34,  6.57s/it] 29%|â–ˆâ–ˆâ–Š       | 645/2256 [1:10:53<2:56:13,  6.56s/it] 29%|â–ˆâ–ˆâ–Š       | 646/2256 [1:11:00<2:56:10,  6.57s/it] 29%|â–ˆâ–ˆâ–Š       | 647/2256 [1:11:06<2:56:08,  6.57s/it] 29%|â–ˆâ–ˆâ–Š       | 648/2256 [1:11:13<2:56:02,  6.57s/it] 29%|â–ˆâ–ˆâ–‰       | 649/2256 [1:11:20<2:55:50,  6.57s/it] 29%|â–ˆâ–ˆâ–‰       | 650/2256 [1:11:26<2:55:36,  6.56s/it]                                                      {'loss': 1.737, 'learning_rate': 4.0439043587231884e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 650/2256 [1:11:26<2:55:36,  6.56s/it]                                                      {'router_ce_loss': 0.9673935770988464, 'old_lang_expert0_score': '0.24 0.03 0.06 0.28 0.89 0.91 0.36 0.95 0.89 0.94 0.35 0.96 0.6 0.93 0.97 0.92 0.94 0.95 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 650/2256 [1:11:27<2:55:36,  6.56s/it] 29%|â–ˆâ–ˆâ–‰       | 651/2256 [1:11:33<2:55:32,  6.56s/it] 29%|â–ˆâ–ˆâ–‰       | 652/2256 [1:11:39<2:55:31,  6.57s/it] 29%|â–ˆâ–ˆâ–‰       | 653/2256 [1:11:46<2:55:19,  6.56s/it] 29%|â–ˆâ–ˆâ–‰       | 654/2256 [1:11:52<2:55:11,  6.56s/it] 29%|â–ˆâ–ˆâ–‰       | 655/2256 [1:11:59<2:55:04,  6.56s/it] 29%|â–ˆâ–ˆâ–‰       | 656/2256 [1:12:05<2:55:04,  6.57s/it] 29%|â–ˆâ–ˆâ–‰       | 657/2256 [1:12:12<2:58:22,  6.69s/it] 29%|â–ˆâ–ˆâ–‰       | 658/2256 [1:12:19<2:57:14,  6.65s/it] 29%|â–ˆâ–ˆâ–‰       | 659/2256 [1:12:26<2:56:21,  6.63s/it] 29%|â–ˆâ–ˆâ–‰       | 660/2256 [1:12:32<2:55:38,  6.60s/it]                                                      {'loss': 1.8573, 'learning_rate': 4.016373756417669e-05, 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 660/2256 [1:12:32<2:55:38,  6.60s/it]                                                      {'router_ce_loss': 0.9772979617118835, 'old_lang_expert0_score': '0.26 0.03 0.06 0.24 0.85 0.87 0.39 0.93 0.87 0.92 0.37 0.95 0.59 0.92 0.96 0.92 0.92 0.92 0.96 0.95 0.95 0.95 0.98 0.98', 'epoch': 0.29}
 29%|â–ˆâ–ˆâ–‰       | 660/2256 [1:12:33<2:55:38,  6.60s/it] 29%|â–ˆâ–ˆâ–‰       | 661/2256 [1:12:39<2:55:13,  6.59s/it] 29%|â–ˆâ–ˆâ–‰       | 662/2256 [1:12:45<2:54:54,  6.58s/it] 29%|â–ˆâ–ˆâ–‰       | 663/2256 [1:12:52<2:54:19,  6.57s/it] 29%|â–ˆâ–ˆâ–‰       | 664/2256 [1:12:58<2:54:11,  6.57s/it] 29%|â–ˆâ–ˆâ–‰       | 665/2256 [1:13:05<2:54:02,  6.56s/it] 30%|â–ˆâ–ˆâ–‰       | 666/2256 [1:13:11<2:53:42,  6.56s/it] 30%|â–ˆâ–ˆâ–‰       | 667/2256 [1:13:18<2:53:45,  6.56s/it] 30%|â–ˆâ–ˆâ–‰       | 668/2256 [1:13:25<2:53:42,  6.56s/it] 30%|â–ˆâ–ˆâ–‰       | 669/2256 [1:13:31<2:53:33,  6.56s/it] 30%|â–ˆâ–ˆâ–‰       | 670/2256 [1:13:38<2:53:21,  6.56s/it]                                                      {'loss': 1.747, 'learning_rate': 3.988549104374581e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 670/2256 [1:13:38<2:53:21,  6.56s/it]                                                      {'router_ce_loss': 0.9794617891311646, 'old_lang_expert0_score': '0.26 0.03 0.04 0.22 0.86 0.88 0.35 0.93 0.87 0.93 0.32 0.95 0.58 0.94 0.96 0.91 0.93 0.93 0.97 0.96 0.96 0.96 0.98 0.99', 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–‰       | 670/2256 [1:13:38<2:53:21,  6.56s/it] 30%|â–ˆâ–ˆâ–‰       | 671/2256 [1:13:44<2:54:06,  6.59s/it] 30%|â–ˆâ–ˆâ–‰       | 672/2256 [1:13:51<2:53:43,  6.58s/it] 30%|â–ˆâ–ˆâ–‰       | 673/2256 [1:13:58<2:53:34,  6.58s/it] 30%|â–ˆâ–ˆâ–‰       | 674/2256 [1:14:04<2:55:05,  6.64s/it] 30%|â–ˆâ–ˆâ–‰       | 675/2256 [1:14:11<2:55:54,  6.68s/it] 30%|â–ˆâ–ˆâ–‰       | 676/2256 [1:14:18<2:55:06,  6.65s/it] 30%|â–ˆâ–ˆâ–ˆ       | 677/2256 [1:14:24<2:54:30,  6.63s/it] 30%|â–ˆâ–ˆâ–ˆ       | 678/2256 [1:14:31<2:53:55,  6.61s/it] 30%|â–ˆâ–ˆâ–ˆ       | 679/2256 [1:14:37<2:53:35,  6.60s/it] 30%|â–ˆâ–ˆâ–ˆ       | 680/2256 [1:14:44<2:53:11,  6.59s/it]                                                      {'loss': 1.7375, 'learning_rate': 3.960435798250236e-05, 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 680/2256 [1:14:44<2:53:11,  6.59s/it]                                                      {'router_ce_loss': 0.9839507341384888, 'old_lang_expert0_score': '0.27 0.03 0.05 0.2 0.83 0.86 0.36 0.94 0.83 0.9 0.36 0.95 0.61 0.94 0.96 0.91 0.9 0.89 0.95 0.94 0.95 0.95 0.98 0.98', 'epoch': 0.3}
 30%|â–ˆâ–ˆâ–ˆ       | 680/2256 [1:14:45<2:53:11,  6.59s/it] 30%|â–ˆâ–ˆâ–ˆ       | 681/2256 [1:14:51<2:53:01,  6.59s/it] 30%|â–ˆâ–ˆâ–ˆ       | 682/2256 [1:14:57<2:52:36,  6.58s/it] 30%|â–ˆâ–ˆâ–ˆ       | 683/2256 [1:15:04<2:52:32,  6.58s/it] 30%|â–ˆâ–ˆâ–ˆ       | 684/2256 [1:15:10<2:52:19,  6.58s/it] 30%|â–ˆâ–ˆâ–ˆ       | 685/2256 [1:15:17<2:52:18,  6.58s/it] 30%|â–ˆâ–ˆâ–ˆ       | 686/2256 [1:15:23<2:52:06,  6.58s/it] 30%|â–ˆâ–ˆâ–ˆ       | 687/2256 [1:15:30<2:51:53,  6.57s/it] 30%|â–ˆâ–ˆâ–ˆ       | 688/2256 [1:15:37<2:51:50,  6.58s/it] 31%|â–ˆâ–ˆâ–ˆ       | 689/2256 [1:15:43<2:51:38,  6.57s/it] 31%|â–ˆâ–ˆâ–ˆ       | 690/2256 [1:15:50<2:51:34,  6.57s/it]                                                      {'loss': 1.7582, 'learning_rate': 3.932039289675708e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 690/2256 [1:15:50<2:51:34,  6.57s/it]                                                      {'router_ce_loss': 0.9822864532470703, 'old_lang_expert0_score': '0.22 0.03 0.07 0.25 0.82 0.86 0.37 0.91 0.85 0.91 0.39 0.94 0.62 0.93 0.96 0.88 0.91 0.93 0.93 0.95 0.95 0.94 0.98 0.98', 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 690/2256 [1:15:50<2:51:34,  6.57s/it] 31%|â–ˆâ–ˆâ–ˆ       | 691/2256 [1:15:56<2:51:33,  6.58s/it] 31%|â–ˆâ–ˆâ–ˆ       | 692/2256 [1:16:03<2:53:11,  6.64s/it] 31%|â–ˆâ–ˆâ–ˆ       | 693/2256 [1:16:10<2:52:23,  6.62s/it] 31%|â–ˆâ–ˆâ–ˆ       | 694/2256 [1:16:16<2:52:03,  6.61s/it] 31%|â–ˆâ–ˆâ–ˆ       | 695/2256 [1:16:23<2:51:34,  6.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 696/2256 [1:16:29<2:51:20,  6.59s/it] 31%|â–ˆâ–ˆâ–ˆ       | 697/2256 [1:16:36<2:50:53,  6.58s/it] 31%|â–ˆâ–ˆâ–ˆ       | 698/2256 [1:16:42<2:50:42,  6.57s/it] 31%|â–ˆâ–ˆâ–ˆ       | 699/2256 [1:16:49<2:50:21,  6.56s/it] 31%|â–ˆâ–ˆâ–ˆ       | 700/2256 [1:16:56<2:50:09,  6.56s/it]                                                      {'loss': 1.8005, 'learning_rate': 3.9033650851996684e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 700/2256 [1:16:56<2:50:09,  6.56s/it]                                                      {'router_ce_loss': 0.9682736396789551, 'old_lang_expert0_score': '0.23 0.02 0.05 0.25 0.89 0.9 0.37 0.94 0.88 0.93 0.47 0.96 0.62 0.94 0.92 0.91 0.92 0.94 0.97 0.98 0.97 0.97 0.99 0.99', 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆ       | 700/2256 [1:16:56<2:50:09,  6.56s/it] 31%|â–ˆâ–ˆâ–ˆ       | 701/2256 [1:17:02<2:50:11,  6.57s/it] 31%|â–ˆâ–ˆâ–ˆ       | 702/2256 [1:17:09<2:49:50,  6.56s/it] 31%|â–ˆâ–ˆâ–ˆ       | 703/2256 [1:17:15<2:49:40,  6.56s/it] 31%|â–ˆâ–ˆâ–ˆ       | 704/2256 [1:17:22<2:49:38,  6.56s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 705/2256 [1:17:28<2:49:36,  6.56s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 706/2256 [1:17:35<2:49:33,  6.56s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 707/2256 [1:17:42<2:49:25,  6.56s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 708/2256 [1:17:48<2:49:25,  6.57s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 709/2256 [1:17:55<2:49:16,  6.57s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 710/2256 [1:18:02<2:52:30,  6.70s/it]                                                      {'loss': 1.7375, 'learning_rate': 3.874418745220577e-05, 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 710/2256 [1:18:02<2:52:30,  6.70s/it]                                                      {'router_ce_loss': 0.9739158749580383, 'old_lang_expert0_score': '0.24 0.02 0.07 0.31 0.88 0.9 0.38 0.92 0.89 0.91 0.35 0.94 0.58 0.92 0.96 0.9 0.93 0.93 0.96 0.96 0.96 0.96 0.98 0.98', 'epoch': 0.31}
 31%|â–ˆâ–ˆâ–ˆâ–      | 710/2256 [1:18:02<2:52:30,  6.70s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 711/2256 [1:18:08<2:51:25,  6.66s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 712/2256 [1:18:15<2:50:33,  6.63s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 713/2256 [1:18:21<2:49:49,  6.60s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 714/2256 [1:18:28<2:49:35,  6.60s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 715/2256 [1:18:34<2:49:06,  6.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 716/2256 [1:18:41<2:48:54,  6.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 717/2256 [1:18:48<2:48:46,  6.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 718/2256 [1:18:54<2:48:34,  6.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 719/2256 [1:19:01<2:48:35,  6.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 720/2256 [1:19:07<2:48:25,  6.58s/it]                                                      {'loss': 1.7817, 'learning_rate': 3.845205882908432e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 720/2256 [1:19:07<2:48:25,  6.58s/it]                                                      {'router_ce_loss': 0.9747684001922607, 'old_lang_expert0_score': '0.25 0.02 0.03 0.21 0.88 0.88 0.36 0.96 0.91 0.93 0.38 0.96 0.64 0.92 0.95 0.91 0.92 0.94 0.97 0.96 0.96 0.96 0.99 0.99', 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 720/2256 [1:19:08<2:48:25,  6.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 721/2256 [1:19:14<2:47:59,  6.57s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 722/2256 [1:19:20<2:47:53,  6.57s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 723/2256 [1:19:27<2:47:48,  6.57s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 724/2256 [1:19:34<2:47:46,  6.57s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 725/2256 [1:19:40<2:47:30,  6.56s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 726/2256 [1:19:47<2:47:21,  6.56s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 727/2256 [1:19:53<2:48:57,  6.63s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 728/2256 [1:20:00<2:49:56,  6.67s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 729/2256 [1:20:07<2:48:59,  6.64s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 730/2256 [1:20:13<2:48:18,  6.62s/it]                                                      {'loss': 1.7312, 'learning_rate': 3.815732163116281e-05, 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 730/2256 [1:20:13<2:48:18,  6.62s/it]                                                      {'router_ce_loss': 0.9658510088920593, 'old_lang_expert0_score': '0.26 0.02 0.05 0.19 0.89 0.89 0.37 0.93 0.91 0.96 0.41 0.95 0.64 0.94 0.96 0.92 0.96 0.97 0.97 0.96 0.97 0.97 0.99 0.99', 'epoch': 0.32}
 32%|â–ˆâ–ˆâ–ˆâ–      | 730/2256 [1:20:14<2:48:18,  6.62s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 731/2256 [1:20:20<2:47:37,  6.60s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 732/2256 [1:20:27<2:47:15,  6.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 733/2256 [1:20:33<2:46:54,  6.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 734/2256 [1:20:40<2:46:47,  6.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 735/2256 [1:20:46<2:46:44,  6.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 736/2256 [1:20:53<2:46:28,  6.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 737/2256 [1:20:59<2:46:20,  6.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 738/2256 [1:21:06<2:46:06,  6.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 739/2256 [1:21:12<2:45:54,  6.56s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 740/2256 [1:21:19<2:45:38,  6.56s/it]                                                      {'loss': 1.7285, 'learning_rate': 3.786003301281717e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 740/2256 [1:21:19<2:45:38,  6.56s/it]                                                      {'router_ce_loss': 0.9641273021697998, 'old_lang_expert0_score': '0.25 0.02 0.06 0.25 0.89 0.93 0.38 0.94 0.9 0.94 0.38 0.95 0.68 0.94 0.96 0.94 0.95 0.94 0.97 0.97 0.97 0.97 0.98 0.99', 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 740/2256 [1:21:20<2:45:38,  6.56s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 741/2256 [1:21:26<2:45:29,  6.55s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 742/2256 [1:21:32<2:45:43,  6.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 743/2256 [1:21:39<2:45:42,  6.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 744/2256 [1:21:45<2:45:37,  6.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 745/2256 [1:21:52<2:47:17,  6.64s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 746/2256 [1:21:59<2:46:32,  6.62s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 747/2256 [1:22:05<2:46:06,  6.60s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 748/2256 [1:22:12<2:45:27,  6.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 749/2256 [1:22:18<2:45:11,  6.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 750/2256 [1:22:25<2:45:05,  6.58s/it]                                                      {'loss': 1.8161, 'learning_rate': 3.756025062318557e-05, 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 750/2256 [1:22:25<2:45:05,  6.58s/it]                                                      {'router_ce_loss': 0.9634854197502136, 'old_lang_expert0_score': '0.24 0.02 0.05 0.26 0.89 0.91 0.37 0.95 0.91 0.95 0.36 0.95 0.71 0.94 0.97 0.94 0.95 0.96 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.33}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 750/2256 [1:22:25<2:45:05,  6.58s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 751/2256 [1:22:31<2:44:42,  6.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 752/2256 [1:22:38<2:44:37,  6.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 753/2256 [1:22:45<2:44:29,  6.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 754/2256 [1:22:51<2:44:27,  6.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 755/2256 [1:22:58<2:44:23,  6.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 756/2256 [1:23:04<2:44:20,  6.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 757/2256 [1:23:11<2:44:10,  6.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 758/2256 [1:23:17<2:44:04,  6.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 759/2256 [1:23:24<2:43:59,  6.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 760/2256 [1:23:31<2:43:47,  6.57s/it]                                                      {'loss': 1.7551, 'learning_rate': 3.725803259498931e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 760/2256 [1:23:31<2:43:47,  6.57s/it]                                                      {'router_ce_loss': 0.9584383368492126, 'old_lang_expert0_score': '0.28 0.04 0.06 0.2 0.81 0.87 0.41 0.95 0.91 0.94 0.53 0.96 0.79 0.93 0.96 0.94 0.93 0.94 0.96 0.96 0.95 0.96 0.99 0.98', 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 760/2256 [1:23:31<2:43:47,  6.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 761/2256 [1:23:37<2:43:40,  6.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 762/2256 [1:23:44<2:43:29,  6.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 763/2256 [1:23:51<2:45:04,  6.63s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 764/2256 [1:23:57<2:46:07,  6.68s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 765/2256 [1:24:04<2:45:02,  6.64s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 766/2256 [1:24:10<2:44:19,  6.62s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 767/2256 [1:24:17<2:43:47,  6.60s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 768/2256 [1:24:24<2:43:14,  6.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 769/2256 [1:24:30<2:43:05,  6.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 770/2256 [1:24:37<2:42:57,  6.58s/it]                                                      {'loss': 1.7165, 'learning_rate': 3.6953437533259986e-05, 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 770/2256 [1:24:37<2:42:57,  6.58s/it]                                                      {'router_ce_loss': 0.9608213901519775, 'old_lang_expert0_score': '0.25 0.02 0.04 0.31 0.92 0.89 0.4 0.94 0.87 0.95 0.38 0.95 0.69 0.92 0.96 0.94 0.95 0.95 0.97 0.97 0.98 0.98 0.99 0.99', 'epoch': 0.34}
 34%|â–ˆâ–ˆâ–ˆâ–      | 770/2256 [1:24:37<2:42:57,  6.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 771/2256 [1:24:43<2:42:51,  6.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 772/2256 [1:24:50<2:43:04,  6.59s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 773/2256 [1:24:56<2:42:43,  6.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 774/2256 [1:25:03<2:42:29,  6.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 775/2256 [1:25:10<2:42:21,  6.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 776/2256 [1:25:16<2:42:13,  6.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 777/2256 [1:25:23<2:42:02,  6.57s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 778/2256 [1:25:29<2:41:47,  6.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 779/2256 [1:25:36<2:41:46,  6.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 780/2256 [1:25:42<2:41:33,  6.57s/it]                                                      {'loss': 1.8249, 'learning_rate': 3.6646524503974955e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 780/2256 [1:25:42<2:41:33,  6.57s/it]                                                      {'router_ce_loss': 0.9735015034675598, 'old_lang_expert0_score': '0.22 0.02 0.04 0.19 0.83 0.88 0.38 0.92 0.84 0.93 0.39 0.96 0.74 0.93 0.97 0.94 0.94 0.94 0.95 0.94 0.97 0.95 0.98 0.98', 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–      | 780/2256 [1:25:43<2:41:33,  6.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 781/2256 [1:25:49<2:43:15,  6.64s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 782/2256 [1:25:56<2:44:09,  6.68s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 783/2256 [1:26:03<2:43:10,  6.65s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 784/2256 [1:26:09<2:42:25,  6.62s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 785/2256 [1:26:16<2:41:58,  6.61s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 786/2256 [1:26:22<2:41:45,  6.60s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 787/2256 [1:26:29<2:41:23,  6.59s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 788/2256 [1:26:35<2:41:10,  6.59s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 789/2256 [1:26:42<2:41:12,  6.59s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 790/2256 [1:26:49<2:41:21,  6.60s/it]                                                      {'loss': 1.733, 'learning_rate': 3.63373530226035e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 790/2256 [1:26:49<2:41:21,  6.60s/it]                                                      {'router_ce_loss': 0.9886832237243652, 'old_lang_expert0_score': '0.24 0.04 0.04 0.22 0.82 0.88 0.38 0.93 0.79 0.91 0.44 0.94 0.71 0.89 0.94 0.84 0.82 0.88 0.93 0.92 0.93 0.93 0.97 0.98', 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 790/2256 [1:26:49<2:41:21,  6.60s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 791/2256 [1:26:55<2:41:52,  6.63s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 792/2256 [1:27:02<2:41:42,  6.63s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 793/2256 [1:27:09<2:41:27,  6.62s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 794/2256 [1:27:15<2:40:56,  6.61s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 795/2256 [1:27:22<2:40:38,  6.60s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 796/2256 [1:27:28<2:40:23,  6.59s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 797/2256 [1:27:35<2:40:06,  6.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 798/2256 [1:27:41<2:39:55,  6.58s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 799/2256 [1:27:48<2:41:28,  6.65s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 800/2256 [1:27:55<2:40:55,  6.63s/it]                                                      {'loss': 1.7127, 'learning_rate': 3.6025983042565795e-05, 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 800/2256 [1:27:55<2:40:55,  6.63s/it]                                                      {'router_ce_loss': 0.964227020740509, 'old_lang_expert0_score': '0.24 0.02 0.03 0.23 0.84 0.84 0.38 0.94 0.88 0.94 0.47 0.97 0.75 0.95 0.97 0.92 0.93 0.96 0.98 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.35}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 800/2256 [1:27:55<2:40:55,  6.63s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 801/2256 [1:28:01<2:40:30,  6.62s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 802/2256 [1:28:08<2:40:03,  6.60s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 803/2256 [1:28:15<2:39:44,  6.60s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 804/2256 [1:28:21<2:39:23,  6.59s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 805/2256 [1:28:28<2:39:18,  6.59s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 806/2256 [1:28:34<2:39:01,  6.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 807/2256 [1:28:41<2:38:54,  6.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 808/2256 [1:28:48<2:39:16,  6.60s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 809/2256 [1:28:54<2:39:08,  6.60s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 810/2256 [1:29:01<2:38:52,  6.59s/it]                                                      {'loss': 1.7803, 'learning_rate': 3.571247494360695e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 810/2256 [1:29:01<2:38:52,  6.59s/it]                                                      {'router_ce_loss': 0.9695844650268555, 'old_lang_expert0_score': '0.24 0.02 0.06 0.24 0.85 0.86 0.4 0.94 0.89 0.94 0.4 0.96 0.68 0.93 0.97 0.92 0.96 0.95 0.97 0.95 0.96 0.95 0.98 0.98', 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 810/2256 [1:29:01<2:38:52,  6.59s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 811/2256 [1:29:07<2:38:44,  6.59s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 812/2256 [1:29:14<2:38:28,  6.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 813/2256 [1:29:20<2:38:24,  6.59s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 814/2256 [1:29:27<2:38:08,  6.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 815/2256 [1:29:34<2:38:02,  6.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 816/2256 [1:29:40<2:37:59,  6.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 817/2256 [1:29:47<2:41:08,  6.72s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 818/2256 [1:29:54<2:40:02,  6.68s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 819/2256 [1:30:00<2:39:11,  6.65s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 820/2256 [1:30:07<2:39:10,  6.65s/it]                                                      {'loss': 1.9022, 'learning_rate': 3.539688952008842e-05, 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 820/2256 [1:30:07<2:39:10,  6.65s/it]                                                      {'router_ce_loss': 0.962688684463501, 'old_lang_expert0_score': '0.24 0.02 0.04 0.23 0.87 0.9 0.4 0.95 0.9 0.94 0.39 0.95 0.73 0.94 0.97 0.93 0.95 0.96 0.97 0.96 0.97 0.97 0.98 0.99', 'epoch': 0.36}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 820/2256 [1:30:08<2:39:10,  6.65s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 821/2256 [1:30:14<2:38:41,  6.64s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 822/2256 [1:30:20<2:38:26,  6.63s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 823/2256 [1:30:27<2:37:57,  6.61s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 824/2256 [1:30:33<2:37:25,  6.60s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 825/2256 [1:30:40<2:37:06,  6.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 826/2256 [1:30:47<2:36:47,  6.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 827/2256 [1:30:53<2:36:47,  6.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 828/2256 [1:31:00<2:36:40,  6.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 829/2256 [1:31:06<2:36:34,  6.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 830/2256 [1:31:13<2:36:28,  6.58s/it]                                                      {'loss': 1.7308, 'learning_rate': 3.507928796919893e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 830/2256 [1:31:13<2:36:28,  6.58s/it]                                                      {'router_ce_loss': 0.9648925065994263, 'old_lang_expert0_score': '0.24 0.02 0.06 0.26 0.88 0.88 0.44 0.94 0.87 0.93 0.44 0.96 0.7 0.94 0.96 0.89 0.94 0.93 0.96 0.96 0.95 0.95 0.98 0.99', 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 830/2256 [1:31:13<2:36:28,  6.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 831/2256 [1:31:19<2:36:17,  6.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 832/2256 [1:31:26<2:36:09,  6.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 833/2256 [1:31:33<2:35:56,  6.58s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 834/2256 [1:31:39<2:37:31,  6.65s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 835/2256 [1:31:46<2:38:42,  6.70s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 836/2256 [1:31:53<2:37:49,  6.67s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 837/2256 [1:31:59<2:37:11,  6.65s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 838/2256 [1:32:06<2:36:41,  6.63s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 839/2256 [1:32:13<2:36:14,  6.62s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 840/2256 [1:32:19<2:35:54,  6.61s/it]                                                      {'loss': 1.8337, 'learning_rate': 3.475973187908737e-05, 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 840/2256 [1:32:19<2:35:54,  6.61s/it]                                                      {'router_ce_loss': 0.9660193920135498, 'old_lang_expert0_score': '0.27 0.03 0.05 0.19 0.83 0.85 0.41 0.94 0.88 0.94 0.47 0.96 0.77 0.94 0.96 0.92 0.91 0.94 0.97 0.96 0.96 0.95 0.99 0.98', 'epoch': 0.37}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 840/2256 [1:32:20<2:35:54,  6.61s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 841/2256 [1:32:26<2:35:26,  6.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 842/2256 [1:32:32<2:35:17,  6.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 843/2256 [1:32:39<2:35:10,  6.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 844/2256 [1:32:45<2:35:01,  6.59s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 845/2256 [1:32:52<2:34:55,  6.59s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 846/2256 [1:32:59<2:35:00,  6.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 847/2256 [1:33:05<2:34:39,  6.59s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 848/2256 [1:33:12<2:34:23,  6.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 849/2256 [1:33:18<2:34:31,  6.59s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 850/2256 [1:33:25<2:34:28,  6.59s/it]                                                      {'loss': 1.8397, 'learning_rate': 3.443828321691986e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 850/2256 [1:33:25<2:34:28,  6.59s/it]                                                      {'router_ce_loss': 0.9546775817871094, 'old_lang_expert0_score': '0.21 0.02 0.04 0.24 0.85 0.88 0.48 0.94 0.91 0.94 0.48 0.97 0.73 0.96 0.98 0.95 0.97 0.96 0.98 0.98 0.97 0.96 0.99 0.99', 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 850/2256 [1:33:26<2:34:28,  6.59s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 851/2256 [1:33:32<2:34:30,  6.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 852/2256 [1:33:38<2:35:50,  6.66s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 853/2256 [1:33:45<2:35:08,  6.63s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 854/2256 [1:33:52<2:34:41,  6.62s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 855/2256 [1:33:58<2:34:29,  6.62s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 856/2256 [1:34:05<2:34:02,  6.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 857/2256 [1:34:11<2:33:47,  6.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 858/2256 [1:34:18<2:33:35,  6.59s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 859/2256 [1:34:25<2:33:18,  6.58s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 860/2256 [1:34:31<2:33:17,  6.59s/it]                                                      {'loss': 1.7601, 'learning_rate': 3.411500431686324e-05, 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 860/2256 [1:34:31<2:33:17,  6.59s/it]                                                      {'router_ce_loss': 0.9642956256866455, 'old_lang_expert0_score': '0.25 0.02 0.05 0.26 0.87 0.9 0.43 0.93 0.89 0.94 0.41 0.94 0.68 0.93 0.96 0.92 0.94 0.95 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.38}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 860/2256 [1:34:32<2:33:17,  6.59s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 861/2256 [1:34:38<2:33:27,  6.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 862/2256 [1:34:44<2:33:20,  6.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 863/2256 [1:34:51<2:33:10,  6.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 864/2256 [1:34:58<2:33:06,  6.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 865/2256 [1:35:04<2:33:03,  6.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 866/2256 [1:35:11<2:32:56,  6.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 867/2256 [1:35:17<2:32:48,  6.60s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 868/2256 [1:35:24<2:32:44,  6.60s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 869/2256 [1:35:31<2:32:28,  6.60s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 870/2256 [1:35:38<2:35:27,  6.73s/it]                                                      {'loss': 1.7751, 'learning_rate': 3.3789957867997517e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 870/2256 [1:35:38<2:35:27,  6.73s/it]                                                      {'router_ce_loss': 0.9576488733291626, 'old_lang_expert0_score': '0.25 0.03 0.06 0.26 0.88 0.91 0.47 0.93 0.89 0.93 0.53 0.94 0.72 0.92 0.95 0.9 0.92 0.95 0.96 0.96 0.96 0.96 0.99 0.99', 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 870/2256 [1:35:38<2:35:27,  6.73s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 871/2256 [1:35:44<2:34:29,  6.69s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 872/2256 [1:35:51<2:33:40,  6.66s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 873/2256 [1:35:57<2:32:56,  6.64s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 874/2256 [1:36:04<2:32:15,  6.61s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 875/2256 [1:36:10<2:32:04,  6.61s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 876/2256 [1:36:17<2:31:41,  6.60s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 877/2256 [1:36:24<2:31:25,  6.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 878/2256 [1:36:30<2:31:12,  6.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 879/2256 [1:36:37<2:31:10,  6.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 880/2256 [1:36:43<2:31:07,  6.59s/it]                                                      {'loss': 1.7584, 'learning_rate': 3.3463206902159395e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 880/2256 [1:36:43<2:31:07,  6.59s/it]                                                      {'router_ce_loss': 0.9554470181465149, 'old_lang_expert0_score': '0.24 0.02 0.07 0.29 0.89 0.9 0.54 0.94 0.89 0.93 0.48 0.93 0.7 0.92 0.96 0.91 0.93 0.95 0.97 0.97 0.97 0.95 0.99 0.99', 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 880/2256 [1:36:44<2:31:07,  6.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 881/2256 [1:36:50<2:31:01,  6.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 882/2256 [1:36:57<2:30:55,  6.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 883/2256 [1:37:03<2:30:34,  6.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 884/2256 [1:37:10<2:30:33,  6.58s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 885/2256 [1:37:16<2:30:30,  6.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 886/2256 [1:37:23<2:30:29,  6.59s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 887/2256 [1:37:30<2:31:39,  6.65s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 888/2256 [1:37:36<2:32:31,  6.69s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 889/2256 [1:37:43<2:31:49,  6.66s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 890/2256 [1:37:50<2:31:04,  6.64s/it]                                                      {'loss': 1.8417, 'learning_rate': 3.313481478171934e-05, 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 890/2256 [1:37:50<2:31:04,  6.64s/it]                                                      {'router_ce_loss': 0.9561514854431152, 'old_lang_expert0_score': '0.26 0.04 0.06 0.3 0.89 0.89 0.49 0.93 0.89 0.96 0.47 0.94 0.72 0.92 0.95 0.92 0.92 0.93 0.97 0.96 0.97 0.97 0.98 0.99', 'epoch': 0.39}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 890/2256 [1:37:50<2:31:04,  6.64s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 891/2256 [1:37:56<2:30:42,  6.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 892/2256 [1:38:03<2:30:32,  6.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 893/2256 [1:38:09<2:30:23,  6.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 894/2256 [1:38:16<2:30:16,  6.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 895/2256 [1:38:23<2:30:06,  6.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 896/2256 [1:38:29<2:29:41,  6.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 897/2256 [1:38:36<2:29:31,  6.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 898/2256 [1:38:42<2:29:19,  6.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 899/2256 [1:38:49<2:29:14,  6.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 900/2256 [1:38:56<2:29:11,  6.60s/it]                                                      {'loss': 1.7225, 'learning_rate': 3.280484518729466e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 900/2256 [1:38:56<2:29:11,  6.60s/it]                                                      {'router_ce_loss': 0.9843244552612305, 'old_lang_expert0_score': '0.24 0.02 0.06 0.19 0.86 0.85 0.41 0.82 0.84 0.93 0.41 0.94 0.66 0.89 0.95 0.86 0.87 0.94 0.96 0.95 0.96 0.97 0.98 0.99', 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 900/2256 [1:38:56<2:29:11,  6.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 901/2256 [1:39:02<2:29:12,  6.61s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 902/2256 [1:39:09<2:29:02,  6.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 903/2256 [1:39:16<2:29:03,  6.61s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 904/2256 [1:39:22<2:28:52,  6.61s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 905/2256 [1:39:29<2:30:23,  6.68s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 906/2256 [1:39:36<2:29:35,  6.65s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 907/2256 [1:39:42<2:29:01,  6.63s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 908/2256 [1:39:49<2:28:40,  6.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 909/2256 [1:39:55<2:28:24,  6.61s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 910/2256 [1:40:02<2:28:10,  6.60s/it]                                                      {'loss': 1.804, 'learning_rate': 3.2473362105400696e-05, 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 910/2256 [1:40:02<2:28:10,  6.60s/it]                                                      {'router_ce_loss': 0.9481009840965271, 'old_lang_expert0_score': '0.24 0.02 0.07 0.34 0.88 0.91 0.58 0.93 0.9 0.93 0.53 0.95 0.68 0.94 0.97 0.9 0.95 0.96 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.4}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 910/2256 [1:40:02<2:28:10,  6.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 911/2256 [1:40:08<2:27:58,  6.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 912/2256 [1:40:15<2:27:47,  6.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 913/2256 [1:40:22<2:27:48,  6.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 914/2256 [1:40:28<2:27:42,  6.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 915/2256 [1:40:35<2:27:27,  6.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 916/2256 [1:40:41<2:27:13,  6.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 917/2256 [1:40:48<2:27:04,  6.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 918/2256 [1:40:55<2:26:52,  6.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 919/2256 [1:41:01<2:26:48,  6.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 920/2256 [1:41:08<2:26:40,  6.59s/it]                                                      {'loss': 1.9606, 'learning_rate': 3.214042981604283e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 920/2256 [1:41:08<2:26:40,  6.59s/it]                                                      {'router_ce_loss': 0.952958345413208, 'old_lang_expert0_score': '0.24 0.02 0.05 0.25 0.87 0.91 0.44 0.96 0.9 0.95 0.54 0.96 0.79 0.94 0.97 0.92 0.93 0.95 0.96 0.96 0.96 0.95 0.98 0.99', 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 920/2256 [1:41:08<2:26:40,  6.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 921/2256 [1:41:14<2:26:54,  6.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 922/2256 [1:41:21<2:26:55,  6.61s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 923/2256 [1:41:28<2:29:58,  6.75s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 924/2256 [1:41:35<2:28:41,  6.70s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 925/2256 [1:41:41<2:27:51,  6.67s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 926/2256 [1:41:48<2:27:16,  6.64s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 927/2256 [1:41:55<2:26:46,  6.63s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 928/2256 [1:42:01<2:26:30,  6.62s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 929/2256 [1:42:08<2:26:11,  6.61s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 930/2256 [1:42:14<2:25:51,  6.60s/it]                                                      {'loss': 1.888, 'learning_rate': 3.180611288025156e-05, 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 930/2256 [1:42:14<2:25:51,  6.60s/it]                                                      {'router_ce_loss': 0.9594426155090332, 'old_lang_expert0_score': '0.24 0.02 0.06 0.29 0.9 0.91 0.47 0.94 0.87 0.92 0.4 0.94 0.7 0.92 0.96 0.93 0.93 0.95 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.41}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 930/2256 [1:42:15<2:25:51,  6.60s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 931/2256 [1:42:21<2:25:29,  6.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 932/2256 [1:42:27<2:25:24,  6.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 933/2256 [1:42:34<2:25:24,  6.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 934/2256 [1:42:41<2:25:11,  6.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 935/2256 [1:42:47<2:24:50,  6.58s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 936/2256 [1:42:54<2:24:42,  6.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 937/2256 [1:43:00<2:24:28,  6.57s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 938/2256 [1:43:07<2:24:29,  6.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 939/2256 [1:43:13<2:24:33,  6.59s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 940/2256 [1:43:20<2:26:00,  6.66s/it]                                                      {'loss': 1.7098, 'learning_rate': 3.147047612756302e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 940/2256 [1:43:20<2:26:00,  6.66s/it]                                                      {'router_ce_loss': 0.9545994400978088, 'old_lang_expert0_score': '0.24 0.02 0.05 0.28 0.89 0.89 0.48 0.93 0.88 0.94 0.51 0.94 0.71 0.95 0.96 0.93 0.95 0.96 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 940/2256 [1:43:21<2:26:00,  6.66s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 941/2256 [1:43:27<2:26:46,  6.70s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 942/2256 [1:43:34<2:25:43,  6.65s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 943/2256 [1:43:40<2:25:10,  6.63s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 944/2256 [1:43:47<2:24:34,  6.61s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 945/2256 [1:43:53<2:24:07,  6.60s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 946/2256 [1:44:00<2:23:59,  6.59s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 947/2256 [1:44:07<2:23:39,  6.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 948/2256 [1:44:13<2:23:27,  6.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 949/2256 [1:44:20<2:23:17,  6.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 950/2256 [1:44:26<2:23:07,  6.58s/it]                                                      {'loss': 1.6742, 'learning_rate': 3.113358464344748e-05, 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 950/2256 [1:44:26<2:23:07,  6.58s/it]                                                      {'router_ce_loss': 0.9524205327033997, 'old_lang_expert0_score': '0.25 0.02 0.05 0.29 0.84 0.88 0.52 0.94 0.9 0.94 0.53 0.96 0.8 0.94 0.95 0.91 0.93 0.95 0.97 0.96 0.94 0.97 0.99 0.99', 'epoch': 0.42}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 950/2256 [1:44:27<2:23:07,  6.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 951/2256 [1:44:33<2:22:55,  6.57s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 952/2256 [1:44:39<2:22:42,  6.57s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 953/2256 [1:44:46<2:22:38,  6.57s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 954/2256 [1:44:52<2:22:21,  6.56s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 955/2256 [1:44:59<2:22:23,  6.57s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 956/2256 [1:45:06<2:22:12,  6.56s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 957/2256 [1:45:12<2:22:03,  6.56s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 958/2256 [1:45:19<2:23:31,  6.63s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 959/2256 [1:45:26<2:23:07,  6.62s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 960/2256 [1:45:32<2:22:40,  6.61s/it]                                                      {'loss': 1.7392, 'learning_rate': 3.079550375668821e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 960/2256 [1:45:32<2:22:40,  6.61s/it]                                                      {'router_ce_loss': 0.9610311388969421, 'old_lang_expert0_score': '0.26 0.03 0.06 0.23 0.86 0.87 0.49 0.93 0.87 0.92 0.54 0.94 0.74 0.92 0.94 0.9 0.92 0.93 0.95 0.95 0.95 0.96 0.99 0.99', 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 960/2256 [1:45:33<2:22:40,  6.61s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 961/2256 [1:45:39<2:22:15,  6.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 962/2256 [1:45:45<2:22:08,  6.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 963/2256 [1:45:52<2:21:51,  6.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 964/2256 [1:45:58<2:21:39,  6.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 965/2256 [1:46:05<2:21:11,  6.56s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 966/2256 [1:46:11<2:21:08,  6.56s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 967/2256 [1:46:18<2:20:51,  6.56s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 968/2256 [1:46:25<2:20:49,  6.56s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 969/2256 [1:46:31<2:20:52,  6.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 970/2256 [1:46:38<2:20:48,  6.57s/it]                                                      {'loss': 1.7742, 'learning_rate': 3.0456299026713158e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 970/2256 [1:46:38<2:20:48,  6.57s/it]                                                      {'router_ce_loss': 0.9441874623298645, 'old_lang_expert0_score': '0.24 0.02 0.06 0.27 0.9 0.91 0.56 0.93 0.89 0.94 0.61 0.95 0.77 0.93 0.97 0.91 0.93 0.95 0.97 0.97 0.98 0.98 0.99 0.99', 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 970/2256 [1:46:38<2:20:48,  6.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 971/2256 [1:46:44<2:20:48,  6.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 972/2256 [1:46:51<2:20:42,  6.58s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 973/2256 [1:46:57<2:20:27,  6.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 974/2256 [1:47:04<2:20:22,  6.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 975/2256 [1:47:11<2:20:21,  6.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 976/2256 [1:47:17<2:21:32,  6.63s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 977/2256 [1:47:24<2:22:25,  6.68s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 978/2256 [1:47:31<2:21:38,  6.65s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 979/2256 [1:47:37<2:21:06,  6.63s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 980/2256 [1:47:44<2:20:29,  6.61s/it]                                                      {'loss': 1.7271, 'learning_rate': 3.0116036230881916e-05, 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 980/2256 [1:47:44<2:20:29,  6.61s/it]                                                      {'router_ce_loss': nan, 'old_lang_expert0_score': 'nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan', 'epoch': 0.43}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 980/2256 [1:47:44<2:20:29,  6.61s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 981/2256 [1:47:50<2:20:15,  6.60s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 982/2256 [1:47:57<2:19:54,  6.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 983/2256 [1:48:04<2:19:48,  6.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 984/2256 [1:48:10<2:19:46,  6.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 985/2256 [1:48:17<2:19:32,  6.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 986/2256 [1:48:23<2:19:23,  6.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 987/2256 [1:48:30<2:19:19,  6.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 988/2256 [1:48:37<2:19:06,  6.58s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 989/2256 [1:48:43<2:19:07,  6.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 990/2256 [1:48:50<2:19:05,  6.59s/it]                                                      {'loss': 1.733, 'learning_rate': 2.977478135173037e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 990/2256 [1:48:50<2:19:05,  6.59s/it]                                                      {'router_ce_loss': 0.9444652795791626, 'old_lang_expert0_score': '0.25 0.02 0.05 0.29 0.9 0.92 0.54 0.94 0.9 0.94 0.54 0.95 0.78 0.94 0.97 0.93 0.94 0.96 0.98 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 990/2256 [1:48:50<2:19:05,  6.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 991/2256 [1:48:56<2:19:18,  6.61s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 992/2256 [1:49:03<2:19:06,  6.60s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 993/2256 [1:49:10<2:18:47,  6.59s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 994/2256 [1:49:16<2:20:07,  6.66s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 995/2256 [1:49:23<2:20:48,  6.70s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 996/2256 [1:49:30<2:19:58,  6.67s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 997/2256 [1:49:36<2:19:26,  6.65s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 998/2256 [1:49:43<2:18:55,  6.63s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 999/2256 [1:49:50<2:18:29,  6.61s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1000/2256 [1:49:56<2:18:08,  6.60s/it]                                                       {'loss': 1.8171, 'learning_rate': 2.943260056417564e-05, 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1000/2256 [1:49:56<2:18:08,  6.60s/it]                                                       {'router_ce_loss': 0.9391272068023682, 'old_lang_expert0_score': '0.24 0.02 0.06 0.3 0.88 0.91 0.52 0.95 0.91 0.96 0.66 0.97 0.84 0.94 0.97 0.93 0.94 0.95 0.97 0.97 0.96 0.98 0.99 0.99', 'epoch': 0.44}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1000/2256 [1:49:57<2:18:08,  6.60s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1001/2256 [1:50:03<2:18:45,  6.63s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1002/2256 [1:50:09<2:18:19,  6.62s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1003/2256 [1:50:16<2:17:52,  6.60s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1004/2256 [1:50:23<2:17:23,  6.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1005/2256 [1:50:29<2:17:17,  6.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1006/2256 [1:50:36<2:16:53,  6.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1007/2256 [1:50:42<2:16:51,  6.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1008/2256 [1:50:49<2:16:45,  6.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1009/2256 [1:50:55<2:16:35,  6.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1010/2256 [1:51:02<2:16:32,  6.57s/it]                                                       {'loss': 1.7542, 'learning_rate': 2.9089560222683675e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1010/2256 [1:51:02<2:16:32,  6.57s/it]                                                       {'router_ce_loss': 0.9365363717079163, 'old_lang_expert0_score': '0.26 0.02 0.06 0.35 0.9 0.93 0.58 0.96 0.89 0.94 0.57 0.94 0.8 0.95 0.96 0.93 0.95 0.96 0.98 0.97 0.96 0.97 0.99 0.99', 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1010/2256 [1:51:03<2:16:32,  6.57s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1011/2256 [1:51:09<2:16:46,  6.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1012/2256 [1:51:15<2:18:01,  6.66s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1013/2256 [1:51:22<2:17:30,  6.64s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1014/2256 [1:51:29<2:16:55,  6.61s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1015/2256 [1:51:35<2:16:28,  6.60s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1016/2256 [1:51:42<2:16:15,  6.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1017/2256 [1:51:48<2:15:59,  6.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1018/2256 [1:51:55<2:15:58,  6.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1019/2256 [1:52:01<2:15:44,  6.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1020/2256 [1:52:08<2:15:34,  6.58s/it]                                                       {'loss': 1.6893, 'learning_rate': 2.8745726848402036e-05, 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1020/2256 [1:52:08<2:15:34,  6.58s/it]                                                       {'router_ce_loss': 0.9441480040550232, 'old_lang_expert0_score': '0.24 0.02 0.06 0.3 0.88 0.92 0.53 0.94 0.9 0.94 0.6 0.95 0.82 0.94 0.96 0.91 0.92 0.94 0.98 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.45}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1020/2256 [1:52:09<2:15:34,  6.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1021/2256 [1:52:15<2:15:38,  6.59s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1022/2256 [1:52:21<2:15:22,  6.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1023/2256 [1:52:28<2:15:10,  6.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1024/2256 [1:52:34<2:15:03,  6.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1025/2256 [1:52:41<2:14:54,  6.58s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1026/2256 [1:52:47<2:14:43,  6.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1027/2256 [1:52:54<2:14:41,  6.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1028/2256 [1:53:01<2:14:28,  6.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1029/2256 [1:53:07<2:14:23,  6.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1030/2256 [1:53:14<2:16:59,  6.70s/it]                                                       {'loss': 1.7003, 'learning_rate': 2.840116711626033e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1030/2256 [1:53:14<2:16:59,  6.70s/it]                                                       {'router_ce_loss': 0.9485033750534058, 'old_lang_expert0_score': '0.23 0.02 0.03 0.26 0.9 0.93 0.51 0.95 0.89 0.94 0.54 0.95 0.79 0.94 0.96 0.93 0.95 0.95 0.96 0.97 0.96 0.97 0.99 0.99', 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1030/2256 [1:53:15<2:16:59,  6.70s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1031/2256 [1:53:21<2:16:39,  6.69s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1032/2256 [1:53:27<2:15:49,  6.66s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1033/2256 [1:53:34<2:15:06,  6.63s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1034/2256 [1:53:41<2:14:40,  6.61s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1035/2256 [1:53:47<2:14:30,  6.61s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1036/2256 [1:53:54<2:14:18,  6.61s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1037/2256 [1:54:00<2:14:01,  6.60s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1038/2256 [1:54:07<2:13:45,  6.59s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1039/2256 [1:54:13<2:13:38,  6.59s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1040/2256 [1:54:20<2:13:29,  6.59s/it]                                                       {'loss': 1.7404, 'learning_rate': 2.8055947842040862e-05, 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1040/2256 [1:54:20<2:13:29,  6.59s/it]                                                       {'router_ce_loss': 0.9575794339179993, 'old_lang_expert0_score': '0.24 0.02 0.06 0.28 0.84 0.86 0.53 0.9 0.85 0.91 0.55 0.95 0.81 0.9 0.96 0.92 0.92 0.95 0.96 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.46}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1040/2256 [1:54:21<2:13:29,  6.59s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1041/2256 [1:54:27<2:13:17,  6.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1042/2256 [1:54:33<2:13:12,  6.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1043/2256 [1:54:40<2:12:55,  6.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1044/2256 [1:54:46<2:12:45,  6.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1045/2256 [1:54:53<2:12:35,  6.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1046/2256 [1:54:59<2:12:20,  6.56s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1047/2256 [1:55:06<2:13:41,  6.64s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1048/2256 [1:55:13<2:14:30,  6.68s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1049/2256 [1:55:20<2:13:47,  6.65s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1050/2256 [1:55:26<2:13:12,  6.63s/it]                                                       {'loss': 1.7209, 'learning_rate': 2.7710135969421974e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1050/2256 [1:55:26<2:13:12,  6.63s/it]                                                       {'router_ce_loss': 0.970240592956543, 'old_lang_expert0_score': '0.24 0.07 0.1 0.3 0.82 0.83 0.46 0.89 0.83 0.92 0.58 0.91 0.7 0.89 0.93 0.89 0.91 0.9 0.94 0.94 0.92 0.94 0.98 0.99', 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1050/2256 [1:55:27<2:13:12,  6.63s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1051/2256 [1:55:33<2:12:49,  6.61s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1052/2256 [1:55:39<2:12:28,  6.60s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1053/2256 [1:55:46<2:12:14,  6.60s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1054/2256 [1:55:52<2:11:52,  6.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1055/2256 [1:55:59<2:11:33,  6.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1056/2256 [1:56:06<2:11:23,  6.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1057/2256 [1:56:12<2:11:20,  6.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1058/2256 [1:56:19<2:11:01,  6.56s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1059/2256 [1:56:25<2:10:54,  6.56s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1060/2256 [1:56:32<2:10:51,  6.56s/it]                                                       {'loss': 1.7216, 'learning_rate': 2.7363798556996555e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1060/2256 [1:56:32<2:10:51,  6.56s/it]                                                       {'router_ce_loss': 0.9549946188926697, 'old_lang_expert0_score': '0.25 0.02 0.05 0.25 0.87 0.89 0.51 0.92 0.86 0.94 0.61 0.94 0.76 0.91 0.95 0.91 0.92 0.93 0.96 0.96 0.97 0.96 0.98 0.99', 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1060/2256 [1:56:32<2:10:51,  6.56s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1061/2256 [1:56:38<2:10:42,  6.56s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1062/2256 [1:56:45<2:10:37,  6.56s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1063/2256 [1:56:52<2:10:34,  6.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1064/2256 [1:56:58<2:10:27,  6.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1065/2256 [1:57:05<2:11:42,  6.64s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1066/2256 [1:57:11<2:11:17,  6.62s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1067/2256 [1:57:18<2:10:54,  6.61s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1068/2256 [1:57:25<2:10:27,  6.59s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1069/2256 [1:57:31<2:10:10,  6.58s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1070/2256 [1:57:38<2:09:57,  6.57s/it]                                                       {'loss': 1.7492, 'learning_rate': 2.701700276526827e-05, 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1070/2256 [1:57:38<2:09:57,  6.57s/it]                                                       {'router_ce_loss': 0.9376683235168457, 'old_lang_expert0_score': '0.25 0.02 0.04 0.31 0.88 0.9 0.61 0.94 0.91 0.93 0.6 0.96 0.83 0.92 0.96 0.93 0.96 0.97 0.99 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.47}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1070/2256 [1:57:38<2:09:57,  6.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1071/2256 [1:57:44<2:10:11,  6.59s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1072/2256 [1:57:51<2:09:54,  6.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1073/2256 [1:57:57<2:09:34,  6.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1074/2256 [1:58:04<2:09:20,  6.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1075/2256 [1:58:11<2:09:16,  6.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1076/2256 [1:58:17<2:09:18,  6.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1077/2256 [1:58:24<2:09:10,  6.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1078/2256 [1:58:30<2:09:04,  6.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1079/2256 [1:58:37<2:08:56,  6.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1080/2256 [1:58:43<2:08:43,  6.57s/it]                                                       {'loss': 1.8019, 'learning_rate': 2.6669815843628042e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1080/2256 [1:58:43<2:08:43,  6.57s/it]                                                       {'router_ce_loss': 0.9462982416152954, 'old_lang_expert0_score': '0.24 0.02 0.05 0.29 0.84 0.85 0.54 0.91 0.87 0.95 0.62 0.95 0.82 0.94 0.97 0.93 0.95 0.97 0.97 0.97 0.97 0.98 0.99 0.99', 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1080/2256 [1:58:44<2:08:43,  6.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1081/2256 [1:58:50<2:08:34,  6.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1082/2256 [1:58:57<2:08:34,  6.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1083/2256 [1:59:04<2:10:58,  6.70s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1084/2256 [1:59:10<2:10:05,  6.66s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1085/2256 [1:59:17<2:09:24,  6.63s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1086/2256 [1:59:23<2:08:52,  6.61s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1087/2256 [1:59:30<2:08:36,  6.60s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1088/2256 [1:59:36<2:08:16,  6.59s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1089/2256 [1:59:43<2:08:00,  6.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1090/2256 [1:59:50<2:07:46,  6.57s/it]                                                       {'loss': 1.7407, 'learning_rate': 2.6322305117313322e-05, 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1090/2256 [1:59:50<2:07:46,  6.57s/it]                                                       {'router_ce_loss': 0.9553189873695374, 'old_lang_expert0_score': '0.3 0.03 0.04 0.19 0.86 0.84 0.54 0.94 0.88 0.94 0.63 0.94 0.82 0.92 0.92 0.91 0.89 0.93 0.95 0.95 0.95 0.95 0.99 0.98', 'epoch': 0.48}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1090/2256 [1:59:50<2:07:46,  6.57s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1091/2256 [1:59:56<2:08:22,  6.61s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1092/2256 [2:00:03<2:07:58,  6.60s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1093/2256 [2:00:09<2:07:43,  6.59s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1094/2256 [2:00:16<2:07:31,  6.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1095/2256 [2:00:23<2:07:20,  6.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1096/2256 [2:00:29<2:07:06,  6.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1097/2256 [2:00:36<2:06:55,  6.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1098/2256 [2:00:42<2:06:48,  6.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1099/2256 [2:00:49<2:06:40,  6.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1100/2256 [2:00:56<2:07:47,  6.63s/it]                                                       {'loss': 1.7606, 'learning_rate': 2.5974537974352592e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1100/2256 [2:00:56<2:07:47,  6.63s/it]                                                       {'router_ce_loss': 0.9440276622772217, 'old_lang_expert0_score': '0.25 0.02 0.05 0.25 0.87 0.89 0.52 0.95 0.91 0.94 0.63 0.95 0.83 0.94 0.96 0.93 0.95 0.95 0.97 0.96 0.97 0.97 0.98 0.99', 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1100/2256 [2:00:56<2:07:47,  6.63s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1101/2256 [2:01:02<2:08:21,  6.67s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1102/2256 [2:01:09<2:07:38,  6.64s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1103/2256 [2:01:15<2:07:09,  6.62s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1104/2256 [2:01:22<2:06:43,  6.60s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1105/2256 [2:01:29<2:06:32,  6.60s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1106/2256 [2:01:35<2:06:16,  6.59s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1107/2256 [2:01:42<2:06:07,  6.59s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1108/2256 [2:01:48<2:05:50,  6.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1109/2256 [2:01:55<2:05:40,  6.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1110/2256 [2:02:01<2:05:24,  6.57s/it]                                                       {'loss': 1.742, 'learning_rate': 2.562658185249771e-05, 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1110/2256 [2:02:01<2:05:24,  6.57s/it]                                                       {'router_ce_loss': 0.9479174613952637, 'old_lang_expert0_score': '0.26 0.03 0.07 0.33 0.91 0.93 0.61 0.92 0.86 0.92 0.5 0.93 0.76 0.93 0.94 0.91 0.93 0.94 0.96 0.97 0.95 0.96 0.99 0.99', 'epoch': 0.49}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1110/2256 [2:02:02<2:05:24,  6.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1111/2256 [2:02:08<2:05:18,  6.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1112/2256 [2:02:15<2:05:18,  6.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1113/2256 [2:02:21<2:05:15,  6.58s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1114/2256 [2:02:28<2:04:59,  6.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1115/2256 [2:02:34<2:04:58,  6.57s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1116/2256 [2:02:41<2:04:49,  6.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1117/2256 [2:02:47<2:04:47,  6.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1118/2256 [2:02:54<2:05:56,  6.64s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1119/2256 [2:03:01<2:05:26,  6.62s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1120/2256 [2:03:07<2:04:58,  6.60s/it]                                                       {'loss': 1.8413, 'learning_rate': 2.5278504226146636e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1120/2256 [2:03:07<2:04:58,  6.60s/it]                                                       {'router_ce_loss': 0.9440701603889465, 'old_lang_expert0_score': '0.23 0.02 0.04 0.28 0.86 0.91 0.62 0.93 0.9 0.93 0.7 0.93 0.82 0.92 0.94 0.89 0.93 0.95 0.96 0.96 0.95 0.97 0.99 0.99', 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1120/2256 [2:03:08<2:04:58,  6.60s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1121/2256 [2:03:14<2:04:42,  6.59s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1122/2256 [2:03:21<2:04:31,  6.59s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1123/2256 [2:03:27<2:04:16,  6.58s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1124/2256 [2:03:34<2:04:00,  6.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1125/2256 [2:03:40<2:03:51,  6.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1126/2256 [2:03:47<2:03:39,  6.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1127/2256 [2:03:53<2:03:34,  6.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1128/2256 [2:04:00<2:03:27,  6.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1129/2256 [2:04:06<2:03:15,  6.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1130/2256 [2:04:13<2:03:08,  6.56s/it]                                                       {'loss': 1.7398, 'learning_rate': 2.4930372593259086e-05, 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1130/2256 [2:04:13<2:03:08,  6.56s/it]                                                       {'router_ce_loss': 0.9677326679229736, 'old_lang_expert0_score': '0.24 0.04 0.06 0.24 0.83 0.86 0.53 0.94 0.9 0.93 0.65 0.92 0.77 0.91 0.94 0.92 0.9 0.92 0.93 0.91 0.86 0.89 0.91 0.99', 'epoch': 0.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1130/2256 [2:04:14<2:03:08,  6.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1131/2256 [2:04:20<2:02:59,  6.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1132/2256 [2:04:26<2:02:52,  6.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1133/2256 [2:04:33<2:02:46,  6.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1134/2256 [2:04:39<2:02:44,  6.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1135/2256 [2:04:46<2:02:38,  6.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1136/2256 [2:04:53<2:05:00,  6.70s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1137/2256 [2:04:59<2:04:02,  6.65s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1138/2256 [2:05:06<2:03:26,  6.62s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1139/2256 [2:05:12<2:02:57,  6.60s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1140/2256 [2:05:19<2:02:28,  6.58s/it]                                                       {'loss': 1.7783, 'learning_rate': 2.4582254462267476e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1140/2256 [2:05:19<2:02:28,  6.58s/it]                                                       {'router_ce_loss': 0.9642186760902405, 'old_lang_expert0_score': '0.23 0.02 0.05 0.25 0.86 0.83 0.48 0.91 0.88 0.93 0.6 0.94 0.81 0.93 0.96 0.91 0.9 0.92 0.95 0.94 0.94 0.93 0.98 0.98', 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1140/2256 [2:05:20<2:02:28,  6.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1141/2256 [2:05:26<2:02:10,  6.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1142/2256 [2:05:32<2:02:02,  6.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1143/2256 [2:05:39<2:01:55,  6.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1144/2256 [2:05:45<2:01:47,  6.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1145/2256 [2:05:52<2:01:37,  6.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1146/2256 [2:05:58<2:01:32,  6.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1147/2256 [2:06:05<2:01:24,  6.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1148/2256 [2:06:12<2:01:20,  6.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1149/2256 [2:06:18<2:01:11,  6.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1150/2256 [2:06:25<2:01:02,  6.57s/it]                                                       {'loss': 1.7058, 'learning_rate': 2.423421733898603e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1150/2256 [2:06:25<2:01:02,  6.57s/it]                                                       {'router_ce_loss': 0.9391968846321106, 'old_lang_expert0_score': '0.24 0.02 0.06 0.28 0.88 0.9 0.61 0.93 0.91 0.93 0.71 0.95 0.82 0.92 0.96 0.9 0.94 0.96 0.97 0.96 0.97 0.96 0.99 0.99', 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1150/2256 [2:06:25<2:01:02,  6.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1151/2256 [2:06:31<2:00:59,  6.57s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1152/2256 [2:06:38<2:00:46,  6.56s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1153/2256 [2:06:45<2:01:54,  6.63s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1154/2256 [2:06:51<2:02:35,  6.67s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1155/2256 [2:06:58<2:01:54,  6.64s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1156/2256 [2:07:04<2:01:14,  6.61s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1157/2256 [2:07:11<2:00:51,  6.60s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1158/2256 [2:07:18<2:00:33,  6.59s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1159/2256 [2:07:24<2:00:18,  6.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1160/2256 [2:07:31<2:00:07,  6.58s/it]                                                       {'loss': 1.7532, 'learning_rate': 2.3886328713520194e-05, 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1160/2256 [2:07:31<2:00:07,  6.58s/it]                                                       {'router_ce_loss': 0.9538329839706421, 'old_lang_expert0_score': '0.24 0.03 0.06 0.34 0.85 0.88 0.56 0.91 0.84 0.93 0.59 0.95 0.76 0.93 0.95 0.91 0.92 0.94 0.96 0.95 0.95 0.95 0.98 0.99', 'epoch': 0.51}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1160/2256 [2:07:31<2:00:07,  6.58s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1161/2256 [2:07:37<1:59:52,  6.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1162/2256 [2:07:44<1:59:41,  6.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1163/2256 [2:07:50<1:59:30,  6.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1164/2256 [2:07:57<1:59:24,  6.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1165/2256 [2:08:04<1:59:18,  6.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1166/2256 [2:08:10<1:59:07,  6.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1167/2256 [2:08:17<1:59:07,  6.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1168/2256 [2:08:23<1:59:03,  6.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1169/2256 [2:08:30<1:58:48,  6.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1170/2256 [2:08:36<1:58:48,  6.56s/it]                                                       {'loss': 1.7871, 'learning_rate': 2.3538656047179324e-05, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1170/2256 [2:08:36<1:58:48,  6.56s/it]                                                       {'router_ce_loss': 0.9317331910133362, 'old_lang_expert0_score': '0.24 0.03 0.07 0.32 0.9 0.92 0.63 0.94 0.9 0.94 0.67 0.96 0.83 0.94 0.97 0.93 0.95 0.96 0.97 0.97 0.95 0.97 0.99 0.99', 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1170/2256 [2:08:37<1:58:48,  6.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1171/2256 [2:08:43<1:59:58,  6.63s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1172/2256 [2:08:50<1:59:27,  6.61s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1173/2256 [2:08:56<1:59:01,  6.59s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1174/2256 [2:09:03<1:58:43,  6.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1175/2256 [2:09:09<1:58:27,  6.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1176/2256 [2:09:16<1:58:21,  6.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1177/2256 [2:09:23<1:58:09,  6.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1178/2256 [2:09:29<1:57:59,  6.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1179/2256 [2:09:36<1:58:01,  6.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1180/2256 [2:09:42<1:57:48,  6.57s/it]                                                       {'loss': 1.7382, 'learning_rate': 2.3191266759394686e-05, 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1180/2256 [2:09:42<1:57:48,  6.57s/it]                                                       {'router_ce_loss': 0.9388006925582886, 'old_lang_expert0_score': '0.25 0.02 0.04 0.27 0.89 0.91 0.6 0.96 0.91 0.94 0.61 0.94 0.81 0.95 0.97 0.94 0.95 0.96 0.98 0.97 0.97 0.96 0.99 0.99', 'epoch': 0.52}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1180/2256 [2:09:43<1:57:48,  6.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1181/2256 [2:09:49<1:57:34,  6.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1182/2256 [2:09:55<1:57:29,  6.56s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1183/2256 [2:10:02<1:57:24,  6.57s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1184/2256 [2:10:08<1:57:20,  6.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1185/2256 [2:10:15<1:57:14,  6.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1186/2256 [2:10:22<1:57:06,  6.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1187/2256 [2:10:28<1:56:56,  6.56s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1188/2256 [2:10:35<1:56:48,  6.56s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1189/2256 [2:10:42<1:57:51,  6.63s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1190/2256 [2:10:48<1:58:36,  6.68s/it]                                                       {'loss': 1.7571, 'learning_rate': 2.284422821464585e-05, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1190/2256 [2:10:48<1:58:36,  6.68s/it]                                                       {'router_ce_loss': 0.9375882148742676, 'old_lang_expert0_score': '0.24 0.02 0.05 0.31 0.9 0.94 0.58 0.94 0.9 0.93 0.67 0.95 0.84 0.94 0.96 0.91 0.92 0.95 0.97 0.97 0.96 0.96 0.98 0.99', 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1190/2256 [2:10:49<1:58:36,  6.68s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1191/2256 [2:10:55<1:57:50,  6.64s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1192/2256 [2:11:01<1:57:18,  6.62s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1193/2256 [2:11:08<1:56:57,  6.60s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1194/2256 [2:11:15<1:56:32,  6.58s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1195/2256 [2:11:21<1:56:26,  6.58s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1196/2256 [2:11:28<1:56:08,  6.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1197/2256 [2:11:34<1:56:02,  6.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1198/2256 [2:11:41<1:55:51,  6.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1199/2256 [2:11:47<1:55:37,  6.56s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1200/2256 [2:11:54<1:55:28,  6.56s/it]                                                       {'loss': 1.7842, 'learning_rate': 2.2497607709397543e-05, 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1200/2256 [2:11:54<1:55:28,  6.56s/it]                                                       {'router_ce_loss': 0.9398179650306702, 'old_lang_expert0_score': '0.25 0.02 0.04 0.27 0.88 0.91 0.58 0.95 0.9 0.94 0.67 0.95 0.85 0.94 0.96 0.93 0.93 0.94 0.97 0.96 0.96 0.96 0.99 0.99', 'epoch': 0.53}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1200/2256 [2:11:54<1:55:28,  6.56s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1201/2256 [2:12:00<1:55:27,  6.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1202/2256 [2:12:07<1:55:21,  6.57s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1203/2256 [2:12:14<1:55:11,  6.56s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1204/2256 [2:12:20<1:55:04,  6.56s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1205/2256 [2:12:27<1:54:56,  6.56s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1206/2256 [2:12:33<1:54:50,  6.56s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1207/2256 [2:12:40<1:55:55,  6.63s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1208/2256 [2:12:47<1:56:39,  6.68s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1209/2256 [2:12:53<1:55:50,  6.64s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1210/2256 [2:13:00<1:55:19,  6.62s/it]                                                       {'loss': 1.7554, 'learning_rate': 2.215147245904977e-05, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1210/2256 [2:13:00<1:55:19,  6.62s/it]                                                       {'router_ce_loss': 0.9475894570350647, 'old_lang_expert0_score': '0.25 0.02 0.06 0.28 0.87 0.9 0.61 0.93 0.87 0.92 0.65 0.94 0.83 0.94 0.97 0.91 0.92 0.93 0.95 0.94 0.94 0.94 0.98 0.98', 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1210/2256 [2:13:00<1:55:19,  6.62s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1211/2256 [2:13:07<1:54:49,  6.59s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1212/2256 [2:13:13<1:54:37,  6.59s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1213/2256 [2:13:20<1:54:25,  6.58s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1214/2256 [2:13:26<1:54:14,  6.58s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1215/2256 [2:13:33<1:53:59,  6.57s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1216/2256 [2:13:39<1:53:48,  6.57s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1217/2256 [2:13:46<1:53:38,  6.56s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1218/2256 [2:13:52<1:53:39,  6.57s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1219/2256 [2:13:59<1:53:32,  6.57s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1220/2256 [2:14:06<1:53:18,  6.56s/it]                                                       {'loss': 1.7072, 'learning_rate': 2.1805889584903667e-05, 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1220/2256 [2:14:06<1:53:18,  6.56s/it]                                                       {'router_ce_loss': 0.9454740285873413, 'old_lang_expert0_score': '0.24 0.02 0.06 0.35 0.89 0.9 0.59 0.93 0.88 0.93 0.62 0.92 0.75 0.91 0.95 0.91 0.93 0.94 0.97 0.96 0.97 0.97 0.99 0.99', 'epoch': 0.54}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1220/2256 [2:14:06<1:53:18,  6.56s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1221/2256 [2:14:12<1:53:15,  6.57s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1222/2256 [2:14:19<1:53:06,  6.56s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1223/2256 [2:14:25<1:53:05,  6.57s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1224/2256 [2:14:32<1:52:59,  6.57s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1225/2256 [2:14:39<1:54:03,  6.64s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1226/2256 [2:14:45<1:53:31,  6.61s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1227/2256 [2:14:52<1:53:08,  6.60s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1228/2256 [2:14:58<1:52:52,  6.59s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1229/2256 [2:15:05<1:52:42,  6.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1230/2256 [2:15:12<1:52:32,  6.58s/it]                                                       {'loss': 1.7683, 'learning_rate': 2.1460926101145547e-05, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1230/2256 [2:15:12<1:52:32,  6.58s/it]                                                       {'router_ce_loss': 0.9430671334266663, 'old_lang_expert0_score': '0.25 0.02 0.05 0.29 0.89 0.93 0.55 0.94 0.9 0.93 0.61 0.93 0.84 0.92 0.95 0.91 0.93 0.96 0.97 0.96 0.97 0.97 0.99 0.99', 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1230/2256 [2:15:12<1:52:32,  6.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1231/2256 [2:15:18<1:52:27,  6.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1232/2256 [2:15:25<1:52:16,  6.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1233/2256 [2:15:31<1:52:02,  6.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1234/2256 [2:15:38<1:51:53,  6.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1235/2256 [2:15:44<1:51:48,  6.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1236/2256 [2:15:51<1:51:39,  6.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1237/2256 [2:15:57<1:51:36,  6.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1238/2256 [2:16:04<1:51:21,  6.56s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1239/2256 [2:16:11<1:51:12,  6.56s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1240/2256 [2:16:17<1:51:08,  6.56s/it]                                                       {'loss': 1.844, 'learning_rate': 2.1116648901851793e-05, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1240/2256 [2:16:17<1:51:08,  6.56s/it]                                                       {'router_ce_loss': 0.9370062947273254, 'old_lang_expert0_score': '0.24 0.02 0.04 0.32 0.9 0.93 0.6 0.94 0.9 0.93 0.68 0.95 0.83 0.94 0.95 0.91 0.93 0.95 0.97 0.96 0.96 0.96 0.99 0.99', 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1240/2256 [2:16:18<1:51:08,  6.56s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1241/2256 [2:16:24<1:51:03,  6.56s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1242/2256 [2:16:30<1:50:56,  6.56s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1243/2256 [2:16:37<1:53:00,  6.69s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1244/2256 [2:16:44<1:52:16,  6.66s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1245/2256 [2:16:50<1:51:38,  6.63s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1246/2256 [2:16:57<1:51:12,  6.61s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1247/2256 [2:17:04<1:50:48,  6.59s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1248/2256 [2:17:10<1:50:34,  6.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1249/2256 [2:17:17<1:50:21,  6.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1250/2256 [2:17:23<1:50:08,  6.57s/it]                                                       {'loss': 1.7733, 'learning_rate': 2.0773124748016956e-05, 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1250/2256 [2:17:23<1:50:08,  6.57s/it]                                                       {'router_ce_loss': 0.9434638619422913, 'old_lang_expert0_score': '0.23 0.01 0.03 0.32 0.9 0.9 0.55 0.96 0.88 0.94 0.69 0.95 0.76 0.89 0.96 0.93 0.94 0.92 0.97 0.97 0.97 0.97 0.98 0.99', 'epoch': 0.55}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1250/2256 [2:17:24<1:50:08,  6.57s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1251/2256 [2:17:30<1:50:08,  6.58s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1252/2256 [2:17:36<1:49:56,  6.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1253/2256 [2:17:43<1:49:43,  6.56s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1254/2256 [2:17:49<1:49:40,  6.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1255/2256 [2:17:56<1:49:29,  6.56s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1256/2256 [2:18:03<1:49:25,  6.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1257/2256 [2:18:09<1:49:19,  6.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1258/2256 [2:18:16<1:49:11,  6.56s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1259/2256 [2:18:22<1:49:04,  6.56s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1260/2256 [2:18:29<1:50:10,  6.64s/it]                                                       {'loss': 1.7346, 'learning_rate': 2.0430420254607748e-05, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1260/2256 [2:18:29<1:50:10,  6.64s/it]                                                       {'router_ce_loss': 0.9441368579864502, 'old_lang_expert0_score': '0.25 0.02 0.04 0.32 0.88 0.9 0.59 0.93 0.9 0.94 0.65 0.94 0.81 0.94 0.96 0.92 0.92 0.95 0.95 0.96 0.94 0.95 0.98 0.99', 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1260/2256 [2:18:30<1:50:10,  6.64s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1261/2256 [2:18:36<1:50:41,  6.67s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1262/2256 [2:18:42<1:50:01,  6.64s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1263/2256 [2:18:49<1:49:23,  6.61s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1264/2256 [2:18:56<1:49:05,  6.60s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1265/2256 [2:19:02<1:48:46,  6.59s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1266/2256 [2:19:09<1:48:32,  6.58s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1267/2256 [2:19:15<1:48:21,  6.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1268/2256 [2:19:22<1:48:10,  6.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1269/2256 [2:19:28<1:48:03,  6.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1270/2256 [2:19:35<1:48:01,  6.57s/it]                                                       {'loss': 1.7627, 'learning_rate': 2.0088601877645245e-05, 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1270/2256 [2:19:35<1:48:01,  6.57s/it]                                                       {'router_ce_loss': 0.9389492869377136, 'old_lang_expert0_score': '0.25 0.02 0.05 0.31 0.88 0.89 0.59 0.93 0.88 0.92 0.68 0.95 0.84 0.94 0.96 0.92 0.94 0.96 0.98 0.96 0.96 0.97 0.98 0.99', 'epoch': 0.56}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1270/2256 [2:19:35<1:48:01,  6.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1271/2256 [2:19:41<1:47:53,  6.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1272/2256 [2:19:48<1:47:46,  6.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1273/2256 [2:19:55<1:47:40,  6.57s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1274/2256 [2:20:01<1:47:35,  6.57s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1275/2256 [2:20:08<1:47:23,  6.57s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1276/2256 [2:20:14<1:47:15,  6.57s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1277/2256 [2:20:21<1:47:13,  6.57s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1278/2256 [2:20:28<1:48:05,  6.63s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1279/2256 [2:20:34<1:47:33,  6.61s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1280/2256 [2:20:41<1:47:12,  6.59s/it]                                                       {'loss': 1.7121, 'learning_rate': 1.974773590131805e-05, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1280/2256 [2:20:41<1:47:12,  6.59s/it]                                                       {'router_ce_loss': 0.9405147433280945, 'old_lang_expert0_score': '0.25 0.01 0.04 0.3 0.9 0.92 0.59 0.92 0.89 0.93 0.65 0.92 0.84 0.91 0.95 0.91 0.93 0.96 0.98 0.96 0.97 0.97 0.99 0.99', 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1280/2256 [2:20:41<1:47:12,  6.59s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1281/2256 [2:20:47<1:46:58,  6.58s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1282/2256 [2:20:54<1:46:46,  6.58s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1283/2256 [2:21:00<1:46:38,  6.58s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1284/2256 [2:21:07<1:46:29,  6.57s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1285/2256 [2:21:14<1:46:22,  6.57s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1286/2256 [2:21:20<1:46:20,  6.58s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1287/2256 [2:21:27<1:46:12,  6.58s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1288/2256 [2:21:33<1:46:04,  6.57s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1289/2256 [2:21:40<1:45:53,  6.57s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1290/2256 [2:21:46<1:45:38,  6.56s/it]                                                       {'loss': 1.7137, 'learning_rate': 1.9407888425128663e-05, 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1290/2256 [2:21:46<1:45:38,  6.56s/it]                                                       {'router_ce_loss': 0.953266441822052, 'old_lang_expert0_score': '0.24 0.02 0.05 0.31 0.85 0.87 0.57 0.92 0.88 0.93 0.67 0.94 0.82 0.94 0.97 0.93 0.92 0.93 0.95 0.94 0.91 0.88 0.95 0.98', 'epoch': 0.57}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1290/2256 [2:21:47<1:45:38,  6.56s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1291/2256 [2:21:53<1:45:31,  6.56s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1292/2256 [2:22:00<1:45:24,  6.56s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1293/2256 [2:22:06<1:45:21,  6.56s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1294/2256 [2:22:13<1:45:14,  6.56s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1295/2256 [2:22:19<1:45:07,  6.56s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1296/2256 [2:22:26<1:47:04,  6.69s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1297/2256 [2:22:33<1:46:18,  6.65s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1298/2256 [2:22:39<1:45:46,  6.62s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1299/2256 [2:22:46<1:45:25,  6.61s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1300/2256 [2:22:53<1:45:04,  6.59s/it]                                                       {'loss': 1.7283, 'learning_rate': 1.9069125351075667e-05, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1300/2256 [2:22:53<1:45:04,  6.59s/it]                                                       {'router_ce_loss': 0.9251596927642822, 'old_lang_expert0_score': '0.27 0.02 0.04 0.35 0.92 0.93 0.68 0.95 0.91 0.94 0.71 0.94 0.84 0.95 0.97 0.94 0.95 0.97 0.97 0.97 0.97 0.97 0.98 0.99', 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1300/2256 [2:22:53<1:45:04,  6.59s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1301/2256 [2:22:59<1:44:54,  6.59s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1302/2256 [2:23:06<1:44:40,  6.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1303/2256 [2:23:12<1:44:27,  6.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1304/2256 [2:23:19<1:44:15,  6.57s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1305/2256 [2:23:25<1:44:13,  6.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1306/2256 [2:23:32<1:44:12,  6.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1307/2256 [2:23:39<1:44:02,  6.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1308/2256 [2:23:45<1:43:53,  6.58s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1309/2256 [2:23:52<1:43:44,  6.57s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1310/2256 [2:23:58<1:43:36,  6.57s/it]                                                       {'loss': 1.8086, 'learning_rate': 1.8731512370874333e-05, 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1310/2256 [2:23:58<1:43:36,  6.57s/it]                                                       {'router_ce_loss': 0.9574238657951355, 'old_lang_expert0_score': '0.24 0.03 0.07 0.34 0.86 0.85 0.54 0.9 0.85 0.93 0.65 0.92 0.75 0.91 0.94 0.87 0.89 0.93 0.96 0.95 0.95 0.96 0.98 0.99', 'epoch': 0.58}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1310/2256 [2:23:59<1:43:36,  6.57s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1311/2256 [2:24:05<1:43:31,  6.57s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1312/2256 [2:24:11<1:43:23,  6.57s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1313/2256 [2:24:18<1:44:20,  6.64s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1314/2256 [2:24:25<1:44:47,  6.68s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1315/2256 [2:24:32<1:44:14,  6.65s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1316/2256 [2:24:38<1:43:48,  6.63s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1317/2256 [2:24:45<1:43:27,  6.61s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1318/2256 [2:24:51<1:43:08,  6.60s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1319/2256 [2:24:58<1:42:50,  6.59s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1320/2256 [2:25:04<1:42:35,  6.58s/it]                                                       {'loss': 1.8005, 'learning_rate': 1.8395114953217852e-05, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1320/2256 [2:25:04<1:42:35,  6.58s/it]                                                       {'router_ce_loss': 0.936141312122345, 'old_lang_expert0_score': '0.24 0.02 0.05 0.38 0.84 0.87 0.62 0.91 0.88 0.94 0.74 0.95 0.87 0.95 0.98 0.93 0.93 0.95 0.95 0.95 0.94 0.95 0.99 0.99', 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1320/2256 [2:25:05<1:42:35,  6.58s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1321/2256 [2:25:11<1:42:28,  6.58s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1322/2256 [2:25:18<1:42:22,  6.58s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1323/2256 [2:25:24<1:42:07,  6.57s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1324/2256 [2:25:31<1:42:02,  6.57s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1325/2256 [2:25:37<1:41:53,  6.57s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1326/2256 [2:25:44<1:41:46,  6.57s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1327/2256 [2:25:50<1:41:30,  6.56s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1328/2256 [2:25:57<1:41:30,  6.56s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1329/2256 [2:26:03<1:41:20,  6.56s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1330/2256 [2:26:10<1:41:23,  6.57s/it]                                                       {'loss': 1.7641, 'learning_rate': 1.805999833108191e-05, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1330/2256 [2:26:10<1:41:23,  6.57s/it]                                                       {'router_ce_loss': 0.9318813681602478, 'old_lang_expert0_score': '0.27 0.03 0.07 0.3 0.87 0.89 0.63 0.95 0.92 0.94 0.77 0.95 0.86 0.94 0.95 0.9 0.92 0.95 0.97 0.96 0.96 0.95 0.99 0.98', 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1330/2256 [2:26:11<1:41:23,  6.57s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1331/2256 [2:26:17<1:42:26,  6.64s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1332/2256 [2:26:23<1:41:50,  6.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1333/2256 [2:26:30<1:41:23,  6.59s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1334/2256 [2:26:36<1:41:12,  6.59s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1335/2256 [2:26:43<1:41:07,  6.59s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1336/2256 [2:26:50<1:41:13,  6.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1337/2256 [2:26:56<1:41:03,  6.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1338/2256 [2:27:03<1:40:47,  6.59s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1339/2256 [2:27:10<1:41:04,  6.61s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1340/2256 [2:27:16<1:40:44,  6.60s/it]                                                       {'loss': 1.7776, 'learning_rate': 1.772622748907493e-05, 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1340/2256 [2:27:16<1:40:44,  6.60s/it]                                                       {'router_ce_loss': 0.9226300120353699, 'old_lang_expert0_score': '0.26 0.02 0.06 0.41 0.91 0.93 0.66 0.94 0.91 0.94 0.7 0.95 0.85 0.94 0.97 0.94 0.95 0.97 0.98 0.97 0.97 0.98 0.99 0.99', 'epoch': 0.59}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1340/2256 [2:27:17<1:40:44,  6.60s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1341/2256 [2:27:23<1:40:32,  6.59s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1342/2256 [2:27:29<1:40:20,  6.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1343/2256 [2:27:36<1:40:11,  6.58s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1344/2256 [2:27:42<1:40:08,  6.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1345/2256 [2:27:49<1:39:59,  6.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1346/2256 [2:27:56<1:39:50,  6.58s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1347/2256 [2:28:02<1:39:42,  6.58s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1348/2256 [2:28:09<1:39:33,  6.58s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1349/2256 [2:28:16<1:41:32,  6.72s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1350/2256 [2:28:22<1:40:48,  6.68s/it]                                                       {'loss': 1.7524, 'learning_rate': 1.739386715083651e-05, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1350/2256 [2:28:22<1:40:48,  6.68s/it]                                                       {'router_ce_loss': 0.9724724888801575, 'old_lang_expert0_score': '0.24 0.02 0.12 0.28 0.74 0.74 0.48 0.81 0.81 0.93 0.7 0.94 0.75 0.88 0.97 0.87 0.88 0.92 0.95 0.94 0.97 0.96 0.98 0.99', 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1350/2256 [2:28:23<1:40:48,  6.68s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1351/2256 [2:28:29<1:40:17,  6.65s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1352/2256 [2:28:36<1:39:54,  6.63s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1353/2256 [2:28:42<1:39:29,  6.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1354/2256 [2:28:49<1:39:22,  6.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1355/2256 [2:28:55<1:39:12,  6.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1356/2256 [2:29:02<1:38:59,  6.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1357/2256 [2:29:08<1:38:52,  6.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1358/2256 [2:29:15<1:38:42,  6.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1359/2256 [2:29:22<1:38:29,  6.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1360/2256 [2:29:28<1:38:22,  6.59s/it]                                                       {'loss': 1.6792, 'learning_rate': 1.7062981766486437e-05, 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1360/2256 [2:29:28<1:38:22,  6.59s/it]                                                       {'router_ce_loss': 0.9353405833244324, 'old_lang_expert0_score': '0.26 0.02 0.05 0.35 0.89 0.92 0.63 0.95 0.88 0.92 0.69 0.94 0.83 0.92 0.94 0.91 0.91 0.95 0.98 0.97 0.97 0.98 0.99 0.99', 'epoch': 0.6}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1360/2256 [2:29:29<1:38:22,  6.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1361/2256 [2:29:35<1:38:14,  6.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1362/2256 [2:29:41<1:38:02,  6.58s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1363/2256 [2:29:48<1:37:58,  6.58s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1364/2256 [2:29:55<1:37:48,  6.58s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1365/2256 [2:30:01<1:37:44,  6.58s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1366/2256 [2:30:08<1:38:40,  6.65s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1367/2256 [2:30:15<1:39:08,  6.69s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1368/2256 [2:30:21<1:38:23,  6.65s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1369/2256 [2:30:28<1:37:55,  6.62s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1370/2256 [2:30:34<1:37:37,  6.61s/it]                                                       {'loss': 1.7798, 'learning_rate': 1.6733635500126742e-05, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1370/2256 [2:30:34<1:37:37,  6.61s/it]                                                       {'router_ce_loss': 0.9386036992073059, 'old_lang_expert0_score': '0.23 0.04 0.07 0.34 0.88 0.89 0.63 0.94 0.88 0.94 0.71 0.95 0.82 0.93 0.95 0.9 0.92 0.94 0.96 0.94 0.95 0.95 0.99 0.99', 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1370/2256 [2:30:35<1:37:37,  6.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1371/2256 [2:30:41<1:37:23,  6.60s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1372/2256 [2:30:48<1:37:10,  6.60s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1373/2256 [2:30:54<1:36:57,  6.59s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1374/2256 [2:31:01<1:36:43,  6.58s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1375/2256 [2:31:07<1:36:36,  6.58s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1376/2256 [2:31:14<1:36:25,  6.57s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1377/2256 [2:31:20<1:36:17,  6.57s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1378/2256 [2:31:27<1:36:11,  6.57s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1379/2256 [2:31:34<1:36:03,  6.57s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1380/2256 [2:31:40<1:35:54,  6.57s/it]                                                       {'loss': 1.7036, 'learning_rate': 1.640589221739926e-05, 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1380/2256 [2:31:40<1:35:54,  6.57s/it]                                                       {'router_ce_loss': 0.9234501719474792, 'old_lang_expert0_score': '0.26 0.02 0.07 0.33 0.89 0.92 0.73 0.95 0.91 0.94 0.75 0.95 0.86 0.94 0.97 0.91 0.95 0.96 0.98 0.97 0.97 0.98 0.98 0.99', 'epoch': 0.61}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1380/2256 [2:31:41<1:35:54,  6.57s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1381/2256 [2:31:47<1:35:46,  6.57s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1382/2256 [2:31:53<1:35:43,  6.57s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1383/2256 [2:32:00<1:35:39,  6.58s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1384/2256 [2:32:07<1:36:33,  6.64s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1385/2256 [2:32:13<1:36:05,  6.62s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1386/2256 [2:32:20<1:35:48,  6.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1387/2256 [2:32:26<1:35:22,  6.58s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1388/2256 [2:32:33<1:35:09,  6.58s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1389/2256 [2:32:39<1:34:57,  6.57s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1390/2256 [2:32:46<1:34:49,  6.57s/it]                                                       {'loss': 1.7461, 'learning_rate': 1.6079815473100972e-05, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1390/2256 [2:32:46<1:34:49,  6.57s/it]                                                       {'router_ce_loss': 0.9352242350578308, 'old_lang_expert0_score': '0.25 0.03 0.07 0.26 0.82 0.87 0.61 0.94 0.9 0.95 0.73 0.96 0.86 0.95 0.98 0.94 0.94 0.95 0.98 0.97 0.96 0.96 0.98 0.98', 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1390/2256 [2:32:47<1:34:49,  6.57s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1391/2256 [2:32:53<1:34:42,  6.57s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1392/2256 [2:32:59<1:34:31,  6.56s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1393/2256 [2:33:06<1:34:26,  6.57s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1394/2256 [2:33:12<1:34:18,  6.56s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1395/2256 [2:33:19<1:34:14,  6.57s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1396/2256 [2:33:25<1:34:04,  6.56s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1397/2256 [2:33:32<1:34:03,  6.57s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1398/2256 [2:33:39<1:33:55,  6.57s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1399/2256 [2:33:45<1:33:53,  6.57s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1400/2256 [2:33:52<1:33:44,  6.57s/it]                                                       {'loss': 1.8128, 'learning_rate': 1.5755468498859777e-05, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1400/2256 [2:33:52<1:33:44,  6.57s/it]                                                       {'router_ce_loss': 0.9403576254844666, 'old_lang_expert0_score': '0.22 0.02 0.04 0.26 0.88 0.9 0.62 0.93 0.84 0.94 0.74 0.96 0.87 0.94 0.96 0.92 0.92 0.93 0.94 0.96 0.96 0.96 0.99 0.99', 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1400/2256 [2:33:52<1:33:44,  6.57s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1401/2256 [2:33:58<1:33:38,  6.57s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1402/2256 [2:34:05<1:34:22,  6.63s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1403/2256 [2:34:12<1:34:59,  6.68s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1404/2256 [2:34:18<1:34:24,  6.65s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1405/2256 [2:34:25<1:34:03,  6.63s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1406/2256 [2:34:32<1:33:51,  6.63s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1407/2256 [2:34:38<1:33:38,  6.62s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1408/2256 [2:34:45<1:33:25,  6.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1409/2256 [2:34:51<1:33:15,  6.61s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1410/2256 [2:34:58<1:33:05,  6.60s/it]                                                       {'loss': 1.6715, 'learning_rate': 1.5432914190872757e-05, 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1410/2256 [2:34:58<1:33:05,  6.60s/it]                                                       {'router_ce_loss': 0.949816882610321, 'old_lang_expert0_score': '0.26 0.04 0.07 0.32 0.86 0.89 0.62 0.91 0.88 0.91 0.7 0.92 0.84 0.93 0.94 0.92 0.9 0.93 0.94 0.93 0.9 0.91 0.98 0.98', 'epoch': 0.62}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1410/2256 [2:34:59<1:33:05,  6.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1411/2256 [2:35:05<1:33:17,  6.62s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1412/2256 [2:35:11<1:32:59,  6.61s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1413/2256 [2:35:18<1:32:36,  6.59s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1414/2256 [2:35:24<1:32:24,  6.59s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1415/2256 [2:35:31<1:32:15,  6.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1416/2256 [2:35:38<1:32:07,  6.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1417/2256 [2:35:44<1:31:54,  6.57s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1418/2256 [2:35:51<1:31:52,  6.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1419/2256 [2:35:57<1:31:46,  6.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1420/2256 [2:36:04<1:32:33,  6.64s/it]                                                       {'loss': 1.8194, 'learning_rate': 1.5112215097709637e-05, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1420/2256 [2:36:04<1:32:33,  6.64s/it]                                                       {'router_ce_loss': 0.9306651949882507, 'old_lang_expert0_score': '0.23 0.03 0.05 0.34 0.87 0.93 0.66 0.94 0.87 0.94 0.76 0.95 0.86 0.94 0.96 0.94 0.93 0.96 0.97 0.97 0.97 0.96 0.99 0.98', 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1420/2256 [2:36:05<1:32:33,  6.64s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1421/2256 [2:36:11<1:33:17,  6.70s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1422/2256 [2:36:17<1:32:39,  6.67s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1423/2256 [2:36:24<1:32:12,  6.64s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1424/2256 [2:36:31<1:31:56,  6.63s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1425/2256 [2:36:37<1:31:36,  6.61s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1426/2256 [2:36:44<1:31:19,  6.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1427/2256 [2:36:50<1:31:11,  6.60s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1428/2256 [2:36:57<1:30:59,  6.59s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1429/2256 [2:37:04<1:30:48,  6.59s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1430/2256 [2:37:10<1:30:38,  6.58s/it]                                                       {'loss': 1.7159, 'learning_rate': 1.4793433408183553e-05, 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1430/2256 [2:37:10<1:30:38,  6.58s/it]                                                       {'router_ce_loss': 0.9553559422492981, 'old_lang_expert0_score': '0.21 0.01 0.01 0.2 0.78 0.92 0.41 0.96 0.93 0.95 0.77 0.95 0.88 0.94 0.98 0.94 0.91 0.93 0.97 0.95 0.94 0.87 0.97 0.98', 'epoch': 0.63}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1430/2256 [2:37:11<1:30:38,  6.58s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1431/2256 [2:37:17<1:31:01,  6.62s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1432/2256 [2:37:23<1:30:44,  6.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1433/2256 [2:37:30<1:30:34,  6.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1434/2256 [2:37:37<1:30:23,  6.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1435/2256 [2:37:43<1:30:11,  6.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1436/2256 [2:37:50<1:29:58,  6.58s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1437/2256 [2:37:56<1:29:51,  6.58s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1438/2256 [2:38:03<1:30:39,  6.65s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1439/2256 [2:38:10<1:30:16,  6.63s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1440/2256 [2:38:16<1:29:55,  6.61s/it]                                                       {'loss': 1.783, 'learning_rate': 1.447663093929163e-05, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1440/2256 [2:38:16<1:29:55,  6.61s/it]                                                       {'router_ce_loss': 0.9364319443702698, 'old_lang_expert0_score': '0.23 0.02 0.07 0.31 0.89 0.89 0.67 0.94 0.89 0.93 0.71 0.94 0.82 0.93 0.96 0.91 0.94 0.94 0.96 0.95 0.95 0.96 0.99 0.99', 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1440/2256 [2:38:17<1:29:55,  6.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1441/2256 [2:38:23<1:29:58,  6.62s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1442/2256 [2:38:30<1:29:44,  6.61s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1443/2256 [2:38:36<1:29:26,  6.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1444/2256 [2:38:43<1:29:14,  6.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1445/2256 [2:38:49<1:29:05,  6.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1446/2256 [2:38:56<1:28:59,  6.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1447/2256 [2:39:02<1:28:51,  6.59s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1448/2256 [2:39:09<1:28:50,  6.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1449/2256 [2:39:16<1:28:43,  6.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1450/2256 [2:39:22<1:28:35,  6.60s/it]                                                       {'loss': 1.8563, 'learning_rate': 1.4161869124227655e-05, 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1450/2256 [2:39:22<1:28:35,  6.60s/it]                                                       {'router_ce_loss': 0.9396675229072571, 'old_lang_expert0_score': '0.22 0.03 0.06 0.36 0.84 0.87 0.56 0.94 0.87 0.92 0.79 0.94 0.89 0.93 0.97 0.92 0.93 0.94 0.96 0.95 0.95 0.95 0.97 0.97', 'epoch': 0.64}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1450/2256 [2:39:23<1:28:35,  6.60s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1451/2256 [2:39:29<1:29:00,  6.63s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1452/2256 [2:39:36<1:28:54,  6.64s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1453/2256 [2:39:42<1:28:43,  6.63s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1454/2256 [2:39:49<1:28:31,  6.62s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1455/2256 [2:39:55<1:28:15,  6.61s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1456/2256 [2:40:02<1:29:36,  6.72s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1457/2256 [2:40:09<1:28:57,  6.68s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1458/2256 [2:40:16<1:28:19,  6.64s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1459/2256 [2:40:22<1:27:52,  6.62s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1460/2256 [2:40:29<1:27:33,  6.60s/it]                                                       {'loss': 1.861, 'learning_rate': 1.3849209000469188e-05, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1460/2256 [2:40:29<1:27:33,  6.60s/it]                                                       {'router_ce_loss': 0.948062539100647, 'old_lang_expert0_score': '0.24 0.02 0.05 0.28 0.85 0.87 0.58 0.92 0.87 0.92 0.75 0.94 0.83 0.92 0.97 0.9 0.9 0.94 0.95 0.95 0.94 0.94 0.98 0.99', 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1460/2256 [2:40:29<1:27:33,  6.60s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1461/2256 [2:40:35<1:27:18,  6.59s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1462/2256 [2:40:42<1:27:05,  6.58s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1463/2256 [2:40:48<1:26:53,  6.57s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1464/2256 [2:40:55<1:26:45,  6.57s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1465/2256 [2:41:01<1:26:39,  6.57s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1466/2256 [2:41:08<1:26:34,  6.58s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1467/2256 [2:41:15<1:26:33,  6.58s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1468/2256 [2:41:21<1:26:20,  6.57s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1469/2256 [2:41:28<1:26:10,  6.57s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1470/2256 [2:41:34<1:26:07,  6.57s/it]                                                       {'loss': 1.8127, 'learning_rate': 1.3538711197941372e-05, 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1470/2256 [2:41:34<1:26:07,  6.57s/it]                                                       {'router_ce_loss': 0.9495654106140137, 'old_lang_expert0_score': '0.24 0.04 0.07 0.29 0.86 0.87 0.55 0.93 0.87 0.92 0.67 0.93 0.83 0.93 0.96 0.9 0.92 0.94 0.97 0.95 0.94 0.94 0.98 0.99', 'epoch': 0.65}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1470/2256 [2:41:35<1:26:07,  6.57s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1471/2256 [2:41:41<1:26:03,  6.58s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1472/2256 [2:41:47<1:25:56,  6.58s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1473/2256 [2:41:54<1:26:45,  6.65s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1474/2256 [2:42:01<1:27:17,  6.70s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1475/2256 [2:42:08<1:26:42,  6.66s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1476/2256 [2:42:14<1:26:16,  6.64s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1477/2256 [2:42:21<1:25:58,  6.62s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1478/2256 [2:42:27<1:25:38,  6.60s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1479/2256 [2:42:34<1:25:26,  6.60s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1480/2256 [2:42:41<1:25:12,  6.59s/it]                                                       {'loss': 1.7121, 'learning_rate': 1.3230435927259797e-05, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1480/2256 [2:42:41<1:25:12,  6.59s/it]                                                       {'router_ce_loss': 0.9363687634468079, 'old_lang_expert0_score': '0.23 0.02 0.05 0.31 0.88 0.92 0.59 0.92 0.89 0.91 0.78 0.94 0.84 0.95 0.96 0.93 0.93 0.95 0.97 0.96 0.96 0.96 0.98 0.99', 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1480/2256 [2:42:41<1:25:12,  6.59s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1481/2256 [2:42:47<1:25:07,  6.59s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1482/2256 [2:42:54<1:24:57,  6.59s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1483/2256 [2:43:00<1:24:52,  6.59s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1484/2256 [2:43:07<1:24:38,  6.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1485/2256 [2:43:13<1:24:30,  6.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1486/2256 [2:43:20<1:24:27,  6.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1487/2256 [2:43:27<1:24:17,  6.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1488/2256 [2:43:33<1:24:11,  6.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1489/2256 [2:43:40<1:24:06,  6.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1490/2256 [2:43:46<1:24:00,  6.58s/it]                                                       {'loss': 1.7169, 'learning_rate': 1.292444296805469e-05, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1490/2256 [2:43:46<1:24:00,  6.58s/it]                                                       {'router_ce_loss': 0.9201710224151611, 'old_lang_expert0_score': '0.26 0.01 0.05 0.36 0.91 0.93 0.69 0.95 0.9 0.93 0.68 0.95 0.85 0.96 0.98 0.98 0.97 0.99 0.99 0.99 0.98 0.98 0.99 0.99', 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1490/2256 [2:43:47<1:24:00,  6.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1491/2256 [2:43:53<1:24:44,  6.65s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1492/2256 [2:44:00<1:24:22,  6.63s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1493/2256 [2:44:06<1:24:02,  6.61s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1494/2256 [2:44:13<1:23:46,  6.60s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1495/2256 [2:44:19<1:23:35,  6.59s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1496/2256 [2:44:26<1:23:28,  6.59s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1497/2256 [2:44:33<1:23:17,  6.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1498/2256 [2:44:39<1:23:11,  6.59s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1499/2256 [2:44:46<1:23:02,  6.58s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1500/2256 [2:44:52<1:22:53,  6.58s/it]                                                       {'loss': 1.7633, 'learning_rate': 1.2620791657378664e-05, 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1500/2256 [2:44:52<1:22:53,  6.58s/it]                                                       {'router_ce_loss': 0.9177913665771484, 'old_lang_expert0_score': '0.25 0.02 0.06 0.36 0.92 0.94 0.71 0.94 0.91 0.94 0.78 0.95 0.89 0.94 0.96 0.94 0.94 0.97 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.66}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1500/2256 [2:44:53<1:22:53,  6.58s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1501/2256 [2:44:59<1:22:49,  6.58s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1502/2256 [2:45:06<1:22:45,  6.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1503/2256 [2:45:12<1:22:45,  6.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1504/2256 [2:45:19<1:22:39,  6.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1505/2256 [2:45:25<1:22:36,  6.60s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1506/2256 [2:45:32<1:22:24,  6.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1507/2256 [2:45:39<1:22:14,  6.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1508/2256 [2:45:45<1:22:10,  6.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1509/2256 [2:45:52<1:23:45,  6.73s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1510/2256 [2:45:59<1:23:06,  6.68s/it]                                                       {'loss': 1.6852, 'learning_rate': 1.2319540878200259e-05, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1510/2256 [2:45:59<1:23:06,  6.68s/it]                                                       {'router_ce_loss': 0.931879997253418, 'old_lang_expert0_score': '0.26 0.02 0.05 0.32 0.89 0.92 0.63 0.92 0.89 0.93 0.77 0.94 0.85 0.94 0.96 0.93 0.93 0.95 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1510/2256 [2:45:59<1:23:06,  6.68s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1511/2256 [2:46:05<1:22:37,  6.65s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1512/2256 [2:46:12<1:22:16,  6.63s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1513/2256 [2:46:19<1:22:00,  6.62s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1514/2256 [2:46:25<1:21:46,  6.61s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1515/2256 [2:46:32<1:21:32,  6.60s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1516/2256 [2:46:38<1:21:22,  6.60s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1517/2256 [2:46:45<1:21:12,  6.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1518/2256 [2:46:51<1:21:01,  6.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1519/2256 [2:46:58<1:20:53,  6.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1520/2256 [2:47:05<1:20:47,  6.59s/it]                                                       {'loss': 1.7527, 'learning_rate': 1.2020749047985627e-05, 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1520/2256 [2:47:05<1:20:47,  6.59s/it]                                                       {'router_ce_loss': 0.9457879662513733, 'old_lang_expert0_score': '0.24 0.02 0.08 0.36 0.84 0.84 0.65 0.89 0.89 0.9 0.61 0.92 0.74 0.94 0.97 0.94 0.95 0.96 0.97 0.98 0.97 0.97 0.99 0.99', 'epoch': 0.67}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1520/2256 [2:47:05<1:20:47,  6.59s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1521/2256 [2:47:11<1:20:38,  6.58s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1522/2256 [2:47:18<1:20:32,  6.58s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1523/2256 [2:47:24<1:20:24,  6.58s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1524/2256 [2:47:31<1:20:14,  6.58s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1525/2256 [2:47:37<1:20:08,  6.58s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1526/2256 [2:47:44<1:20:54,  6.65s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1527/2256 [2:47:51<1:21:17,  6.69s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1528/2256 [2:47:58<1:20:45,  6.66s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1529/2256 [2:48:04<1:20:19,  6.63s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1530/2256 [2:48:11<1:19:56,  6.61s/it]                                                       {'loss': 1.7579, 'learning_rate': 1.172447410737035e-05, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1530/2256 [2:48:11<1:19:56,  6.61s/it]                                                       {'router_ce_loss': 0.9246668219566345, 'old_lang_expert0_score': '0.21 0.02 0.07 0.32 0.87 0.91 0.69 0.96 0.91 0.93 0.79 0.96 0.88 0.96 0.98 0.96 0.95 0.96 0.97 0.97 0.96 0.97 0.95 0.98', 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1530/2256 [2:48:11<1:19:56,  6.61s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1531/2256 [2:48:17<1:19:46,  6.60s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1532/2256 [2:48:24<1:19:37,  6.60s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1533/2256 [2:48:31<1:19:27,  6.59s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1534/2256 [2:48:37<1:19:19,  6.59s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1535/2256 [2:48:44<1:19:13,  6.59s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1536/2256 [2:48:50<1:19:06,  6.59s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1537/2256 [2:48:57<1:18:58,  6.59s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1538/2256 [2:49:03<1:18:51,  6.59s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1539/2256 [2:49:10<1:18:43,  6.59s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1540/2256 [2:49:17<1:18:34,  6.58s/it]                                                       {'loss': 1.7415, 'learning_rate': 1.1430773508923882e-05, 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1540/2256 [2:49:17<1:18:34,  6.58s/it]                                                       {'router_ce_loss': 0.9162428379058838, 'old_lang_expert0_score': '0.24 0.02 0.08 0.4 0.84 0.94 0.73 0.95 0.93 0.97 0.8 0.96 0.87 0.96 0.98 0.95 0.95 0.96 0.98 0.97 0.98 0.98 0.99 0.99', 'epoch': 0.68}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1540/2256 [2:49:17<1:18:34,  6.58s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1541/2256 [2:49:23<1:18:31,  6.59s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1542/2256 [2:49:30<1:18:22,  6.59s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1543/2256 [2:49:36<1:18:11,  6.58s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1544/2256 [2:49:43<1:18:53,  6.65s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1545/2256 [2:49:50<1:18:35,  6.63s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1546/2256 [2:49:56<1:18:13,  6.61s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1547/2256 [2:50:03<1:18:04,  6.61s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1548/2256 [2:50:10<1:17:53,  6.60s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1549/2256 [2:50:16<1:17:40,  6.59s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1550/2256 [2:50:23<1:17:33,  6.59s/it]                                                       {'loss': 1.7326, 'learning_rate': 1.1139704206008495e-05, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1550/2256 [2:50:23<1:17:33,  6.59s/it]                                                       {'router_ce_loss': 0.9263895153999329, 'old_lang_expert0_score': '0.25 0.02 0.05 0.36 0.91 0.92 0.67 0.94 0.9 0.94 0.73 0.94 0.85 0.95 0.97 0.94 0.95 0.96 0.97 0.96 0.97 0.97 0.99 0.99', 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1550/2256 [2:50:23<1:17:33,  6.59s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1551/2256 [2:50:29<1:17:23,  6.59s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1552/2256 [2:50:36<1:17:16,  6.59s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1553/2256 [2:50:42<1:17:08,  6.58s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1554/2256 [2:50:49<1:17:00,  6.58s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1555/2256 [2:50:56<1:16:54,  6.58s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1556/2256 [2:51:02<1:16:49,  6.58s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1557/2256 [2:51:09<1:16:36,  6.58s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1558/2256 [2:51:15<1:16:30,  6.58s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1559/2256 [2:51:22<1:16:27,  6.58s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1560/2256 [2:51:29<1:16:21,  6.58s/it]                                                       {'loss': 1.6917, 'learning_rate': 1.0851322641735118e-05, 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1560/2256 [2:51:29<1:16:21,  6.58s/it]                                                       {'router_ce_loss': 0.9414297938346863, 'old_lang_expert0_score': '0.21 0.02 0.06 0.26 0.78 0.92 0.54 0.95 0.9 0.95 0.8 0.95 0.86 0.94 0.97 0.93 0.93 0.95 0.97 0.96 0.96 0.92 0.99 0.99', 'epoch': 0.69}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1560/2256 [2:51:29<1:16:21,  6.58s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1561/2256 [2:51:35<1:16:09,  6.57s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1562/2256 [2:51:42<1:17:34,  6.71s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1563/2256 [2:51:49<1:17:01,  6.67s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1564/2256 [2:51:55<1:16:30,  6.63s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1565/2256 [2:52:02<1:16:14,  6.62s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1566/2256 [2:52:08<1:15:58,  6.61s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1567/2256 [2:52:15<1:15:47,  6.60s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1568/2256 [2:52:22<1:15:35,  6.59s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1569/2256 [2:52:28<1:15:26,  6.59s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1570/2256 [2:52:35<1:15:15,  6.58s/it]                                                       {'loss': 1.7285, 'learning_rate': 1.0565684738018087e-05, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1570/2256 [2:52:35<1:15:15,  6.58s/it]                                                       {'router_ce_loss': 0.9332602620124817, 'old_lang_expert0_score': '0.24 0.02 0.04 0.32 0.9 0.92 0.67 0.93 0.89 0.92 0.72 0.94 0.82 0.93 0.96 0.93 0.94 0.96 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1570/2256 [2:52:35<1:15:15,  6.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1571/2256 [2:52:41<1:15:08,  6.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1572/2256 [2:52:48<1:15:01,  6.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1573/2256 [2:52:54<1:14:53,  6.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1574/2256 [2:53:01<1:14:49,  6.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1575/2256 [2:53:08<1:14:45,  6.59s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1576/2256 [2:53:14<1:14:38,  6.59s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1577/2256 [2:53:21<1:14:24,  6.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1578/2256 [2:53:27<1:14:17,  6.57s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1579/2256 [2:53:34<1:14:59,  6.65s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1580/2256 [2:53:41<1:15:17,  6.68s/it]                                                       {'loss': 1.7366, 'learning_rate': 1.0282845884730932e-05, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1580/2256 [2:53:41<1:15:17,  6.68s/it]                                                       {'router_ce_loss': 0.9278176426887512, 'old_lang_expert0_score': '0.24 0.03 0.06 0.3 0.88 0.9 0.62 0.94 0.88 0.92 0.79 0.95 0.91 0.95 0.97 0.95 0.95 0.96 0.98 0.98 0.97 0.97 0.99 0.99', 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1580/2256 [2:53:41<1:15:17,  6.68s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1581/2256 [2:53:47<1:14:45,  6.64s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1582/2256 [2:53:54<1:14:21,  6.62s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1583/2256 [2:54:01<1:14:07,  6.61s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1584/2256 [2:54:07<1:13:50,  6.59s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1585/2256 [2:54:14<1:13:42,  6.59s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1586/2256 [2:54:20<1:13:31,  6.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1587/2256 [2:54:27<1:13:25,  6.59s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1588/2256 [2:54:33<1:13:16,  6.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1589/2256 [2:54:40<1:13:08,  6.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1590/2256 [2:54:47<1:13:01,  6.58s/it]                                                       {'loss': 1.8149, 'learning_rate': 1.0002860928965453e-05, 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1590/2256 [2:54:47<1:13:01,  6.58s/it]                                                       {'router_ce_loss': 0.9328248500823975, 'old_lang_expert0_score': '0.28 0.03 0.03 0.28 0.88 0.91 0.67 0.94 0.9 0.93 0.76 0.93 0.84 0.93 0.96 0.93 0.94 0.96 0.97 0.97 0.96 0.96 0.99 0.99', 'epoch': 0.7}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1590/2256 [2:54:47<1:13:01,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1591/2256 [2:54:53<1:12:56,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1592/2256 [2:55:00<1:12:50,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1593/2256 [2:55:06<1:12:43,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1594/2256 [2:55:13<1:12:37,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1595/2256 [2:55:20<1:12:28,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1596/2256 [2:55:26<1:12:19,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1597/2256 [2:55:33<1:13:02,  6.65s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1598/2256 [2:55:39<1:12:42,  6.63s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1599/2256 [2:55:46<1:12:27,  6.62s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1600/2256 [2:55:53<1:12:14,  6.61s/it]                                                       {'loss': 1.773, 'learning_rate': 9.72578416439587e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1600/2256 [2:55:53<1:12:14,  6.61s/it]                                                       {'router_ce_loss': 0.9378197193145752, 'old_lang_expert0_score': '0.24 0.02 0.05 0.29 0.9 0.89 0.64 0.94 0.88 0.92 0.73 0.95 0.85 0.93 0.97 0.9 0.93 0.95 0.97 0.96 0.96 0.96 0.99 0.99', 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1600/2256 [2:55:53<1:12:14,  6.61s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1601/2256 [2:55:59<1:11:59,  6.59s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1602/2256 [2:56:06<1:11:44,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1603/2256 [2:56:12<1:11:39,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1604/2256 [2:56:19<1:11:31,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1605/2256 [2:56:26<1:11:25,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1606/2256 [2:56:32<1:11:17,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1607/2256 [2:56:39<1:11:08,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1608/2256 [2:56:45<1:11:03,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1609/2256 [2:56:52<1:10:51,  6.57s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1610/2256 [2:56:58<1:10:47,  6.58s/it]                                                       {'loss': 1.7735, 'learning_rate': 9.451669320750484e-06, 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1610/2256 [2:56:58<1:10:47,  6.58s/it]                                                       {'router_ce_loss': 0.9138562083244324, 'old_lang_expert0_score': '0.21 0.02 0.03 0.41 0.93 0.95 0.77 0.95 0.9 0.95 0.78 0.95 0.89 0.97 0.96 0.94 0.97 0.96 0.98 0.97 0.97 0.98 0.99 1.0', 'epoch': 0.71}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1610/2256 [2:56:59<1:10:47,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1611/2256 [2:57:05<1:10:41,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1612/2256 [2:57:12<1:10:34,  6.58s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1613/2256 [2:57:18<1:10:29,  6.58s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1614/2256 [2:57:25<1:10:19,  6.57s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1615/2256 [2:57:31<1:10:56,  6.64s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1616/2256 [2:57:38<1:11:16,  6.68s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1617/2256 [2:57:45<1:10:53,  6.66s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1618/2256 [2:57:51<1:10:35,  6.64s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1619/2256 [2:57:58<1:10:23,  6.63s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1620/2256 [2:58:05<1:10:11,  6.62s/it]                                                       {'loss': 1.7111, 'learning_rate': 9.180569553392535e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1620/2256 [2:58:05<1:10:11,  6.62s/it]                                                       {'router_ce_loss': 0.932495653629303, 'old_lang_expert0_score': '0.26 0.02 0.05 0.34 0.91 0.91 0.64 0.94 0.9 0.94 0.68 0.94 0.85 0.95 0.96 0.93 0.95 0.95 0.98 0.96 0.95 0.97 0.98 0.99', 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1620/2256 [2:58:05<1:10:11,  6.62s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1621/2256 [2:58:11<1:09:56,  6.61s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1622/2256 [2:58:18<1:09:46,  6.60s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1623/2256 [2:58:24<1:09:39,  6.60s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1624/2256 [2:58:31<1:09:25,  6.59s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1625/2256 [2:58:38<1:09:16,  6.59s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1626/2256 [2:58:44<1:09:06,  6.58s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1627/2256 [2:58:51<1:09:03,  6.59s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1628/2256 [2:58:57<1:08:56,  6.59s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1629/2256 [2:59:04<1:08:53,  6.59s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1630/2256 [2:59:11<1:08:41,  6.58s/it]                                                       {'loss': 1.7319, 'learning_rate': 8.912537433012544e-06, 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1630/2256 [2:59:11<1:08:41,  6.58s/it]                                                       {'router_ce_loss': 0.9177219271659851, 'old_lang_expert0_score': '0.26 0.02 0.05 0.41 0.92 0.94 0.74 0.95 0.89 0.93 0.72 0.94 0.86 0.95 0.97 0.96 0.97 0.97 0.98 0.97 0.98 0.97 0.99 0.99', 'epoch': 0.72}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1630/2256 [2:59:11<1:08:41,  6.58s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1631/2256 [2:59:17<1:08:36,  6.59s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1632/2256 [2:59:24<1:08:30,  6.59s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1633/2256 [2:59:30<1:09:02,  6.65s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1634/2256 [2:59:37<1:09:27,  6.70s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1635/2256 [2:59:44<1:08:56,  6.66s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1636/2256 [2:59:50<1:08:33,  6.63s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1637/2256 [2:59:57<1:08:17,  6.62s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1638/2256 [3:00:04<1:08:03,  6.61s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1639/2256 [3:00:10<1:07:50,  6.60s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1640/2256 [3:00:17<1:07:43,  6.60s/it]                                                       {'loss': 1.745, 'learning_rate': 8.647624935433948e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1640/2256 [3:00:17<1:07:43,  6.60s/it]                                                       {'router_ce_loss': 0.9258467555046082, 'old_lang_expert0_score': '0.23 0.02 0.05 0.33 0.9 0.92 0.65 0.93 0.93 0.95 0.75 0.95 0.87 0.93 0.96 0.94 0.96 0.97 0.98 0.98 0.98 0.97 0.99 0.99', 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1640/2256 [3:00:17<1:07:43,  6.60s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1641/2256 [3:00:23<1:07:37,  6.60s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1642/2256 [3:00:30<1:07:28,  6.59s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1643/2256 [3:00:37<1:07:19,  6.59s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1644/2256 [3:00:43<1:07:13,  6.59s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1645/2256 [3:00:50<1:07:02,  6.58s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1646/2256 [3:00:56<1:06:52,  6.58s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1647/2256 [3:01:03<1:06:46,  6.58s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1648/2256 [3:01:09<1:06:39,  6.58s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1649/2256 [3:01:16<1:06:30,  6.57s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1650/2256 [3:01:23<1:06:23,  6.57s/it]                                                       {'loss': 1.7478, 'learning_rate': 8.385883431534195e-06, 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1650/2256 [3:01:23<1:06:23,  6.57s/it]                                                       {'router_ce_loss': 0.9295513033866882, 'old_lang_expert0_score': '0.25 0.02 0.04 0.36 0.9 0.93 0.68 0.93 0.9 0.94 0.77 0.93 0.84 0.94 0.95 0.9 0.93 0.96 0.97 0.96 0.97 0.97 0.99 0.99', 'epoch': 0.73}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1650/2256 [3:01:23<1:06:23,  6.57s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1651/2256 [3:01:29<1:06:57,  6.64s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1652/2256 [3:01:36<1:06:37,  6.62s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1653/2256 [3:01:42<1:06:22,  6.60s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1654/2256 [3:01:49<1:06:07,  6.59s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1655/2256 [3:01:56<1:05:59,  6.59s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1656/2256 [3:02:02<1:05:52,  6.59s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1657/2256 [3:02:09<1:05:43,  6.58s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1658/2256 [3:02:15<1:05:37,  6.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1659/2256 [3:02:22<1:05:23,  6.57s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1660/2256 [3:02:28<1:05:15,  6.57s/it]                                                       {'loss': 1.79, 'learning_rate': 8.127363677283059e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1660/2256 [3:02:28<1:05:15,  6.57s/it]                                                       {'router_ce_loss': 0.9415274262428284, 'old_lang_expert0_score': '0.25 0.06 0.1 0.37 0.82 0.83 0.61 0.91 0.87 0.93 0.66 0.96 0.81 0.94 0.96 0.9 0.93 0.95 0.96 0.95 0.96 0.97 0.99 0.99', 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1660/2256 [3:02:29<1:05:15,  6.57s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1661/2256 [3:02:35<1:05:12,  6.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1662/2256 [3:02:42<1:05:07,  6.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1663/2256 [3:02:48<1:05:00,  6.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1664/2256 [3:02:55<1:04:56,  6.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1665/2256 [3:03:01<1:04:52,  6.59s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1666/2256 [3:03:08<1:04:47,  6.59s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1667/2256 [3:03:15<1:04:36,  6.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1668/2256 [3:03:21<1:04:27,  6.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1669/2256 [3:03:28<1:05:36,  6.71s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1670/2256 [3:03:35<1:05:04,  6.66s/it]                                                       {'loss': 1.7423, 'learning_rate': 7.872115803900198e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1670/2256 [3:03:35<1:05:04,  6.66s/it]                                                       {'router_ce_loss': 0.915960431098938, 'old_lang_expert0_score': '0.25 0.02 0.06 0.43 0.89 0.93 0.75 0.94 0.91 0.95 0.81 0.95 0.87 0.94 0.97 0.94 0.94 0.97 0.98 0.96 0.97 0.97 0.98 0.99', 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1670/2256 [3:03:35<1:05:04,  6.66s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1671/2256 [3:03:41<1:04:42,  6.64s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1672/2256 [3:03:48<1:04:22,  6.61s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1673/2256 [3:03:54<1:04:11,  6.61s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1674/2256 [3:04:01<1:04:02,  6.60s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1675/2256 [3:04:08<1:03:53,  6.60s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1676/2256 [3:04:14<1:03:39,  6.59s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1677/2256 [3:04:21<1:03:30,  6.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1678/2256 [3:04:27<1:03:22,  6.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1679/2256 [3:04:34<1:03:15,  6.58s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1680/2256 [3:04:41<1:03:13,  6.59s/it]                                                       {'loss': 1.7106, 'learning_rate': 7.620189308133943e-06, 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1680/2256 [3:04:41<1:03:13,  6.59s/it]                                                       {'router_ce_loss': 0.9409586787223816, 'old_lang_expert0_score': '0.27 0.02 0.04 0.24 0.87 0.9 0.66 0.94 0.86 0.93 0.76 0.93 0.85 0.91 0.95 0.91 0.91 0.9 0.95 0.95 0.96 0.96 0.99 0.99', 'epoch': 0.74}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1680/2256 [3:04:41<1:03:13,  6.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1681/2256 [3:04:47<1:03:06,  6.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1682/2256 [3:04:54<1:02:55,  6.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1683/2256 [3:05:00<1:02:51,  6.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1684/2256 [3:05:07<1:02:46,  6.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1685/2256 [3:05:13<1:02:37,  6.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1686/2256 [3:05:20<1:03:08,  6.65s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1687/2256 [3:05:27<1:03:23,  6.68s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1688/2256 [3:05:34<1:02:56,  6.65s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1689/2256 [3:05:40<1:02:35,  6.62s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1690/2256 [3:05:47<1:02:20,  6.61s/it]                                                       {'loss': 1.7171, 'learning_rate': 7.371633042663031e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1690/2256 [3:05:47<1:02:20,  6.61s/it]                                                       {'router_ce_loss': 0.925037145614624, 'old_lang_expert0_score': '0.24 0.02 0.06 0.38 0.9 0.92 0.69 0.93 0.9 0.93 0.77 0.94 0.86 0.94 0.97 0.93 0.94 0.96 0.97 0.97 0.97 0.96 0.99 0.99', 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1690/2256 [3:05:47<1:02:20,  6.61s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1691/2256 [3:05:53<1:02:04,  6.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1692/2256 [3:06:00<1:01:54,  6.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1693/2256 [3:06:06<1:01:48,  6.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1694/2256 [3:06:13<1:01:42,  6.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1695/2256 [3:06:20<1:01:30,  6.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1696/2256 [3:06:26<1:01:26,  6.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1697/2256 [3:06:33<1:01:17,  6.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1698/2256 [3:06:39<1:01:09,  6.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1699/2256 [3:06:46<1:01:05,  6.58s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1700/2256 [3:06:52<1:01:01,  6.59s/it]                                                       {'loss': 1.7246, 'learning_rate': 7.126495206623285e-06, 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1700/2256 [3:06:52<1:01:01,  6.59s/it]                                                       {'router_ce_loss': 0.9220180511474609, 'old_lang_expert0_score': '0.25 0.02 0.05 0.38 0.91 0.93 0.66 0.94 0.9 0.95 0.82 0.94 0.88 0.94 0.96 0.93 0.93 0.96 0.97 0.97 0.97 0.96 0.99 0.99', 'epoch': 0.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1700/2256 [3:06:53<1:01:01,  6.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1701/2256 [3:06:59<1:00:58,  6.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1702/2256 [3:07:06<1:00:50,  6.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1703/2256 [3:07:12<1:00:40,  6.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1704/2256 [3:07:19<1:01:11,  6.65s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1705/2256 [3:07:26<1:00:54,  6.63s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1706/2256 [3:07:32<1:00:42,  6.62s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1707/2256 [3:07:39<1:00:27,  6.61s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1708/2256 [3:07:45<1:00:13,  6.59s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1709/2256 [3:07:52<1:00:04,  6.59s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1710/2256 [3:07:59<59:55,  6.58s/it]                                                       {'loss': 1.7791, 'learning_rate': 6.8848233362609815e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1710/2256 [3:07:59<59:55,  6.58s/it]                                                     {'router_ce_loss': 0.9261681437492371, 'old_lang_expert0_score': '0.25 0.02 0.04 0.34 0.9 0.91 0.7 0.94 0.91 0.92 0.73 0.94 0.88 0.94 0.96 0.93 0.95 0.97 0.97 0.97 0.98 0.97 0.99 0.99', 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1710/2256 [3:07:59<59:55,  6.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1711/2256 [3:08:05<59:47,  6.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1712/2256 [3:08:12<59:38,  6.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1713/2256 [3:08:18<59:32,  6.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1714/2256 [3:08:25<59:27,  6.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1715/2256 [3:08:31<59:20,  6.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1716/2256 [3:08:38<59:13,  6.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1717/2256 [3:08:45<59:08,  6.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1718/2256 [3:08:51<59:02,  6.59s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1719/2256 [3:08:58<58:57,  6.59s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1720/2256 [3:09:04<58:49,  6.58s/it]                                                     {'loss': 1.7917, 'learning_rate': 6.646664295714836e-06, 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1720/2256 [3:09:04<58:49,  6.58s/it]                                                     {'router_ce_loss': 0.9340968728065491, 'old_lang_expert0_score': '0.25 0.02 0.06 0.36 0.9 0.89 0.65 0.93 0.88 0.92 0.73 0.94 0.84 0.91 0.95 0.92 0.92 0.95 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.76}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1720/2256 [3:09:05<58:49,  6.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1721/2256 [3:09:11<58:42,  6.58s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1722/2256 [3:09:18<59:44,  6.71s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1723/2256 [3:09:25<59:14,  6.67s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1724/2256 [3:09:31<58:54,  6.64s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1725/2256 [3:09:38<58:37,  6.62s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1726/2256 [3:09:44<58:21,  6.61s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1727/2256 [3:09:51<58:09,  6.60s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1728/2256 [3:09:57<57:57,  6.59s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1729/2256 [3:10:04<57:49,  6.58s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1730/2256 [3:10:11<57:39,  6.58s/it]                                                     {'loss': 1.7227, 'learning_rate': 6.4120642679282635e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1730/2256 [3:10:11<57:39,  6.58s/it]                                                     {'router_ce_loss': 0.930636465549469, 'old_lang_expert0_score': '0.28 0.02 0.05 0.29 0.85 0.86 0.65 0.94 0.9 0.93 0.79 0.95 0.89 0.94 0.97 0.93 0.94 0.94 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1730/2256 [3:10:11<57:39,  6.58s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1731/2256 [3:10:17<57:32,  6.58s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1732/2256 [3:10:24<57:26,  6.58s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1733/2256 [3:10:30<57:19,  6.58s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1734/2256 [3:10:37<57:14,  6.58s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1735/2256 [3:10:43<57:08,  6.58s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1736/2256 [3:10:50<57:05,  6.59s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1737/2256 [3:10:57<56:57,  6.58s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1738/2256 [3:11:03<56:49,  6.58s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1739/2256 [3:11:10<57:14,  6.64s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1740/2256 [3:11:17<57:27,  6.68s/it]                                                     {'loss': 1.7372, 'learning_rate': 6.181068745693716e-06, 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1740/2256 [3:11:17<57:27,  6.68s/it]                                                     {'router_ce_loss': 0.934911847114563, 'old_lang_expert0_score': '0.23 0.03 0.08 0.32 0.86 0.89 0.68 0.92 0.89 0.92 0.79 0.95 0.83 0.93 0.96 0.92 0.93 0.95 0.97 0.96 0.96 0.96 0.99 0.99', 'epoch': 0.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1740/2256 [3:11:17<57:27,  6.68s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1741/2256 [3:11:23<57:03,  6.65s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1742/2256 [3:11:30<56:43,  6.62s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1743/2256 [3:11:36<56:30,  6.61s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1744/2256 [3:11:43<56:19,  6.60s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1745/2256 [3:11:50<56:09,  6.59s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1746/2256 [3:11:56<56:00,  6.59s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1747/2256 [3:12:03<55:48,  6.58s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1748/2256 [3:12:09<55:40,  6.58s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1749/2256 [3:12:16<55:37,  6.58s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1750/2256 [3:12:22<55:30,  6.58s/it]                                                     {'loss': 1.6963, 'learning_rate': 5.953722522830943e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1750/2256 [3:12:22<55:30,  6.58s/it]                                                     {'router_ce_loss': 0.9178954362869263, 'old_lang_expert0_score': '0.23 0.02 0.07 0.38 0.91 0.95 0.72 0.96 0.9 0.94 0.81 0.95 0.86 0.95 0.96 0.92 0.95 0.97 0.98 0.97 0.98 0.98 0.99 0.99', 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1750/2256 [3:12:23<55:30,  6.58s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1751/2256 [3:12:29<55:21,  6.58s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1752/2256 [3:12:36<55:17,  6.58s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1753/2256 [3:12:42<55:13,  6.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1754/2256 [3:12:49<55:06,  6.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1755/2256 [3:12:55<55:01,  6.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1756/2256 [3:13:02<54:52,  6.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1757/2256 [3:13:09<55:18,  6.65s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1758/2256 [3:13:15<55:01,  6.63s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1759/2256 [3:13:22<54:48,  6.62s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1760/2256 [3:13:29<54:36,  6.61s/it]                                                     {'loss': 1.7609, 'learning_rate': 5.730069685500669e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1760/2256 [3:13:29<54:36,  6.61s/it]                                                     {'router_ce_loss': 0.9391002058982849, 'old_lang_expert0_score': '0.24 0.01 0.04 0.24 0.91 0.87 0.57 0.94 0.91 0.94 0.79 0.95 0.88 0.94 0.98 0.89 0.92 0.94 0.97 0.97 0.97 0.96 0.99 0.99', 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1760/2256 [3:13:29<54:36,  6.61s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1761/2256 [3:13:35<54:26,  6.60s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1762/2256 [3:13:42<54:17,  6.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1763/2256 [3:13:48<54:08,  6.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1764/2256 [3:13:55<53:56,  6.58s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1765/2256 [3:14:01<53:52,  6.58s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1766/2256 [3:14:08<53:48,  6.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1767/2256 [3:14:15<53:40,  6.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1768/2256 [3:14:21<53:35,  6.59s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1769/2256 [3:14:28<53:24,  6.58s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1770/2256 [3:14:34<53:16,  6.58s/it]                                                     {'loss': 1.7067, 'learning_rate': 5.510153603655621e-06, 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1770/2256 [3:14:34<53:16,  6.58s/it]                                                     {'router_ce_loss': 0.9179584980010986, 'old_lang_expert0_score': '0.26 0.02 0.05 0.35 0.91 0.93 0.72 0.94 0.92 0.95 0.76 0.94 0.92 0.95 0.98 0.96 0.96 0.97 0.98 0.97 0.98 0.98 0.99 0.99', 'epoch': 0.78}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1770/2256 [3:14:35<53:16,  6.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1771/2256 [3:14:41<53:10,  6.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1772/2256 [3:14:47<53:04,  6.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1773/2256 [3:14:54<52:59,  6.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1774/2256 [3:15:01<52:50,  6.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1775/2256 [3:15:08<53:45,  6.71s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1776/2256 [3:15:14<53:20,  6.67s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1777/2256 [3:15:21<52:58,  6.64s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1778/2256 [3:15:27<52:45,  6.62s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1779/2256 [3:15:34<52:31,  6.61s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1780/2256 [3:15:41<52:19,  6.59s/it]                                                     {'loss': 1.6664, 'learning_rate': 5.29401692263036e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1780/2256 [3:15:41<52:19,  6.59s/it]                                                     {'router_ce_loss': 0.924633264541626, 'old_lang_expert0_score': '0.26 0.02 0.06 0.33 0.9 0.92 0.7 0.92 0.89 0.93 0.78 0.94 0.87 0.94 0.97 0.95 0.94 0.97 0.98 0.96 0.98 0.97 0.98 0.99', 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1780/2256 [3:15:41<52:19,  6.59s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1781/2256 [3:15:47<52:08,  6.59s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1782/2256 [3:15:54<52:01,  6.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1783/2256 [3:16:00<51:51,  6.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1784/2256 [3:16:07<51:44,  6.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1785/2256 [3:16:13<51:36,  6.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1786/2256 [3:16:20<51:30,  6.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1787/2256 [3:16:27<51:22,  6.57s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1788/2256 [3:16:33<51:21,  6.58s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1789/2256 [3:16:40<51:15,  6.59s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1790/2256 [3:16:46<51:08,  6.59s/it]                                                     {'loss': 1.6889, 'learning_rate': 5.081701554871643e-06, 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1790/2256 [3:16:46<51:08,  6.59s/it]                                                     {'router_ce_loss': 0.9297615885734558, 'old_lang_expert0_score': '0.25 0.02 0.04 0.31 0.89 0.91 0.67 0.94 0.91 0.93 0.76 0.94 0.86 0.93 0.96 0.94 0.94 0.95 0.96 0.98 0.97 0.97 0.99 0.99', 'epoch': 0.79}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1790/2256 [3:16:47<51:08,  6.59s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1791/2256 [3:16:53<50:56,  6.57s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1792/2256 [3:17:00<51:21,  6.64s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1793/2256 [3:17:06<51:41,  6.70s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1794/2256 [3:17:13<51:18,  6.66s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1795/2256 [3:17:20<50:59,  6.64s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1796/2256 [3:17:26<50:46,  6.62s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1797/2256 [3:17:33<50:32,  6.61s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1798/2256 [3:17:39<50:21,  6.60s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1799/2256 [3:17:46<50:09,  6.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1800/2256 [3:17:52<49:59,  6.58s/it]                                                     {'loss': 1.737, 'learning_rate': 4.873248671810928e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1800/2256 [3:17:52<49:59,  6.58s/it]                                                     {'router_ce_loss': 0.9377520084381104, 'old_lang_expert0_score': '0.25 0.02 0.04 0.32 0.88 0.91 0.67 0.93 0.9 0.93 0.75 0.93 0.84 0.93 0.94 0.92 0.91 0.94 0.96 0.95 0.95 0.95 0.99 0.99', 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1800/2256 [3:17:53<49:59,  6.58s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1801/2256 [3:17:59<49:56,  6.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1802/2256 [3:18:06<49:51,  6.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1803/2256 [3:18:12<49:45,  6.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1804/2256 [3:18:19<49:36,  6.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1805/2256 [3:18:25<49:28,  6.58s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1806/2256 [3:18:32<49:19,  6.58s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1807/2256 [3:18:39<49:09,  6.57s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1808/2256 [3:18:45<49:02,  6.57s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1809/2256 [3:18:52<48:56,  6.57s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1810/2256 [3:18:59<49:21,  6.64s/it]                                                     {'loss': 1.8237, 'learning_rate': 4.6686986958805415e-06, 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1810/2256 [3:18:59<49:21,  6.64s/it]                                                     {'router_ce_loss': 0.9379894733428955, 'old_lang_expert0_score': '0.25 0.02 0.04 0.29 0.88 0.9 0.59 0.93 0.91 0.96 0.81 0.95 0.86 0.93 0.96 0.9 0.91 0.94 0.96 0.94 0.94 0.95 0.99 0.99', 'epoch': 0.8}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1810/2256 [3:18:59<49:21,  6.64s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1811/2256 [3:19:05<49:01,  6.61s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1812/2256 [3:19:12<48:51,  6.60s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1813/2256 [3:19:18<48:41,  6.60s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1814/2256 [3:19:25<48:32,  6.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1815/2256 [3:19:31<48:23,  6.58s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1816/2256 [3:19:38<48:13,  6.58s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1817/2256 [3:19:44<48:04,  6.57s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1818/2256 [3:19:51<47:58,  6.57s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1819/2256 [3:19:58<47:52,  6.57s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1820/2256 [3:20:04<47:46,  6.57s/it]                                                     {'loss': 1.8269, 'learning_rate': 4.468091292675127e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1820/2256 [3:20:04<47:46,  6.57s/it]                                                     {'router_ce_loss': 0.9382347464561462, 'old_lang_expert0_score': '0.24 0.02 0.06 0.35 0.89 0.88 0.65 0.92 0.88 0.93 0.72 0.93 0.84 0.94 0.97 0.93 0.92 0.94 0.97 0.95 0.96 0.95 0.98 0.98', 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1820/2256 [3:20:05<47:46,  6.57s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1821/2256 [3:20:11<47:39,  6.57s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1822/2256 [3:20:17<47:33,  6.58s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1823/2256 [3:20:24<47:27,  6.58s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1824/2256 [3:20:31<47:21,  6.58s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1825/2256 [3:20:37<47:17,  6.58s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1826/2256 [3:20:44<47:10,  6.58s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1827/2256 [3:20:50<47:00,  6.57s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1828/2256 [3:20:57<47:20,  6.64s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1829/2256 [3:21:04<47:33,  6.68s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1830/2256 [3:21:10<47:12,  6.65s/it]                                                     {'loss': 1.7006, 'learning_rate': 4.271465363259811e-06, 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1830/2256 [3:21:10<47:12,  6.65s/it]                                                     {'router_ce_loss': 0.9145796298980713, 'old_lang_expert0_score': '0.26 0.02 0.04 0.33 0.92 0.94 0.73 0.95 0.91 0.95 0.8 0.94 0.91 0.94 0.98 0.96 0.96 0.98 0.99 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.81}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1830/2256 [3:21:11<47:12,  6.65s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1831/2256 [3:21:17<46:57,  6.63s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1832/2256 [3:21:24<46:44,  6.62s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1833/2256 [3:21:30<46:34,  6.61s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1834/2256 [3:21:37<46:28,  6.61s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1835/2256 [3:21:43<46:17,  6.60s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1836/2256 [3:21:50<46:08,  6.59s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1837/2256 [3:21:57<46:04,  6.60s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1838/2256 [3:22:03<45:55,  6.59s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1839/2256 [3:22:10<45:48,  6.59s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1840/2256 [3:22:16<45:44,  6.60s/it]                                                     {'loss': 1.7252, 'learning_rate': 4.078859036626676e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1840/2256 [3:22:16<45:44,  6.60s/it]                                                     {'router_ce_loss': 0.9178054332733154, 'old_lang_expert0_score': '0.24 0.02 0.08 0.42 0.89 0.91 0.76 0.93 0.91 0.94 0.81 0.95 0.85 0.95 0.97 0.94 0.96 0.97 0.98 0.96 0.96 0.96 0.99 0.99', 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1840/2256 [3:22:17<45:44,  6.60s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1841/2256 [3:22:23<45:40,  6.60s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1842/2256 [3:22:29<45:31,  6.60s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1843/2256 [3:22:36<45:25,  6.60s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1844/2256 [3:22:43<45:22,  6.61s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1845/2256 [3:22:49<45:15,  6.61s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1846/2256 [3:22:56<45:36,  6.68s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1847/2256 [3:23:03<45:41,  6.70s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1848/2256 [3:23:09<45:16,  6.66s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1849/2256 [3:23:16<44:58,  6.63s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1850/2256 [3:23:23<44:44,  6.61s/it]                                                     {'loss': 1.6973, 'learning_rate': 3.890309662300904e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1850/2256 [3:23:23<44:44,  6.61s/it]                                                     {'router_ce_loss': 0.9338641166687012, 'old_lang_expert0_score': '0.25 0.02 0.05 0.31 0.89 0.92 0.62 0.94 0.88 0.92 0.78 0.94 0.86 0.94 0.94 0.91 0.92 0.95 0.96 0.96 0.97 0.96 0.98 0.99', 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1850/2256 [3:23:23<44:44,  6.61s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1851/2256 [3:23:29<44:34,  6.60s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1852/2256 [3:23:36<44:23,  6.59s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1853/2256 [3:23:42<44:14,  6.59s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1854/2256 [3:23:49<44:08,  6.59s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1855/2256 [3:23:55<43:57,  6.58s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1856/2256 [3:24:02<43:50,  6.58s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1857/2256 [3:24:09<43:42,  6.57s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1858/2256 [3:24:15<43:36,  6.57s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1859/2256 [3:24:22<43:28,  6.57s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1860/2256 [3:24:28<43:21,  6.57s/it]                                                     {'loss': 1.7664, 'learning_rate': 3.7058538030980942e-06, 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1860/2256 [3:24:28<43:21,  6.57s/it]                                                     {'router_ce_loss': 0.919174313545227, 'old_lang_expert0_score': '0.23 0.02 0.04 0.32 0.91 0.94 0.69 0.96 0.91 0.96 0.86 0.96 0.88 0.95 0.97 0.95 0.95 0.95 0.97 0.97 0.98 0.96 0.99 0.99', 'epoch': 0.82}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1860/2256 [3:24:29<43:21,  6.57s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1861/2256 [3:24:35<43:17,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1862/2256 [3:24:41<43:11,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1863/2256 [3:24:48<43:04,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1864/2256 [3:24:55<43:24,  6.64s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1865/2256 [3:25:01<43:12,  6.63s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1866/2256 [3:25:08<43:00,  6.62s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1867/2256 [3:25:15<42:45,  6.59s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1868/2256 [3:25:21<42:37,  6.59s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1869/2256 [3:25:28<42:27,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1870/2256 [3:25:34<42:20,  6.58s/it]                                                     {'loss': 1.7136, 'learning_rate': 3.5255272280341452e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1870/2256 [3:25:34<42:20,  6.58s/it]                                                     {'router_ce_loss': 0.9232017993927002, 'old_lang_expert0_score': '0.25 0.02 0.05 0.38 0.89 0.93 0.67 0.95 0.91 0.94 0.75 0.94 0.85 0.95 0.98 0.95 0.95 0.96 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1870/2256 [3:25:35<42:20,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1871/2256 [3:25:41<42:11,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1872/2256 [3:25:47<42:05,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1873/2256 [3:25:54<41:58,  6.57s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1874/2256 [3:26:01<41:49,  6.57s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1875/2256 [3:26:07<41:46,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1876/2256 [3:26:14<41:40,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1877/2256 [3:26:20<41:33,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1878/2256 [3:26:27<41:26,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1879/2256 [3:26:34<41:19,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1880/2256 [3:26:40<41:13,  6.58s/it]                                                     {'loss': 1.7852, 'learning_rate': 3.3493649053890326e-06, 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1880/2256 [3:26:40<41:13,  6.58s/it]                                                     {'router_ce_loss': 0.9296971559524536, 'old_lang_expert0_score': '0.26 0.02 0.03 0.26 0.9 0.91 0.67 0.94 0.89 0.92 0.75 0.94 0.89 0.94 0.96 0.93 0.94 0.96 0.98 0.97 0.97 0.96 0.99 0.99', 'epoch': 0.83}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1880/2256 [3:26:41<41:13,  6.58s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1881/2256 [3:26:47<41:05,  6.57s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1882/2256 [3:26:54<41:49,  6.71s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1883/2256 [3:27:00<41:28,  6.67s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1884/2256 [3:27:07<41:11,  6.64s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1885/2256 [3:27:13<40:59,  6.63s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1886/2256 [3:27:20<40:48,  6.62s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1887/2256 [3:27:27<40:38,  6.61s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1888/2256 [3:27:33<40:29,  6.60s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1889/2256 [3:27:40<40:19,  6.59s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1890/2256 [3:27:46<40:11,  6.59s/it]                                                     {'loss': 1.7432, 'learning_rate': 3.1774009959259194e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1890/2256 [3:27:46<40:11,  6.59s/it]                                                     {'router_ce_loss': 0.9299782514572144, 'old_lang_expert0_score': '0.25 0.03 0.07 0.37 0.87 0.92 0.75 0.93 0.93 0.95 0.78 0.93 0.84 0.92 0.95 0.9 0.9 0.95 0.96 0.95 0.95 0.95 0.97 0.99', 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1890/2256 [3:27:47<40:11,  6.59s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1891/2256 [3:27:53<40:02,  6.58s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1892/2256 [3:27:59<39:56,  6.58s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1893/2256 [3:28:06<39:50,  6.58s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1894/2256 [3:28:13<39:42,  6.58s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1895/2256 [3:28:19<39:35,  6.58s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1896/2256 [3:28:26<39:26,  6.57s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1897/2256 [3:28:32<39:21,  6.58s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1898/2256 [3:28:39<39:15,  6.58s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1899/2256 [3:28:46<39:35,  6.65s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1900/2256 [3:28:53<39:42,  6.69s/it]                                                     {'loss': 1.7959, 'learning_rate': 3.0096688462667777e-06, 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1900/2256 [3:28:53<39:42,  6.69s/it]                                                     {'router_ce_loss': 0.9271233677864075, 'old_lang_expert0_score': '0.24 0.02 0.06 0.33 0.86 0.89 0.68 0.94 0.92 0.94 0.85 0.93 0.84 0.95 0.96 0.94 0.95 0.96 0.96 0.96 0.96 0.96 0.99 0.99', 'epoch': 0.84}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1900/2256 [3:28:53<39:42,  6.69s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1901/2256 [3:28:59<39:23,  6.66s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1902/2256 [3:29:06<39:07,  6.63s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1903/2256 [3:29:12<38:56,  6.62s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1904/2256 [3:29:19<38:43,  6.60s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1905/2256 [3:29:25<38:33,  6.59s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1906/2256 [3:29:32<38:25,  6.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1907/2256 [3:29:39<38:18,  6.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1908/2256 [3:29:45<38:08,  6.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1909/2256 [3:29:52<38:02,  6.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1910/2256 [3:29:58<37:58,  6.59s/it]                                                     {'loss': 1.7257, 'learning_rate': 2.8462009824259923e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1910/2256 [3:29:58<37:58,  6.59s/it]                                                     {'router_ce_loss': 0.930701494216919, 'old_lang_expert0_score': '0.24 0.02 0.05 0.32 0.89 0.9 0.67 0.94 0.91 0.95 0.76 0.94 0.87 0.95 0.97 0.93 0.94 0.95 0.97 0.96 0.95 0.95 0.98 0.99', 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1910/2256 [3:29:59<37:58,  6.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1911/2256 [3:30:05<37:52,  6.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1912/2256 [3:30:12<37:46,  6.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1913/2256 [3:30:18<37:39,  6.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1914/2256 [3:30:25<37:31,  6.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1915/2256 [3:30:31<37:24,  6.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1916/2256 [3:30:38<37:18,  6.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1917/2256 [3:30:45<37:37,  6.66s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1918/2256 [3:30:51<37:26,  6.65s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1919/2256 [3:30:58<37:17,  6.64s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1920/2256 [3:31:05<37:07,  6.63s/it]                                                     {'loss': 1.7975, 'learning_rate': 2.687029103502972e-06, 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1920/2256 [3:31:05<37:07,  6.63s/it]                                                     {'router_ce_loss': 0.9291368126869202, 'old_lang_expert0_score': '0.25 0.01 0.04 0.35 0.91 0.93 0.73 0.92 0.92 0.95 0.79 0.94 0.81 0.91 0.95 0.88 0.94 0.95 0.97 0.94 0.96 0.96 0.97 0.98', 'epoch': 0.85}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1920/2256 [3:31:05<37:07,  6.63s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1921/2256 [3:31:11<36:58,  6.62s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1922/2256 [3:31:18<36:47,  6.61s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1923/2256 [3:31:24<36:37,  6.60s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1924/2256 [3:31:31<36:27,  6.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1925/2256 [3:31:37<36:22,  6.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1926/2256 [3:31:44<36:13,  6.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1927/2256 [3:31:51<36:08,  6.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1928/2256 [3:31:57<36:01,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1929/2256 [3:32:04<35:55,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1930/2256 [3:32:10<35:47,  6.59s/it]                                                     {'loss': 1.8621, 'learning_rate': 2.5321840755352156e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1930/2256 [3:32:10<35:47,  6.59s/it]                                                     {'router_ce_loss': 0.92671799659729, 'old_lang_expert0_score': '0.25 0.02 0.04 0.32 0.89 0.9 0.68 0.94 0.91 0.95 0.8 0.94 0.87 0.93 0.96 0.93 0.94 0.96 0.97 0.97 0.98 0.96 0.98 0.99', 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1930/2256 [3:32:11<35:47,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1931/2256 [3:32:17<35:39,  6.58s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1932/2256 [3:32:24<35:33,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1933/2256 [3:32:30<35:25,  6.58s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1934/2256 [3:32:37<35:18,  6.58s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1935/2256 [3:32:44<35:50,  6.70s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1936/2256 [3:32:50<35:32,  6.66s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1937/2256 [3:32:57<35:18,  6.64s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1938/2256 [3:33:03<35:05,  6.62s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1939/2256 [3:33:10<34:52,  6.60s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1940/2256 [3:33:17<34:43,  6.59s/it]                                                     {'loss': 1.7046, 'learning_rate': 2.381695925512878e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1940/2256 [3:33:17<34:43,  6.59s/it]                                                     {'router_ce_loss': 0.9388239979743958, 'old_lang_expert0_score': '0.24 0.02 0.04 0.28 0.88 0.89 0.61 0.94 0.89 0.93 0.77 0.93 0.86 0.95 0.97 0.92 0.93 0.94 0.96 0.94 0.95 0.95 0.98 0.99', 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1940/2256 [3:33:17<34:43,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1941/2256 [3:33:23<34:36,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1942/2256 [3:33:30<34:29,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1943/2256 [3:33:36<34:23,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1944/2256 [3:33:43<34:14,  6.58s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1945/2256 [3:33:49<34:08,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1946/2256 [3:33:56<34:02,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1947/2256 [3:34:03<33:57,  6.60s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1948/2256 [3:34:09<33:50,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1949/2256 [3:34:16<33:44,  6.59s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1950/2256 [3:34:22<33:34,  6.58s/it]                                                     {'loss': 1.7359, 'learning_rate': 2.2355938355560102e-06, 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1950/2256 [3:34:22<33:34,  6.58s/it]                                                     {'router_ce_loss': 0.9271594285964966, 'old_lang_expert0_score': '0.24 0.02 0.05 0.34 0.89 0.91 0.68 0.94 0.89 0.94 0.79 0.94 0.83 0.95 0.97 0.93 0.94 0.95 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.86}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1950/2256 [3:34:23<33:34,  6.58s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1951/2256 [3:34:29<33:26,  6.58s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1952/2256 [3:34:36<33:43,  6.66s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1953/2256 [3:34:43<33:48,  6.70s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1954/2256 [3:34:49<33:32,  6.66s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1955/2256 [3:34:56<33:17,  6.63s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1956/2256 [3:35:02<33:03,  6.61s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1957/2256 [3:35:09<32:55,  6.61s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1958/2256 [3:35:15<32:45,  6.60s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1959/2256 [3:35:22<32:38,  6.59s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1960/2256 [3:35:29<32:30,  6.59s/it]                                                     {'loss': 1.8218, 'learning_rate': 2.0939061372557134e-06, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1960/2256 [3:35:29<32:30,  6.59s/it]                                                     {'router_ce_loss': 0.9238559603691101, 'old_lang_expert0_score': '0.25 0.02 0.06 0.33 0.89 0.92 0.7 0.95 0.92 0.95 0.79 0.94 0.87 0.93 0.96 0.9 0.94 0.97 0.98 0.96 0.97 0.97 0.99 0.99', 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1960/2256 [3:35:29<32:30,  6.59s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1961/2256 [3:35:35<32:24,  6.59s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1962/2256 [3:35:42<32:15,  6.58s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1963/2256 [3:35:48<32:08,  6.58s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1964/2256 [3:35:55<32:01,  6.58s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1965/2256 [3:36:02<31:55,  6.58s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1966/2256 [3:36:08<31:46,  6.58s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1967/2256 [3:36:15<31:40,  6.57s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1968/2256 [3:36:21<31:32,  6.57s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1969/2256 [3:36:28<31:26,  6.57s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1970/2256 [3:36:35<31:40,  6.64s/it]                                                     {'loss': 1.7637, 'learning_rate': 1.9566603061801415e-06, 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1970/2256 [3:36:35<31:40,  6.64s/it]                                                     {'router_ce_loss': 0.9108221530914307, 'old_lang_expert0_score': '0.25 0.03 0.07 0.38 0.92 0.95 0.74 0.95 0.92 0.94 0.81 0.94 0.89 0.96 0.97 0.95 0.96 0.97 0.99 0.98 0.99 0.98 0.99 0.99', 'epoch': 0.87}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1970/2256 [3:36:35<31:40,  6.64s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1971/2256 [3:36:41<31:27,  6.62s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1972/2256 [3:36:48<31:18,  6.62s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1973/2256 [3:36:54<31:08,  6.60s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1974/2256 [3:37:01<31:02,  6.60s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1975/2256 [3:37:08<30:54,  6.60s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1976/2256 [3:37:14<30:48,  6.60s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1977/2256 [3:37:21<30:39,  6.59s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1978/2256 [3:37:27<30:30,  6.58s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1979/2256 [3:37:34<30:24,  6.59s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1980/2256 [3:37:40<30:15,  6.58s/it]                                                     {'loss': 1.7812, 'learning_rate': 1.823882956546566e-06, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1980/2256 [3:37:40<30:15,  6.58s/it]                                                     {'router_ce_loss': 0.9290244579315186, 'old_lang_expert0_score': '0.25 0.02 0.05 0.31 0.91 0.92 0.7 0.94 0.9 0.94 0.77 0.94 0.85 0.93 0.96 0.9 0.94 0.96 0.97 0.96 0.96 0.96 0.98 0.99', 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1980/2256 [3:37:41<30:15,  6.58s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1981/2256 [3:37:47<30:08,  6.57s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1982/2256 [3:37:54<30:00,  6.57s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1983/2256 [3:38:00<29:54,  6.57s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1984/2256 [3:38:07<29:47,  6.57s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1985/2256 [3:38:13<29:40,  6.57s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1986/2256 [3:38:20<29:35,  6.58s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1987/2256 [3:38:26<29:28,  6.57s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1988/2256 [3:38:34<29:59,  6.72s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1989/2256 [3:38:40<29:41,  6.67s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1990/2256 [3:38:47<29:26,  6.64s/it]                                                     {'loss': 1.7859, 'learning_rate': 1.6955998360604186e-06, 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1990/2256 [3:38:47<29:26,  6.64s/it]                                                     {'router_ce_loss': 0.9400460124015808, 'old_lang_expert0_score': '0.31 0.02 0.02 0.17 0.88 0.88 0.63 0.96 0.9 0.96 0.83 0.93 0.86 0.94 0.95 0.9 0.89 0.92 0.95 0.94 0.95 0.93 0.99 0.99', 'epoch': 0.88}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1990/2256 [3:38:47<29:26,  6.64s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1991/2256 [3:38:53<29:16,  6.63s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1992/2256 [3:39:00<29:05,  6.61s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1993/2256 [3:39:06<28:56,  6.60s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1994/2256 [3:39:13<28:45,  6.59s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1995/2256 [3:39:20<28:38,  6.58s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1996/2256 [3:39:26<28:30,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1997/2256 [3:39:33<28:23,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1998/2256 [3:39:39<28:17,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1999/2256 [3:39:46<28:13,  6.59s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2000/2256 [3:39:52<28:05,  6.58s/it]                                                     {'loss': 1.8405, 'learning_rate': 1.5718358209224153e-06, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2000/2256 [3:39:52<28:05,  6.58s/it]                                                     {'router_ce_loss': 0.9242373108863831, 'old_lang_expert0_score': '0.25 0.02 0.06 0.37 0.89 0.92 0.67 0.94 0.88 0.93 0.76 0.95 0.88 0.94 0.96 0.93 0.95 0.97 0.98 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2000/2256 [3:39:53<28:05,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2001/2256 [3:39:59<27:57,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2002/2256 [3:40:06<27:50,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2003/2256 [3:40:12<27:43,  6.57s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2004/2256 [3:40:19<27:37,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2005/2256 [3:40:26<27:46,  6.64s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2006/2256 [3:40:32<27:51,  6.69s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2007/2256 [3:40:39<27:35,  6.65s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2008/2256 [3:40:45<27:24,  6.63s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2009/2256 [3:40:52<27:13,  6.62s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2010/2256 [3:40:59<27:04,  6.60s/it]                                                     {'loss': 1.714, 'learning_rate': 1.4526149110046266e-06, 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2010/2256 [3:40:59<27:04,  6.60s/it]                                                     {'router_ce_loss': 0.9147703647613525, 'old_lang_expert0_score': '0.24 0.04 0.08 0.35 0.9 0.94 0.76 0.98 0.93 0.95 0.85 0.94 0.86 0.95 0.94 0.91 0.94 0.95 0.98 0.99 0.99 0.99 1.0 1.0', 'epoch': 0.89}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2010/2256 [3:40:59<27:04,  6.60s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2011/2256 [3:41:05<26:55,  6.59s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2012/2256 [3:41:12<26:47,  6.59s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2013/2256 [3:41:18<26:39,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2014/2256 [3:41:25<26:32,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2015/2256 [3:41:32<26:25,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2016/2256 [3:41:38<26:18,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2017/2256 [3:41:45<26:12,  6.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2018/2256 [3:41:51<26:04,  6.57s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2019/2256 [3:41:58<25:57,  6.57s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2020/2256 [3:42:04<25:52,  6.58s/it]                                                     {'loss': 1.7868, 'learning_rate': 1.3379602251965245e-06, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2020/2256 [3:42:04<25:52,  6.58s/it]                                                     {'router_ce_loss': 0.9237818717956543, 'old_lang_expert0_score': '0.23 0.02 0.05 0.38 0.88 0.89 0.7 0.93 0.9 0.94 0.79 0.95 0.88 0.96 0.98 0.95 0.95 0.96 0.98 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2020/2256 [3:42:05<25:52,  6.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2021/2256 [3:42:11<25:46,  6.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2022/2256 [3:42:18<25:41,  6.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2023/2256 [3:42:24<25:53,  6.67s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2024/2256 [3:42:31<25:43,  6.65s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2025/2256 [3:42:38<25:32,  6.63s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2026/2256 [3:42:44<25:22,  6.62s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2027/2256 [3:42:51<25:17,  6.63s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2028/2256 [3:42:57<25:11,  6.63s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2029/2256 [3:43:04<25:02,  6.62s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2030/2256 [3:43:11<24:54,  6.61s/it]                                                     {'loss': 1.7104, 'learning_rate': 1.2278939969218556e-06, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2030/2256 [3:43:11<24:54,  6.61s/it]                                                     {'router_ce_loss': 0.920365571975708, 'old_lang_expert0_score': '0.26 0.02 0.05 0.41 0.92 0.93 0.77 0.93 0.89 0.93 0.76 0.92 0.85 0.93 0.96 0.93 0.95 0.96 0.98 0.98 0.97 0.98 0.99 0.99', 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2030/2256 [3:43:11<24:54,  6.61s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2031/2256 [3:43:17<24:46,  6.60s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2032/2256 [3:43:24<24:37,  6.60s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2033/2256 [3:43:30<24:29,  6.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2034/2256 [3:43:37<24:21,  6.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2035/2256 [3:43:44<24:14,  6.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2036/2256 [3:43:50<24:09,  6.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2037/2256 [3:43:57<24:03,  6.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2038/2256 [3:44:03<23:56,  6.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2039/2256 [3:44:10<23:49,  6.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2040/2256 [3:44:17<23:41,  6.58s/it]                                                     {'loss': 1.8195, 'learning_rate': 1.1224375698271894e-06, 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2040/2256 [3:44:17<23:41,  6.58s/it]                                                     {'router_ce_loss': 0.9121944308280945, 'old_lang_expert0_score': '0.24 0.02 0.09 0.38 0.87 0.92 0.78 0.95 0.94 0.96 0.86 0.97 0.88 0.96 0.98 0.91 0.96 0.97 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.9}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2040/2256 [3:44:17<23:41,  6.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2041/2256 [3:44:23<23:48,  6.65s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2042/2256 [3:44:30<23:49,  6.68s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2043/2256 [3:44:37<23:34,  6.64s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2044/2256 [3:44:43<23:23,  6.62s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2045/2256 [3:44:50<23:15,  6.61s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2046/2256 [3:44:56<23:04,  6.59s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2047/2256 [3:45:03<22:58,  6.59s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2048/2256 [3:45:10<22:51,  6.59s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2049/2256 [3:45:16<22:43,  6.59s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2050/2256 [3:45:23<22:34,  6.58s/it]                                                     {'loss': 1.7363, 'learning_rate': 1.021611393643071e-06, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2050/2256 [3:45:23<22:34,  6.58s/it]                                                     {'router_ce_loss': 0.9383848905563354, 'old_lang_expert0_score': '0.24 0.03 0.07 0.3 0.82 0.84 0.63 0.92 0.91 0.95 0.78 0.95 0.85 0.95 0.97 0.92 0.91 0.96 0.97 0.96 0.95 0.96 0.99 0.99', 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2050/2256 [3:45:23<22:34,  6.58s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2051/2256 [3:45:29<22:27,  6.57s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2052/2256 [3:45:36<22:19,  6.56s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2053/2256 [3:45:42<22:12,  6.56s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2054/2256 [3:45:49<22:05,  6.56s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2055/2256 [3:45:55<21:59,  6.57s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2056/2256 [3:46:02<21:53,  6.57s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2057/2256 [3:46:09<21:46,  6.57s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2058/2256 [3:46:15<21:40,  6.57s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2059/2256 [3:46:22<21:45,  6.63s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2060/2256 [3:46:29<21:47,  6.67s/it]                                                     {'loss': 1.7672, 'learning_rate': 9.254350202184487e-07, 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2060/2256 [3:46:29<21:47,  6.67s/it]                                                     {'router_ce_loss': 0.9252161383628845, 'old_lang_expert0_score': '0.24 0.02 0.05 0.35 0.88 0.9 0.66 0.94 0.92 0.95 0.81 0.96 0.91 0.95 0.97 0.94 0.94 0.95 0.97 0.97 0.95 0.95 0.99 0.99', 'epoch': 0.91}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2060/2256 [3:46:29<21:47,  6.67s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2061/2256 [3:46:35<21:35,  6.64s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2062/2256 [3:46:42<21:25,  6.63s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2063/2256 [3:46:48<21:14,  6.61s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2064/2256 [3:46:55<21:05,  6.59s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2065/2256 [3:47:02<20:57,  6.58s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2066/2256 [3:47:08<20:49,  6.58s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2067/2256 [3:47:15<20:41,  6.57s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2068/2256 [3:47:21<20:35,  6.57s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2069/2256 [3:47:28<20:29,  6.57s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2070/2256 [3:47:34<20:21,  6.57s/it]                                                     {'loss': 1.7617, 'learning_rate': 8.339270997292814e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2070/2256 [3:47:34<20:21,  6.57s/it]                                                     {'router_ce_loss': 0.9282057285308838, 'old_lang_expert0_score': '0.25 0.03 0.05 0.27 0.88 0.91 0.71 0.94 0.9 0.94 0.79 0.95 0.87 0.93 0.96 0.94 0.94 0.95 0.97 0.97 0.97 0.96 0.99 0.99', 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2070/2256 [3:47:35<20:21,  6.57s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2071/2256 [3:47:41<20:15,  6.57s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2072/2256 [3:47:48<20:08,  6.57s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2073/2256 [3:47:54<20:00,  6.56s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2074/2256 [3:48:01<19:55,  6.57s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2075/2256 [3:48:07<19:48,  6.56s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2076/2256 [3:48:14<19:41,  6.56s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2077/2256 [3:48:21<19:46,  6.63s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2078/2256 [3:48:27<19:36,  6.61s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2079/2256 [3:48:34<19:27,  6.60s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2080/2256 [3:48:40<19:19,  6.59s/it]                                                     {'loss': 1.7765, 'learning_rate': 7.471053770619352e-07, 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2080/2256 [3:48:40<19:19,  6.59s/it]                                                     {'router_ce_loss': 0.9236786961555481, 'old_lang_expert0_score': '0.25 0.02 0.05 0.32 0.88 0.92 0.74 0.94 0.9 0.93 0.75 0.94 0.86 0.94 0.97 0.94 0.97 0.97 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.92}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2080/2256 [3:48:41<19:19,  6.59s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2081/2256 [3:48:47<19:12,  6.59s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2082/2256 [3:48:53<19:05,  6.58s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2083/2256 [3:49:00<18:58,  6.58s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2084/2256 [3:49:07<18:52,  6.58s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2085/2256 [3:49:13<18:44,  6.58s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2086/2256 [3:49:20<18:38,  6.58s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2087/2256 [3:49:26<18:33,  6.59s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2088/2256 [3:49:33<18:28,  6.60s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2089/2256 [3:49:40<18:21,  6.59s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2090/2256 [3:49:46<18:12,  6.58s/it]                                                     {'loss': 1.8993, 'learning_rate': 6.649866883721822e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2090/2256 [3:49:46<18:12,  6.58s/it]                                                     {'router_ce_loss': 0.936697781085968, 'old_lang_expert0_score': '0.3 0.02 0.04 0.31 0.87 0.9 0.65 0.92 0.88 0.91 0.74 0.92 0.87 0.92 0.94 0.93 0.91 0.93 0.96 0.95 0.95 0.96 0.99 0.99', 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2090/2256 [3:49:47<18:12,  6.58s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2091/2256 [3:49:53<18:05,  6.58s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2092/2256 [3:49:59<17:58,  6.58s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2093/2256 [3:50:06<17:51,  6.57s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2094/2256 [3:50:12<17:44,  6.57s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2095/2256 [3:50:19<17:59,  6.71s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2096/2256 [3:50:26<17:46,  6.66s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2097/2256 [3:50:32<17:34,  6.63s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2098/2256 [3:50:39<17:24,  6.61s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2099/2256 [3:50:46<17:16,  6.60s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2100/2256 [3:50:52<17:07,  6.58s/it]                                                     {'loss': 1.724, 'learning_rate': 5.875869578203824e-07, 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2100/2256 [3:50:52<17:07,  6.58s/it]                                                     {'router_ce_loss': 0.9344205260276794, 'old_lang_expert0_score': '0.25 0.01 0.04 0.25 0.88 0.89 0.62 0.95 0.91 0.95 0.79 0.96 0.89 0.95 0.98 0.93 0.93 0.94 0.97 0.96 0.96 0.94 0.99 0.99', 'epoch': 0.93}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2100/2256 [3:50:53<17:07,  6.58s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2101/2256 [3:50:59<16:59,  6.58s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2102/2256 [3:51:05<16:51,  6.57s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2103/2256 [3:51:12<16:44,  6.57s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2104/2256 [3:51:18<16:38,  6.57s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2105/2256 [3:51:25<16:31,  6.56s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2106/2256 [3:51:32<16:25,  6.57s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2107/2256 [3:51:38<16:19,  6.58s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2108/2256 [3:51:45<16:13,  6.58s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2109/2256 [3:51:51<16:08,  6.59s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2110/2256 [3:51:58<16:01,  6.58s/it]                                                     {'loss': 1.7862, 'learning_rate': 5.149211944835197e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2110/2256 [3:51:58<16:01,  6.58s/it]                                                     {'router_ce_loss': 0.9240917563438416, 'old_lang_expert0_score': '0.26 0.02 0.04 0.35 0.89 0.93 0.7 0.94 0.92 0.95 0.81 0.93 0.86 0.94 0.95 0.93 0.94 0.95 0.96 0.97 0.97 0.96 0.99 0.99', 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2110/2256 [3:51:58<16:01,  6.58s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2111/2256 [3:52:04<15:54,  6.58s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2112/2256 [3:52:11<15:56,  6.64s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2113/2256 [3:52:18<15:55,  6.68s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2114/2256 [3:52:25<15:43,  6.64s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2115/2256 [3:52:31<15:33,  6.62s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2116/2256 [3:52:38<15:24,  6.60s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2117/2256 [3:52:44<15:16,  6.60s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2118/2256 [3:52:51<15:09,  6.59s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2119/2256 [3:52:57<15:02,  6.59s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2120/2256 [3:53:04<14:54,  6.58s/it]                                                     {'loss': 1.7696, 'learning_rate': 4.470034894447195e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2120/2256 [3:53:04<14:54,  6.58s/it]                                                     {'router_ce_loss': 0.9239675402641296, 'old_lang_expert0_score': '0.24 0.01 0.04 0.3 0.89 0.91 0.67 0.95 0.92 0.96 0.83 0.94 0.88 0.95 0.98 0.94 0.95 0.97 0.98 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2120/2256 [3:53:05<14:54,  6.58s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2121/2256 [3:53:11<14:47,  6.57s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2122/2256 [3:53:17<14:40,  6.57s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2123/2256 [3:53:24<14:34,  6.57s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2124/2256 [3:53:30<14:27,  6.57s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2125/2256 [3:53:37<14:20,  6.57s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2126/2256 [3:53:43<14:13,  6.57s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2127/2256 [3:53:50<14:07,  6.57s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2128/2256 [3:53:57<14:00,  6.56s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2129/2256 [3:54:03<13:53,  6.57s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2130/2256 [3:54:10<13:57,  6.65s/it]                                                     {'loss': 1.7443, 'learning_rate': 3.838470130607258e-07, 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2130/2256 [3:54:10<13:57,  6.65s/it]                                                     {'router_ce_loss': 0.9263468980789185, 'old_lang_expert0_score': '0.24 0.02 0.06 0.36 0.91 0.91 0.73 0.92 0.91 0.94 0.78 0.92 0.8 0.94 0.97 0.92 0.95 0.97 0.98 0.97 0.96 0.97 0.99 0.99', 'epoch': 0.94}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2130/2256 [3:54:10<13:57,  6.65s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2131/2256 [3:54:17<13:47,  6.62s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2132/2256 [3:54:23<13:38,  6.60s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2133/2256 [3:54:30<13:30,  6.59s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2134/2256 [3:54:36<13:23,  6.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2135/2256 [3:54:43<13:16,  6.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2136/2256 [3:54:49<13:09,  6.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2137/2256 [3:54:56<13:02,  6.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2138/2256 [3:55:03<12:56,  6.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2139/2256 [3:55:09<12:49,  6.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2140/2256 [3:55:16<12:43,  6.58s/it]                                                     {'loss': 1.7977, 'learning_rate': 3.2546401240798607e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2140/2256 [3:55:16<12:43,  6.58s/it]                                                     {'router_ce_loss': 0.9320766925811768, 'old_lang_expert0_score': '0.24 0.02 0.05 0.34 0.88 0.92 0.68 0.94 0.88 0.93 0.76 0.93 0.83 0.95 0.95 0.93 0.94 0.95 0.96 0.96 0.96 0.97 0.99 0.99', 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2140/2256 [3:55:16<12:43,  6.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2141/2256 [3:55:22<12:35,  6.57s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2142/2256 [3:55:29<12:29,  6.57s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2143/2256 [3:55:35<12:23,  6.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2144/2256 [3:55:42<12:15,  6.57s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2145/2256 [3:55:49<12:09,  6.57s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2146/2256 [3:55:55<12:02,  6.57s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2147/2256 [3:56:02<11:56,  6.57s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2148/2256 [3:56:09<12:04,  6.71s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2149/2256 [3:56:15<11:53,  6.67s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2150/2256 [3:56:22<11:43,  6.64s/it]                                                     {'loss': 1.6961, 'learning_rate': 2.7186580890771476e-07, 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2150/2256 [3:56:22<11:43,  6.64s/it]                                                     {'router_ce_loss': 0.9189179539680481, 'old_lang_expert0_score': '0.25 0.02 0.04 0.38 0.91 0.94 0.73 0.95 0.91 0.93 0.81 0.94 0.86 0.95 0.96 0.94 0.94 0.96 0.98 0.98 0.97 0.97 0.99 0.99', 'epoch': 0.95}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2150/2256 [3:56:22<11:43,  6.64s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2151/2256 [3:56:28<11:34,  6.62s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2152/2256 [3:56:35<11:27,  6.61s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2153/2256 [3:56:42<11:19,  6.60s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2154/2256 [3:56:48<11:12,  6.59s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2155/2256 [3:56:55<11:05,  6.59s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2156/2256 [3:57:01<10:58,  6.58s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2157/2256 [3:57:08<10:51,  6.58s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2158/2256 [3:57:14<10:44,  6.58s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2159/2256 [3:57:21<10:37,  6.57s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2160/2256 [3:57:28<10:30,  6.57s/it]                                                     {'loss': 1.8464, 'learning_rate': 2.230627961304993e-07, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2160/2256 [3:57:28<10:30,  6.57s/it]                                                     {'router_ce_loss': 0.9241237640380859, 'old_lang_expert0_score': '0.25 0.02 0.06 0.32 0.85 0.89 0.69 0.94 0.9 0.94 0.83 0.96 0.89 0.95 0.98 0.94 0.95 0.96 0.97 0.97 0.97 0.96 0.99 0.99', 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2160/2256 [3:57:28<10:30,  6.57s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2161/2256 [3:57:34<10:23,  6.57s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2162/2256 [3:57:41<10:16,  6.56s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2163/2256 [3:57:47<10:10,  6.57s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2164/2256 [3:57:54<10:04,  6.57s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2165/2256 [3:58:01<10:04,  6.64s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2166/2256 [3:58:07<10:01,  6.68s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2167/2256 [3:58:14<09:51,  6.65s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2168/2256 [3:58:21<09:42,  6.62s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2169/2256 [3:58:27<09:34,  6.60s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2170/2256 [3:58:34<09:27,  6.59s/it]                                                     {'loss': 1.7148, 'learning_rate': 1.7906443778080984e-07, 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2170/2256 [3:58:34<09:27,  6.59s/it]                                                     {'router_ce_loss': 0.9222590923309326, 'old_lang_expert0_score': '0.25 0.02 0.05 0.38 0.9 0.93 0.74 0.93 0.88 0.93 0.77 0.94 0.85 0.93 0.97 0.92 0.94 0.96 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.96}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2170/2256 [3:58:34<09:27,  6.59s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2171/2256 [3:58:40<09:20,  6.59s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2172/2256 [3:58:47<09:13,  6.59s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2173/2256 [3:58:53<09:05,  6.58s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2174/2256 [3:59:00<08:59,  6.58s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2175/2256 [3:59:07<08:52,  6.57s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2176/2256 [3:59:13<08:45,  6.57s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2177/2256 [3:59:20<08:38,  6.57s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2178/2256 [3:59:26<08:32,  6.57s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2179/2256 [3:59:33<08:25,  6.57s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2180/2256 [3:59:39<08:19,  6.57s/it]                                                     {'loss': 1.7832, 'learning_rate': 1.398792658618392e-07, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2180/2256 [3:59:39<08:19,  6.57s/it]                                                     {'router_ce_loss': 0.9208798408508301, 'old_lang_expert0_score': '0.24 0.02 0.07 0.37 0.88 0.91 0.7 0.95 0.92 0.96 0.81 0.96 0.88 0.94 0.98 0.93 0.94 0.96 0.97 0.97 0.97 0.96 0.98 0.99', 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2180/2256 [3:59:40<08:19,  6.57s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2181/2256 [3:59:46<08:13,  6.57s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2182/2256 [3:59:53<08:06,  6.57s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2183/2256 [3:59:59<08:04,  6.64s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2184/2256 [4:00:06<07:56,  6.62s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2185/2256 [4:00:12<07:48,  6.60s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2186/2256 [4:00:19<07:41,  6.59s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2187/2256 [4:00:26<07:34,  6.59s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2188/2256 [4:00:32<07:27,  6.58s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2189/2256 [4:00:39<07:20,  6.57s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2190/2256 [4:00:45<07:13,  6.57s/it]                                                     {'loss': 1.8968, 'learning_rate': 1.0551487902100143e-07, 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2190/2256 [4:00:45<07:13,  6.57s/it]                                                     {'router_ce_loss': 0.9116455912590027, 'old_lang_expert0_score': '0.22 0.02 0.09 0.37 0.84 0.92 0.73 0.95 0.95 0.97 0.87 0.98 0.9 0.97 0.99 0.97 0.95 0.97 0.99 0.98 0.99 0.98 0.99 0.99', 'epoch': 0.97}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2190/2256 [4:00:46<07:13,  6.57s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2191/2256 [4:00:52<07:07,  6.58s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2192/2256 [4:00:58<07:00,  6.57s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2193/2256 [4:01:05<06:53,  6.56s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2194/2256 [4:01:12<06:46,  6.56s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2195/2256 [4:01:18<06:40,  6.56s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2196/2256 [4:01:25<06:33,  6.56s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2197/2256 [4:01:31<06:26,  6.56s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2198/2256 [4:01:38<06:20,  6.56s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2199/2256 [4:01:44<06:14,  6.57s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2200/2256 [4:01:51<06:07,  6.56s/it]                                                     {'loss': 1.7279, 'learning_rate': 7.597794107641887e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2200/2256 [4:01:51<06:07,  6.56s/it]                                                     {'router_ce_loss': 0.9331094026565552, 'old_lang_expert0_score': '0.25 0.01 0.02 0.31 0.91 0.92 0.63 0.94 0.9 0.94 0.75 0.94 0.87 0.94 0.96 0.93 0.92 0.95 0.97 0.97 0.96 0.96 0.99 0.99', 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2200/2256 [4:01:51<06:07,  6.56s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2201/2256 [4:01:58<06:08,  6.70s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2202/2256 [4:02:04<05:58,  6.65s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2203/2256 [4:02:11<05:50,  6.62s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2204/2256 [4:02:18<05:43,  6.60s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2205/2256 [4:02:24<05:35,  6.59s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2206/2256 [4:02:31<05:28,  6.58s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2207/2256 [4:02:37<05:21,  6.57s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2208/2256 [4:02:44<05:15,  6.57s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2209/2256 [4:02:50<05:08,  6.56s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2210/2256 [4:02:57<05:02,  6.57s/it]                                                     {'loss': 1.7327, 'learning_rate': 5.1274179724730694e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2210/2256 [4:02:57<05:02,  6.57s/it]                                                     {'router_ce_loss': 0.9309149384498596, 'old_lang_expert0_score': '0.25 0.03 0.06 0.28 0.86 0.89 0.62 0.95 0.9 0.94 0.84 0.94 0.88 0.92 0.95 0.92 0.93 0.95 0.97 0.97 0.97 0.96 0.99 0.98', 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2210/2256 [4:02:57<05:02,  6.57s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2211/2256 [4:03:04<04:55,  6.58s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2212/2256 [4:03:10<04:49,  6.57s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2213/2256 [4:03:17<04:42,  6.57s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2214/2256 [4:03:23<04:36,  6.57s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2215/2256 [4:03:30<04:29,  6.58s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2216/2256 [4:03:36<04:22,  6.57s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2217/2256 [4:03:43<04:16,  6.58s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2218/2256 [4:03:50<04:12,  6.65s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2219/2256 [4:03:57<04:07,  6.69s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2220/2256 [4:04:03<03:59,  6.65s/it]                                                     {'loss': 1.7271, 'learning_rate': 3.1408385430356516e-08, 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2220/2256 [4:04:03<03:59,  6.65s/it]                                                     {'router_ce_loss': 0.9331483840942383, 'old_lang_expert0_score': '0.24 0.02 0.05 0.25 0.85 0.87 0.66 0.94 0.89 0.94 0.77 0.97 0.85 0.97 0.98 0.94 0.95 0.97 0.98 0.97 0.95 0.96 0.99 0.99', 'epoch': 0.98}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2220/2256 [4:04:04<03:59,  6.65s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2221/2256 [4:04:10<03:52,  6.63s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2222/2256 [4:04:16<03:44,  6.61s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2223/2256 [4:04:23<03:37,  6.60s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2224/2256 [4:04:29<03:30,  6.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2225/2256 [4:04:36<03:23,  6.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2226/2256 [4:04:43<03:17,  6.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2227/2256 [4:04:49<03:10,  6.57s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2228/2256 [4:04:56<03:03,  6.57s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2229/2256 [4:05:02<02:57,  6.57s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2230/2256 [4:05:09<02:50,  6.57s/it]                                                     {'loss': 1.7327, 'learning_rate': 1.638441049658379e-08, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2230/2256 [4:05:09<02:50,  6.57s/it]                                                     {'router_ce_loss': 0.9262953400611877, 'old_lang_expert0_score': '0.27 0.02 0.05 0.28 0.89 0.9 0.72 0.93 0.91 0.93 0.81 0.93 0.86 0.93 0.97 0.94 0.93 0.95 0.98 0.97 0.98 0.98 0.99 0.99', 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2230/2256 [4:05:09<02:50,  6.57s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2231/2256 [4:05:15<02:44,  6.57s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2232/2256 [4:05:22<02:37,  6.57s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2233/2256 [4:05:29<02:31,  6.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2234/2256 [4:05:35<02:24,  6.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2235/2256 [4:05:42<02:18,  6.58s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2236/2256 [4:05:49<02:12,  6.64s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2237/2256 [4:05:55<02:05,  6.62s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2238/2256 [4:06:02<01:59,  6.61s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2239/2256 [4:06:08<01:52,  6.60s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2240/2256 [4:06:15<01:45,  6.60s/it]                                                     {'loss': 1.7816, 'learning_rate': 6.205168318523802e-09, 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2240/2256 [4:06:15<01:45,  6.60s/it]                                                     {'router_ce_loss': 0.9806311130523682, 'old_lang_expert0_score': '0.25 0.04 0.07 0.3 0.82 0.82 0.63 0.85 0.81 0.87 0.71 0.88 0.78 0.88 0.9 0.84 0.89 0.9 0.91 0.91 0.92 0.91 0.93 0.94', 'epoch': 0.99}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2240/2256 [4:06:15<01:45,  6.60s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2241/2256 [4:06:21<01:38,  6.60s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2242/2256 [4:06:28<01:32,  6.59s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2243/2256 [4:06:35<01:25,  6.59s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2244/2256 [4:06:41<01:19,  6.59s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2245/2256 [4:06:48<01:12,  6.59s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2246/2256 [4:06:54<01:05,  6.60s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2247/2256 [4:07:01<00:59,  6.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2248/2256 [4:07:08<00:52,  6.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2249/2256 [4:07:14<00:45,  6.57s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2250/2256 [4:07:21<00:39,  6.57s/it]                                                     {'loss': 1.7714, 'learning_rate': 8.726328181551946e-10, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2250/2256 [4:07:21<00:39,  6.57s/it]                                                     {'router_ce_loss': 0.9363441467285156, 'old_lang_expert0_score': '0.22 0.01 0.06 0.32 0.88 0.88 0.59 0.92 0.9 0.93 0.78 0.94 0.88 0.95 0.97 0.93 0.93 0.94 0.96 0.95 0.96 0.96 0.99 0.99', 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2250/2256 [4:07:21<00:39,  6.57s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2251/2256 [4:07:27<00:32,  6.56s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2252/2256 [4:07:34<00:26,  6.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2253/2256 [4:07:40<00:19,  6.57s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2254/2256 [4:07:47<00:13,  6.64s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2255/2256 [4:07:54<00:06,  6.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [4:08:01<00:00,  6.67s/it][INFO|trainer.py:1989] 2024-07-11 15:44:29,191 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 14881.0972, 'train_samples_per_second': 29.108, 'train_steps_per_second': 0.152, 'train_loss': 1.7722763985606795, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [4:08:01<00:00,  6.67s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [4:08:01<00:00,  6.60s/it]
[INFO|trainer.py:2981] 2024-07-11 15:44:35,267 >> Saving model checkpoint to /home/nfs04/wangzj/checkpoints/moe/test
/home/nfs02/wangzj/aliyun/temp_data/peft/src/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B - will assume that the vocabulary was not modified.
  warnings.warn(
[2024-07-11 15:44:37,248] [INFO] [launch.py:347:main] Process 33987 exits successfully.
[2024-07-11 15:44:37,249] [INFO] [launch.py:347:main] Process 33986 exits successfully.
[INFO|tokenization_utils_base.py:2435] 2024-07-11 15:44:47,875 >> tokenizer config file saved in /home/nfs04/wangzj/checkpoints/moe/test/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-07-11 15:44:47,876 >> Special tokens file saved in /home/nfs04/wangzj/checkpoints/moe/test/special_tokens_map.json
[INFO|tokenization_utils_base.py:2495] 2024-07-11 15:44:47,877 >> added tokens file saved in /home/nfs04/wangzj/checkpoints/moe/test/added_tokens.json
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     1.7723
  train_runtime            = 4:08:01.09
  train_samples_per_second =     29.108
  train_steps_per_second   =      0.152
Figure saved: /home/nfs04/wangzj/checkpoints/moe/test/training_loss.png
07/11/2024 15:44:49 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-07-11 15:44:49,328 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-07-11 15:44:51,266] [INFO] [launch.py:347:main] Process 33985 exits successfully.
