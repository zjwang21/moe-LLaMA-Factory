[2024-07-09 11:26:09,822] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 11:26:12,626] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-09 11:26:12,673] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/alimoe/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/nfs04/wangzj/models/Qwen1.5-1.8B --adapter_name_or_path /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01 --train_only_router --ce_loss_coef 0.05 --do_train --dataset skypile_1b --max_samples 50000 --preprocessing_num_workers 16 --cutoff_len 512 --finetuning_type moe --output_dir /home/nfs04/wangzj/checkpoints/moe/test --overwrite_output_dir --per_device_train_batch_size 8 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 100 --save_steps 10000 --save_only_model --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-07-09 11:26:14,092] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 11:26:16,006] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [5, 6, 7]}
[2024-07-09 11:26:16,006] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=3, node_rank=0
[2024-07-09 11:26:16,006] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2]})
[2024-07-09 11:26:16,006] [INFO] [launch.py:163:main] dist_world_size=3
[2024-07-09 11:26:16,006] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=5,6,7
[2024-07-09 11:26:21,658] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 11:26:21,658] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 11:26:21,658] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 11:26:31,324] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-09 11:26:31,325] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-09 11:26:31,325] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-09 11:26:31,325] [INFO] [comm.py:637:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
07/09/2024 11:26:32 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/09/2024 11:26:32 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul09_11-26-31_2080ti-2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
07/09/2024 11:26:32 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/09/2024 11:26:32 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul09_11-26-31_2080ti-2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-07-09 11:26:32,438 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2027] 2024-07-09 11:26:32,438 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2027] 2024-07-09 11:26:32,438 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-07-09 11:26:32,438 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-07-09 11:26:32,438 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-07-09 11:26:32,438 >> loading file tokenizer.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
07/09/2024 11:26:32 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/09/2024 11:26:32 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul09_11-26-31_2080ti-2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[WARNING|logging.py:314] 2024-07-09 11:26:32,866 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:727] 2024-07-09 11:26:32,868 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/config.json
[INFO|configuration_utils.py:792] 2024-07-09 11:26:32,871 >> Model config Qwen2Config {
  "_name_or_path": "/home/nfs04/wangzj/models/Qwen1.5-1.8B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3334] 2024-07-09 11:26:33,053 >> loading weights file /home/nfs04/wangzj/models/Qwen1.5-1.8B/model.safetensors
[INFO|modeling_utils.py:1459] 2024-07-09 11:26:33,320 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:827] 2024-07-09 11:26:33,322 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}

07/09/2024 11:26:36 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/09/2024 11:26:36 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/09/2024 11:26:36 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
[INFO|modeling_utils.py:4070] 2024-07-09 11:26:36,292 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4078] 2024-07-09 11:26:36,293 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/nfs04/wangzj/models/Qwen1.5-1.8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-07-09 11:26:36,310 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/generation_config.json
[INFO|configuration_utils.py:827] 2024-07-09 11:26:36,310 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "max_new_tokens": 2048
}

07/09/2024 11:26:36 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/09/2024 11:26:36 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/09/2024 11:26:36 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
07/09/2024 11:26:36 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/09/2024 11:26:36 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/09/2024 11:26:36 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
07/09/2024 11:27:31 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
07/09/2024 11:27:31 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/09/2024 11:27:31 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/09/2024 11:27:31 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/09/2024 11:27:31 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
07/09/2024 11:27:31 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/09/2024 11:27:31 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
07/09/2024 11:27:31 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/09/2024 11:27:31 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/09/2024 11:27:31 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/09/2024 11:27:31 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/09/2024 11:27:31 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/09/2024 11:27:43 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
Using custom data configuration default-5d974cfe78f38e3a
Loading Dataset Infos from /home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Downloading and preparing dataset json/default to /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 5338 examples [00:00, 44212.12 examples/s]Generating train split: 12594 examples [00:00, 58873.51 examples/s]Generating train split: 19675 examples [00:00, 63262.22 examples/s]Generating train split: 28828 examples [00:00, 67389.30 examples/s]Generating train split: 39436 examples [00:00, 68244.23 examples/s]Generating train split: 46551 examples [00:00, 68474.66 examples/s]Generating train split: 57132 examples [00:00, 70557.39 examples/s]Generating train split: 65984 examples [00:00, 71190.68 examples/s]Generating train split: 75061 examples [00:01, 72489.97 examples/s]Generating train split: 84092 examples [00:01, 72887.51 examples/s]Generating train split: 93028 examples [00:01, 73279.69 examples/s]Generating train split: 101918 examples [00:01, 73026.32 examples/s]Generating train split: 114427 examples [00:01, 71755.28 examples/s]Generating train split: 125297 examples [00:01, 72445.98 examples/s]Generating train split: 135877 examples [00:01, 72120.58 examples/s]Generating train split: 143138 examples [00:02, 71958.00 examples/s]Generating train split: 155649 examples [00:02, 72068.92 examples/s]Generating train split: 164509 examples [00:02, 72133.56 examples/s]Generating train split: 171751 examples [00:02, 71822.66 examples/s]Generating train split: 182405 examples [00:02, 72203.71 examples/s]Generating train split: 194930 examples [00:02, 71645.37 examples/s]Generating train split: 203808 examples [00:02, 71257.03 examples/s]Generating train split: 214534 examples [00:03, 71479.41 examples/s]Generating train split: 223471 examples [00:03, 71833.57 examples/s]Generating train split: 234234 examples [00:03, 72310.10 examples/s]Generating train split: 243163 examples [00:03, 72331.44 examples/s]Generating train split: 253992 examples [00:03, 73590.47 examples/s]Generating train split: 264799 examples [00:03, 73646.43 examples/s]Generating train split: 277206 examples [00:03, 73344.78 examples/s]Generating train split: 286066 examples [00:04, 72819.13 examples/s]Generating train split: 296795 examples [00:04, 73500.77 examples/s]Generating train split: 305565 examples [00:04, 73179.63 examples/s]Generating train split: 316320 examples [00:04, 74266.73 examples/s]Generating train split: 327112 examples [00:04, 74027.45 examples/s]Generating train split: 336042 examples [00:04, 73261.54 examples/s]Generating train split: 346761 examples [00:04, 73355.76 examples/s]Generating train split: 355751 examples [00:04, 72356.05 examples/s]Generating train split: 366514 examples [00:05, 69388.27 examples/s]Generating train split: 375576 examples [00:05, 69971.25 examples/s]Generating train split: 384471 examples [00:05, 70716.15 examples/s]Generating train split: 395142 examples [00:05, 71991.24 examples/s]Generating train split: 405766 examples [00:05, 71404.35 examples/s]Generating train split: 414714 examples [00:05, 71852.37 examples/s]Generating train split: 425649 examples [00:05, 73240.39 examples/s]Generating train split: 436407 examples [00:06, 74626.94 examples/s]Generating train split: 447196 examples [00:06, 74561.11 examples/s]Generating train split: 457923 examples [00:06, 75647.03 examples/s]Generating train split: 468669 examples [00:06, 75783.47 examples/s]Generating train split: 479581 examples [00:06, 75920.22 examples/s]Generating train split: 490369 examples [00:06, 76263.78 examples/s]Generating train split: 501205 examples [00:06, 77707.09 examples/s]Generating train split: 511930 examples [00:07, 77373.31 examples/s]Generating train split: 522748 examples [00:07, 76544.40 examples/s]Generating train split: 533429 examples [00:07, 75663.56 examples/s]Generating train split: 544198 examples [00:07, 75897.27 examples/s]Generating train split: 554914 examples [00:07, 76558.41 examples/s]Generating train split: 565679 examples [00:07, 76480.38 examples/s]Generating train split: 576504 examples [00:07, 77234.32 examples/s]Generating train split: 587217 examples [00:08, 76882.86 examples/s]Generating train split: 597959 examples [00:08, 77525.13 examples/s]Generating train split: 608798 examples [00:08, 77268.89 examples/s]Generating train split: 619698 examples [00:08, 76793.56 examples/s]Generating train split: 630500 examples [00:08, 77445.94 examples/s]Generating train split: 641330 examples [00:08, 77988.87 examples/s]Generating train split: 652070 examples [00:08, 77811.56 examples/s]Generating train split: 661595 examples [00:08, 78765.50 examples/s]Generating train split: 661595 examples [00:08, 73588.56 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00000_of_00016.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00001_of_00016.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00002_of_00016.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00003_of_00016.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00004_of_00016.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00005_of_00016.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00006_of_00016.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00007_of_00016.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00008_of_00016.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00009_of_00016.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00010_of_00016.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00011_of_00016.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00012_of_00016.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00013_of_00016.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00014_of_00016.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00015_of_00016.arrow
Spawning 16 processes
Converting format of dataset (num_proc=16):   0%|          | 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00000_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00004_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00003_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00006_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00001_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00002_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00010_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00007_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00005_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00011_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00012_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00009_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00014_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00013_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00008_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4b078eb7a394907c_00015_of_00016.arrow
Converting format of dataset (num_proc=16):   2%|▏         | 1000/50000 [00:00<00:43, 1122.54 examples/s]Converting format of dataset (num_proc=16):  90%|█████████ | 45250/50000 [00:02<00:00, 20878.82 examples/s]Converting format of dataset (num_proc=16):  94%|█████████▍| 47000/50000 [00:03<00:00, 15828.04 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 50000/50000 [00:03<00:00, 15131.92 examples/s]
Concatenating 16 shards
Map:   0%|          | 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-36ba184b3ca450cc.arrow
Map:   2%|▏         | 973/50000 [00:00<00:05, 9392.41 examples/s]Map:   4%|▍         | 2250/50000 [00:00<00:05, 8791.75 examples/s]Map:   7%|▋         | 3693/50000 [00:00<00:05, 9195.05 examples/s]Map:   9%|▉         | 4706/50000 [00:00<00:04, 9490.48 examples/s]Map:  11%|█▏        | 5719/50000 [00:00<00:04, 9687.54 examples/s]Map:  13%|█▎        | 6723/50000 [00:00<00:04, 9795.80 examples/s]Map:  15%|█▌        | 7748/50000 [00:00<00:04, 9931.80 examples/s]Map:  18%|█▊        | 8809/50000 [00:00<00:04, 10135.83 examples/s]Map:  20%|█▉        | 9861/50000 [00:01<00:03, 10250.52 examples/s]Map:  23%|██▎       | 11288/50000 [00:01<00:03, 9956.28 examples/s]Map:  25%|██▍       | 12337/50000 [00:01<00:03, 10098.60 examples/s]Map:  27%|██▋       | 13376/50000 [00:01<00:03, 10176.81 examples/s]Map:  29%|██▉       | 14423/50000 [00:01<00:03, 10256.26 examples/s]Map:  31%|███       | 15617/50000 [00:01<00:03, 10400.20 examples/s]Map:  33%|███▎      | 16665/50000 [00:01<00:03, 10421.37 examples/s]Map:  36%|███▌      | 18102/50000 [00:01<00:03, 10099.34 examples/s]Map:  38%|███▊      | 19160/50000 [00:01<00:03, 10223.14 examples/s]Map:  40%|████      | 20220/50000 [00:02<00:02, 10325.15 examples/s]Map:  43%|████▎     | 21278/50000 [00:02<00:02, 10395.12 examples/s]Map:  46%|████▌     | 22912/50000 [00:02<00:02, 10579.22 examples/s]Map:  49%|████▉     | 24395/50000 [00:02<00:02, 10334.39 examples/s]Map:  51%|█████     | 25622/50000 [00:02<00:02, 10475.32 examples/s]Map:  53%|█████▎    | 26688/50000 [00:02<00:02, 10520.46 examples/s]Map:  56%|█████▋    | 28142/50000 [00:02<00:02, 10224.05 examples/s]Map:  58%|█████▊    | 29196/50000 [00:02<00:02, 10302.64 examples/s]Map:  60%|██████    | 30234/50000 [00:02<00:01, 10320.67 examples/s]Map:  63%|██████▎   | 31282/50000 [00:03<00:01, 10360.98 examples/s]Map:  65%|██████▍   | 32339/50000 [00:03<00:01, 10418.41 examples/s]Map:  67%|██████▋   | 33401/50000 [00:03<00:01, 10474.22 examples/s]Map:  69%|██████▉   | 34617/50000 [00:03<00:01, 10572.39 examples/s]Map:  72%|███████▏  | 36091/50000 [00:03<00:01, 10287.21 examples/s]Map:  75%|███████▌  | 37695/50000 [00:03<00:01, 10426.12 examples/s]Map:  77%|███████▋  | 38741/50000 [00:03<00:01, 10430.97 examples/s]Map:  80%|███████▉  | 39794/50000 [00:03<00:00, 10454.92 examples/s]Map:  82%|████████▏ | 40853/50000 [00:03<00:00, 10491.23 examples/s]Map:  85%|████████▍ | 42332/50000 [00:04<00:00, 10248.26 examples/s]Map:  87%|████████▋ | 43373/50000 [00:04<00:00, 10288.09 examples/s]Map:  89%|████████▉ | 44413/50000 [00:04<00:00, 10315.62 examples/s]Map:  91%|█████████ | 45621/50000 [00:04<00:00, 10434.73 examples/s]Map:  93%|█████████▎| 46681/50000 [00:04<00:00, 10478.01 examples/s]Map:  96%|█████████▋| 48159/50000 [00:04<00:00, 10241.47 examples/s]Map:  98%|█████████▊| 49204/50000 [00:04<00:00, 10292.54 examples/s]Map: 100%|██████████| 50000/50000 [00:10<00:00, 4942.67 examples/s] 
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00000_of_00016.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00001_of_00016.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00002_of_00016.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00003_of_00016.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00004_of_00016.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00005_of_00016.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00006_of_00016.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00007_of_00016.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00008_of_00016.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00009_of_00016.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00010_of_00016.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00011_of_00016.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00012_of_00016.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00013_of_00016.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00014_of_00016.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00015_of_00016.arrow
Spawning 16 processes
Running tokenizer on dataset (num_proc=16):   0%|          | 0/50000 [00:00<?, ? examples/s]07/09/2024 11:28:27 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
07/09/2024 11:28:28 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00000_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   2%|▏         | 1000/50000 [00:35<29:03, 28.10 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00001_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   4%|▍         | 2000/50000 [00:36<12:22, 64.65 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00002_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   6%|▌         | 3000/50000 [00:40<07:42, 101.53 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00003_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   8%|▊         | 4000/50000 [00:44<05:51, 130.91 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00004_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  10%|█         | 5000/50000 [00:46<04:09, 180.11 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-07-09 11:29:17,886 >> Token indices sequence length is longer than the specified maximum sequence length for this model (40175 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00005_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  12%|█▏        | 6000/50000 [00:52<04:08, 177.14 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00006_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  14%|█▍        | 7000/50000 [00:54<03:18, 216.84 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00007_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  16%|█▌        | 8000/50000 [00:58<02:59, 233.78 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00008_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  18%|█▊        | 9000/50000 [00:59<02:12, 309.44 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00010_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  20%|██        | 10000/50000 [01:08<03:21, 198.72 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00011_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  22%|██▏       | 11000/50000 [01:11<02:51, 227.62 examples/s]Running tokenizer on dataset (num_proc=16):  24%|██▍       | 12000/50000 [01:11<02:02, 310.70 examples/s]Running tokenizer on dataset (num_proc=16):  26%|██▌       | 13000/50000 [01:11<01:25, 434.85 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00009_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  28%|██▊       | 14000/50000 [01:13<01:20, 446.34 examples/s]Running tokenizer on dataset (num_proc=16):  30%|███       | 15000/50000 [01:17<01:28, 397.09 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 16000/50000 [01:17<01:05, 521.90 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00012_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  34%|███▍      | 17000/50000 [01:19<01:00, 544.32 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00013_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  36%|███▌      | 18000/50000 [01:19<00:47, 679.32 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 19000/50000 [01:19<00:33, 931.65 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00014_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  40%|████      | 20000/50000 [01:21<00:35, 845.28 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6fafb1c8eb644953_00015_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  42%|████▏     | 21000/50000 [01:23<00:39, 742.95 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 22000/50000 [01:26<00:56, 495.21 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▌     | 23000/50000 [01:29<01:04, 418.86 examples/s]Running tokenizer on dataset (num_proc=16):  48%|████▊     | 24000/50000 [01:33<01:12, 360.19 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 25000/50000 [01:37<01:21, 308.50 examples/s]Running tokenizer on dataset (num_proc=16):  52%|█████▏    | 26000/50000 [01:44<01:40, 238.27 examples/s]Running tokenizer on dataset (num_proc=16):  54%|█████▍    | 27000/50000 [01:46<01:24, 271.10 examples/s]Running tokenizer on dataset (num_proc=16):  54%|█████▍    | 27125/50000 [01:48<01:31, 249.40 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 28125/50000 [01:48<00:56, 388.16 examples/s]Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 29125/50000 [01:49<00:44, 471.94 examples/s]Running tokenizer on dataset (num_proc=16):  60%|██████    | 30250/50000 [01:53<00:50, 393.12 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 31375/50000 [01:53<00:34, 534.61 examples/s]Running tokenizer on dataset (num_proc=16):  65%|██████▍   | 32375/50000 [01:56<00:39, 450.47 examples/s]Running tokenizer on dataset (num_proc=16):  67%|██████▋   | 33500/50000 [01:57<00:29, 555.15 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 34500/50000 [01:58<00:22, 680.05 examples/s]Running tokenizer on dataset (num_proc=16):  71%|███████   | 35500/50000 [02:00<00:22, 657.10 examples/s]Running tokenizer on dataset (num_proc=16):  73%|███████▎  | 36500/50000 [02:02<00:22, 611.69 examples/s]Running tokenizer on dataset (num_proc=16):  73%|███████▎  | 36625/50000 [02:04<00:33, 404.94 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 37625/50000 [02:05<00:22, 553.67 examples/s]Running tokenizer on dataset (num_proc=16):  76%|███████▌  | 37750/50000 [02:05<00:23, 524.55 examples/s]Running tokenizer on dataset (num_proc=16):  78%|███████▊  | 38750/50000 [02:06<00:19, 589.28 examples/s]Running tokenizer on dataset (num_proc=16):  80%|███████▉  | 39750/50000 [02:07<00:12, 805.32 examples/s]Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 40750/50000 [02:08<00:09, 929.97 examples/s]Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 40875/50000 [02:09<00:13, 652.72 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▍ | 41875/50000 [02:10<00:12, 650.19 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▍ | 42000/50000 [02:11<00:13, 601.08 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▍ | 42125/50000 [02:14<00:30, 256.18 examples/s]Running tokenizer on dataset (num_proc=16):  86%|████████▋ | 43125/50000 [02:19<00:30, 225.98 examples/s]Running tokenizer on dataset (num_proc=16):  86%|████████▋ | 43250/50000 [02:23<00:43, 156.02 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 44250/50000 [02:23<00:21, 272.40 examples/s]Running tokenizer on dataset (num_proc=16):  90%|█████████ | 45250/50000 [02:27<00:18, 263.22 examples/s]Running tokenizer on dataset (num_proc=16):  92%|█████████▎| 46250/50000 [02:28<00:09, 406.12 examples/s]Running tokenizer on dataset (num_proc=16):  93%|█████████▎| 46500/50000 [02:31<00:13, 261.30 examples/s]Running tokenizer on dataset (num_proc=16):  95%|█████████▌| 47500/50000 [02:31<00:05, 426.04 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 48750/50000 [02:37<00:03, 318.14 examples/s]Running tokenizer on dataset (num_proc=16): 100%|█████████▉| 49750/50000 [02:39<00:00, 348.68 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 50000/50000 [02:42<00:00, 250.46 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 50000/50000 [02:43<00:00, 306.58 examples/s]
Concatenating 16 shards
input_ids:
[14777, 48888, 105854, 3837, 99313, 99685, 99829, 99395, 65770, 104603, 39953, 3837, 16, 13, 23, 99294, 40, 12, 19, 104798, 3837, 99278, 112440, 9370, 16, 94299, 19589, 51, 3837, 108897, 105216, 6313, 89982, 30534, 104363, 5122, 12, 107563, 28330, 24562, 59956, 108144, 12, 42192, 107605, 101040, 12, 116463, 64064, 12, 100168, 101044, 102705, 12, 105710, 101159, 12, 100931, 35727, 100237, 12, 100415, 27369, 54021, 12, 100171, 100144, 84104, 2446, 56, 608, 15814, 12, 29526, 5122, 12, 99805, 39953, 109360, 12, 110501, 104650, 12, 100931, 101143, 64689, 12, 101939, 99464, 100504, 198, 55338, 103978, 99472, 67338, 20637, 9909, 105600, 100343, 7552, 16, 20, 20, 47882, 33424, 97, 33477, 101978, 90395, 100136, 103965, 99553, 20, 15, 15, 15, 16017, 57191, 24, 15, 35727, 35987, 39953, 117403, 6313, 103965, 36605, 14777, 73296, 99318, 10, 17, 32571, 71134, 116165, 10, 99796, 103965, 63703, 99620, 104116, 47874, 6313, 198, 117134, 13253, 35931, 100338, 9909, 105274, 100082, 100343, 100338, 7552, 111538, 20637, 9909, 105600, 100343, 7552, 102503, 100135, 3837, 103926, 113823, 100343, 47874, 6313, 100437, 36407, 99366, 8997, 151643, 75023, 102310, 99372, 63109, 102598, 107269, 114110, 104273, 17714, 12, 17, 15, 144105, 21216, 16, 15, 20, 144105, 3837, 85106, 13343, 101910, 109386, 101082, 30440, 107269, 105377, 104273, 9370, 114110, 3837, 106584, 102310, 5373, 102339, 5373, 112414, 5373, 104244, 5373, 116458, 5373, 101083, 5373, 107365, 5373, 101117, 5373, 110628, 54542, 33108, 106726, 105753, 111425, 100751, 107269, 100646, 109985, 9370, 57191, 110263, 100791, 9370, 113080, 52510, 9370, 114110, 8997, 14777, 5373, 75023, 24300, 102576, 28330, 102310, 99372, 63109, 102598, 82700, 113608, 28311, 75023, 24300, 102310, 102598, 20412, 23990, 52334, 23990, 99544, 9909, 102390, 69041, 118124, 7552, 100588, 102768, 28330, 99372, 63109, 102598, 3837, 83744, 107269, 109852, 115320, 107561, 100629, 109985, 33071, 5373, 104957, 26381, 101412, 52510, 9370, 109181, 1773, 41146, 113743, 5373, 61191, 22382, 102111, 33108, 105356, 49567, 59355, 101910, 100142, 24167, 17, 23, 20, 23, 3837, 100629, 102111, 101121, 80942, 5373, 101991, 44636, 91956, 44991, 32108, 854, 100021, 44636, 33108, 104440, 101147, 113598, 3837, 41146, 101991, 56006, 37, 24300, 102598, 101200, 100627, 20, 130696, 3837, 20412, 99599, 101955, 9370, 101487, 82700, 8997, 75023, 24300, 102310, 102598, 107269, 114110, 104273, 17714, 12, 17, 15, 144105, 21216, 16, 15, 20, 144105, 3837, 85106, 13343, 101910, 109386, 101082, 30440, 107269, 105377, 104273, 9370, 114110, 3837, 106584, 102310, 5373, 102339, 5373, 112414, 5373, 104244, 5373, 116458, 5373, 101083, 5373, 107365, 5373, 101117, 5373, 110628, 54542, 33108, 106726, 105753, 111425, 100751, 107269, 100646, 109985, 9370, 57191, 110263, 100791, 9370, 113080, 52510, 9370, 114110, 8997, 109068, 102598, 40952, 100338, 103921, 99878, 99553, 75023, 102310, 99372, 63109, 102598, 49567, 82700, 27369, 3837, 100437, 110969, 100703, 6313, 151643, 107699, 99185, 22704, 9370, 105834, 112741, 101899, 30, 105682, 101899, 101160, 3837, 50404, 105045, 100634, 104771, 100799, 3837, 105834, 101158, 99491, 103456, 99549, 9370, 101160, 3837, 101899, 99793, 100000, 99792, 103985, 9370, 3837, 99999, 101924, 107427, 50404, 52801, 105327, 9370, 100634, 100363, 3837, 101160, 100000, 104083, 101899, 9370, 1773, 100624, 3837, 107699, 99185, 22704, 9370, 105834, 112741, 101899, 30, 110404, 99258, 106091, 15946, 99233, 105834, 100634, 9370, 36407, 102585, 109432, 8997, 107699, 99185, 22704, 9370, 105834, 112741, 101899, 5267]
inputs:
一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁
所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！
隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。
<|endoftext|>IH化工离心泵输送介质温度为-20℃～105℃，需要时采用冷却措施可输送更高温度的介质，适用于化工、石油、冶金、电力、造纸、食品、制药、环保、废水处理和合成纤维等行业用于输送各种腐蚀的或不允许污染的类似于水的介质。
一、IH型卧式化工离心泵产品概述：
IH型化工泵是单级单吸（轴向吸入）悬臂式离心泵，供输送不含固体颗粒具有腐蚀性、粘度类似水的液体。其标记、额定性能和尺寸等效采用标准ISO2858，具有性能范围广、效率高、“三化”水平高和维修方便等特点，其效率比F型泵平均提高5％，是国家推广的节能产品。
IH型化工泵输送介质温度为-20℃～105℃，需要时采用冷却措施可输送更高温度的介质，适用于化工、石油、冶金、电力、造纸、食品、制药、环保、废水处理和合成纤维等行业用于输送各种腐蚀的或不允许污染的类似于水的介质。
太平洋泵业集团有限公司专业提供IH化工离心泵等产品信息，欢迎来电咨询！<|endoftext|>处在发展期的白癜风应该如何治疗?要想治疗疾病，选择正确的医院是非常重要的，白癜风是一种非常顽固的疾病，治疗起来也是比较困难的，所以患者如果没有选择好正规的医院的话，疾病也是很难治疗的。那么，处在发展期的白癜风应该如何治疗?下面就让兰州中研白癜风医院的来给大家介绍一下。
处在发展期的白癜风应该如何治疗?

Caching indices mapping at /home/wangzj/.cache/huggingface/datasets/json/default-5d974cfe78f38e3a/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-e50f49dd057b9d81.arrow
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 64385
})
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): Qwen2ForCausalLM(
      (model): Qwen2Model(
        (embed_tokens): Embedding(151936, 2048)
        (layers): ModuleList(
          (0): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (1): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (2): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (3): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (4): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (5): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (6): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (7): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (8): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (9): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (10): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (11): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (12): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (13): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (14): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (15): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (16): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (17): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (18): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (19): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (20): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (21): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (22): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (23): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
        )
        (norm): Qwen2RMSNorm()
      )
      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
    )
  )
)
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
[INFO|trainer.py:586] 2024-07-09 11:31:09,628 >> Using auto half precision backend
[2024-07-09 11:31:10,030] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 64385
})
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 64385
})
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
[2024-07-09 11:31:24,193] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-09 11:31:24,196] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-07-09 11:31:24,196] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-07-09 11:31:24,200] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-07-09 11:31:24,200] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-07-09 11:31:24,200] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-07-09 11:31:24,200] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-07-09 11:31:24,200] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-07-09 11:31:24,200] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: False
[2024-07-09 11:31:24,200] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-07-09 11:31:25,254] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-07-09 11:31:25,255] [INFO] [utils.py:803:see_memory_usage] MA 8.47 GB         Max_MA 8.47 GB         CA 8.76 GB         Max_CA 9 GB 
[2024-07-09 11:31:25,255] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 42.47 GB, percent = 11.3%
[2024-07-09 11:31:25,453] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-07-09 11:31:25,454] [INFO] [utils.py:803:see_memory_usage] MA 8.47 GB         Max_MA 8.47 GB         CA 8.76 GB         Max_CA 9 GB 
[2024-07-09 11:31:25,454] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 42.47 GB, percent = 11.3%
[2024-07-09 11:31:25,454] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-07-09 11:31:25,647] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-07-09 11:31:25,648] [INFO] [utils.py:803:see_memory_usage] MA 8.47 GB         Max_MA 8.47 GB         CA 8.76 GB         Max_CA 9 GB 
[2024-07-09 11:31:25,648] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 42.47 GB, percent = 11.3%
[2024-07-09 11:31:25,648] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-07-09 11:31:25,649] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-07-09 11:31:25,649] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-07-09 11:31:25,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2024-07-09 11:31:25,651] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-07-09 11:31:25,651] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-09 11:31:25,651] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-09 11:31:25,651] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-07-09 11:31:25,651] [INFO] [config.py:978:print]   amp_params ................... False
[2024-07-09 11:31:25,651] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-09 11:31:25,651] [INFO] [config.py:978:print]   bfloat16_enabled ............. True
[2024-07-09 11:31:25,651] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-07-09 11:31:25,651] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-07-09 11:31:25,651] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-07-09 11:31:25,651] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6761ad8be0>
[2024-07-09 11:31:25,651] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   dump_state ................... False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... None
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   fp16_auto_cast ............... None
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   fp16_enabled ................. False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 4
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 1
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   loss_scale ................... 1.0
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-07-09 11:31:25,652] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   pld_params ................... False
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   train_batch_size ............. 96
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  8
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   world_size ................... 3
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-09 11:31:25,653] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-07-09 11:31:25,653] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 96, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1748] 2024-07-09 11:31:25,653 >> ***** Running training *****
[INFO|trainer.py:1749] 2024-07-09 11:31:25,653 >>   Num examples = 64,385
[INFO|trainer.py:1750] 2024-07-09 11:31:25,653 >>   Num Epochs = 1
[INFO|trainer.py:1751] 2024-07-09 11:31:25,653 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1754] 2024-07-09 11:31:25,653 >>   Total train batch size (w. parallel, distributed & accumulation) = 96
[INFO|trainer.py:1755] 2024-07-09 11:31:25,653 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1756] 2024-07-09 11:31:25,653 >>   Total optimization steps = 670
[INFO|trainer.py:1757] 2024-07-09 11:31:25,656 >>   Number of trainable parameters = 196,608
07/09/2024 11:31:25 - WARNING - llmtuner.extras.callbacks - Previous log file in this folder will be deleted.
  0%|          | 0/670 [00:00<?, ?it/s]  0%|          | 1/670 [00:04<52:37,  4.72s/it]  0%|          | 2/670 [00:07<36:52,  3.31s/it]  0%|          | 3/670 [00:09<31:53,  2.87s/it]  1%|          | 4/670 [00:11<29:32,  2.66s/it]  1%|          | 5/670 [00:14<28:15,  2.55s/it]  1%|          | 6/670 [00:16<27:31,  2.49s/it]  1%|          | 7/670 [00:18<27:02,  2.45s/it]  1%|          | 8/670 [00:21<26:42,  2.42s/it]  1%|▏         | 9/670 [00:23<26:24,  2.40s/it]  1%|▏         | 10/670 [00:25<26:13,  2.38s/it]                                                {'loss': 5.7585, 'learning_rate': 4.997252228714279e-05, 'epoch': 0.01}
  1%|▏         | 10/670 [00:25<26:13,  2.38s/it]                                                {'router_ce_loss': 1.2382844686508179, 'old_lang_expert0_score': '0.21 0.1 0.09 0.12 0.52 0.52 0.12 0.59 0.3 0.67 0.19 0.77 0.13 0.78 0.82 0.54 0.61 0.62 0.6 0.55 0.4 0.58 0.33 0.39', 'epoch': 0.01}
  1%|▏         | 10/670 [00:26<26:13,  2.38s/it]  2%|▏         | 11/670 [00:28<26:06,  2.38s/it]  2%|▏         | 12/670 [00:30<26:17,  2.40s/it]  2%|▏         | 13/670 [00:33<26:39,  2.43s/it]  2%|▏         | 14/670 [00:35<26:52,  2.46s/it]  2%|▏         | 15/670 [00:38<27:01,  2.48s/it]  2%|▏         | 16/670 [00:40<26:51,  2.46s/it]  3%|▎         | 17/670 [00:43<26:34,  2.44s/it]  3%|▎         | 18/670 [00:45<27:19,  2.51s/it]  3%|▎         | 19/670 [00:48<26:52,  2.48s/it]  3%|▎         | 20/670 [00:50<26:37,  2.46s/it]                                                {'loss': 5.2851, 'learning_rate': 4.9890149550547454e-05, 'epoch': 0.03}
  3%|▎         | 20/670 [00:50<26:37,  2.46s/it]                                                {'router_ce_loss': 1.211790919303894, 'old_lang_expert0_score': '0.21 0.08 0.06 0.09 0.53 0.58 0.11 0.64 0.35 0.73 0.19 0.8 0.16 0.84 0.87 0.63 0.65 0.66 0.63 0.58 0.47 0.6 0.29 0.52', 'epoch': 0.03}
  3%|▎         | 20/670 [00:50<26:37,  2.46s/it]  3%|▎         | 21/670 [00:52<26:17,  2.43s/it]  3%|▎         | 22/670 [00:55<26:04,  2.41s/it]  3%|▎         | 23/670 [00:57<25:55,  2.40s/it]  4%|▎         | 24/670 [01:00<25:47,  2.40s/it]  4%|▎         | 25/670 [01:02<25:39,  2.39s/it]  4%|▍         | 26/670 [01:04<25:34,  2.38s/it]  4%|▍         | 27/670 [01:07<25:32,  2.38s/it]  4%|▍         | 28/670 [01:09<25:28,  2.38s/it]  4%|▍         | 29/670 [01:11<25:18,  2.37s/it]  4%|▍         | 30/670 [01:14<25:11,  2.36s/it]                                                {'loss': 5.0462, 'learning_rate': 4.9753062863366276e-05, 'epoch': 0.04}
  4%|▍         | 30/670 [01:14<25:11,  2.36s/it]                                                {'router_ce_loss': 1.2315036058425903, 'old_lang_expert0_score': '0.21 0.1 0.08 0.09 0.48 0.5 0.11 0.6 0.21 0.72 0.16 0.79 0.09 0.77 0.82 0.56 0.6 0.65 0.66 0.62 0.54 0.55 0.2 0.71', 'epoch': 0.04}
  4%|▍         | 30/670 [01:14<25:11,  2.36s/it]  5%|▍         | 31/670 [01:16<25:06,  2.36s/it]  5%|▍         | 32/670 [01:18<25:02,  2.35s/it]  5%|▍         | 33/670 [01:21<24:58,  2.35s/it]  5%|▌         | 34/670 [01:23<24:54,  2.35s/it]  5%|▌         | 35/670 [01:26<25:37,  2.42s/it]  5%|▌         | 36/670 [01:28<25:21,  2.40s/it]  6%|▌         | 37/670 [01:30<25:08,  2.38s/it]  6%|▌         | 38/670 [01:33<25:02,  2.38s/it]  6%|▌         | 39/670 [01:35<24:55,  2.37s/it]  6%|▌         | 40/670 [01:37<24:49,  2.36s/it]                                                {'loss': 4.7499, 'learning_rate': 4.95615635718894e-05, 'epoch': 0.06}
  6%|▌         | 40/670 [01:37<24:49,  2.36s/it]                                                {'router_ce_loss': 1.177100658416748, 'old_lang_expert0_score': '0.19 0.08 0.06 0.09 0.53 0.54 0.08 0.68 0.27 0.81 0.14 0.83 0.12 0.88 0.92 0.71 0.78 0.82 0.75 0.7 0.56 0.68 0.24 0.84', 'epoch': 0.06}
  6%|▌         | 40/670 [01:38<24:49,  2.36s/it]  6%|▌         | 41/670 [01:40<24:46,  2.36s/it]  6%|▋         | 42/670 [01:42<24:41,  2.36s/it]  6%|▋         | 43/670 [01:45<24:40,  2.36s/it]  7%|▋         | 44/670 [01:47<24:37,  2.36s/it]  7%|▋         | 45/670 [01:49<24:37,  2.36s/it]  7%|▋         | 46/670 [01:52<24:34,  2.36s/it]  7%|▋         | 47/670 [01:54<24:31,  2.36s/it]  7%|▋         | 48/670 [01:56<24:30,  2.36s/it]  7%|▋         | 49/670 [01:59<24:28,  2.36s/it]  7%|▋         | 50/670 [02:01<24:26,  2.37s/it]                                                {'loss': 4.5735, 'learning_rate': 4.931607263312032e-05, 'epoch': 0.07}
  7%|▋         | 50/670 [02:01<24:26,  2.37s/it]                                                {'router_ce_loss': 1.1463485956192017, 'old_lang_expert0_score': '0.19 0.07 0.06 0.1 0.58 0.64 0.11 0.75 0.32 0.84 0.13 0.87 0.14 0.91 0.93 0.8 0.83 0.85 0.83 0.74 0.68 0.71 0.24 0.85', 'epoch': 0.07}
  7%|▋         | 50/670 [02:01<24:26,  2.37s/it]  8%|▊         | 51/670 [02:03<24:24,  2.37s/it]  8%|▊         | 52/670 [02:06<24:24,  2.37s/it]  8%|▊         | 53/670 [02:08<25:07,  2.44s/it]  8%|▊         | 54/670 [02:11<24:52,  2.42s/it]  8%|▊         | 55/670 [02:13<24:41,  2.41s/it]  8%|▊         | 56/670 [02:16<24:32,  2.40s/it]  9%|▊         | 57/670 [02:18<24:27,  2.39s/it]  9%|▊         | 58/670 [02:20<24:22,  2.39s/it]  9%|▉         | 59/670 [02:23<24:18,  2.39s/it]  9%|▉         | 60/670 [02:25<24:14,  2.38s/it]                                                {'loss': 4.4227, 'learning_rate': 4.9017129689421e-05, 'epoch': 0.09}
  9%|▉         | 60/670 [02:25<24:14,  2.38s/it]                                                {'router_ce_loss': 1.175150990486145, 'old_lang_expert0_score': '0.21 0.08 0.07 0.09 0.57 0.63 0.09 0.67 0.23 0.8 0.11 0.84 0.09 0.85 0.89 0.69 0.75 0.79 0.8 0.74 0.68 0.7 0.17 0.9', 'epoch': 0.09}
  9%|▉         | 60/670 [02:25<24:14,  2.38s/it]  9%|▉         | 61/670 [02:27<24:11,  2.38s/it]  9%|▉         | 62/670 [02:30<24:09,  2.38s/it]  9%|▉         | 63/670 [02:32<24:07,  2.38s/it] 10%|▉         | 64/670 [02:35<24:06,  2.39s/it] 10%|▉         | 65/670 [02:37<24:05,  2.39s/it] 10%|▉         | 66/670 [02:39<24:04,  2.39s/it] 10%|█         | 67/670 [02:42<24:03,  2.39s/it] 10%|█         | 68/670 [02:44<24:02,  2.40s/it] 10%|█         | 69/670 [02:47<24:08,  2.41s/it] 10%|█         | 70/670 [02:49<24:14,  2.42s/it]                                                {'loss': 4.3088, 'learning_rate': 4.8665391882260856e-05, 'epoch': 0.1}
 10%|█         | 70/670 [02:49<24:14,  2.42s/it]                                                {'router_ce_loss': 1.1649900674819946, 'old_lang_expert0_score': '0.19 0.07 0.07 0.08 0.55 0.65 0.1 0.7 0.22 0.8 0.1 0.83 0.1 0.87 0.9 0.75 0.8 0.85 0.83 0.8 0.7 0.77 0.1 0.92', 'epoch': 0.1}
 10%|█         | 70/670 [02:49<24:14,  2.42s/it] 11%|█         | 71/670 [02:52<24:55,  2.50s/it] 11%|█         | 72/670 [02:54<24:49,  2.49s/it] 11%|█         | 73/670 [02:57<24:45,  2.49s/it] 11%|█         | 74/670 [02:59<24:54,  2.51s/it] 11%|█         | 75/670 [03:02<25:02,  2.53s/it] 11%|█▏        | 76/670 [03:04<24:58,  2.52s/it] 11%|█▏        | 77/670 [03:07<25:10,  2.55s/it] 12%|█▏        | 78/670 [03:10<25:16,  2.56s/it] 12%|█▏        | 79/670 [03:12<25:25,  2.58s/it] 12%|█▏        | 80/670 [03:15<25:38,  2.61s/it]                                                {'loss': 4.2051, 'learning_rate': 4.8261632407677174e-05, 'epoch': 0.12}
 12%|█▏        | 80/670 [03:15<25:38,  2.61s/it]                                                {'router_ce_loss': 1.1625078916549683, 'old_lang_expert0_score': '0.19 0.06 0.07 0.1 0.63 0.71 0.12 0.7 0.22 0.8 0.12 0.85 0.08 0.85 0.89 0.73 0.79 0.83 0.82 0.8 0.74 0.74 0.09 0.91', 'epoch': 0.12}
 12%|█▏        | 80/670 [03:15<25:38,  2.61s/it] 12%|█▏        | 81/670 [03:17<25:35,  2.61s/it] 12%|█▏        | 82/670 [03:20<25:38,  2.62s/it] 12%|█▏        | 83/670 [03:23<26:04,  2.67s/it] 13%|█▎        | 84/670 [03:26<26:06,  2.67s/it] 13%|█▎        | 85/670 [03:28<26:22,  2.71s/it] 13%|█▎        | 86/670 [03:31<26:32,  2.73s/it] 13%|█▎        | 87/670 [03:34<26:48,  2.76s/it] 13%|█▎        | 88/670 [03:37<27:21,  2.82s/it] 13%|█▎        | 89/670 [03:40<26:52,  2.78s/it] 13%|█▎        | 90/670 [03:43<27:06,  2.80s/it]                                                {'loss': 4.1413, 'learning_rate': 4.780673881662242e-05, 'epoch': 0.13}
 13%|█▎        | 90/670 [03:43<27:06,  2.80s/it]                                                {'router_ce_loss': 1.1336084604263306, 'old_lang_expert0_score': '0.19 0.07 0.06 0.09 0.6 0.69 0.09 0.75 0.22 0.87 0.12 0.91 0.09 0.91 0.94 0.83 0.87 0.9 0.89 0.84 0.84 0.87 0.1 0.94', 'epoch': 0.13}
 13%|█▎        | 90/670 [03:43<27:06,  2.80s/it] 14%|█▎        | 91/670 [03:45<27:28,  2.85s/it] 14%|█▎        | 92/670 [03:48<27:32,  2.86s/it] 14%|█▍        | 93/670 [03:51<27:47,  2.89s/it] 14%|█▍        | 94/670 [03:55<29:40,  3.09s/it] 14%|█▍        | 95/670 [03:58<29:23,  3.07s/it] 14%|█▍        | 96/670 [04:01<28:40,  3.00s/it] 14%|█▍        | 97/670 [04:04<28:39,  3.00s/it] 15%|█▍        | 98/670 [04:07<28:52,  3.03s/it] 15%|█▍        | 99/670 [04:10<29:01,  3.05s/it] 15%|█▍        | 100/670 [04:13<29:40,  3.12s/it]                                                 {'loss': 4.0667, 'learning_rate': 4.730171106393466e-05, 'epoch': 0.15}
 15%|█▍        | 100/670 [04:13<29:40,  3.12s/it]                                                 {'router_ce_loss': 1.1087275743484497, 'old_lang_expert0_score': '0.19 0.07 0.07 0.09 0.69 0.77 0.12 0.8 0.28 0.9 0.12 0.91 0.11 0.93 0.96 0.87 0.91 0.92 0.92 0.9 0.87 0.89 0.11 0.96', 'epoch': 0.15}
 15%|█▍        | 100/670 [04:13<29:40,  3.12s/it] 15%|█▌        | 101/670 [04:16<29:58,  3.16s/it] 15%|█▌        | 102/670 [04:19<29:23,  3.10s/it] 15%|█▌        | 103/670 [04:22<28:46,  3.04s/it] 16%|█▌        | 104/670 [04:26<29:08,  3.09s/it] 16%|█▌        | 105/670 [04:29<29:17,  3.11s/it] 16%|█▌        | 106/670 [04:32<29:58,  3.19s/it] 16%|█▌        | 107/670 [04:35<29:57,  3.19s/it] 16%|█▌        | 108/670 [04:38<29:54,  3.19s/it] 16%|█▋        | 109/670 [04:42<29:55,  3.20s/it] 16%|█▋        | 110/670 [04:45<30:00,  3.21s/it]                                                 {'loss': 4.0112, 'learning_rate': 4.674765931021976e-05, 'epoch': 0.16}
 16%|█▋        | 110/670 [04:45<30:00,  3.21s/it]                                                 {'router_ce_loss': 1.1107641458511353, 'old_lang_expert0_score': '0.19 0.05 0.06 0.08 0.67 0.75 0.11 0.82 0.28 0.9 0.11 0.92 0.1 0.92 0.97 0.9 0.93 0.94 0.92 0.91 0.91 0.89 0.07 0.97', 'epoch': 0.16}
 16%|█▋        | 110/670 [04:45<30:00,  3.21s/it] 17%|█▋        | 111/670 [04:48<30:00,  3.22s/it] 17%|█▋        | 112/670 [04:51<29:54,  3.22s/it] 17%|█▋        | 113/670 [04:55<29:53,  3.22s/it] 17%|█▋        | 114/670 [04:58<29:47,  3.21s/it] 17%|█▋        | 115/670 [05:01<29:41,  3.21s/it] 17%|█▋        | 116/670 [05:04<29:43,  3.22s/it] 17%|█▋        | 117/670 [05:07<29:39,  3.22s/it] 18%|█▊        | 118/670 [05:11<29:34,  3.21s/it] 18%|█▊        | 119/670 [05:14<29:27,  3.21s/it] 18%|█▊        | 120/670 [05:17<29:26,  3.21s/it]                                                 {'loss': 4.0084, 'learning_rate': 4.614580148147744e-05, 'epoch': 0.18}
 18%|█▊        | 120/670 [05:17<29:26,  3.21s/it]                                                 {'router_ce_loss': 1.1164228916168213, 'old_lang_expert0_score': '0.19 0.05 0.06 0.08 0.69 0.76 0.12 0.81 0.24 0.88 0.1 0.9 0.06 0.9 0.95 0.87 0.91 0.93 0.93 0.92 0.92 0.9 0.07 0.96', 'epoch': 0.18}
 18%|█▊        | 120/670 [05:17<29:26,  3.21s/it] 18%|█▊        | 121/670 [05:20<29:23,  3.21s/it] 18%|█▊        | 122/670 [05:24<29:36,  3.24s/it] 18%|█▊        | 123/670 [05:27<30:24,  3.34s/it] 19%|█▊        | 124/670 [05:31<30:53,  3.39s/it] 19%|█▊        | 125/670 [05:35<32:13,  3.55s/it] 19%|█▉        | 126/670 [05:38<32:04,  3.54s/it] 19%|█▉        | 127/670 [05:42<32:00,  3.54s/it] 19%|█▉        | 128/670 [05:45<31:49,  3.52s/it] 19%|█▉        | 129/670 [05:49<31:43,  3.52s/it] 19%|█▉        | 130/670 [05:52<31:39,  3.52s/it]                                                 {'loss': 3.9013, 'learning_rate': 4.5497460591835615e-05, 'epoch': 0.19}
 19%|█▉        | 130/670 [05:52<31:39,  3.52s/it]                                                 {'router_ce_loss': 1.1217700242996216, 'old_lang_expert0_score': '0.19 0.06 0.06 0.08 0.69 0.77 0.11 0.78 0.23 0.85 0.08 0.91 0.08 0.9 0.94 0.86 0.9 0.93 0.93 0.91 0.89 0.88 0.07 0.97', 'epoch': 0.19}
 19%|█▉        | 130/670 [05:52<31:39,  3.52s/it] 20%|█▉        | 131/670 [05:56<31:33,  3.51s/it] 20%|█▉        | 132/670 [05:59<31:26,  3.51s/it] 20%|█▉        | 133/670 [06:03<31:19,  3.50s/it] 20%|██        | 134/670 [06:06<31:19,  3.51s/it] 20%|██        | 135/670 [06:10<31:12,  3.50s/it] 20%|██        | 136/670 [06:13<31:07,  3.50s/it] 20%|██        | 137/670 [06:17<31:07,  3.50s/it] 21%|██        | 138/670 [06:20<30:58,  3.49s/it] 21%|██        | 139/670 [06:24<30:47,  3.48s/it] 21%|██        | 140/670 [06:27<30:41,  3.47s/it]                                                 {'loss': 3.9134, 'learning_rate': 4.480406183527823e-05, 'epoch': 0.21}
 21%|██        | 140/670 [06:27<30:41,  3.47s/it]                                                 {'router_ce_loss': 1.1144531965255737, 'old_lang_expert0_score': '0.18 0.06 0.07 0.07 0.67 0.78 0.12 0.83 0.26 0.88 0.08 0.9 0.06 0.92 0.95 0.89 0.92 0.94 0.94 0.93 0.91 0.89 0.05 0.96', 'epoch': 0.21}
 21%|██        | 140/670 [06:27<30:41,  3.47s/it] 21%|██        | 141/670 [06:30<30:34,  3.47s/it] 21%|██        | 142/670 [06:34<31:00,  3.52s/it] 21%|██▏       | 143/670 [06:38<30:45,  3.50s/it] 21%|██▏       | 144/670 [06:41<30:34,  3.49s/it] 22%|██▏       | 145/670 [06:44<30:26,  3.48s/it] 22%|██▏       | 146/670 [06:48<30:20,  3.47s/it] 22%|██▏       | 147/670 [06:51<30:12,  3.46s/it] 22%|██▏       | 148/670 [06:55<30:12,  3.47s/it] 22%|██▏       | 149/670 [06:58<30:03,  3.46s/it] 22%|██▏       | 150/670 [07:02<29:57,  3.46s/it]                                                 {'loss': 3.8784, 'learning_rate': 4.406712945275955e-05, 'epoch': 0.22}
 22%|██▏       | 150/670 [07:02<29:57,  3.46s/it]                                                 {'router_ce_loss': 1.1135082244873047, 'old_lang_expert0_score': '0.18 0.05 0.07 0.09 0.69 0.81 0.14 0.82 0.22 0.88 0.09 0.92 0.07 0.91 0.95 0.87 0.92 0.93 0.93 0.92 0.92 0.89 0.07 0.96', 'epoch': 0.22}
 22%|██▏       | 150/670 [07:02<29:57,  3.46s/it] 23%|██▎       | 151/670 [07:05<29:56,  3.46s/it] 23%|██▎       | 152/670 [07:09<29:51,  3.46s/it] 23%|██▎       | 153/670 [07:12<29:39,  3.44s/it] 23%|██▎       | 154/670 [07:16<29:37,  3.44s/it] 23%|██▎       | 155/670 [07:19<29:34,  3.45s/it] 23%|██▎       | 156/670 [07:22<29:30,  3.45s/it] 23%|██▎       | 157/670 [07:26<29:24,  3.44s/it] 24%|██▎       | 158/670 [07:29<29:24,  3.45s/it] 24%|██▎       | 159/670 [07:33<29:21,  3.45s/it] 24%|██▍       | 160/670 [07:36<29:46,  3.50s/it]                                                 {'loss': 3.8652, 'learning_rate': 4.328828338159173e-05, 'epoch': 0.24}
 24%|██▍       | 160/670 [07:36<29:46,  3.50s/it]                                                 {'router_ce_loss': 1.1068899631500244, 'old_lang_expert0_score': '0.18 0.05 0.07 0.08 0.71 0.81 0.11 0.83 0.21 0.89 0.08 0.92 0.06 0.93 0.96 0.91 0.94 0.95 0.95 0.94 0.95 0.92 0.05 0.97', 'epoch': 0.24}
 24%|██▍       | 160/670 [07:37<29:46,  3.50s/it] 24%|██▍       | 161/670 [07:40<29:30,  3.48s/it] 24%|██▍       | 162/670 [07:43<29:20,  3.47s/it] 24%|██▍       | 163/670 [07:47<29:09,  3.45s/it] 24%|██▍       | 164/670 [07:50<28:52,  3.42s/it] 25%|██▍       | 165/670 [07:53<28:51,  3.43s/it] 25%|██▍       | 166/670 [07:57<28:46,  3.43s/it] 25%|██▍       | 167/670 [08:00<28:44,  3.43s/it] 25%|██▌       | 168/670 [08:04<28:37,  3.42s/it] 25%|██▌       | 169/670 [08:07<28:33,  3.42s/it] 25%|██▌       | 170/670 [08:11<28:27,  3.41s/it]                                                 {'loss': 3.8287, 'learning_rate': 4.2469235694471043e-05, 'epoch': 0.25}
 25%|██▌       | 170/670 [08:11<28:27,  3.41s/it]                                                 {'router_ce_loss': 1.0958725214004517, 'old_lang_expert0_score': '0.19 0.04 0.05 0.08 0.76 0.83 0.13 0.88 0.32 0.9 0.11 0.94 0.12 0.92 0.97 0.91 0.93 0.94 0.94 0.93 0.95 0.93 0.07 0.97', 'epoch': 0.25}
 25%|██▌       | 170/670 [08:11<28:27,  3.41s/it] 26%|██▌       | 171/670 [08:14<28:15,  3.40s/it] 26%|██▌       | 172/670 [08:17<28:17,  3.41s/it] 26%|██▌       | 173/670 [08:21<28:10,  3.40s/it] 26%|██▌       | 174/670 [08:24<28:06,  3.40s/it] 26%|██▌       | 175/670 [08:27<27:56,  3.39s/it] 26%|██▋       | 176/670 [08:31<27:52,  3.39s/it] 26%|██▋       | 177/670 [08:34<27:47,  3.38s/it] 27%|██▋       | 178/670 [08:38<28:13,  3.44s/it] 27%|██▋       | 179/670 [08:41<28:00,  3.42s/it] 27%|██▋       | 180/670 [08:45<27:45,  3.40s/it]                                                 {'loss': 3.8444, 'learning_rate': 4.161178683597054e-05, 'epoch': 0.27}
 27%|██▋       | 180/670 [08:45<27:45,  3.40s/it]                                                 {'router_ce_loss': 1.0926982164382935, 'old_lang_expert0_score': '0.2 0.07 0.07 0.09 0.75 0.85 0.14 0.87 0.27 0.92 0.11 0.94 0.06 0.94 0.96 0.92 0.94 0.94 0.95 0.95 0.95 0.91 0.08 0.96', 'epoch': 0.27}
 27%|██▋       | 180/670 [08:45<27:45,  3.40s/it] 27%|██▋       | 181/670 [08:48<27:38,  3.39s/it] 27%|██▋       | 182/670 [08:51<27:27,  3.38s/it] 27%|██▋       | 183/670 [08:55<27:21,  3.37s/it] 27%|██▋       | 184/670 [08:58<27:16,  3.37s/it] 28%|██▊       | 185/670 [09:01<27:12,  3.37s/it] 28%|██▊       | 186/670 [09:05<27:07,  3.36s/it] 28%|██▊       | 187/670 [09:08<27:03,  3.36s/it] 28%|██▊       | 188/670 [09:11<27:05,  3.37s/it] 28%|██▊       | 189/670 [09:15<26:59,  3.37s/it] 28%|██▊       | 190/670 [09:18<26:52,  3.36s/it]                                                 {'loss': 3.8035, 'learning_rate': 4.071782166477213e-05, 'epoch': 0.28}
 28%|██▊       | 190/670 [09:18<26:52,  3.36s/it]                                                 {'router_ce_loss': 1.1006417274475098, 'old_lang_expert0_score': '0.18 0.05 0.07 0.08 0.75 0.83 0.15 0.86 0.28 0.89 0.1 0.92 0.08 0.92 0.95 0.89 0.94 0.93 0.95 0.94 0.95 0.93 0.04 0.97', 'epoch': 0.28}
 28%|██▊       | 190/670 [09:18<26:52,  3.36s/it] 29%|██▊       | 191/670 [09:22<26:49,  3.36s/it] 29%|██▊       | 192/670 [09:25<26:45,  3.36s/it] 29%|██▉       | 193/670 [09:28<26:39,  3.35s/it] 29%|██▉       | 194/670 [09:32<26:34,  3.35s/it] 29%|██▉       | 195/670 [09:35<26:54,  3.40s/it] 29%|██▉       | 196/670 [09:38<26:45,  3.39s/it] 29%|██▉       | 197/670 [09:42<26:35,  3.37s/it] 30%|██▉       | 198/670 [09:45<26:30,  3.37s/it] 30%|██▉       | 199/670 [09:48<26:25,  3.37s/it] 30%|██▉       | 200/670 [09:52<26:18,  3.36s/it]                                                 {'loss': 3.7606, 'learning_rate': 3.978930531033807e-05, 'epoch': 0.3}
 30%|██▉       | 200/670 [09:52<26:18,  3.36s/it]                                                 {'router_ce_loss': 1.0892707109451294, 'old_lang_expert0_score': '0.18 0.04 0.05 0.07 0.76 0.84 0.15 0.9 0.32 0.94 0.12 0.95 0.1 0.95 0.98 0.92 0.96 0.97 0.97 0.96 0.95 0.95 0.04 0.98', 'epoch': 0.3}
 30%|██▉       | 200/670 [09:52<26:18,  3.36s/it] 30%|███       | 201/670 [09:55<26:12,  3.35s/it] 30%|███       | 202/670 [09:59<26:09,  3.35s/it] 30%|███       | 203/670 [10:02<26:06,  3.36s/it] 30%|███       | 204/670 [10:05<26:04,  3.36s/it] 31%|███       | 205/670 [10:09<25:55,  3.34s/it] 31%|███       | 206/670 [10:12<25:50,  3.34s/it] 31%|███       | 207/670 [10:15<25:47,  3.34s/it] 31%|███       | 208/670 [10:19<25:46,  3.35s/it] 31%|███       | 209/670 [10:22<25:42,  3.35s/it] 31%|███▏      | 210/670 [10:25<25:37,  3.34s/it]                                                 {'loss': 3.7596, 'learning_rate': 3.882827885312999e-05, 'epoch': 0.31}
 31%|███▏      | 210/670 [10:25<25:37,  3.34s/it]                                                 {'router_ce_loss': 1.1026216745376587, 'old_lang_expert0_score': '0.19 0.05 0.05 0.07 0.74 0.81 0.12 0.85 0.27 0.89 0.1 0.93 0.1 0.92 0.96 0.89 0.94 0.93 0.96 0.95 0.95 0.95 0.06 0.97', 'epoch': 0.31}
 31%|███▏      | 210/670 [10:26<25:37,  3.34s/it] 31%|███▏      | 211/670 [10:29<25:36,  3.35s/it] 32%|███▏      | 212/670 [10:32<25:34,  3.35s/it] 32%|███▏      | 213/670 [10:36<25:56,  3.41s/it] 32%|███▏      | 214/670 [10:39<25:39,  3.38s/it] 32%|███▏      | 215/670 [10:42<25:30,  3.36s/it] 32%|███▏      | 216/670 [10:45<25:23,  3.36s/it] 32%|███▏      | 217/670 [10:49<25:18,  3.35s/it] 33%|███▎      | 218/670 [10:52<25:13,  3.35s/it] 33%|███▎      | 219/670 [10:56<25:11,  3.35s/it] 33%|███▎      | 220/670 [10:59<25:06,  3.35s/it]                                                 {'loss': 3.7328, 'learning_rate': 3.783685483787105e-05, 'epoch': 0.33}
 33%|███▎      | 220/670 [10:59<25:06,  3.35s/it]                                                 {'router_ce_loss': 1.0966342687606812, 'old_lang_expert0_score': '0.18 0.05 0.06 0.07 0.76 0.85 0.14 0.87 0.27 0.91 0.1 0.94 0.08 0.93 0.96 0.91 0.95 0.96 0.96 0.95 0.96 0.94 0.05 0.97', 'epoch': 0.33}
 33%|███▎      | 220/670 [10:59<25:06,  3.35s/it] 33%|███▎      | 221/670 [11:02<25:04,  3.35s/it] 33%|███▎      | 222/670 [11:06<25:00,  3.35s/it] 33%|███▎      | 223/670 [11:09<24:57,  3.35s/it] 33%|███▎      | 224/670 [11:12<24:55,  3.35s/it] 34%|███▎      | 225/670 [11:16<24:50,  3.35s/it] 34%|███▎      | 226/670 [11:19<24:48,  3.35s/it] 34%|███▍      | 227/670 [11:22<24:40,  3.34s/it] 34%|███▍      | 228/670 [11:26<24:39,  3.35s/it] 34%|███▍      | 229/670 [11:29<24:32,  3.34s/it] 34%|███▍      | 230/670 [11:32<24:31,  3.34s/it]                                                 {'loss': 3.7667, 'learning_rate': 3.681721262971413e-05, 'epoch': 0.34}
 34%|███▍      | 230/670 [11:32<24:31,  3.34s/it]                                                 {'router_ce_loss': 1.100037693977356, 'old_lang_expert0_score': '0.17 0.05 0.06 0.07 0.72 0.84 0.16 0.85 0.26 0.9 0.1 0.93 0.07 0.93 0.96 0.9 0.93 0.96 0.96 0.96 0.95 0.94 0.05 0.97', 'epoch': 0.34}
 34%|███▍      | 230/670 [11:33<24:31,  3.34s/it] 34%|███▍      | 231/670 [11:36<24:53,  3.40s/it] 35%|███▍      | 232/670 [11:39<24:39,  3.38s/it] 35%|███▍      | 233/670 [11:43<24:33,  3.37s/it] 35%|███▍      | 234/670 [11:46<24:28,  3.37s/it] 35%|███▌      | 235/670 [11:49<24:19,  3.35s/it] 35%|███▌      | 236/670 [11:53<24:13,  3.35s/it] 35%|███▌      | 237/670 [11:56<24:08,  3.35s/it] 36%|███▌      | 238/670 [11:59<24:01,  3.34s/it] 36%|███▌      | 239/670 [12:03<24:01,  3.34s/it] 36%|███▌      | 240/670 [12:06<23:59,  3.35s/it]                                                 {'loss': 3.736, 'learning_rate': 3.5771593623524265e-05, 'epoch': 0.36}
 36%|███▌      | 240/670 [12:06<23:59,  3.35s/it]                                                 {'router_ce_loss': 1.0980113744735718, 'old_lang_expert0_score': '0.17 0.04 0.06 0.08 0.76 0.85 0.16 0.87 0.27 0.9 0.09 0.94 0.07 0.92 0.96 0.91 0.95 0.96 0.97 0.96 0.95 0.94 0.03 0.96', 'epoch': 0.36}
 36%|███▌      | 240/670 [12:06<23:59,  3.35s/it] 36%|███▌      | 241/670 [12:09<24:00,  3.36s/it] 36%|███▌      | 242/670 [12:13<23:57,  3.36s/it] 36%|███▋      | 243/670 [12:16<23:46,  3.34s/it] 36%|███▋      | 244/670 [12:19<23:37,  3.33s/it] 37%|███▋      | 245/670 [12:23<23:35,  3.33s/it] 37%|███▋      | 246/670 [12:26<23:35,  3.34s/it] 37%|███▋      | 247/670 [12:29<23:29,  3.33s/it] 37%|███▋      | 248/670 [12:33<23:52,  3.39s/it] 37%|███▋      | 249/670 [12:36<23:37,  3.37s/it] 37%|███▋      | 250/670 [12:39<23:25,  3.35s/it]                                                 {'loss': 3.7319, 'learning_rate': 3.4702296316806244e-05, 'epoch': 0.37}
 37%|███▋      | 250/670 [12:39<23:25,  3.35s/it]                                                 {'router_ce_loss': 1.090812087059021, 'old_lang_expert0_score': '0.17 0.05 0.06 0.07 0.77 0.85 0.17 0.88 0.32 0.9 0.11 0.94 0.08 0.93 0.96 0.92 0.95 0.96 0.96 0.96 0.96 0.95 0.05 0.97', 'epoch': 0.37}
 37%|███▋      | 250/670 [12:40<23:25,  3.35s/it] 37%|███▋      | 251/670 [12:43<23:15,  3.33s/it] 38%|███▊      | 252/670 [12:46<23:11,  3.33s/it] 38%|███▊      | 253/670 [12:49<23:06,  3.33s/it] 38%|███▊      | 254/670 [12:53<23:02,  3.32s/it] 38%|███▊      | 255/670 [12:56<22:53,  3.31s/it] 38%|███▊      | 256/670 [12:59<22:49,  3.31s/it] 38%|███▊      | 257/670 [13:03<22:49,  3.32s/it] 39%|███▊      | 258/670 [13:06<22:41,  3.30s/it] 39%|███▊      | 259/670 [13:09<22:41,  3.31s/it] 39%|███▉      | 260/670 [13:13<22:44,  3.33s/it]                                                 {'loss': 3.6897, 'learning_rate': 3.361167125710832e-05, 'epoch': 0.39}
 39%|███▉      | 260/670 [13:13<22:44,  3.33s/it]                                                 {'router_ce_loss': 1.0954934358596802, 'old_lang_expert0_score': '0.18 0.05 0.06 0.08 0.77 0.84 0.16 0.87 0.29 0.89 0.11 0.92 0.09 0.93 0.96 0.89 0.95 0.96 0.96 0.96 0.95 0.94 0.02 0.97', 'epoch': 0.39}
 39%|███▉      | 260/670 [13:13<22:44,  3.33s/it] 39%|███▉      | 261/670 [13:16<22:39,  3.32s/it] 39%|███▉      | 262/670 [13:19<22:33,  3.32s/it] 39%|███▉      | 263/670 [13:23<22:30,  3.32s/it] 39%|███▉      | 264/670 [13:26<22:30,  3.33s/it] 40%|███▉      | 265/670 [13:29<22:23,  3.32s/it] 40%|███▉      | 266/670 [13:33<22:39,  3.37s/it] 40%|███▉      | 267/670 [13:36<22:27,  3.34s/it] 40%|████      | 268/670 [13:39<22:19,  3.33s/it] 40%|████      | 269/670 [13:43<22:14,  3.33s/it] 40%|████      | 270/670 [13:46<22:09,  3.32s/it]                                                 {'loss': 3.7068, 'learning_rate': 3.2502115875008524e-05, 'epoch': 0.4}
 40%|████      | 270/670 [13:46<22:09,  3.32s/it]                                                 {'router_ce_loss': 1.0845648050308228, 'old_lang_expert0_score': '0.18 0.04 0.06 0.08 0.8 0.89 0.19 0.9 0.31 0.92 0.08 0.94 0.07 0.94 0.97 0.94 0.96 0.97 0.97 0.97 0.97 0.96 0.04 0.98', 'epoch': 0.4}
 40%|████      | 270/670 [13:46<22:09,  3.32s/it] 40%|████      | 271/670 [13:49<22:03,  3.32s/it] 41%|████      | 272/670 [13:52<21:58,  3.31s/it] 41%|████      | 273/670 [13:56<21:51,  3.30s/it] 41%|████      | 274/670 [13:59<21:49,  3.31s/it] 41%|████      | 275/670 [14:02<21:42,  3.30s/it] 41%|████      | 276/670 [14:06<21:39,  3.30s/it] 41%|████▏     | 277/670 [14:09<21:38,  3.30s/it] 41%|████▏     | 278/670 [14:12<21:35,  3.30s/it] 42%|████▏     | 279/670 [14:16<21:35,  3.31s/it] 42%|████▏     | 280/670 [14:19<21:32,  3.31s/it]                                                 {'loss': 3.701, 'learning_rate': 3.1376069214041913e-05, 'epoch': 0.42}
 42%|████▏     | 280/670 [14:19<21:32,  3.31s/it]                                                 {'router_ce_loss': 1.089493751525879, 'old_lang_expert0_score': '0.17 0.05 0.07 0.08 0.78 0.86 0.21 0.87 0.33 0.91 0.1 0.93 0.07 0.92 0.96 0.93 0.95 0.96 0.96 0.96 0.96 0.94 0.04 0.97', 'epoch': 0.42}
 42%|████▏     | 280/670 [14:19<21:32,  3.31s/it] 42%|████▏     | 281/670 [14:22<21:27,  3.31s/it] 42%|████▏     | 282/670 [14:26<21:25,  3.31s/it] 42%|████▏     | 283/670 [14:29<21:20,  3.31s/it] 42%|████▏     | 284/670 [14:32<21:39,  3.37s/it] 43%|████▎     | 285/670 [14:36<21:30,  3.35s/it] 43%|████▎     | 286/670 [14:39<21:25,  3.35s/it] 43%|████▎     | 287/670 [14:42<21:17,  3.33s/it] 43%|████▎     | 288/670 [14:46<21:11,  3.33s/it] 43%|████▎     | 289/670 [14:49<21:02,  3.31s/it] 43%|████▎     | 290/670 [14:52<20:58,  3.31s/it]                                                 {'loss': 3.6936, 'learning_rate': 3.0236006569153617e-05, 'epoch': 0.43}
 43%|████▎     | 290/670 [14:52<20:58,  3.31s/it]                                                 {'router_ce_loss': 1.0822116136550903, 'old_lang_expert0_score': '0.17 0.04 0.05 0.07 0.81 0.89 0.21 0.91 0.39 0.93 0.09 0.94 0.08 0.94 0.97 0.93 0.96 0.97 0.97 0.96 0.97 0.94 0.03 0.98', 'epoch': 0.43}
 43%|████▎     | 290/670 [14:52<20:58,  3.31s/it] 43%|████▎     | 291/670 [14:56<20:55,  3.31s/it] 44%|████▎     | 292/670 [14:59<20:52,  3.31s/it] 44%|████▎     | 293/670 [15:02<20:49,  3.31s/it] 44%|████▍     | 294/670 [15:05<20:47,  3.32s/it] 44%|████▍     | 295/670 [15:09<20:46,  3.32s/it] 44%|████▍     | 296/670 [15:12<20:40,  3.32s/it] 44%|████▍     | 297/670 [15:15<20:37,  3.32s/it] 44%|████▍     | 298/670 [15:19<20:35,  3.32s/it] 45%|████▍     | 299/670 [15:22<20:30,  3.32s/it] 45%|████▍     | 300/670 [15:25<20:27,  3.32s/it]                                                 {'loss': 3.6797, 'learning_rate': 2.9084434045463255e-05, 'epoch': 0.45}
 45%|████▍     | 300/670 [15:25<20:27,  3.32s/it]                                                 {'router_ce_loss': 1.0937119722366333, 'old_lang_expert0_score': '0.17 0.04 0.06 0.08 0.79 0.87 0.19 0.87 0.32 0.9 0.09 0.93 0.06 0.92 0.94 0.9 0.93 0.96 0.97 0.96 0.96 0.95 0.04 0.97', 'epoch': 0.45}
 45%|████▍     | 300/670 [15:26<20:27,  3.32s/it] 45%|████▍     | 301/670 [15:29<20:44,  3.37s/it] 45%|████▌     | 302/670 [15:32<20:30,  3.34s/it] 45%|████▌     | 303/670 [15:35<20:24,  3.34s/it] 45%|████▌     | 304/670 [15:39<20:17,  3.33s/it] 46%|████▌     | 305/670 [15:42<20:09,  3.31s/it] 46%|████▌     | 306/670 [15:45<20:05,  3.31s/it] 46%|████▌     | 307/670 [15:49<20:00,  3.31s/it] 46%|████▌     | 308/670 [15:52<19:56,  3.31s/it] 46%|████▌     | 309/670 [15:55<19:57,  3.32s/it] 46%|████▋     | 310/670 [15:59<19:53,  3.32s/it]                                                 {'loss': 3.7061, 'learning_rate': 2.792388304930207e-05, 'epoch': 0.46}
 46%|████▋     | 310/670 [15:59<19:53,  3.32s/it]                                                 {'router_ce_loss': 1.0835943222045898, 'old_lang_expert0_score': '0.17 0.03 0.05 0.07 0.8 0.87 0.21 0.9 0.37 0.93 0.09 0.95 0.06 0.94 0.98 0.94 0.96 0.97 0.97 0.97 0.97 0.96 0.03 0.98', 'epoch': 0.46}
 46%|████▋     | 310/670 [15:59<19:53,  3.32s/it] 46%|████▋     | 311/670 [16:02<19:46,  3.31s/it] 47%|████▋     | 312/670 [16:05<19:40,  3.30s/it] 47%|████▋     | 313/670 [16:09<19:39,  3.30s/it] 47%|████▋     | 314/670 [16:12<19:35,  3.30s/it] 47%|████▋     | 315/670 [16:15<19:32,  3.30s/it] 47%|████▋     | 316/670 [16:18<19:30,  3.31s/it] 47%|████▋     | 317/670 [16:22<19:28,  3.31s/it] 47%|████▋     | 318/670 [16:25<19:32,  3.33s/it] 48%|████▊     | 319/670 [16:29<19:48,  3.39s/it] 48%|████▊     | 320/670 [16:32<19:32,  3.35s/it]                                                 {'loss': 3.6976, 'learning_rate': 2.6756904723632324e-05, 'epoch': 0.48}
 48%|████▊     | 320/670 [16:32<19:32,  3.35s/it]                                                 {'router_ce_loss': 1.0823572874069214, 'old_lang_expert0_score': '0.16 0.04 0.06 0.06 0.77 0.88 0.2 0.91 0.37 0.93 0.1 0.95 0.06 0.96 0.98 0.95 0.97 0.98 0.98 0.98 0.98 0.96 0.03 0.98', 'epoch': 0.48}
 48%|████▊     | 320/670 [16:32<19:32,  3.35s/it] 48%|████▊     | 321/670 [16:35<19:25,  3.34s/it] 48%|████▊     | 322/670 [16:39<19:20,  3.33s/it] 48%|████▊     | 323/670 [16:42<19:13,  3.32s/it] 48%|████▊     | 324/670 [16:45<19:05,  3.31s/it] 49%|████▊     | 325/670 [16:48<19:01,  3.31s/it] 49%|████▊     | 326/670 [16:52<18:56,  3.30s/it] 49%|████▉     | 327/670 [16:55<18:50,  3.30s/it] 49%|████▉     | 328/670 [16:58<18:45,  3.29s/it] 49%|████▉     | 329/670 [17:02<18:45,  3.30s/it] 49%|████▉     | 330/670 [17:05<18:40,  3.29s/it]                                                 {'loss': 3.6339, 'learning_rate': 2.5586064340081516e-05, 'epoch': 0.49}
 49%|████▉     | 330/670 [17:05<18:40,  3.29s/it]                                                 {'router_ce_loss': 1.0901180505752563, 'old_lang_expert0_score': '0.17 0.04 0.05 0.07 0.79 0.87 0.18 0.88 0.35 0.91 0.08 0.93 0.06 0.93 0.96 0.93 0.96 0.98 0.97 0.97 0.97 0.95 0.03 0.97', 'epoch': 0.49}
 49%|████▉     | 330/670 [17:05<18:40,  3.29s/it] 49%|████▉     | 331/670 [17:08<18:37,  3.30s/it] 50%|████▉     | 332/670 [17:12<18:36,  3.30s/it] 50%|████▉     | 333/670 [17:15<18:29,  3.29s/it] 50%|████▉     | 334/670 [17:18<18:32,  3.31s/it] 50%|█████     | 335/670 [17:21<18:32,  3.32s/it] 50%|█████     | 336/670 [17:25<18:29,  3.32s/it] 50%|█████     | 337/670 [17:28<18:22,  3.31s/it] 50%|█████     | 338/670 [17:32<18:37,  3.37s/it] 51%|█████     | 339/670 [17:35<18:25,  3.34s/it] 51%|█████     | 340/670 [17:38<18:16,  3.32s/it]                                                 {'loss': 3.6894, 'learning_rate': 2.441393565991849e-05, 'epoch': 0.51}
 51%|█████     | 340/670 [17:38<18:16,  3.32s/it]                                                 {'router_ce_loss': 1.0737074613571167, 'old_lang_expert0_score': '0.18 0.04 0.07 0.08 0.81 0.89 0.23 0.92 0.44 0.94 0.08 0.95 0.06 0.96 0.98 0.95 0.97 0.98 0.98 0.97 0.98 0.96 0.03 0.98', 'epoch': 0.51}
 51%|█████     | 340/670 [17:38<18:16,  3.32s/it] 51%|█████     | 341/670 [17:41<18:06,  3.30s/it] 51%|█████     | 342/670 [17:45<18:02,  3.30s/it] 51%|█████     | 343/670 [17:48<17:59,  3.30s/it] 51%|█████▏    | 344/670 [17:51<17:55,  3.30s/it] 51%|█████▏    | 345/670 [17:55<17:52,  3.30s/it] 52%|█████▏    | 346/670 [17:58<17:47,  3.29s/it] 52%|█████▏    | 347/670 [18:01<17:48,  3.31s/it] 52%|█████▏    | 348/670 [18:04<17:40,  3.29s/it] 52%|█████▏    | 349/670 [18:08<17:37,  3.29s/it] 52%|█████▏    | 350/670 [18:11<17:40,  3.31s/it]                                                 {'loss': 3.6429, 'learning_rate': 2.3243095276367685e-05, 'epoch': 0.52}
 52%|█████▏    | 350/670 [18:11<17:40,  3.31s/it]                                                 {'router_ce_loss': 1.0718821287155151, 'old_lang_expert0_score': '0.18 0.04 0.05 0.09 0.81 0.9 0.26 0.92 0.45 0.94 0.1 0.95 0.07 0.96 0.97 0.96 0.97 0.98 0.98 0.97 0.98 0.96 0.04 0.98', 'epoch': 0.52}
 52%|█████▏    | 350/670 [18:11<17:40,  3.31s/it] 52%|█████▏    | 351/670 [18:14<17:34,  3.31s/it] 53%|█████▎    | 352/670 [18:18<17:30,  3.30s/it] 53%|█████▎    | 353/670 [18:21<17:32,  3.32s/it] 53%|█████▎    | 354/670 [18:24<17:27,  3.31s/it] 53%|█████▎    | 355/670 [18:28<17:41,  3.37s/it] 53%|█████▎    | 356/670 [18:31<17:31,  3.35s/it] 53%|█████▎    | 357/670 [18:34<17:21,  3.33s/it] 53%|█████▎    | 358/670 [18:38<17:13,  3.31s/it] 54%|█████▎    | 359/670 [18:41<17:10,  3.32s/it] 54%|█████▎    | 360/670 [18:44<17:01,  3.30s/it]                                                 {'loss': 3.6338, 'learning_rate': 2.207611695069794e-05, 'epoch': 0.54}
 54%|█████▎    | 360/670 [18:44<17:01,  3.30s/it]                                                 {'router_ce_loss': 1.0933631658554077, 'old_lang_expert0_score': '0.17 0.04 0.07 0.08 0.78 0.86 0.22 0.86 0.36 0.88 0.09 0.91 0.07 0.91 0.95 0.9 0.94 0.95 0.97 0.95 0.96 0.94 0.03 0.96', 'epoch': 0.54}
 54%|█████▎    | 360/670 [18:45<17:01,  3.30s/it] 54%|█████▍    | 361/670 [18:48<16:57,  3.29s/it] 54%|█████▍    | 362/670 [18:51<16:54,  3.29s/it] 54%|█████▍    | 363/670 [18:54<16:52,  3.30s/it] 54%|█████▍    | 364/670 [18:57<16:48,  3.30s/it] 54%|█████▍    | 365/670 [19:01<16:45,  3.30s/it] 55%|█████▍    | 366/670 [19:04<16:45,  3.31s/it] 55%|█████▍    | 367/670 [19:07<16:38,  3.30s/it] 55%|█████▍    | 368/670 [19:11<16:35,  3.30s/it] 55%|█████▌    | 369/670 [19:14<16:30,  3.29s/it] 55%|█████▌    | 370/670 [19:17<16:28,  3.29s/it]                                                 {'loss': 3.6459, 'learning_rate': 2.0915565954536744e-05, 'epoch': 0.55}
 55%|█████▌    | 370/670 [19:17<16:28,  3.29s/it]                                                 {'router_ce_loss': 1.088411808013916, 'old_lang_expert0_score': '0.17 0.05 0.05 0.06 0.78 0.87 0.18 0.87 0.41 0.9 0.08 0.94 0.09 0.93 0.96 0.91 0.95 0.96 0.97 0.97 0.97 0.95 0.03 0.97', 'epoch': 0.55}
 55%|█████▌    | 370/670 [19:18<16:28,  3.29s/it] 55%|█████▌    | 371/670 [19:21<16:25,  3.29s/it] 56%|█████▌    | 372/670 [19:24<16:21,  3.30s/it] 56%|█████▌    | 373/670 [19:27<16:36,  3.36s/it] 56%|█████▌    | 374/670 [19:31<16:26,  3.33s/it] 56%|█████▌    | 375/670 [19:34<16:18,  3.32s/it] 56%|█████▌    | 376/670 [19:37<16:11,  3.31s/it] 56%|█████▋    | 377/670 [19:40<16:04,  3.29s/it] 56%|█████▋    | 378/670 [19:44<16:00,  3.29s/it] 57%|█████▋    | 379/670 [19:47<15:57,  3.29s/it] 57%|█████▋    | 380/670 [19:50<15:52,  3.28s/it]                                                 {'loss': 3.6578, 'learning_rate': 1.9763993430846395e-05, 'epoch': 0.57}
 57%|█████▋    | 380/670 [19:50<15:52,  3.28s/it]                                                 {'router_ce_loss': 1.0742112398147583, 'old_lang_expert0_score': '0.16 0.03 0.05 0.07 0.82 0.89 0.23 0.93 0.45 0.94 0.08 0.95 0.07 0.95 0.98 0.95 0.98 0.98 0.98 0.98 0.98 0.97 0.02 0.98', 'epoch': 0.57}
 57%|█████▋    | 380/670 [19:51<15:52,  3.28s/it] 57%|█████▋    | 381/670 [19:54<15:50,  3.29s/it] 57%|█████▋    | 382/670 [19:57<15:47,  3.29s/it] 57%|█████▋    | 383/670 [20:00<15:41,  3.28s/it] 57%|█████▋    | 384/670 [20:03<15:40,  3.29s/it] 57%|█████▋    | 385/670 [20:07<15:39,  3.30s/it] 58%|█████▊    | 386/670 [20:10<15:34,  3.29s/it] 58%|█████▊    | 387/670 [20:13<15:30,  3.29s/it] 58%|█████▊    | 388/670 [20:17<15:26,  3.28s/it] 58%|█████▊    | 389/670 [20:20<15:24,  3.29s/it] 58%|█████▊    | 390/670 [20:23<15:21,  3.29s/it]                                                 {'loss': 3.6391, 'learning_rate': 1.8623930785958092e-05, 'epoch': 0.58}
 58%|█████▊    | 390/670 [20:23<15:21,  3.29s/it]                                                 {'router_ce_loss': 1.0720252990722656, 'old_lang_expert0_score': '0.17 0.03 0.04 0.07 0.81 0.9 0.25 0.93 0.49 0.94 0.12 0.95 0.08 0.95 0.98 0.94 0.96 0.97 0.98 0.97 0.97 0.96 0.03 0.97', 'epoch': 0.58}
 58%|█████▊    | 390/670 [20:23<15:21,  3.29s/it] 58%|█████▊    | 391/670 [20:27<15:30,  3.34s/it] 59%|█████▊    | 392/670 [20:30<15:22,  3.32s/it] 59%|█████▊    | 393/670 [20:33<15:17,  3.31s/it] 59%|█████▉    | 394/670 [20:37<15:14,  3.31s/it] 59%|█████▉    | 395/670 [20:40<15:07,  3.30s/it] 59%|█████▉    | 396/670 [20:43<15:03,  3.30s/it] 59%|█████▉    | 397/670 [20:46<15:00,  3.30s/it] 59%|█████▉    | 398/670 [20:50<14:55,  3.29s/it] 60%|█████▉    | 399/670 [20:53<14:49,  3.28s/it] 60%|█████▉    | 400/670 [20:56<14:47,  3.29s/it]                                                 {'loss': 3.6641, 'learning_rate': 1.749788412499149e-05, 'epoch': 0.6}
 60%|█████▉    | 400/670 [20:56<14:47,  3.29s/it]                                                 {'router_ce_loss': 1.0806220769882202, 'old_lang_expert0_score': '0.17 0.04 0.05 0.06 0.78 0.87 0.21 0.91 0.44 0.93 0.09 0.95 0.08 0.94 0.97 0.94 0.97 0.97 0.98 0.97 0.97 0.96 0.04 0.98', 'epoch': 0.6}
 60%|█████▉    | 400/670 [20:56<14:47,  3.29s/it] 60%|█████▉    | 401/670 [21:00<14:45,  3.29s/it] 60%|██████    | 402/670 [21:03<14:42,  3.29s/it] 60%|██████    | 403/670 [21:06<14:38,  3.29s/it] 60%|██████    | 404/670 [21:09<14:34,  3.29s/it] 60%|██████    | 405/670 [21:13<14:30,  3.28s/it] 61%|██████    | 406/670 [21:16<14:28,  3.29s/it] 61%|██████    | 407/670 [21:19<14:22,  3.28s/it] 61%|██████    | 408/670 [21:23<14:38,  3.35s/it] 61%|██████    | 409/670 [21:26<14:30,  3.33s/it] 61%|██████    | 410/670 [21:29<14:19,  3.31s/it]                                                 {'loss': 3.6473, 'learning_rate': 1.638832874289168e-05, 'epoch': 0.61}
 61%|██████    | 410/670 [21:29<14:19,  3.31s/it]                                                 {'router_ce_loss': 1.0790457725524902, 'old_lang_expert0_score': '0.17 0.04 0.05 0.06 0.81 0.88 0.23 0.9 0.46 0.93 0.08 0.95 0.06 0.94 0.97 0.94 0.96 0.97 0.98 0.97 0.97 0.95 0.03 0.97', 'epoch': 0.61}
 61%|██████    | 410/670 [21:30<14:19,  3.31s/it] 61%|██████▏   | 411/670 [21:33<14:16,  3.31s/it] 61%|██████▏   | 412/670 [21:36<14:10,  3.30s/it] 62%|██████▏   | 413/670 [21:39<14:07,  3.30s/it] 62%|██████▏   | 414/670 [21:42<14:04,  3.30s/it] 62%|██████▏   | 415/670 [21:46<13:59,  3.29s/it] 62%|██████▏   | 416/670 [21:49<13:57,  3.30s/it] 62%|██████▏   | 417/670 [21:52<13:52,  3.29s/it] 62%|██████▏   | 418/670 [21:56<13:47,  3.28s/it] 63%|██████▎   | 419/670 [21:59<13:45,  3.29s/it] 63%|██████▎   | 420/670 [22:02<13:41,  3.29s/it]                                                 {'loss': 3.6712, 'learning_rate': 1.5297703683193752e-05, 'epoch': 0.63}
 63%|██████▎   | 420/670 [22:02<13:41,  3.29s/it]                                                 {'router_ce_loss': 1.068228006362915, 'old_lang_expert0_score': '0.17 0.04 0.06 0.08 0.82 0.9 0.26 0.93 0.49 0.94 0.11 0.95 0.11 0.95 0.98 0.95 0.97 0.97 0.98 0.98 0.98 0.96 0.05 0.97', 'epoch': 0.63}
 63%|██████▎   | 420/670 [22:02<13:41,  3.29s/it] 63%|██████▎   | 421/670 [22:05<13:37,  3.28s/it] 63%|██████▎   | 422/670 [22:09<13:34,  3.28s/it] 63%|██████▎   | 423/670 [22:12<13:31,  3.29s/it] 63%|██████▎   | 424/670 [22:15<13:26,  3.28s/it] 63%|██████▎   | 425/670 [22:19<13:23,  3.28s/it] 64%|██████▎   | 426/670 [22:22<13:44,  3.38s/it] 64%|██████▎   | 427/670 [22:25<13:33,  3.35s/it] 64%|██████▍   | 428/670 [22:29<13:24,  3.32s/it] 64%|██████▍   | 429/670 [22:32<13:16,  3.30s/it] 64%|██████▍   | 430/670 [22:35<13:11,  3.30s/it]                                                 {'loss': 3.6239, 'learning_rate': 1.4228406376475742e-05, 'epoch': 0.64}
 64%|██████▍   | 430/670 [22:35<13:11,  3.30s/it]                                                 {'router_ce_loss': 1.0773829221725464, 'old_lang_expert0_score': '0.17 0.03 0.05 0.07 0.8 0.87 0.22 0.91 0.45 0.93 0.12 0.95 0.11 0.95 0.98 0.93 0.96 0.97 0.97 0.97 0.98 0.97 0.03 0.98', 'epoch': 0.64}
 64%|██████▍   | 430/670 [22:36<13:11,  3.30s/it] 64%|██████▍   | 431/670 [22:39<13:06,  3.29s/it] 64%|██████▍   | 432/670 [22:42<12:59,  3.27s/it] 65%|██████▍   | 433/670 [22:45<12:57,  3.28s/it] 65%|██████▍   | 434/670 [22:48<12:53,  3.28s/it] 65%|██████▍   | 435/670 [22:52<12:51,  3.28s/it] 65%|██████▌   | 436/670 [22:55<12:47,  3.28s/it] 65%|██████▌   | 437/670 [22:58<12:42,  3.27s/it] 65%|██████▌   | 438/670 [23:01<12:41,  3.28s/it] 66%|██████▌   | 439/670 [23:05<12:38,  3.28s/it] 66%|██████▌   | 440/670 [23:08<12:36,  3.29s/it]                                                 {'loss': 3.6307, 'learning_rate': 1.3182787370285865e-05, 'epoch': 0.66}
 66%|██████▌   | 440/670 [23:08<12:36,  3.29s/it]                                                 {'router_ce_loss': 1.0786617994308472, 'old_lang_expert0_score': '0.17 0.04 0.06 0.07 0.79 0.88 0.24 0.9 0.46 0.92 0.07 0.95 0.06 0.94 0.97 0.94 0.96 0.97 0.97 0.98 0.98 0.96 0.03 0.97', 'epoch': 0.66}
 66%|██████▌   | 440/670 [23:08<12:36,  3.29s/it] 66%|██████▌   | 441/670 [23:11<12:33,  3.29s/it] 66%|██████▌   | 442/670 [23:15<12:32,  3.30s/it] 66%|██████▌   | 443/670 [23:18<12:28,  3.30s/it] 66%|██████▋   | 444/670 [23:21<12:37,  3.35s/it] 66%|██████▋   | 445/670 [23:25<12:28,  3.33s/it] 67%|██████▋   | 446/670 [23:28<12:22,  3.32s/it] 67%|██████▋   | 447/670 [23:31<12:16,  3.30s/it] 67%|██████▋   | 448/670 [23:35<12:11,  3.29s/it] 67%|██████▋   | 449/670 [23:38<12:07,  3.29s/it] 67%|██████▋   | 450/670 [23:41<12:02,  3.29s/it]                                                 {'loss': 3.6447, 'learning_rate': 1.2163145162128947e-05, 'epoch': 0.67}
 67%|██████▋   | 450/670 [23:41<12:02,  3.29s/it]                                                 {'router_ce_loss': 1.0835341215133667, 'old_lang_expert0_score': '0.17 0.04 0.06 0.07 0.79 0.88 0.25 0.9 0.44 0.91 0.1 0.93 0.1 0.92 0.95 0.91 0.96 0.96 0.95 0.96 0.96 0.94 0.04 0.97', 'epoch': 0.67}
 67%|██████▋   | 450/670 [23:41<12:02,  3.29s/it] 67%|██████▋   | 451/670 [23:44<11:59,  3.28s/it] 67%|██████▋   | 452/670 [23:48<11:55,  3.28s/it] 68%|██████▊   | 453/670 [23:51<11:52,  3.28s/it] 68%|██████▊   | 454/670 [23:54<11:48,  3.28s/it] 68%|██████▊   | 455/670 [23:57<11:45,  3.28s/it] 68%|██████▊   | 456/670 [24:01<11:41,  3.28s/it] 68%|██████▊   | 457/670 [24:04<11:36,  3.27s/it] 68%|██████▊   | 458/670 [24:07<11:36,  3.28s/it] 69%|██████▊   | 459/670 [24:11<11:31,  3.28s/it] 69%|██████▊   | 460/670 [24:14<11:27,  3.28s/it]                                                 {'loss': 3.6268, 'learning_rate': 1.1171721146870015e-05, 'epoch': 0.69}
 69%|██████▊   | 460/670 [24:14<11:27,  3.28s/it]                                                 {'router_ce_loss': 1.074761152267456, 'old_lang_expert0_score': '0.17 0.03 0.05 0.07 0.81 0.89 0.22 0.92 0.5 0.93 0.09 0.95 0.08 0.95 0.97 0.94 0.97 0.98 0.97 0.97 0.98 0.97 0.03 0.98', 'epoch': 0.69}
 69%|██████▊   | 460/670 [24:14<11:27,  3.28s/it] 69%|██████▉   | 461/670 [24:17<11:36,  3.33s/it] 69%|██████▉   | 462/670 [24:21<11:30,  3.32s/it] 69%|██████▉   | 463/670 [24:24<11:22,  3.30s/it] 69%|██████▉   | 464/670 [24:27<11:18,  3.29s/it] 69%|██████▉   | 465/670 [24:30<11:15,  3.29s/it] 70%|██████▉   | 466/670 [24:34<11:10,  3.28s/it] 70%|██████▉   | 467/670 [24:37<11:06,  3.28s/it] 70%|██████▉   | 468/670 [24:40<11:02,  3.28s/it] 70%|███████   | 469/670 [24:44<10:59,  3.28s/it] 70%|███████   | 470/670 [24:47<10:54,  3.27s/it]                                                 {'loss': 3.6295, 'learning_rate': 1.021069468966194e-05, 'epoch': 0.7}
 70%|███████   | 470/670 [24:47<10:54,  3.27s/it]                                                 {'router_ce_loss': 1.071980357170105, 'old_lang_expert0_score': '0.18 0.04 0.07 0.08 0.81 0.88 0.25 0.9 0.51 0.94 0.11 0.95 0.12 0.95 0.98 0.94 0.96 0.97 0.97 0.97 0.96 0.93 0.04 0.97', 'epoch': 0.7}
 70%|███████   | 470/670 [24:47<10:54,  3.27s/it] 70%|███████   | 471/670 [24:50<10:51,  3.27s/it] 70%|███████   | 472/670 [24:53<10:48,  3.28s/it] 71%|███████   | 473/670 [24:57<10:43,  3.26s/it] 71%|███████   | 474/670 [25:00<10:40,  3.27s/it] 71%|███████   | 475/670 [25:03<10:37,  3.27s/it] 71%|███████   | 476/670 [25:06<10:35,  3.27s/it] 71%|███████   | 477/670 [25:10<10:30,  3.27s/it] 71%|███████▏  | 478/670 [25:13<10:27,  3.27s/it] 71%|███████▏  | 479/670 [25:16<10:38,  3.34s/it] 72%|███████▏  | 480/670 [25:20<10:32,  3.33s/it]                                                 {'loss': 3.6259, 'learning_rate': 9.282178335227884e-06, 'epoch': 0.72}
 72%|███████▏  | 480/670 [25:20<10:32,  3.33s/it]                                                 {'router_ce_loss': 1.0726581811904907, 'old_lang_expert0_score': '0.17 0.05 0.06 0.07 0.83 0.9 0.3 0.91 0.5 0.9 0.08 0.93 0.07 0.93 0.96 0.93 0.97 0.97 0.97 0.97 0.97 0.96 0.04 0.97', 'epoch': 0.72}
 72%|███████▏  | 480/670 [25:20<10:32,  3.33s/it] 72%|███████▏  | 481/670 [25:23<10:25,  3.31s/it] 72%|███████▏  | 482/670 [25:26<10:19,  3.29s/it] 72%|███████▏  | 483/670 [25:30<10:14,  3.28s/it] 72%|███████▏  | 484/670 [25:33<10:10,  3.28s/it] 72%|███████▏  | 485/670 [25:36<10:06,  3.28s/it] 73%|███████▎  | 486/670 [25:39<10:03,  3.28s/it] 73%|███████▎  | 487/670 [25:43<10:00,  3.28s/it] 73%|███████▎  | 488/670 [25:46<09:57,  3.28s/it] 73%|███████▎  | 489/670 [25:49<09:53,  3.28s/it] 73%|███████▎  | 490/670 [25:52<09:49,  3.28s/it]                                                 {'loss': 3.6304, 'learning_rate': 8.38821316402946e-06, 'epoch': 0.73}
 73%|███████▎  | 490/670 [25:52<09:49,  3.28s/it]                                                 {'router_ce_loss': 1.0754749774932861, 'old_lang_expert0_score': '0.16 0.04 0.06 0.07 0.8 0.9 0.26 0.91 0.47 0.94 0.1 0.95 0.06 0.95 0.97 0.94 0.96 0.98 0.98 0.98 0.97 0.96 0.02 0.98', 'epoch': 0.73}
 73%|███████▎  | 490/670 [25:53<09:49,  3.28s/it] 73%|███████▎  | 491/670 [25:56<09:45,  3.27s/it] 73%|███████▎  | 492/670 [25:59<09:42,  3.27s/it] 74%|███████▎  | 493/670 [26:02<09:38,  3.27s/it] 74%|███████▎  | 494/670 [26:06<09:34,  3.26s/it] 74%|███████▍  | 495/670 [26:09<09:32,  3.27s/it] 74%|███████▍  | 496/670 [26:12<09:29,  3.28s/it] 74%|███████▍  | 497/670 [26:16<09:37,  3.34s/it] 74%|███████▍  | 498/670 [26:19<09:30,  3.32s/it] 74%|███████▍  | 499/670 [26:22<09:24,  3.30s/it] 75%|███████▍  | 500/670 [26:25<09:25,  3.32s/it]                                                 {'loss': 3.6462, 'learning_rate': 7.530764305528959e-06, 'epoch': 0.75}
 75%|███████▍  | 500/670 [26:25<09:25,  3.32s/it]                                                 {'router_ce_loss': 1.0683096647262573, 'old_lang_expert0_score': '0.16 0.04 0.07 0.07 0.82 0.89 0.27 0.93 0.5 0.94 0.1 0.95 0.07 0.96 0.98 0.95 0.98 0.98 0.98 0.98 0.98 0.97 0.03 0.98', 'epoch': 0.75}
 75%|███████▍  | 500/670 [26:26<09:25,  3.32s/it] 75%|███████▍  | 501/670 [26:29<09:18,  3.30s/it] 75%|███████▍  | 502/670 [26:32<09:13,  3.30s/it] 75%|███████▌  | 503/670 [26:35<09:07,  3.28s/it] 75%|███████▌  | 504/670 [26:39<09:03,  3.28s/it] 75%|███████▌  | 505/670 [26:42<09:00,  3.28s/it] 76%|███████▌  | 506/670 [26:45<08:56,  3.27s/it] 76%|███████▌  | 507/670 [26:48<08:52,  3.27s/it] 76%|███████▌  | 508/670 [26:52<08:51,  3.28s/it] 76%|███████▌  | 509/670 [26:55<08:48,  3.28s/it] 76%|███████▌  | 510/670 [26:58<08:44,  3.28s/it]                                                 {'loss': 3.6469, 'learning_rate': 6.711716618408281e-06, 'epoch': 0.76}
 76%|███████▌  | 510/670 [26:58<08:44,  3.28s/it]                                                 {'router_ce_loss': 1.077040195465088, 'old_lang_expert0_score': '0.17 0.04 0.05 0.06 0.81 0.89 0.28 0.91 0.48 0.92 0.08 0.94 0.06 0.93 0.97 0.93 0.97 0.97 0.98 0.97 0.97 0.95 0.03 0.98', 'epoch': 0.76}
 76%|███████▌  | 510/670 [26:58<08:44,  3.28s/it] 76%|███████▋  | 511/670 [27:01<08:40,  3.27s/it] 76%|███████▋  | 512/670 [27:05<08:36,  3.27s/it] 77%|███████▋  | 513/670 [27:08<08:33,  3.27s/it] 77%|███████▋  | 514/670 [27:11<08:39,  3.33s/it] 77%|███████▋  | 515/670 [27:15<08:33,  3.31s/it] 77%|███████▋  | 516/670 [27:18<08:28,  3.30s/it] 77%|███████▋  | 517/670 [27:21<08:23,  3.29s/it] 77%|███████▋  | 518/670 [27:25<08:19,  3.29s/it] 77%|███████▋  | 519/670 [27:28<08:13,  3.27s/it] 78%|███████▊  | 520/670 [27:31<08:11,  3.28s/it]                                                 {'loss': 3.5726, 'learning_rate': 5.932870547240454e-06, 'epoch': 0.78}
 78%|███████▊  | 520/670 [27:31<08:11,  3.28s/it]                                                 {'router_ce_loss': 1.0827863216400146, 'old_lang_expert0_score': '0.17 0.04 0.06 0.07 0.8 0.87 0.22 0.9 0.47 0.91 0.09 0.94 0.08 0.93 0.96 0.93 0.95 0.96 0.96 0.97 0.97 0.94 0.03 0.98', 'epoch': 0.78}
 78%|███████▊  | 520/670 [27:31<08:11,  3.28s/it] 78%|███████▊  | 521/670 [27:34<08:06,  3.26s/it] 78%|███████▊  | 522/670 [27:38<08:00,  3.25s/it] 78%|███████▊  | 523/670 [27:41<07:58,  3.25s/it] 78%|███████▊  | 524/670 [27:44<07:57,  3.27s/it] 78%|███████▊  | 525/670 [27:47<07:54,  3.28s/it] 79%|███████▊  | 526/670 [27:51<07:51,  3.27s/it] 79%|███████▊  | 527/670 [27:54<07:48,  3.27s/it] 79%|███████▉  | 528/670 [27:57<07:44,  3.27s/it] 79%|███████▉  | 529/670 [28:00<07:40,  3.27s/it] 79%|███████▉  | 530/670 [28:04<07:38,  3.28s/it]                                                 {'loss': 3.6438, 'learning_rate': 5.1959381647217666e-06, 'epoch': 0.79}
 79%|███████▉  | 530/670 [28:04<07:38,  3.28s/it]                                                 {'router_ce_loss': 1.0719075202941895, 'old_lang_expert0_score': '0.16 0.04 0.06 0.08 0.8 0.9 0.26 0.92 0.49 0.94 0.1 0.95 0.06 0.95 0.97 0.94 0.97 0.97 0.98 0.97 0.98 0.97 0.03 0.97', 'epoch': 0.79}
 79%|███████▉  | 530/670 [28:04<07:38,  3.28s/it] 79%|███████▉  | 531/670 [28:07<07:35,  3.28s/it] 79%|███████▉  | 532/670 [28:10<07:40,  3.34s/it] 80%|███████▉  | 533/670 [28:14<07:33,  3.31s/it] 80%|███████▉  | 534/670 [28:17<07:27,  3.29s/it] 80%|███████▉  | 535/670 [28:20<07:23,  3.29s/it] 80%|████████  | 536/670 [28:24<07:19,  3.28s/it] 80%|████████  | 537/670 [28:27<07:15,  3.27s/it] 80%|████████  | 538/670 [28:30<07:12,  3.27s/it] 80%|████████  | 539/670 [28:33<07:09,  3.28s/it] 81%|████████  | 540/670 [28:37<07:10,  3.31s/it]                                                 {'loss': 3.6448, 'learning_rate': 4.502539408164386e-06, 'epoch': 0.81}
 81%|████████  | 540/670 [28:37<07:10,  3.31s/it]                                                 {'router_ce_loss': 1.080195426940918, 'old_lang_expert0_score': '0.17 0.05 0.08 0.08 0.78 0.87 0.26 0.89 0.47 0.9 0.08 0.92 0.07 0.92 0.95 0.92 0.96 0.97 0.97 0.97 0.97 0.96 0.04 0.97', 'epoch': 0.81}
 81%|████████  | 540/670 [28:37<07:10,  3.31s/it] 81%|████████  | 541/670 [28:40<07:04,  3.29s/it] 81%|████████  | 542/670 [28:43<07:00,  3.29s/it] 81%|████████  | 543/670 [28:46<06:56,  3.28s/it] 81%|████████  | 544/670 [28:50<06:53,  3.28s/it] 81%|████████▏ | 545/670 [28:53<06:48,  3.27s/it] 81%|████████▏ | 546/670 [28:56<06:44,  3.26s/it] 82%|████████▏ | 547/670 [28:59<06:39,  3.25s/it] 82%|████████▏ | 548/670 [29:03<06:37,  3.26s/it] 82%|████████▏ | 549/670 [29:06<06:34,  3.26s/it] 82%|████████▏ | 550/670 [29:09<06:31,  3.27s/it]                                                 {'loss': 3.6164, 'learning_rate': 3.8541985185225645e-06, 'epoch': 0.82}
 82%|████████▏ | 550/670 [29:09<06:31,  3.27s/it]                                                 {'router_ce_loss': 1.0737669467926025, 'old_lang_expert0_score': '0.17 0.04 0.06 0.07 0.79 0.89 0.27 0.91 0.5 0.92 0.09 0.94 0.07 0.95 0.97 0.94 0.97 0.97 0.98 0.97 0.98 0.96 0.03 0.97', 'epoch': 0.82}
 82%|████████▏ | 550/670 [29:10<06:31,  3.27s/it] 82%|████████▏ | 551/670 [29:13<06:36,  3.34s/it] 82%|████████▏ | 552/670 [29:16<06:30,  3.31s/it] 83%|████████▎ | 553/670 [29:19<06:25,  3.30s/it] 83%|████████▎ | 554/670 [29:23<06:21,  3.29s/it] 83%|████████▎ | 555/670 [29:26<06:16,  3.28s/it] 83%|████████▎ | 556/670 [29:29<06:12,  3.27s/it] 83%|████████▎ | 557/670 [29:32<06:08,  3.26s/it] 83%|████████▎ | 558/670 [29:36<06:09,  3.30s/it] 83%|████████▎ | 559/670 [29:39<06:04,  3.28s/it] 84%|████████▎ | 560/670 [29:42<06:00,  3.28s/it]                                                 {'loss': 3.6266, 'learning_rate': 3.252340689780245e-06, 'epoch': 0.83}
 84%|████████▎ | 560/670 [29:42<06:00,  3.28s/it]                                                 {'router_ce_loss': 1.0829838514328003, 'old_lang_expert0_score': '0.17 0.04 0.06 0.07 0.78 0.87 0.24 0.88 0.42 0.91 0.09 0.94 0.06 0.94 0.95 0.93 0.95 0.97 0.98 0.97 0.98 0.95 0.03 0.97', 'epoch': 0.83}
 84%|████████▎ | 560/670 [29:43<06:00,  3.28s/it] 84%|████████▎ | 561/670 [29:46<06:04,  3.34s/it] 84%|████████▍ | 562/670 [29:49<05:59,  3.33s/it] 84%|████████▍ | 563/670 [29:52<05:54,  3.32s/it] 84%|████████▍ | 564/670 [29:56<05:50,  3.31s/it] 84%|████████▍ | 565/670 [29:59<05:46,  3.30s/it] 84%|████████▍ | 566/670 [30:02<05:42,  3.29s/it] 85%|████████▍ | 567/670 [30:05<05:37,  3.28s/it] 85%|████████▍ | 568/670 [30:09<05:39,  3.33s/it] 85%|████████▍ | 569/670 [30:12<05:33,  3.30s/it] 85%|████████▌ | 570/670 [30:15<05:28,  3.29s/it]                                                 {'loss': 3.6014, 'learning_rate': 2.6982889360653377e-06, 'epoch': 0.85}
 85%|████████▌ | 570/670 [30:15<05:28,  3.29s/it]                                                 {'router_ce_loss': 1.0743619203567505, 'old_lang_expert0_score': '0.16 0.05 0.06 0.07 0.79 0.88 0.25 0.91 0.5 0.93 0.09 0.94 0.07 0.95 0.97 0.95 0.97 0.98 0.98 0.98 0.98 0.96 0.02 0.98', 'epoch': 0.85}
 85%|████████▌ | 570/670 [30:16<05:28,  3.29s/it] 85%|████████▌ | 571/670 [30:19<05:28,  3.32s/it] 85%|████████▌ | 572/670 [30:22<05:22,  3.29s/it] 86%|████████▌ | 573/670 [30:25<05:18,  3.29s/it] 86%|████████▌ | 574/670 [30:29<05:15,  3.29s/it] 86%|████████▌ | 575/670 [30:32<05:10,  3.26s/it] 86%|████████▌ | 576/670 [30:35<05:06,  3.26s/it] 86%|████████▌ | 577/670 [30:38<05:03,  3.27s/it] 86%|████████▋ | 578/670 [30:42<05:00,  3.26s/it] 86%|████████▋ | 579/670 [30:45<04:56,  3.26s/it] 87%|████████▋ | 580/670 [30:48<04:54,  3.28s/it]                                                 {'loss': 3.6059, 'learning_rate': 2.1932611833775846e-06, 'epoch': 0.86}
 87%|████████▋ | 580/670 [30:48<04:54,  3.28s/it]                                                 {'router_ce_loss': 1.0719243288040161, 'old_lang_expert0_score': '0.16 0.04 0.05 0.07 0.82 0.9 0.29 0.92 0.51 0.92 0.08 0.94 0.06 0.95 0.97 0.94 0.97 0.98 0.98 0.98 0.98 0.97 0.03 0.98', 'epoch': 0.86}
 87%|████████▋ | 580/670 [30:48<04:54,  3.28s/it] 87%|████████▋ | 581/670 [30:51<04:51,  3.27s/it] 87%|████████▋ | 582/670 [30:55<04:47,  3.27s/it] 87%|████████▋ | 583/670 [30:58<04:43,  3.26s/it] 87%|████████▋ | 584/670 [31:01<04:40,  3.26s/it] 87%|████████▋ | 585/670 [31:04<04:37,  3.27s/it] 87%|████████▋ | 586/670 [31:08<04:38,  3.32s/it] 88%|████████▊ | 587/670 [31:11<04:34,  3.31s/it] 88%|████████▊ | 588/670 [31:14<04:29,  3.29s/it] 88%|████████▊ | 589/670 [31:18<04:25,  3.28s/it] 88%|████████▊ | 590/670 [31:21<04:21,  3.27s/it]                                                 {'loss': 3.6376, 'learning_rate': 1.738367592322837e-06, 'epoch': 0.88}
 88%|████████▊ | 590/670 [31:21<04:21,  3.27s/it]                                                 {'router_ce_loss': 1.066789984703064, 'old_lang_expert0_score': '0.16 0.03 0.06 0.07 0.84 0.9 0.3 0.93 0.53 0.94 0.09 0.95 0.08 0.96 0.98 0.95 0.97 0.98 0.98 0.98 0.98 0.97 0.03 0.98', 'epoch': 0.88}
 88%|████████▊ | 590/670 [31:21<04:21,  3.27s/it] 88%|████████▊ | 591/670 [31:24<04:18,  3.27s/it] 88%|████████▊ | 592/670 [31:27<04:14,  3.27s/it] 89%|████████▊ | 593/670 [31:31<04:11,  3.26s/it] 89%|████████▊ | 594/670 [31:34<04:08,  3.27s/it] 89%|████████▉ | 595/670 [31:37<04:04,  3.26s/it] 89%|████████▉ | 596/670 [31:40<04:01,  3.26s/it] 89%|████████▉ | 597/670 [31:44<03:58,  3.26s/it] 89%|████████▉ | 598/670 [31:47<03:54,  3.26s/it] 89%|████████▉ | 599/670 [31:50<03:51,  3.26s/it] 90%|████████▉ | 600/670 [31:53<03:48,  3.27s/it]                                                 {'loss': 3.6588, 'learning_rate': 1.3346081177391472e-06, 'epoch': 0.89}
 90%|████████▉ | 600/670 [31:54<03:48,  3.27s/it]                                                 {'router_ce_loss': 1.0662130117416382, 'old_lang_expert0_score': '0.17 0.04 0.07 0.07 0.82 0.89 0.28 0.94 0.54 0.94 0.1 0.96 0.08 0.95 0.98 0.95 0.97 0.97 0.98 0.97 0.98 0.96 0.03 0.97', 'epoch': 0.89}
 90%|████████▉ | 600/670 [31:54<03:48,  3.27s/it] 90%|████████▉ | 601/670 [31:57<03:45,  3.26s/it] 90%|████████▉ | 602/670 [32:00<03:41,  3.26s/it] 90%|█████████ | 603/670 [32:03<03:38,  3.26s/it] 90%|█████████ | 604/670 [32:07<03:39,  3.33s/it] 90%|█████████ | 605/670 [32:10<03:34,  3.31s/it] 90%|█████████ | 606/670 [32:13<03:30,  3.29s/it] 91%|█████████ | 607/670 [32:16<03:26,  3.27s/it] 91%|█████████ | 608/670 [32:20<03:22,  3.26s/it] 91%|█████████ | 609/670 [32:23<03:18,  3.26s/it] 91%|█████████ | 610/670 [32:26<03:15,  3.25s/it]                                                 {'loss': 3.6337, 'learning_rate': 9.828703105789983e-07, 'epoch': 0.91}
 91%|█████████ | 610/670 [32:26<03:15,  3.25s/it]                                                 {'router_ce_loss': 1.0711978673934937, 'old_lang_expert0_score': '0.17 0.04 0.06 0.08 0.8 0.89 0.31 0.91 0.51 0.93 0.11 0.94 0.07 0.95 0.97 0.94 0.96 0.97 0.98 0.97 0.97 0.94 0.03 0.97', 'epoch': 0.91}
 91%|█████████ | 610/670 [32:26<03:15,  3.25s/it] 91%|█████████ | 611/670 [32:29<03:11,  3.25s/it] 91%|█████████▏| 612/670 [32:33<03:08,  3.25s/it] 91%|█████████▏| 613/670 [32:36<03:05,  3.25s/it] 92%|█████████▏| 614/670 [32:39<03:02,  3.26s/it] 92%|█████████▏| 615/670 [32:43<02:59,  3.26s/it] 92%|█████████▏| 616/670 [32:46<02:55,  3.25s/it] 92%|█████████▏| 617/670 [32:49<02:52,  3.25s/it] 92%|█████████▏| 618/670 [32:52<02:49,  3.25s/it] 92%|█████████▏| 619/670 [32:56<02:47,  3.27s/it] 93%|█████████▎| 620/670 [32:59<02:43,  3.27s/it]                                                 {'loss': 3.6095, 'learning_rate': 6.839273668796747e-07, 'epoch': 0.92}
 93%|█████████▎| 620/670 [32:59<02:43,  3.27s/it]                                                 {'router_ce_loss': 1.0743725299835205, 'old_lang_expert0_score': '0.17 0.03 0.05 0.06 0.8 0.88 0.24 0.92 0.5 0.94 0.1 0.95 0.07 0.95 0.98 0.94 0.97 0.98 0.98 0.98 0.98 0.97 0.03 0.98', 'epoch': 0.92}
 93%|█████████▎| 620/670 [32:59<02:43,  3.27s/it] 93%|█████████▎| 621/670 [33:02<02:42,  3.32s/it] 93%|█████████▎| 622/670 [33:05<02:38,  3.29s/it] 93%|█████████▎| 623/670 [33:09<02:33,  3.28s/it] 93%|█████████▎| 624/670 [33:12<02:30,  3.28s/it] 93%|█████████▎| 625/670 [33:15<02:27,  3.28s/it] 93%|█████████▎| 626/670 [33:19<02:23,  3.27s/it] 94%|█████████▎| 627/670 [33:22<02:20,  3.26s/it] 94%|█████████▎| 628/670 [33:25<02:16,  3.25s/it] 94%|█████████▍| 629/670 [33:28<02:13,  3.26s/it] 94%|█████████▍| 630/670 [33:32<02:10,  3.26s/it]                                                 {'loss': 3.5949, 'learning_rate': 4.3843642811059737e-07, 'epoch': 0.94}
 94%|█████████▍| 630/670 [33:32<02:10,  3.26s/it]                                                 {'router_ce_loss': 1.0735934972763062, 'old_lang_expert0_score': '0.17 0.03 0.05 0.08 0.78 0.87 0.29 0.9 0.49 0.93 0.12 0.95 0.11 0.95 0.97 0.93 0.96 0.96 0.97 0.97 0.97 0.94 0.04 0.97', 'epoch': 0.94}
 94%|█████████▍| 630/670 [33:32<02:10,  3.26s/it] 94%|█████████▍| 631/670 [33:35<02:07,  3.26s/it] 94%|█████████▍| 632/670 [33:38<02:03,  3.26s/it] 94%|█████████▍| 633/670 [33:41<02:00,  3.26s/it] 95%|█████████▍| 634/670 [33:45<01:57,  3.27s/it] 95%|█████████▍| 635/670 [33:48<01:53,  3.26s/it] 95%|█████████▍| 636/670 [33:51<01:50,  3.26s/it] 95%|█████████▌| 637/670 [33:54<01:47,  3.26s/it] 95%|█████████▌| 638/670 [33:58<01:44,  3.25s/it] 95%|█████████▌| 639/670 [34:01<01:43,  3.33s/it] 96%|█████████▌| 640/670 [34:04<01:39,  3.31s/it]                                                 {'loss': 3.6235, 'learning_rate': 2.4693713663372644e-07, 'epoch': 0.95}
 96%|█████████▌| 640/670 [34:04<01:39,  3.31s/it]                                                 {'router_ce_loss': 1.071304202079773, 'old_lang_expert0_score': '0.16 0.03 0.05 0.06 0.81 0.89 0.3 0.92 0.53 0.93 0.11 0.94 0.1 0.93 0.97 0.92 0.96 0.97 0.98 0.98 0.98 0.97 0.03 0.97', 'epoch': 0.95}
 96%|█████████▌| 640/670 [34:05<01:39,  3.31s/it] 96%|█████████▌| 641/670 [34:08<01:35,  3.29s/it] 96%|█████████▌| 642/670 [34:11<01:31,  3.28s/it] 96%|█████████▌| 643/670 [34:14<01:29,  3.30s/it] 96%|█████████▌| 644/670 [34:17<01:25,  3.29s/it] 96%|█████████▋| 645/670 [34:21<01:22,  3.28s/it] 96%|█████████▋| 646/670 [34:24<01:18,  3.27s/it] 97%|█████████▋| 647/670 [34:27<01:15,  3.27s/it] 97%|█████████▋| 648/670 [34:31<01:11,  3.27s/it] 97%|█████████▋| 649/670 [34:34<01:08,  3.27s/it] 97%|█████████▋| 650/670 [34:37<01:05,  3.26s/it]                                                 {'loss': 3.6249, 'learning_rate': 1.0985044945254764e-07, 'epoch': 0.97}
 97%|█████████▋| 650/670 [34:37<01:05,  3.26s/it]                                                 {'router_ce_loss': 1.066562533378601, 'old_lang_expert0_score': '0.17 0.04 0.05 0.07 0.81 0.89 0.27 0.94 0.59 0.95 0.1 0.96 0.08 0.97 0.98 0.96 0.97 0.98 0.98 0.97 0.98 0.95 0.02 0.98', 'epoch': 0.97}
 97%|█████████▋| 650/670 [34:37<01:05,  3.26s/it] 97%|█████████▋| 651/670 [34:40<01:01,  3.26s/it] 97%|█████████▋| 652/670 [34:44<00:58,  3.26s/it] 97%|█████████▋| 653/670 [34:47<00:55,  3.26s/it] 98%|█████████▊| 654/670 [34:50<00:52,  3.25s/it] 98%|█████████▊| 655/670 [34:53<00:48,  3.25s/it] 98%|█████████▊| 656/670 [34:57<00:45,  3.27s/it] 98%|█████████▊| 657/670 [35:00<00:43,  3.33s/it] 98%|█████████▊| 658/670 [35:03<00:39,  3.31s/it] 98%|█████████▊| 659/670 [35:07<00:36,  3.30s/it] 99%|█████████▊| 660/670 [35:10<00:32,  3.29s/it]                                                 {'loss': 3.6267, 'learning_rate': 2.7477712857215677e-08, 'epoch': 0.98}
 99%|█████████▊| 660/670 [35:10<00:32,  3.29s/it]                                                 {'router_ce_loss': 1.0690538883209229, 'old_lang_expert0_score': '0.16 0.04 0.05 0.08 0.83 0.91 0.29 0.93 0.53 0.93 0.09 0.95 0.05 0.95 0.98 0.95 0.97 0.98 0.98 0.97 0.98 0.96 0.03 0.98', 'epoch': 0.98}
 99%|█████████▊| 660/670 [35:10<00:32,  3.29s/it] 99%|█████████▊| 661/670 [35:13<00:29,  3.28s/it] 99%|█████████▉| 662/670 [35:16<00:26,  3.30s/it] 99%|█████████▉| 663/670 [35:20<00:23,  3.29s/it] 99%|█████████▉| 664/670 [35:23<00:19,  3.29s/it] 99%|█████████▉| 665/670 [35:26<00:16,  3.28s/it] 99%|█████████▉| 666/670 [35:30<00:13,  3.27s/it]100%|█████████▉| 667/670 [35:33<00:09,  3.26s/it]100%|█████████▉| 668/670 [35:36<00:06,  3.26s/it]100%|█████████▉| 669/670 [35:39<00:03,  3.26s/it]100%|██████████| 670/670 [35:43<00:00,  3.26s/it]                                                 {'loss': 3.6335, 'learning_rate': 0.0, 'epoch': 1.0}
100%|██████████| 670/670 [35:43<00:00,  3.26s/it][INFO|trainer.py:1989] 2024-07-09 12:07:08,746 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 2143.0901, 'train_samples_per_second': 30.043, 'train_steps_per_second': 0.313, 'train_loss': 3.8381999741739303, 'epoch': 1.0}
100%|██████████| 670/670 [35:43<00:00,  3.26s/it]100%|██████████| 670/670 [35:43<00:00,  3.20s/it]
[INFO|trainer.py:2981] 2024-07-09 12:07:14,786 >> Saving model checkpoint to /home/nfs04/wangzj/checkpoints/moe/test
/home/nfs02/wangzj/aliyun/temp_data/peft/src/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B - will assume that the vocabulary was not modified.
  warnings.warn(
[2024-07-09 12:07:17,683] [INFO] [launch.py:347:main] Process 2340126 exits successfully.
[2024-07-09 12:07:17,684] [INFO] [launch.py:347:main] Process 2340127 exits successfully.
[INFO|tokenization_utils_base.py:2435] 2024-07-09 12:08:03,212 >> tokenizer config file saved in /home/nfs04/wangzj/checkpoints/moe/test/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-07-09 12:08:03,213 >> Special tokens file saved in /home/nfs04/wangzj/checkpoints/moe/test/special_tokens_map.json
[INFO|tokenization_utils_base.py:2495] 2024-07-09 12:08:03,214 >> added tokens file saved in /home/nfs04/wangzj/checkpoints/moe/test/added_tokens.json
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     3.8382
  train_runtime            = 0:35:43.09
  train_samples_per_second =     30.043
  train_steps_per_second   =      0.313
Figure saved: /home/nfs04/wangzj/checkpoints/moe/test/training_loss.png
07/09/2024 12:08:04 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-07-09 12:08:04,635 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-07-09 12:08:06,738] [INFO] [launch.py:347:main] Process 2340125 exits successfully.
