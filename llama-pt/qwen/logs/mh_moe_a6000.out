[2024-05-20 10:57:04,862] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-20 10:57:07,614] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-05-20 10:57:07,660] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/wzjsz/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed ./llama-pt/config/ds_config_cpu.json --stage pt --model_name_or_path /home/nfs04/wangzj/models/Qwen1.5-1.8B --do_train --flash_attn --dataset ar_2b,de_2b,is_2b,hi_2b --mix_strategy concat --preprocessing_num_workers 32 --cache_path /home/nfs03/wangzj/dataset/pretrain/arderu6b --cutoff_len 1024 --finetuning_type moe --moe_every_k_layers 1 --moe_router_type mh_moe --moe_num_experts 4 --moe_num_heads 4 --topk 2 --moe_with_aux --output_dir /home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe --overwrite_output_dir --per_device_train_batch_size 16 --gradient_accumulation_steps 16 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 1 --save_only_model --save_steps 10000 --learning_rate 1e-4 --num_train_epochs 1.0 --plot_loss --bf16
[2024-05-20 10:57:09,186] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-20 10:57:11,382] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-20 10:57:11,382] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-20 10:57:11,382] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-20 10:57:11,382] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-20 10:57:11,382] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-20 10:57:16,945] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-20 10:57:16,945] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-20 10:57:16,947] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-20 10:57:16,947] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-20 10:57:24,536] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-20 10:57:24,537] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-20 10:57:24,537] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-20 10:57:24,568] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-20 10:57:24,568] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
05/20/2024 10:57:25 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/20/2024 10:57:25 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/20/2024 10:57:25 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/20/2024 10:57:25 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe/runs/May20_10-57-24_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/20/2024 10:57:25 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe/runs/May20_10-57-24_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/20/2024 10:57:25 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe/runs/May20_10-57-24_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
05/20/2024 10:57:25 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/20/2024 10:57:25 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe/runs/May20_10-57-24_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-05-20 10:57:25,586 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2027] 2024-05-20 10:57:25,586 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2027] 2024-05-20 10:57:25,586 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-05-20 10:57:25,586 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-05-20 10:57:25,586 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-05-20 10:57:25,586 >> loading file tokenizer.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-05-20 10:57:25,925 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
05/20/2024 10:57:25 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[INFO|configuration_utils.py:727] 2024-05-20 10:57:25,943 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/config.json
[INFO|configuration_utils.py:792] 2024-05-20 10:57:25,945 >> Model config Qwen2Config {
  "_name_or_path": "/home/nfs04/wangzj/models/Qwen1.5-1.8B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

05/20/2024 10:57:25 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
05/20/2024 10:57:25 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
[WARNING|modeling_utils.py:2946] 2024-05-20 10:57:25,948 >> The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
05/20/2024 10:57:25 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[INFO|modeling_utils.py:3334] 2024-05-20 10:57:25,989 >> loading weights file /home/nfs04/wangzj/models/Qwen1.5-1.8B/model.safetensors
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO|modeling_utils.py:1459] 2024-05-20 10:57:26,013 >> Instantiating Qwen2ForCausalMoeLM model under default dtype torch.bfloat16.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[WARNING|logging.py:329] 2024-05-20 10:57:26,014 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[WARNING|logging.py:329] 2024-05-20 10:57:26,017 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:827] 2024-05-20 10:57:26,019 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}

05/20/2024 10:57:29 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/20/2024 10:57:29 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
[INFO|modeling_utils.py:4070] 2024-05-20 10:57:30,132 >> All model checkpoint weights were used when initializing Qwen2ForCausalMoeLM.

[INFO|modeling_utils.py:4078] 2024-05-20 10:57:30,133 >> All the weights of Qwen2ForCausalMoeLM were initialized from the model checkpoint at /home/nfs04/wangzj/models/Qwen1.5-1.8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalMoeLM for predictions without further training.
05/20/2024 10:57:30 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/20/2024 10:57:30 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/20/2024 10:57:30 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/20/2024 10:57:30 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
[INFO|configuration_utils.py:780] 2024-05-20 10:57:30,146 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/generation_config.json
[INFO|configuration_utils.py:827] 2024-05-20 10:57:30,147 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "max_new_tokens": 2048
}

05/20/2024 10:57:30 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/20/2024 10:57:30 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/20/2024 10:57:40 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/20/2024 10:57:40 - WARNING - llmtuner.data.loader - Loading dataset from disk will ignore other data arguments.
05/20/2024 10:57:40 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/20/2024 10:57:40 - WARNING - llmtuner.data.loader - Loading dataset from disk will ignore other data arguments.
05/20/2024 10:57:40 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/20/2024 10:57:40 - WARNING - llmtuner.data.loader - Loading dataset from disk will ignore other data arguments.
Loading cached shuffled indices for dataset at /home/nfs03/wangzj/dataset/pretrain/arderu6b/cache-32ee8c5574b2e83c.arrow
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 5872167
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 5872167
})
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): Qwen2ForCausalMoeLM(
      (model): Qwen2Model(
        (embed_tokens): Embedding(151936, 2048)
        (layers): ModuleList(
          (0): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (1): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (2): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (3): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (4): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (5): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (6): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (7): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (8): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (9): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (10): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (11): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (12): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (13): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (14): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (15): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (16): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (17): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (18): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (19): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (20): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (21): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (22): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (23): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
        )
        (norm): Qwen2RMSNorm()
      )
      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
    )
  )
)
[INFO|trainer.py:586] 2024-05-20 10:57:40,321 >> Using auto half precision backend
05/20/2024 10:57:40 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/20/2024 10:57:40 - WARNING - llmtuner.data.loader - Loading dataset from disk will ignore other data arguments.
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 5872167
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 5872167
})
[INFO|deepspeed.py:325] 2024-05-20 10:57:40,707 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/wangzj/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.004044771194458 seconds
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.0448689460754395 seconds
Time to load cpu_adam op: 3.0446436405181885 seconds
Time to load cpu_adam op: 3.045625686645508 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-05-20 10:57:49,049] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
[2024-05-20 10:57:52,114] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-20 10:57:52,117] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-05-20 10:57:52,117] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-20 10:57:52,159] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-20 10:57:52,159] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-20 10:57:52,159] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-05-20 10:57:52,160] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-05-20 10:57:52,160] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-05-20 10:57:52,160] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: True
[2024-05-20 10:57:52,160] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-05-20 10:58:01,545] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-05-20 10:58:01,546] [INFO] [utils.py:803:see_memory_usage] MA 5.73 GB         Max_MA 5.73 GB         CA 6.02 GB         Max_CA 6 GB 
[2024-05-20 10:58:01,546] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 73.35 GB, percent = 19.5%
[2024-05-20 10:58:03,109] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-05-20 10:58:03,110] [INFO] [utils.py:803:see_memory_usage] MA 5.73 GB         Max_MA 5.73 GB         CA 6.02 GB         Max_CA 6 GB 
[2024-05-20 10:58:03,110] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 80.46 GB, percent = 21.4%
[2024-05-20 10:58:03,110] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-05-20 10:58:03,327] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-05-20 10:58:03,327] [INFO] [utils.py:803:see_memory_usage] MA 5.73 GB         Max_MA 5.73 GB         CA 6.02 GB         Max_CA 6 GB 
[2024-05-20 10:58:03,328] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 80.74 GB, percent = 21.4%
[2024-05-20 10:58:03,332] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-05-20 10:58:03,332] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-05-20 10:58:03,333] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-05-20 10:58:03,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999)]
[2024-05-20 10:58:03,335] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-05-20 10:58:03,335] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-05-20 10:58:03,335] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-05-20 10:58:03,335] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-05-20 10:58:03,335] [INFO] [config.py:978:print]   amp_params ................... False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   bfloat16_enabled ............. True
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f065c2d06a0>
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   dump_state ................... False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... None
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-05-20 10:58:03,336] [INFO] [config.py:978:print]   fp16_auto_cast ............... None
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   fp16_enabled ................. False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 16
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 1
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   loss_scale ................... 1.0
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   pld_params ................... False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   train_batch_size ............. 1024
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  16
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   world_size ................... 4
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-05-20 10:58:03,337] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-05-20 10:58:03,338] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-05-20 10:58:03,338] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-05-20 10:58:03,338] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 1.024000e+03, 
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 16, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1747] 2024-05-20 10:58:03,338 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-05-20 10:58:03,338 >>   Num examples = 5,872,167
[INFO|trainer.py:1749] 2024-05-20 10:58:03,338 >>   Num Epochs = 1
[INFO|trainer.py:1750] 2024-05-20 10:58:03,338 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1753] 2024-05-20 10:58:03,338 >>   Total train batch size (w. parallel, distributed & accumulation) = 1,024
[INFO|trainer.py:1754] 2024-05-20 10:58:03,338 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1755] 2024-05-20 10:58:03,338 >>   Total optimization steps = 5,734
[INFO|trainer.py:1756] 2024-05-20 10:58:03,341 >>   Number of trainable parameters = 1,013,071,872
  0%|          | 0/5734 [00:00<?, ?it/s]  0%|          | 1/5734 [01:31<145:33:10, 91.40s/it]  0%|          | 2/5734 [03:01<144:42:47, 90.89s/it]  0%|          | 3/5734 [04:33<145:05:42, 91.14s/it]  0%|          | 4/5734 [06:07<147:06:34, 92.42s/it]  0%|          | 5/5734 [07:50<153:07:29, 96.22s/it]  0%|          | 6/5734 [09:33<156:22:58, 98.29s/it]  0%|          | 7/5734 [11:15<158:40:55, 99.75s/it]  0%|          | 8/5734 [13:00<160:57:33, 101.20s/it]  0%|          | 9/5734 [14:43<161:51:03, 101.78s/it]  0%|          | 10/5734 [16:25<162:20:12, 102.10s/it]                                                      {'loss': 10.1228, 'learning_rate': 9.999924954735877e-05, 'epoch': 0.0}
  0%|          | 10/5734 [16:26<162:20:12, 102.10s/it]  0%|          | 11/5734 [18:09<162:52:26, 102.45s/it]  0%|          | 12/5734 [19:49<161:49:31, 101.81s/it]  0%|          | 13/5734 [21:29<160:41:23, 101.12s/it]  0%|          | 14/5734 [23:08<159:40:49, 100.50s/it]  0%|          | 15/5734 [24:48<159:26:59, 100.37s/it]  0%|          | 16/5734 [26:27<159:06:19, 100.17s/it]  0%|          | 17/5734 [28:06<158:30:08, 99.81s/it]   0%|          | 18/5734 [29:46<158:21:40, 99.74s/it]  0%|          | 19/5734 [31:28<159:35:43, 100.53s/it]  0%|          | 20/5734 [33:07<158:44:32, 100.01s/it]                                                      {'loss': 7.0543, 'learning_rate': 9.999699821196226e-05, 'epoch': 0.0}
  0%|          | 20/5734 [33:07<158:44:32, 100.01s/it]  0%|          | 21/5734 [34:45<157:49:10, 99.45s/it]   0%|          | 22/5734 [36:30<160:14:43, 100.99s/it]  0%|          | 23/5734 [38:15<162:22:28, 102.35s/it]  0%|          | 24/5734 [40:24<174:43:41, 110.16s/it]  0%|          | 25/5734 [42:34<184:03:39, 116.07s/it]  0%|          | 26/5734 [44:19<178:55:51, 112.85s/it]  0%|          | 27/5734 [45:57<172:00:33, 108.50s/it]  0%|          | 28/5734 [47:36<167:05:43, 105.42s/it]  1%|          | 29/5734 [49:14<163:35:14, 103.23s/it]  1%|          | 30/5734 [50:51<160:58:36, 101.60s/it]                                                      {'loss': 6.3405, 'learning_rate': 9.99932460613913e-05, 'epoch': 0.01}
  1%|          | 30/5734 [50:51<160:58:36, 101.60s/it]  1%|          | 31/5734 [52:29<159:04:53, 100.42s/it]  1%|          | 32/5734 [54:07<157:49:50, 99.65s/it]   1%|          | 33/5734 [55:45<157:02:31, 99.17s/it]  1%|          | 34/5734 [57:22<155:53:16, 98.46s/it]  1%|          | 35/5734 [59:00<155:40:42, 98.34s/it]  1%|          | 36/5734 [1:00:39<155:53:11, 98.49s/it]  1%|          | 37/5734 [1:02:15<154:51:23, 97.86s/it]  1%|          | 38/5734 [1:03:57<156:39:16, 99.01s/it]  1%|          | 39/5734 [1:05:36<156:31:31, 98.95s/it]  1%|          | 40/5734 [1:07:13<155:45:29, 98.48s/it]                                                       {'loss': 5.7679, 'learning_rate': 9.998799320827835e-05, 'epoch': 0.01}
  1%|          | 40/5734 [1:07:13<155:45:29, 98.48s/it]  1%|          | 41/5734 [1:08:51<155:25:07, 98.28s/it]  1%|          | 42/5734 [1:10:28<154:52:56, 97.96s/it]  1%|          | 43/5734 [1:12:05<154:12:22, 97.55s/it]  1%|          | 44/5734 [1:13:42<154:07:46, 97.52s/it]  1%|          | 45/5734 [1:15:19<153:50:24, 97.35s/it]  1%|          | 46/5734 [1:16:56<153:31:29, 97.17s/it]  1%|          | 47/5734 [1:18:33<153:21:41, 97.08s/it]  1%|          | 48/5734 [1:20:09<153:00:10, 96.87s/it]  1%|          | 49/5734 [1:21:46<152:57:24, 96.86s/it]  1%|          | 50/5734 [1:23:22<152:45:33, 96.75s/it]                                                       {'loss': 5.2477, 'learning_rate': 9.998123981030406e-05, 'epoch': 0.01}
  1%|          | 50/5734 [1:23:22<152:45:33, 96.75s/it]  1%|          | 51/5734 [1:24:59<152:39:44, 96.71s/it]  1%|          | 52/5734 [1:26:35<152:13:09, 96.44s/it]  1%|          | 53/5734 [1:28:10<151:40:35, 96.12s/it]  1%|          | 54/5734 [1:29:45<151:09:56, 95.81s/it]  1%|          | 55/5734 [1:31:22<151:27:14, 96.01s/it]  1%|          | 56/5734 [1:32:57<151:05:00, 95.79s/it]  1%|          | 57/5734 [1:34:32<150:43:23, 95.58s/it]  1%|          | 58/5734 [1:36:07<150:25:05, 95.40s/it]  1%|          | 59/5734 [1:37:42<150:18:29, 95.35s/it]  1%|          | 60/5734 [1:39:17<150:12:19, 95.30s/it]                                                       {'loss': 4.8523, 'learning_rate': 9.997298607019269e-05, 'epoch': 0.01}
  1%|          | 60/5734 [1:39:17<150:12:19, 95.30s/it]  1%|          | 61/5734 [1:40:53<150:11:37, 95.31s/it]  1%|          | 62/5734 [1:42:28<150:00:35, 95.21s/it]  1%|          | 63/5734 [1:44:03<149:49:50, 95.11s/it]  1%|          | 64/5734 [1:45:38<149:52:09, 95.16s/it]  1%|          | 65/5734 [1:47:13<149:46:56, 95.12s/it]  1%|          | 66/5734 [1:48:49<150:01:42, 95.29s/it]  1%|          | 67/5734 [1:50:23<149:47:41, 95.16s/it]  1%|          | 68/5734 [1:51:59<149:42:16, 95.12s/it]  1%|          | 69/5734 [1:53:33<149:32:32, 95.03s/it]  1%|          | 70/5734 [1:55:08<149:30:02, 95.02s/it]                                                       {'loss': 4.6125, 'learning_rate': 9.996323223570588e-05, 'epoch': 0.01}
  1%|          | 70/5734 [1:55:08<149:30:02, 95.02s/it]  1%|          | 71/5734 [1:56:43<149:23:42, 94.97s/it]  1%|▏         | 72/5734 [1:58:18<149:11:07, 94.85s/it]  1%|▏         | 73/5734 [1:59:53<149:07:07, 94.83s/it]  1%|▏         | 74/5734 [2:01:27<149:02:04, 94.79s/it]  1%|▏         | 75/5734 [2:03:02<149:02:05, 94.81s/it]  1%|▏         | 76/5734 [2:04:37<148:58:01, 94.78s/it]  1%|▏         | 77/5734 [2:06:11<148:52:29, 94.74s/it]  1%|▏         | 78/5734 [2:07:46<148:48:03, 94.71s/it]  1%|▏         | 79/5734 [2:09:20<148:27:43, 94.51s/it]  1%|▏         | 80/5734 [2:10:55<148:31:14, 94.57s/it]                                                       {'loss': 4.4338, 'learning_rate': 9.995197859963524e-05, 'epoch': 0.01}
  1%|▏         | 80/5734 [2:10:55<148:31:14, 94.57s/it]  1%|▏         | 81/5734 [2:12:29<148:28:03, 94.55s/it]  1%|▏         | 82/5734 [2:14:04<148:20:15, 94.48s/it]  1%|▏         | 83/5734 [2:15:38<148:09:50, 94.39s/it]  1%|▏         | 84/5734 [2:17:12<148:02:55, 94.33s/it]  1%|▏         | 85/5734 [2:18:47<148:14:38, 94.47s/it]  1%|▏         | 86/5734 [2:20:21<148:08:48, 94.43s/it]  2%|▏         | 87/5734 [2:21:56<148:06:09, 94.42s/it]  2%|▏         | 88/5734 [2:23:30<148:05:37, 94.43s/it]  2%|▏         | 89/5734 [2:25:04<147:46:39, 94.24s/it]  2%|▏         | 90/5734 [2:26:38<147:40:40, 94.20s/it]                                                       {'loss': 4.2944, 'learning_rate': 9.993922549979362e-05, 'epoch': 0.02}
  2%|▏         | 90/5734 [2:26:38<147:40:40, 94.20s/it]  2%|▏         | 91/5734 [2:28:13<147:59:22, 94.41s/it]  2%|▏         | 92/5734 [2:29:47<147:50:15, 94.33s/it]  2%|▏         | 93/5734 [2:31:21<147:42:33, 94.27s/it]  2%|▏         | 94/5734 [2:32:55<147:36:07, 94.21s/it]  2%|▏         | 95/5734 [2:34:29<147:31:47, 94.18s/it]  2%|▏         | 96/5734 [2:36:03<147:19:12, 94.07s/it]  2%|▏         | 97/5734 [2:37:37<147:15:47, 94.05s/it]  2%|▏         | 98/5734 [2:39:11<147:21:29, 94.13s/it]  2%|▏         | 99/5734 [2:40:45<147:14:39, 94.07s/it]  2%|▏         | 100/5734 [2:42:19<147:12:13, 94.06s/it]                                                        {'loss': 4.2026, 'learning_rate': 9.992497331900492e-05, 'epoch': 0.02}
  2%|▏         | 100/5734 [2:42:19<147:12:13, 94.06s/it]  2%|▏         | 101/5734 [2:43:53<147:10:47, 94.06s/it]  2%|▏         | 102/5734 [2:45:27<147:03:14, 94.00s/it]  2%|▏         | 103/5734 [2:47:01<147:05:19, 94.04s/it]  2%|▏         | 104/5734 [2:48:35<147:02:36, 94.02s/it]  2%|▏         | 105/5734 [2:50:09<146:46:49, 93.87s/it]  2%|▏         | 106/5734 [2:51:43<146:42:39, 93.85s/it]  2%|▏         | 107/5734 [2:53:17<146:47:21, 93.91s/it]  2%|▏         | 108/5734 [2:54:50<146:31:33, 93.76s/it]  2%|▏         | 109/5734 [2:56:24<146:28:06, 93.74s/it]  2%|▏         | 110/5734 [2:57:59<147:02:41, 94.13s/it]                                                        {'loss': 4.1111, 'learning_rate': 9.99092224850926e-05, 'epoch': 0.02}
  2%|▏         | 110/5734 [2:57:59<147:02:41, 94.13s/it]  2%|▏         | 111/5734 [2:59:33<146:56:21, 94.07s/it]  2%|▏         | 112/5734 [3:01:06<146:37:22, 93.89s/it]  2%|▏         | 113/5734 [3:02:40<146:29:14, 93.82s/it]  2%|▏         | 114/5734 [3:04:13<146:15:05, 93.68s/it]  2%|▏         | 115/5734 [3:05:47<146:15:57, 93.71s/it]  2%|▏         | 116/5734 [3:07:21<146:23:44, 93.81s/it]  2%|▏         | 117/5734 [3:08:55<146:10:25, 93.68s/it]  2%|▏         | 118/5734 [3:10:28<146:01:10, 93.60s/it]  2%|▏         | 119/5734 [3:12:02<146:12:27, 93.74s/it]  2%|▏         | 120/5734 [3:13:36<146:05:37, 93.68s/it]                                                        {'loss': 4.0442, 'learning_rate': 9.989197347086689e-05, 'epoch': 0.02}
  2%|▏         | 120/5734 [3:13:36<146:05:37, 93.68s/it]  2%|▏         | 121/5734 [3:15:09<146:07:33, 93.72s/it]  2%|▏         | 122/5734 [3:16:42<145:48:55, 93.54s/it]  2%|▏         | 123/5734 [3:18:16<145:44:28, 93.51s/it]  2%|▏         | 124/5734 [3:19:49<145:35:15, 93.43s/it]  2%|▏         | 125/5734 [3:21:24<146:11:32, 93.83s/it]  2%|▏         | 126/5734 [3:22:59<146:43:35, 94.19s/it]  2%|▏         | 127/5734 [3:24:34<147:06:16, 94.45s/it]  2%|▏         | 128/5734 [3:26:09<147:30:43, 94.73s/it]  2%|▏         | 129/5734 [3:27:45<147:54:36, 95.00s/it]  2%|▏         | 130/5734 [3:29:24<149:38:36, 96.13s/it]                                                        {'loss': 3.9784, 'learning_rate': 9.987322679411048e-05, 'epoch': 0.02}
  2%|▏         | 130/5734 [3:29:24<149:38:36, 96.13s/it]  2%|▏         | 131/5734 [3:31:08<153:35:06, 98.68s/it]  2%|▏         | 132/5734 [3:32:48<153:51:59, 98.88s/it]  2%|▏         | 133/5734 [3:34:22<151:39:07, 97.47s/it]  2%|▏         | 134/5734 [3:36:03<153:25:52, 98.63s/it]  2%|▏         | 135/5734 [3:37:42<153:35:18, 98.75s/it]  2%|▏         | 136/5734 [3:39:37<160:59:00, 103.53s/it]  2%|▏         | 137/5734 [3:41:18<159:44:51, 102.75s/it]  2%|▏         | 138/5734 [3:42:57<157:47:39, 101.51s/it]  2%|▏         | 139/5734 [3:44:36<156:39:16, 100.80s/it]  2%|▏         | 140/5734 [3:46:22<159:13:57, 102.47s/it]                                                         {'loss': 3.9113, 'learning_rate': 9.98529830175631e-05, 'epoch': 0.02}
  2%|▏         | 140/5734 [3:46:22<159:13:57, 102.47s/it]  2%|▏         | 141/5734 [3:48:10<161:37:52, 104.04s/it]  2%|▏         | 142/5734 [3:49:55<162:20:01, 104.51s/it]  2%|▏         | 143/5734 [3:51:36<160:22:00, 103.26s/it]  3%|▎         | 144/5734 [3:53:15<158:40:39, 102.19s/it]  3%|▎         | 145/5734 [3:54:56<157:58:37, 101.76s/it]  3%|▎         | 146/5734 [3:56:40<158:43:26, 102.26s/it]  3%|▎         | 147/5734 [3:58:20<157:53:55, 101.74s/it]  3%|▎         | 148/5734 [4:00:00<157:00:25, 101.19s/it]  3%|▎         | 149/5734 [4:01:41<156:42:37, 101.01s/it]  3%|▎         | 150/5734 [4:03:21<156:25:35, 100.85s/it]                                                         {'loss': 3.8691, 'learning_rate': 9.98312427489046e-05, 'epoch': 0.03}
  3%|▎         | 150/5734 [4:03:21<156:25:35, 100.85s/it]  3%|▎         | 151/5734 [4:05:02<156:31:22, 100.93s/it]  3%|▎         | 152/5734 [4:06:42<156:00:03, 100.61s/it]  3%|▎         | 153/5734 [4:08:22<155:28:21, 100.29s/it]  3%|▎         | 154/5734 [4:10:03<155:50:26, 100.54s/it]  3%|▎         | 155/5734 [4:11:45<156:23:49, 100.92s/it]  3%|▎         | 156/5734 [4:13:28<157:36:31, 101.72s/it]  3%|▎         | 157/5734 [4:15:12<158:23:20, 102.24s/it]  3%|▎         | 158/5734 [4:16:57<159:47:33, 103.17s/it]  3%|▎         | 159/5734 [4:18:38<158:40:37, 102.46s/it]  3%|▎         | 160/5734 [4:20:16<156:56:05, 101.36s/it]                                                         {'loss': 3.8395, 'learning_rate': 9.980800664073663e-05, 'epoch': 0.03}
  3%|▎         | 160/5734 [4:20:17<156:56:05, 101.36s/it]  3%|▎         | 161/5734 [4:21:57<156:41:38, 101.22s/it]  3%|▎         | 162/5734 [4:23:33<154:13:32, 99.64s/it]   3%|▎         | 163/5734 [4:25:10<152:52:07, 98.78s/it]  3%|▎         | 164/5734 [4:26:47<152:08:02, 98.33s/it]  3%|▎         | 165/5734 [4:28:26<152:13:26, 98.40s/it]  3%|▎         | 166/5734 [4:30:03<151:39:35, 98.06s/it]  3%|▎         | 167/5734 [4:31:41<151:33:04, 98.00s/it]  3%|▎         | 168/5734 [4:33:20<151:43:43, 98.14s/it]  3%|▎         | 169/5734 [4:34:57<151:35:20, 98.06s/it]  3%|▎         | 170/5734 [4:36:34<150:48:07, 97.57s/it]                                                        {'loss': 3.7892, 'learning_rate': 9.978327539056315e-05, 'epoch': 0.03}
  3%|▎         | 170/5734 [4:36:34<150:48:07, 97.57s/it]  3%|▎         | 171/5734 [4:38:11<150:30:07, 97.39s/it]  3%|▎         | 172/5734 [4:39:48<150:21:22, 97.32s/it]  3%|▎         | 173/5734 [4:41:25<149:58:53, 97.09s/it]  3%|▎         | 174/5734 [4:43:03<150:40:49, 97.56s/it]  3%|▎         | 175/5734 [4:44:39<150:01:39, 97.16s/it]  3%|▎         | 176/5734 [4:46:16<149:48:45, 97.04s/it]  3%|▎         | 177/5734 [4:47:52<149:19:15, 96.73s/it]  3%|▎         | 178/5734 [4:49:28<149:05:08, 96.60s/it]  3%|▎         | 179/5734 [4:51:05<148:55:53, 96.52s/it]  3%|▎         | 180/5734 [4:52:43<149:38:33, 97.00s/it]                                                        {'loss': 3.7632, 'learning_rate': 9.975704974076946e-05, 'epoch': 0.03}
  3%|▎         | 180/5734 [4:52:43<149:38:33, 97.00s/it]  3%|▎         | 181/5734 [4:54:20<149:26:42, 96.89s/it]  3%|▎         | 182/5734 [4:55:57<149:40:48, 97.05s/it]  3%|▎         | 183/5734 [4:57:33<149:16:03, 96.80s/it]  3%|▎         | 184/5734 [4:59:09<148:59:22, 96.64s/it]  3%|▎         | 185/5734 [5:00:45<148:27:10, 96.31s/it]  3%|▎         | 186/5734 [5:02:20<147:54:22, 95.97s/it]  3%|▎         | 187/5734 [5:03:57<148:10:17, 96.16s/it]  3%|▎         | 188/5734 [5:05:33<148:06:20, 96.14s/it]  3%|▎         | 189/5734 [5:07:09<148:03:33, 96.12s/it]  3%|▎         | 190/5734 [5:08:45<148:05:43, 96.17s/it]                                                        {'loss': 3.7245, 'learning_rate': 9.972933047859986e-05, 'epoch': 0.03}
  3%|▎         | 190/5734 [5:08:45<148:05:43, 96.17s/it]  3%|▎         | 191/5734 [5:10:21<147:42:32, 95.93s/it]  3%|▎         | 192/5734 [5:11:57<147:39:43, 95.92s/it]  3%|▎         | 193/5734 [5:13:33<148:03:22, 96.19s/it]  3%|▎         | 194/5734 [5:15:10<148:12:22, 96.31s/it]  3%|▎         | 195/5734 [5:16:46<148:16:19, 96.37s/it]  3%|▎         | 196/5734 [5:18:23<148:18:34, 96.41s/it]  3%|▎         | 197/5734 [5:20:00<148:23:29, 96.48s/it]  3%|▎         | 198/5734 [5:21:36<148:12:47, 96.38s/it]  3%|▎         | 199/5734 [5:23:13<148:33:12, 96.62s/it]  3%|▎         | 200/5734 [5:24:50<148:50:19, 96.82s/it]                                                        {'loss': 3.6835, 'learning_rate': 9.970011843613411e-05, 'epoch': 0.03}
  3%|▎         | 200/5734 [5:24:50<148:50:19, 96.82s/it]  4%|▎         | 201/5734 [5:26:30<150:23:42, 97.85s/it]  4%|▎         | 202/5734 [5:28:08<150:25:59, 97.90s/it]  4%|▎         | 203/5734 [5:29:46<150:09:55, 97.74s/it]  4%|▎         | 204/5734 [5:31:22<149:28:44, 97.31s/it]  4%|▎         | 205/5734 [5:32:58<148:56:07, 96.97s/it]  4%|▎         | 206/5734 [5:34:35<148:57:01, 97.00s/it]  4%|▎         | 207/5734 [5:36:13<149:02:30, 97.08s/it]  4%|▎         | 208/5734 [5:37:48<148:16:54, 96.60s/it]  4%|▎         | 209/5734 [5:39:25<148:14:08, 96.59s/it]  4%|▎         | 210/5734 [5:41:01<148:01:58, 96.47s/it]                                                        {'loss': 3.6598, 'learning_rate': 9.966941449026237e-05, 'epoch': 0.04}
  4%|▎         | 210/5734 [5:41:01<148:01:58, 96.47s/it]  4%|▎         | 211/5734 [5:42:38<148:15:59, 96.64s/it]  4%|▎         | 212/5734 [5:44:16<148:40:46, 96.93s/it]  4%|▎         | 213/5734 [5:45:51<148:08:13, 96.59s/it]  4%|▎         | 214/5734 [5:47:29<148:49:03, 97.05s/it]  4%|▎         | 215/5734 [5:49:06<148:35:01, 96.92s/it]  4%|▍         | 216/5734 [5:50:42<148:17:27, 96.75s/it]  4%|▍         | 217/5734 [5:52:18<147:52:22, 96.49s/it]  4%|▍         | 218/5734 [5:53:54<147:36:48, 96.34s/it]  4%|▍         | 219/5734 [5:55:30<147:15:26, 96.12s/it]  4%|▍         | 220/5734 [5:57:07<147:26:33, 96.26s/it]                                                        {'loss': 3.6442, 'learning_rate': 9.963721956265893e-05, 'epoch': 0.04}
  4%|▍         | 220/5734 [5:57:07<147:26:33, 96.26s/it]  4%|▍         | 221/5734 [5:58:42<147:10:37, 96.11s/it]  4%|▍         | 222/5734 [6:00:18<147:00:16, 96.01s/it]  4%|▍         | 223/5734 [6:01:53<146:25:44, 95.65s/it]  4%|▍         | 224/5734 [6:03:29<146:42:27, 95.85s/it]  4%|▍         | 225/5734 [6:05:06<147:08:55, 96.16s/it]  4%|▍         | 226/5734 [6:06:42<147:06:14, 96.15s/it]  4%|▍         | 227/5734 [6:08:18<146:55:10, 96.04s/it]  4%|▍         | 228/5734 [6:09:54<146:47:05, 95.97s/it]  4%|▍         | 229/5734 [6:11:29<146:24:49, 95.75s/it]  4%|▍         | 230/5734 [6:13:04<146:11:57, 95.62s/it]                                                        {'loss': 3.6099, 'learning_rate': 9.960353461975454e-05, 'epoch': 0.04}
  4%|▍         | 230/5734 [6:13:04<146:11:57, 95.62s/it]  4%|▍         | 231/5734 [6:14:41<146:51:00, 96.07s/it]  4%|▍         | 232/5734 [6:16:18<146:56:44, 96.15s/it]  4%|▍         | 233/5734 [6:17:54<147:10:05, 96.31s/it]  4%|▍         | 234/5734 [6:19:31<147:05:56, 96.28s/it]  4%|▍         | 235/5734 [6:21:07<147:04:24, 96.28s/it]  4%|▍         | 236/5734 [6:22:43<146:47:35, 96.12s/it]  4%|▍         | 237/5734 [6:24:19<146:55:22, 96.22s/it]  4%|▍         | 238/5734 [6:25:54<146:21:01, 95.86s/it]  4%|▍         | 239/5734 [6:27:28<145:26:56, 95.29s/it]  4%|▍         | 240/5734 [6:29:03<145:24:01, 95.28s/it]                                                        {'loss': 3.5834, 'learning_rate': 9.956836067270738e-05, 'epoch': 0.04}
  4%|▍         | 240/5734 [6:29:03<145:24:01, 95.28s/it]  4%|▍         | 241/5734 [6:30:39<145:41:04, 95.48s/it]  4%|▍         | 242/5734 [6:32:16<146:07:04, 95.78s/it]  4%|▍         | 243/5734 [6:33:53<146:44:37, 96.21s/it]  4%|▍         | 244/5734 [6:35:28<146:05:34, 95.80s/it]  4%|▍         | 245/5734 [6:37:02<145:30:32, 95.43s/it]  4%|▍         | 246/5734 [6:38:38<145:24:22, 95.38s/it]  4%|▍         | 247/5734 [6:40:14<145:41:47, 95.59s/it]  4%|▍         | 248/5734 [6:41:50<146:00:27, 95.81s/it]  4%|▍         | 249/5734 [6:43:29<147:24:21, 96.75s/it]  4%|▍         | 250/5734 [6:45:07<147:48:45, 97.03s/it]                                                        {'loss': 3.5725, 'learning_rate': 9.95316987773727e-05, 'epoch': 0.04}
  4%|▍         | 250/5734 [6:45:07<147:48:45, 97.03s/it]  4%|▍         | 251/5734 [6:46:45<148:13:52, 97.32s/it]  4%|▍         | 252/5734 [6:48:24<148:56:59, 97.81s/it]  4%|▍         | 253/5734 [6:50:00<148:24:14, 97.47s/it]  4%|▍         | 254/5734 [6:51:36<147:36:03, 96.96s/it]  4%|▍         | 255/5734 [6:53:13<147:23:14, 96.84s/it]  4%|▍         | 256/5734 [6:54:49<146:57:18, 96.58s/it]  4%|▍         | 257/5734 [6:56:23<146:01:54, 95.99s/it]  4%|▍         | 258/5734 [6:58:00<146:14:53, 96.15s/it]  5%|▍         | 259/5734 [6:59:36<146:18:57, 96.21s/it]  5%|▍         | 260/5734 [7:01:12<145:57:18, 95.99s/it]                                                        {'loss': 3.5465, 'learning_rate': 9.949355003427114e-05, 'epoch': 0.05}
  5%|▍         | 260/5734 [7:01:12<145:57:18, 95.99s/it]  5%|▍         | 261/5734 [7:02:49<146:45:41, 96.54s/it]  5%|▍         | 262/5734 [7:04:25<146:24:25, 96.32s/it]  5%|▍         | 263/5734 [7:06:04<147:28:09, 97.04s/it]  5%|▍         | 264/5734 [7:07:40<146:50:38, 96.64s/it]  5%|▍         | 265/5734 [7:09:17<147:07:21, 96.84s/it]  5%|▍         | 266/5734 [7:10:52<146:13:24, 96.27s/it]  5%|▍         | 267/5734 [7:12:29<146:39:15, 96.57s/it]  5%|▍         | 268/5734 [7:14:04<145:54:25, 96.10s/it]  5%|▍         | 269/5734 [7:15:39<145:11:37, 95.64s/it]  5%|▍         | 270/5734 [7:17:16<145:57:46, 96.17s/it]                                                        {'loss': 3.5226, 'learning_rate': 9.945391558855571e-05, 'epoch': 0.05}
  5%|▍         | 270/5734 [7:17:16<145:57:46, 96.17s/it]  5%|▍         | 271/5734 [7:18:53<146:14:54, 96.37s/it]  5%|▍         | 272/5734 [7:20:29<145:50:51, 96.13s/it]  5%|▍         | 273/5734 [7:22:04<145:27:31, 95.89s/it]  5%|▍         | 274/5734 [7:23:40<145:36:21, 96.00s/it]  5%|▍         | 275/5734 [7:25:18<146:17:44, 96.48s/it]  5%|▍         | 276/5734 [7:26:53<145:54:39, 96.24s/it]  5%|▍         | 277/5734 [7:28:28<145:16:20, 95.84s/it]  5%|▍         | 278/5734 [7:30:04<145:21:15, 95.91s/it]  5%|▍         | 279/5734 [7:31:42<146:01:58, 96.37s/it]  5%|▍         | 280/5734 [7:33:16<144:55:44, 95.66s/it]                                                        {'loss': 3.512, 'learning_rate': 9.941279662997739e-05, 'epoch': 0.05}
  5%|▍         | 280/5734 [7:33:16<144:55:44, 95.66s/it]  5%|▍         | 281/5734 [7:34:52<145:13:23, 95.87s/it]  5%|▍         | 282/5734 [7:36:29<145:23:35, 96.00s/it]  5%|▍         | 283/5734 [7:38:07<146:36:18, 96.82s/it]  5%|▍         | 284/5734 [7:39:43<145:57:46, 96.42s/it]  5%|▍         | 285/5734 [7:41:17<144:52:58, 95.72s/it]  5%|▍         | 286/5734 [7:42:55<146:01:10, 96.49s/it]  5%|▌         | 287/5734 [7:44:33<146:38:52, 96.92s/it]  5%|▌         | 288/5734 [7:46:10<146:23:23, 96.77s/it]  5%|▌         | 289/5734 [7:47:45<145:38:03, 96.29s/it]  5%|▌         | 290/5734 [7:49:20<145:15:43, 96.06s/it]                                                        {'loss': 3.4794, 'learning_rate': 9.937019439284943e-05, 'epoch': 0.05}
  5%|▌         | 290/5734 [7:49:20<145:15:43, 96.06s/it]  5%|▌         | 291/5734 [7:50:56<144:53:41, 95.83s/it]  5%|▌         | 292/5734 [7:52:31<144:51:55, 95.83s/it]  5%|▌         | 293/5734 [7:54:06<144:25:29, 95.56s/it]  5%|▌         | 294/5734 [7:55:41<144:01:41, 95.31s/it]  5%|▌         | 295/5734 [7:57:16<143:53:25, 95.24s/it]  5%|▌         | 296/5734 [7:58:52<144:18:56, 95.54s/it]  5%|▌         | 297/5734 [8:00:28<144:15:00, 95.51s/it]  5%|▌         | 298/5734 [8:02:03<143:57:03, 95.33s/it]  5%|▌         | 299/5734 [8:03:38<144:05:14, 95.44s/it]  5%|▌         | 300/5734 [8:05:13<143:53:39, 95.33s/it]                                                        {'loss': 3.4739, 'learning_rate': 9.932611015601025e-05, 'epoch': 0.05}
  5%|▌         | 300/5734 [8:05:13<143:53:39, 95.33s/it]  5%|▌         | 301/5734 [8:06:49<143:50:27, 95.31s/it]  5%|▌         | 302/5734 [8:08:24<143:54:11, 95.37s/it]  5%|▌         | 303/5734 [8:09:58<143:08:42, 94.89s/it]  5%|▌         | 304/5734 [8:11:34<143:51:40, 95.38s/it]  5%|▌         | 305/5734 [8:13:10<144:00:25, 95.49s/it]  5%|▌         | 306/5734 [8:14:45<143:50:01, 95.39s/it]  5%|▌         | 307/5734 [8:16:20<143:28:20, 95.17s/it]  5%|▌         | 308/5734 [8:17:56<143:37:01, 95.29s/it]  5%|▌         | 309/5734 [8:19:30<143:09:38, 95.00s/it]  5%|▌         | 310/5734 [8:21:05<143:05:31, 94.97s/it]                                                        {'loss': 3.4449, 'learning_rate': 9.928054524278518e-05, 'epoch': 0.05}
  5%|▌         | 310/5734 [8:21:05<143:05:31, 94.97s/it]  5%|▌         | 311/5734 [8:22:41<143:32:36, 95.29s/it]  5%|▌         | 312/5734 [8:24:15<142:56:14, 94.90s/it]  5%|▌         | 313/5734 [8:25:48<142:11:15, 94.42s/it]  5%|▌         | 314/5734 [8:27:22<141:45:37, 94.16s/it]  5%|▌         | 315/5734 [8:28:56<141:47:59, 94.20s/it]  6%|▌         | 316/5734 [8:30:31<142:18:59, 94.56s/it]  6%|▌         | 317/5734 [8:32:06<142:15:26, 94.54s/it]  6%|▌         | 318/5734 [8:33:41<142:28:11, 94.70s/it]  6%|▌         | 319/5734 [8:35:16<142:32:05, 94.76s/it]  6%|▌         | 320/5734 [8:36:56<144:47:06, 96.27s/it]                                                        {'loss': 3.4273, 'learning_rate': 9.923350102094655e-05, 'epoch': 0.06}
  6%|▌         | 320/5734 [8:36:56<144:47:06, 96.27s/it]  6%|▌         | 321/5734 [8:38:38<147:23:39, 98.03s/it]  6%|▌         | 322/5734 [8:40:22<150:13:37, 99.93s/it]  6%|▌         | 323/5734 [8:42:08<152:48:53, 101.67s/it]  6%|▌         | 324/5734 [8:43:56<155:33:15, 103.51s/it]  6%|▌         | 325/5734 [8:45:37<154:30:37, 102.84s/it]  6%|▌         | 326/5734 [8:47:17<153:03:30, 101.89s/it]  6%|▌         | 327/5734 [8:48:50<149:22:24, 99.45s/it]   6%|▌         | 328/5734 [8:50:24<146:39:08, 97.66s/it]  6%|▌         | 329/5734 [8:51:58<145:10:25, 96.69s/it]  6%|▌         | 330/5734 [8:53:34<144:35:12, 96.32s/it]                                                        {'loss': 3.4197, 'learning_rate': 9.918497890267282e-05, 'epoch': 0.06}
  6%|▌         | 330/5734 [8:53:34<144:35:12, 96.32s/it]  6%|▌         | 331/5734 [8:55:09<143:55:41, 95.90s/it]  6%|▌         | 332/5734 [8:56:43<143:07:08, 95.38s/it]  6%|▌         | 333/5734 [8:58:18<142:55:16, 95.26s/it]  6%|▌         | 334/5734 [8:59:52<142:22:54, 94.92s/it]  6%|▌         | 335/5734 [9:01:26<141:59:45, 94.68s/it]  6%|▌         | 336/5734 [9:03:01<142:16:03, 94.88s/it]  6%|▌         | 337/5734 [9:04:35<141:29:08, 94.38s/it]  6%|▌         | 338/5734 [9:06:09<141:17:27, 94.26s/it]  6%|▌         | 339/5734 [9:07:43<141:28:00, 94.40s/it]  6%|▌         | 340/5734 [9:09:18<141:32:55, 94.47s/it]                                                        {'loss': 3.4132, 'learning_rate': 9.913498034450603e-05, 'epoch': 0.06}
  6%|▌         | 340/5734 [9:09:18<141:32:55, 94.47s/it]  6%|▌         | 341/5734 [9:10:54<141:59:14, 94.78s/it]  6%|▌         | 342/5734 [9:12:29<142:03:33, 94.85s/it]  6%|▌         | 343/5734 [9:14:04<142:26:26, 95.12s/it]  6%|▌         | 344/5734 [9:15:39<142:11:59, 94.98s/it]  6%|▌         | 345/5734 [9:17:15<142:36:11, 95.26s/it]  6%|▌         | 346/5734 [9:18:50<142:36:22, 95.28s/it]  6%|▌         | 347/5734 [9:20:25<142:16:58, 95.08s/it]  6%|▌         | 348/5734 [9:22:00<142:11:43, 95.04s/it]  6%|▌         | 349/5734 [9:23:36<142:31:28, 95.28s/it]  6%|▌         | 350/5734 [9:25:09<141:45:44, 94.79s/it]                                                        {'loss': 3.3965, 'learning_rate': 9.908350684730821e-05, 'epoch': 0.06}
  6%|▌         | 350/5734 [9:25:09<141:45:44, 94.79s/it]  6%|▌         | 351/5734 [9:26:42<141:02:41, 94.33s/it]  6%|▌         | 352/5734 [9:28:16<140:42:43, 94.12s/it]  6%|▌         | 353/5734 [9:29:51<141:01:24, 94.35s/it]  6%|▌         | 354/5734 [9:31:26<141:15:56, 94.53s/it]  6%|▌         | 355/5734 [9:33:01<141:17:41, 94.56s/it]  6%|▌         | 356/5734 [9:34:36<141:34:58, 94.77s/it]  6%|▌         | 357/5734 [9:36:11<141:40:04, 94.85s/it]  6%|▌         | 358/5734 [9:37:46<141:35:44, 94.82s/it]  6%|▋         | 359/5734 [9:39:21<141:55:13, 95.05s/it]  6%|▋         | 360/5734 [9:40:56<141:45:28, 94.96s/it]                                                        {'loss': 3.3777, 'learning_rate': 9.903055995621623e-05, 'epoch': 0.06}
  6%|▋         | 360/5734 [9:40:56<141:45:28, 94.96s/it]  6%|▋         | 361/5734 [9:42:31<141:33:38, 94.85s/it]  6%|▋         | 362/5734 [9:44:05<141:31:09, 94.84s/it]  6%|▋         | 363/5734 [9:45:40<141:13:01, 94.65s/it]  6%|▋         | 364/5734 [9:47:14<141:04:58, 94.58s/it]  6%|▋         | 365/5734 [9:48:50<141:31:41, 94.90s/it]  6%|▋         | 366/5734 [9:50:24<141:22:13, 94.81s/it]  6%|▋         | 367/5734 [9:51:58<141:03:22, 94.62s/it]  6%|▋         | 368/5734 [9:53:32<140:32:16, 94.29s/it]  6%|▋         | 369/5734 [9:55:06<140:19:01, 94.16s/it]  6%|▋         | 370/5734 [9:56:40<140:12:34, 94.10s/it]                                                        {'loss': 3.3612, 'learning_rate': 9.897614126059545e-05, 'epoch': 0.06}
  6%|▋         | 370/5734 [9:56:40<140:12:34, 94.10s/it]  6%|▋         | 371/5734 [9:58:14<140:04:21, 94.03s/it]  6%|▋         | 372/5734 [9:59:48<140:14:08, 94.15s/it]  7%|▋         | 373/5734 [10:01:22<140:19:51, 94.23s/it]  7%|▋         | 374/5734 [10:02:57<140:22:22, 94.28s/it]  7%|▋         | 375/5734 [10:04:31<140:13:36, 94.20s/it]  7%|▋         | 376/5734 [10:06:05<140:18:32, 94.27s/it]  7%|▋         | 377/5734 [10:07:41<140:51:38, 94.66s/it]  7%|▋         | 378/5734 [10:09:16<140:58:46, 94.76s/it]  7%|▋         | 379/5734 [10:10:51<141:06:27, 94.86s/it]  7%|▋         | 380/5734 [10:12:26<141:10:41, 94.93s/it]                                                         {'loss': 3.3612, 'learning_rate': 9.892025239399203e-05, 'epoch': 0.07}
  7%|▋         | 380/5734 [10:12:26<141:10:41, 94.93s/it]  7%|▋         | 381/5734 [10:14:01<141:19:10, 95.04s/it]  7%|▋         | 382/5734 [10:15:37<141:25:07, 95.12s/it]  7%|▋         | 383/5734 [10:17:11<141:09:36, 94.97s/it]  7%|▋         | 384/5734 [10:18:46<141:05:53, 94.94s/it]  7%|▋         | 385/5734 [10:20:22<141:14:42, 95.06s/it]  7%|▋         | 386/5734 [10:21:55<140:35:43, 94.64s/it]  7%|▋         | 387/5734 [10:23:29<140:13:58, 94.42s/it]  7%|▋         | 388/5734 [10:25:03<140:11:45, 94.41s/it]  7%|▋         | 389/5734 [10:26:38<140:23:01, 94.55s/it]  7%|▋         | 390/5734 [10:28:13<140:17:07, 94.50s/it]                                                         {'loss': 3.3442, 'learning_rate': 9.886289503408387e-05, 'epoch': 0.07}
  7%|▋         | 390/5734 [10:28:13<140:17:07, 94.50s/it]  7%|▋         | 391/5734 [10:29:48<140:48:07, 94.87s/it]  7%|▋         | 392/5734 [10:31:23<140:42:46, 94.83s/it]  7%|▋         | 393/5734 [10:32:58<140:46:00, 94.88s/it]  7%|▋         | 394/5734 [10:34:33<140:37:09, 94.80s/it]  7%|▋         | 395/5734 [10:36:08<140:41:02, 94.86s/it]  7%|▋         | 396/5734 [10:37:43<140:49:39, 94.98s/it]  7%|▋         | 397/5734 [10:39:18<140:50:01, 95.00s/it]  7%|▋         | 398/5734 [10:40:53<140:49:53, 95.01s/it]  7%|▋         | 399/5734 [10:42:27<140:19:22, 94.69s/it]  7%|▋         | 400/5734 [10:44:02<140:17:20, 94.68s/it]                                                         {'loss': 3.3485, 'learning_rate': 9.880407090263027e-05, 'epoch': 0.07}
  7%|▋         | 400/5734 [10:44:02<140:17:20, 94.68s/it]  7%|▋         | 401/5734 [10:45:37<140:19:59, 94.73s/it]  7%|▋         | 402/5734 [10:47:11<140:14:30, 94.69s/it]  7%|▋         | 403/5734 [10:48:45<139:57:17, 94.51s/it]  7%|▋         | 404/5734 [10:50:20<139:56:24, 94.52s/it]  7%|▋         | 405/5734 [10:51:54<139:56:43, 94.54s/it]  7%|▋         | 406/5734 [10:53:28<139:42:35, 94.40s/it]  7%|▋         | 407/5734 [10:55:03<139:45:08, 94.44s/it]  7%|▋         | 408/5734 [10:56:38<139:47:29, 94.49s/it]  7%|▋         | 409/5734 [10:58:12<139:47:07, 94.50s/it]  7%|▋         | 410/5734 [10:59:46<139:34:39, 94.38s/it]                                                         {'loss': 3.3204, 'learning_rate': 9.874378176542021e-05, 'epoch': 0.07}
  7%|▋         | 410/5734 [10:59:46<139:34:39, 94.38s/it]  7%|▋         | 411/5734 [11:01:20<139:29:35, 94.34s/it]  7%|▋         | 412/5734 [11:02:54<138:56:31, 93.99s/it]  7%|▋         | 413/5734 [11:04:27<138:34:59, 93.76s/it]  7%|▋         | 414/5734 [11:06:02<139:00:48, 94.07s/it]  7%|▋         | 415/5734 [11:07:38<139:48:03, 94.62s/it]  7%|▋         | 416/5734 [11:09:12<139:32:54, 94.47s/it]  7%|▋         | 417/5734 [11:10:46<139:35:12, 94.51s/it]  7%|▋         | 418/5734 [11:12:21<139:41:20, 94.60s/it]  7%|▋         | 419/5734 [11:13:56<139:49:19, 94.71s/it]  7%|▋         | 420/5734 [11:15:30<139:20:17, 94.40s/it]                                                         {'loss': 3.316, 'learning_rate': 9.868202943221939e-05, 'epoch': 0.07}
  7%|▋         | 420/5734 [11:15:30<139:20:17, 94.40s/it]  7%|▋         | 421/5734 [11:17:04<139:09:02, 94.29s/it]  7%|▋         | 422/5734 [11:18:37<138:41:48, 94.00s/it]  7%|▋         | 423/5734 [11:20:11<138:41:44, 94.01s/it]  7%|▋         | 424/5734 [11:21:45<138:45:52, 94.08s/it]  7%|▋         | 425/5734 [11:23:20<138:48:43, 94.13s/it]  7%|▋         | 426/5734 [11:24:55<139:15:15, 94.45s/it]  7%|▋         | 427/5734 [11:26:29<139:05:09, 94.35s/it]  7%|▋         | 428/5734 [11:28:04<139:14:44, 94.48s/it]  7%|▋         | 429/5734 [11:29:39<139:43:26, 94.82s/it]  7%|▋         | 430/5734 [11:31:13<139:21:10, 94.58s/it]                                                         {'loss': 3.3003, 'learning_rate': 9.861881575671585e-05, 'epoch': 0.07}
  7%|▋         | 430/5734 [11:31:13<139:21:10, 94.58s/it]  8%|▊         | 431/5734 [11:32:47<139:00:00, 94.36s/it]  8%|▊         | 432/5734 [11:34:22<139:06:01, 94.45s/it]  8%|▊         | 433/5734 [11:35:57<139:14:21, 94.56s/it]  8%|▊         | 434/5734 [11:37:31<138:57:20, 94.38s/it]  8%|▊         | 435/5734 [11:39:05<139:02:08, 94.46s/it]  8%|▊         | 436/5734 [11:40:40<139:03:28, 94.49s/it]  8%|▊         | 437/5734 [11:42:14<138:56:26, 94.43s/it]  8%|▊         | 438/5734 [11:43:49<139:17:23, 94.68s/it]  8%|▊         | 439/5734 [11:45:24<139:12:38, 94.65s/it]  8%|▊         | 440/5734 [11:46:59<139:15:34, 94.70s/it]                                                         {'loss': 3.2976, 'learning_rate': 9.855414263646441e-05, 'epoch': 0.08}
  8%|▊         | 440/5734 [11:46:59<139:15:34, 94.70s/it]  8%|▊         | 441/5734 [11:48:34<139:16:21, 94.73s/it]  8%|▊         | 442/5734 [11:50:08<139:03:43, 94.60s/it]  8%|▊         | 443/5734 [11:51:43<139:11:44, 94.71s/it]  8%|▊         | 444/5734 [11:53:17<138:54:00, 94.53s/it]  8%|▊         | 445/5734 [11:54:50<138:25:16, 94.22s/it]  8%|▊         | 446/5734 [11:56:25<138:27:08, 94.26s/it]  8%|▊         | 447/5734 [11:58:00<138:39:51, 94.42s/it]  8%|▊         | 448/5734 [11:59:34<138:34:37, 94.38s/it]  8%|▊         | 449/5734 [12:01:08<138:39:56, 94.46s/it]  8%|▊         | 450/5734 [12:02:43<138:42:46, 94.51s/it]                                                         {'loss': 3.2861, 'learning_rate': 9.84880120128296e-05, 'epoch': 0.08}
  8%|▊         | 450/5734 [12:02:43<138:42:46, 94.51s/it]  8%|▊         | 451/5734 [12:04:18<138:58:31, 94.70s/it]  8%|▊         | 452/5734 [12:05:53<139:02:22, 94.76s/it]  8%|▊         | 453/5734 [12:07:28<138:56:50, 94.72s/it]  8%|▊         | 454/5734 [12:09:03<138:57:22, 94.74s/it]  8%|▊         | 455/5734 [12:10:38<139:13:02, 94.94s/it]  8%|▊         | 456/5734 [12:12:12<138:51:42, 94.71s/it]  8%|▊         | 457/5734 [12:13:46<138:34:17, 94.53s/it]  8%|▊         | 458/5734 [12:15:20<138:23:51, 94.43s/it]  8%|▊         | 459/5734 [12:16:55<138:15:49, 94.36s/it]  8%|▊         | 460/5734 [12:18:29<138:15:05, 94.37s/it]                                                         {'loss': 3.2731, 'learning_rate': 9.842042587092751e-05, 'epoch': 0.08}
  8%|▊         | 460/5734 [12:18:29<138:15:05, 94.37s/it]  8%|▊         | 461/5734 [12:20:04<138:22:45, 94.47s/it]  8%|▊         | 462/5734 [12:21:39<138:27:51, 94.55s/it]  8%|▊         | 463/5734 [12:23:13<138:35:00, 94.65s/it]  8%|▊         | 464/5734 [12:24:49<138:47:40, 94.81s/it]  8%|▊         | 465/5734 [12:26:24<138:56:18, 94.93s/it]  8%|▊         | 466/5734 [12:27:59<138:55:18, 94.94s/it]  8%|▊         | 467/5734 [12:29:34<138:59:43, 95.00s/it]  8%|▊         | 468/5734 [12:31:09<139:09:17, 95.13s/it]  8%|▊         | 469/5734 [12:32:45<139:18:37, 95.26s/it]  8%|▊         | 470/5734 [12:34:20<139:14:48, 95.23s/it]                                                         {'loss': 3.263, 'learning_rate': 9.835138623956603e-05, 'epoch': 0.08}
  8%|▊         | 470/5734 [12:34:20<139:14:48, 95.23s/it]  8%|▊         | 471/5734 [12:35:55<138:59:52, 95.08s/it]  8%|▊         | 472/5734 [12:37:30<138:59:18, 95.09s/it]  8%|▊         | 473/5734 [12:39:05<139:05:25, 95.18s/it]  8%|▊         | 474/5734 [12:40:40<138:49:45, 95.02s/it]  8%|▊         | 475/5734 [12:42:15<138:41:26, 94.94s/it]  8%|▊         | 476/5734 [12:43:51<139:11:47, 95.30s/it]  8%|▊         | 477/5734 [12:45:26<138:56:18, 95.15s/it]  8%|▊         | 478/5734 [12:47:01<138:52:57, 95.13s/it]  8%|▊         | 479/5734 [12:48:36<138:56:47, 95.19s/it]  8%|▊         | 480/5734 [12:50:11<138:44:34, 95.07s/it]                                                         {'loss': 3.2591, 'learning_rate': 9.828089519118413e-05, 'epoch': 0.08}
  8%|▊         | 480/5734 [12:50:11<138:44:34, 95.07s/it]  8%|▊         | 481/5734 [12:51:45<138:19:46, 94.80s/it]  8%|▊         | 482/5734 [12:53:20<138:12:47, 94.74s/it]  8%|▊         | 483/5734 [12:54:54<138:03:21, 94.65s/it]  8%|▊         | 484/5734 [12:56:28<137:43:34, 94.44s/it]  8%|▊         | 485/5734 [12:58:02<137:38:00, 94.40s/it]  8%|▊         | 486/5734 [12:59:36<137:31:35, 94.34s/it]  8%|▊         | 487/5734 [13:01:11<137:35:07, 94.40s/it]  9%|▊         | 488/5734 [13:02:45<137:24:38, 94.30s/it]  9%|▊         | 489/5734 [13:04:20<137:28:02, 94.35s/it]  9%|▊         | 490/5734 [13:05:53<137:12:52, 94.20s/it]                                                         {'loss': 3.2532, 'learning_rate': 9.820895484178958e-05, 'epoch': 0.09}
  9%|▊         | 490/5734 [13:05:53<137:12:52, 94.20s/it]  9%|▊         | 491/5734 [13:07:27<137:01:25, 94.08s/it]  9%|▊         | 492/5734 [13:09:02<137:08:41, 94.19s/it]  9%|▊         | 493/5734 [13:10:36<137:17:35, 94.31s/it]  9%|▊         | 494/5734 [13:12:10<137:15:15, 94.30s/it]  9%|▊         | 495/5734 [13:13:46<137:41:30, 94.62s/it]  9%|▊         | 496/5734 [13:15:21<137:57:22, 94.82s/it]  9%|▊         | 497/5734 [13:16:56<137:47:05, 94.72s/it]  9%|▊         | 498/5734 [13:18:30<137:39:24, 94.65s/it]  9%|▊         | 499/5734 [13:20:05<137:55:46, 94.85s/it]  9%|▊         | 500/5734 [13:21:40<137:56:35, 94.88s/it]                                                         {'loss': 3.2484, 'learning_rate': 9.813556735089536e-05, 'epoch': 0.09}
  9%|▊         | 500/5734 [13:21:40<137:56:35, 94.88s/it]  9%|▊         | 501/5734 [13:23:16<138:15:31, 95.11s/it]  9%|▉         | 502/5734 [13:24:51<138:15:13, 95.13s/it]  9%|▉         | 503/5734 [13:26:26<138:08:43, 95.07s/it]  9%|▉         | 504/5734 [13:28:00<137:38:19, 94.74s/it]  9%|▉         | 505/5734 [13:29:36<137:54:29, 94.95s/it]  9%|▉         | 506/5734 [13:31:11<138:02:24, 95.05s/it]  9%|▉         | 507/5734 [13:32:46<137:56:43, 95.01s/it]  9%|▉         | 508/5734 [13:34:21<138:03:12, 95.10s/it]  9%|▉         | 509/5734 [13:35:55<137:44:48, 94.91s/it]  9%|▉         | 510/5734 [13:37:31<137:57:40, 95.07s/it]                                                         {'loss': 3.2389, 'learning_rate': 9.80607349214549e-05, 'epoch': 0.09}
  9%|▉         | 510/5734 [13:37:31<137:57:40, 95.07s/it]  9%|▉         | 511/5734 [13:39:07<138:10:04, 95.23s/it]  9%|▉         | 512/5734 [13:40:42<138:06:20, 95.21s/it]  9%|▉         | 513/5734 [13:42:16<137:33:41, 94.85s/it]  9%|▉         | 514/5734 [13:43:50<137:27:04, 94.79s/it]  9%|▉         | 515/5734 [13:45:26<137:43:07, 95.00s/it]  9%|▉         | 516/5734 [13:47:01<137:46:47, 95.06s/it]  9%|▉         | 517/5734 [13:48:36<137:54:49, 95.17s/it]  9%|▉         | 518/5734 [13:50:11<137:33:54, 94.95s/it]  9%|▉         | 519/5734 [13:51:46<137:33:04, 94.95s/it]  9%|▉         | 520/5734 [13:53:21<137:44:32, 95.10s/it]                                                         {'loss': 3.2322, 'learning_rate': 9.798445979979602e-05, 'epoch': 0.09}
  9%|▉         | 520/5734 [13:53:21<137:44:32, 95.10s/it]  9%|▉         | 521/5734 [13:54:57<137:58:32, 95.28s/it]  9%|▉         | 522/5734 [13:56:32<137:35:23, 95.04s/it]  9%|▉         | 523/5734 [13:58:06<137:07:06, 94.73s/it]  9%|▉         | 524/5734 [13:59:41<137:36:45, 95.09s/it]  9%|▉         | 525/5734 [14:01:17<137:36:28, 95.10s/it]  9%|▉         | 526/5734 [14:02:52<137:50:14, 95.28s/it]  9%|▉         | 527/5734 [14:04:27<137:33:07, 95.10s/it]  9%|▉         | 528/5734 [14:06:01<137:08:01, 94.83s/it]  9%|▉         | 529/5734 [14:07:36<137:08:51, 94.86s/it]  9%|▉         | 530/5734 [14:09:11<137:19:21, 95.00s/it]                                                         {'loss': 3.214, 'learning_rate': 9.790674427555335e-05, 'epoch': 0.09}
  9%|▉         | 530/5734 [14:09:11<137:19:21, 95.00s/it]  9%|▉         | 531/5734 [14:10:47<137:28:51, 95.12s/it]  9%|▉         | 532/5734 [14:12:21<136:51:07, 94.71s/it]  9%|▉         | 533/5734 [14:13:56<136:56:28, 94.79s/it]  9%|▉         | 534/5734 [14:15:31<137:07:27, 94.93s/it]  9%|▉         | 535/5734 [14:17:06<137:09:45, 94.98s/it]  9%|▉         | 536/5734 [14:18:41<137:07:24, 94.97s/it]  9%|▉         | 537/5734 [14:20:15<136:42:26, 94.70s/it]  9%|▉         | 538/5734 [14:21:49<136:33:42, 94.62s/it]  9%|▉         | 539/5734 [14:23:24<136:40:06, 94.71s/it]  9%|▉         | 540/5734 [14:25:00<136:56:57, 94.92s/it]                                                         {'loss': 3.2151, 'learning_rate': 9.782759068159974e-05, 'epoch': 0.09}
  9%|▉         | 540/5734 [14:25:00<136:56:57, 94.92s/it]  9%|▉         | 541/5734 [14:26:34<136:42:12, 94.77s/it]  9%|▉         | 542/5734 [14:28:09<136:32:02, 94.67s/it]  9%|▉         | 543/5734 [14:29:44<136:49:43, 94.89s/it]  9%|▉         | 544/5734 [14:31:19<136:49:50, 94.91s/it] 10%|▉         | 545/5734 [14:32:54<136:47:14, 94.90s/it] 10%|▉         | 546/5734 [14:34:28<136:27:19, 94.69s/it] 10%|▉         | 547/5734 [14:36:02<136:11:02, 94.52s/it] 10%|▉         | 548/5734 [14:37:37<136:25:47, 94.71s/it] 10%|▉         | 549/5734 [14:39:13<136:52:38, 95.04s/it] 10%|▉         | 550/5734 [14:40:48<136:51:40, 95.04s/it]                                                         {'loss': 3.2015, 'learning_rate': 9.77470013939761e-05, 'epoch': 0.1}
 10%|▉         | 550/5734 [14:40:48<136:51:40, 95.04s/it] 10%|▉         | 551/5734 [14:42:22<136:14:14, 94.63s/it] 10%|▉         | 552/5734 [14:43:56<136:15:48, 94.66s/it] 10%|▉         | 553/5734 [14:45:32<136:28:25, 94.83s/it] 10%|▉         | 554/5734 [14:47:07<136:39:46, 94.98s/it] 10%|▉         | 555/5734 [14:48:42<136:37:49, 94.97s/it] 10%|▉         | 556/5734 [14:50:17<136:28:41, 94.89s/it] 10%|▉         | 557/5734 [14:51:51<136:10:53, 94.70s/it] 10%|▉         | 558/5734 [14:53:26<136:16:55, 94.79s/it] 10%|▉         | 559/5734 [14:55:01<136:23:00, 94.88s/it] 10%|▉         | 560/5734 [14:56:36<136:14:21, 94.79s/it]                                                         {'loss': 3.2086, 'learning_rate': 9.766497883182018e-05, 'epoch': 0.1}
 10%|▉         | 560/5734 [14:56:36<136:14:21, 94.79s/it] 10%|▉         | 561/5734 [14:58:10<136:01:16, 94.66s/it] 10%|▉         | 562/5734 [14:59:45<136:12:25, 94.81s/it] 10%|▉         | 563/5734 [15:01:20<136:22:10, 94.94s/it] 10%|▉         | 564/5734 [15:02:55<136:23:07, 94.97s/it] 10%|▉         | 565/5734 [15:04:30<136:22:00, 94.97s/it] 10%|▉         | 566/5734 [15:06:05<135:58:52, 94.72s/it] 10%|▉         | 567/5734 [15:07:39<135:57:02, 94.72s/it] 10%|▉         | 568/5734 [15:09:15<136:14:36, 94.94s/it] 10%|▉         | 569/5734 [15:10:49<136:00:39, 94.80s/it] 10%|▉         | 570/5734 [15:12:23<135:31:06, 94.47s/it]                                                         {'loss': 3.2004, 'learning_rate': 9.758152545729395e-05, 'epoch': 0.1}
 10%|▉         | 570/5734 [15:12:23<135:31:06, 94.47s/it] 10%|▉         | 571/5734 [15:13:58<135:35:51, 94.55s/it] 10%|▉         | 572/5734 [15:15:33<135:44:02, 94.66s/it] 10%|▉         | 573/5734 [15:17:08<136:01:24, 94.88s/it] 10%|█         | 574/5734 [15:18:43<136:01:39, 94.90s/it] 10%|█         | 575/5734 [15:20:17<135:41:02, 94.68s/it] 10%|█         | 576/5734 [15:21:52<135:37:24, 94.66s/it] 10%|█         | 577/5734 [15:23:26<135:35:16, 94.65s/it] 10%|█         | 578/5734 [15:25:02<136:08:50, 95.06s/it] 10%|█         | 579/5734 [15:26:37<136:03:23, 95.02s/it] 10%|█         | 580/5734 [15:28:11<135:34:47, 94.70s/it]                                                         {'loss': 3.1903, 'learning_rate': 9.74966437755096e-05, 'epoch': 0.1}
 10%|█         | 580/5734 [15:28:11<135:34:47, 94.70s/it] 10%|█         | 581/5734 [15:29:46<135:43:46, 94.82s/it] 10%|█         | 582/5734 [15:31:21<135:45:43, 94.86s/it] 10%|█         | 583/5734 [15:32:57<135:57:47, 95.02s/it] 10%|█         | 584/5734 [15:34:31<135:47:05, 94.92s/it] 10%|█         | 585/5734 [15:36:06<135:39:15, 94.84s/it] 10%|█         | 586/5734 [15:37:41<135:43:50, 94.92s/it] 10%|█         | 587/5734 [15:39:17<135:58:41, 95.11s/it] 10%|█         | 588/5734 [15:40:51<135:46:46, 94.99s/it] 10%|█         | 589/5734 [15:42:25<135:24:37, 94.75s/it] 10%|█         | 590/5734 [15:44:00<135:21:03, 94.72s/it]                                                         {'loss': 3.1857, 'learning_rate': 9.741033633445442e-05, 'epoch': 0.1}
 10%|█         | 590/5734 [15:44:00<135:21:03, 94.72s/it] 10%|█         | 591/5734 [15:45:35<135:31:50, 94.87s/it] 10%|█         | 592/5734 [15:47:10<135:36:01, 94.94s/it] 10%|█         | 593/5734 [15:48:46<135:44:26, 95.05s/it] 10%|█         | 594/5734 [15:50:20<135:19:28, 94.78s/it] 10%|█         | 595/5734 [15:51:55<135:14:03, 94.74s/it] 10%|█         | 596/5734 [15:53:29<135:12:28, 94.74s/it] 10%|█         | 597/5734 [15:55:05<135:36:16, 95.03s/it] 10%|█         | 598/5734 [15:56:40<135:24:56, 94.92s/it] 10%|█         | 599/5734 [15:58:13<134:51:59, 94.55s/it] 10%|█         | 600/5734 [15:59:48<135:03:26, 94.70s/it]                                                         {'loss': 3.1804, 'learning_rate': 9.732260572491429e-05, 'epoch': 0.1}
 10%|█         | 600/5734 [15:59:48<135:03:26, 94.70s/it] 10%|█         | 601/5734 [16:01:23<135:09:45, 94.80s/it] 10%|█         | 602/5734 [16:02:59<135:22:13, 94.96s/it] 11%|█         | 603/5734 [16:04:33<135:13:25, 94.88s/it] 11%|█         | 604/5734 [16:06:09<135:17:46, 94.94s/it] 11%|█         | 605/5734 [16:07:44<135:21:14, 95.00s/it] 11%|█         | 606/5734 [16:09:19<135:33:38, 95.17s/it] 11%|█         | 607/5734 [16:10:54<135:20:14, 95.03s/it] 11%|█         | 608/5734 [16:12:28<134:48:22, 94.67s/it] 11%|█         | 609/5734 [16:14:02<134:46:55, 94.68s/it] 11%|█         | 610/5734 [16:15:38<135:05:44, 94.92s/it]                                                         {'loss': 3.1804, 'learning_rate': 9.723345458039594e-05, 'epoch': 0.11}
 11%|█         | 610/5734 [16:15:38<135:05:44, 94.92s/it] 11%|█         | 611/5734 [16:17:13<135:09:39, 94.98s/it] 11%|█         | 612/5734 [16:18:48<135:16:18, 95.08s/it] 11%|█         | 613/5734 [16:20:23<134:56:06, 94.86s/it] 11%|█         | 614/5734 [16:21:57<134:44:18, 94.74s/it] 11%|█         | 615/5734 [16:23:32<134:44:05, 94.75s/it] 11%|█         | 616/5734 [16:25:08<135:11:50, 95.10s/it] 11%|█         | 617/5734 [16:26:42<134:54:29, 94.91s/it] 11%|█         | 618/5734 [16:28:16<134:30:26, 94.65s/it] 11%|█         | 619/5734 [16:29:52<134:46:19, 94.85s/it] 11%|█         | 620/5734 [16:31:27<134:58:34, 95.02s/it]                                                         {'loss': 3.1684, 'learning_rate': 9.714288557704782e-05, 'epoch': 0.11}
 11%|█         | 620/5734 [16:31:27<134:58:34, 95.02s/it] 11%|█         | 621/5734 [16:33:02<134:55:01, 94.99s/it] 11%|█         | 622/5734 [16:34:37<134:43:07, 94.87s/it] 11%|█         | 623/5734 [16:36:11<134:23:18, 94.66s/it] 11%|█         | 624/5734 [16:37:45<134:16:38, 94.60s/it] 11%|█         | 625/5734 [16:39:21<134:39:43, 94.89s/it] 11%|█         | 626/5734 [16:40:56<134:36:52, 94.87s/it] 11%|█         | 627/5734 [16:42:30<134:13:22, 94.62s/it] 11%|█         | 628/5734 [16:44:04<134:09:34, 94.59s/it] 11%|█         | 629/5734 [16:45:43<135:42:58, 95.71s/it] 11%|█         | 630/5734 [16:47:21<136:48:08, 96.49s/it]                                                         {'loss': 3.1539, 'learning_rate': 9.705090143357987e-05, 'epoch': 0.11}
 11%|█         | 630/5734 [16:47:21<136:48:08, 96.49s/it] 11%|█         | 631/5734 [16:48:58<137:02:05, 96.67s/it] 11%|█         | 632/5734 [16:50:42<140:09:25, 98.90s/it] 11%|█         | 633/5734 [16:52:21<139:59:37, 98.80s/it] 11%|█         | 634/5734 [16:54:06<142:54:35, 100.88s/it] 11%|█         | 635/5734 [16:55:43<140:57:05, 99.51s/it]  11%|█         | 636/5734 [16:57:19<139:30:23, 98.51s/it] 11%|█         | 637/5734 [16:58:55<138:15:41, 97.65s/it] 11%|█         | 638/5734 [17:00:30<137:22:59, 97.05s/it] 11%|█         | 639/5734 [17:02:08<137:50:00, 97.39s/it] 11%|█         | 640/5734 [17:03:43<136:29:14, 96.46s/it]                                                         {'loss': 3.1439, 'learning_rate': 9.695750491118178e-05, 'epoch': 0.11}
 11%|█         | 640/5734 [17:03:43<136:29:14, 96.46s/it] 11%|█         | 641/5734 [17:05:16<135:11:38, 95.56s/it] 11%|█         | 642/5734 [17:06:49<134:14:28, 94.91s/it] 11%|█         | 643/5734 [17:08:23<133:45:44, 94.59s/it] 11%|█         | 644/5734 [17:09:57<133:28:08, 94.40s/it] 11%|█         | 645/5734 [17:11:30<132:56:09, 94.04s/it] 11%|█▏        | 646/5734 [17:13:04<132:47:14, 93.95s/it] 11%|█▏        | 647/5734 [17:14:38<132:45:56, 93.96s/it] 11%|█▏        | 648/5734 [17:16:12<132:32:13, 93.81s/it] 11%|█▏        | 649/5734 [17:17:45<132:11:37, 93.59s/it] 11%|█▏        | 650/5734 [17:19:19<132:27:13, 93.79s/it]                                                         {'loss': 3.1529, 'learning_rate': 9.686269881344028e-05, 'epoch': 0.11}
 11%|█▏        | 650/5734 [17:19:19<132:27:13, 93.79s/it] 11%|█▏        | 651/5734 [17:20:53<132:28:16, 93.82s/it] 11%|█▏        | 652/5734 [17:22:28<133:00:10, 94.22s/it] 11%|█▏        | 653/5734 [17:24:01<132:24:10, 93.81s/it] 11%|█▏        | 654/5734 [17:25:35<132:20:28, 93.79s/it] 11%|█▏        | 655/5734 [17:27:08<132:13:32, 93.72s/it] 11%|█▏        | 656/5734 [17:28:41<132:01:37, 93.60s/it] 11%|█▏        | 657/5734 [17:30:14<131:44:52, 93.42s/it] 11%|█▏        | 658/5734 [17:31:48<131:55:11, 93.56s/it] 11%|█▏        | 659/5734 [17:33:23<132:18:51, 93.86s/it] 12%|█▏        | 660/5734 [17:34:57<132:13:04, 93.81s/it]                                                         {'loss': 3.1323, 'learning_rate': 9.676648598625477e-05, 'epoch': 0.12}
 12%|█▏        | 660/5734 [17:34:57<132:13:04, 93.81s/it] 12%|█▏        | 661/5734 [17:36:30<131:58:16, 93.65s/it] 12%|█▏        | 662/5734 [17:38:03<131:48:26, 93.55s/it] 12%|█▏        | 663/5734 [17:39:37<131:42:40, 93.50s/it] 12%|█▏        | 664/5734 [17:41:10<131:44:52, 93.55s/it] 12%|█▏        | 665/5734 [17:42:44<131:40:30, 93.52s/it] 12%|█▏        | 666/5734 [17:44:17<131:41:03, 93.54s/it] 12%|█▏        | 667/5734 [17:45:51<131:42:32, 93.58s/it] 12%|█▏        | 668/5734 [17:47:25<132:03:06, 93.84s/it] 12%|█▏        | 669/5734 [17:48:59<131:47:55, 93.68s/it] 12%|█▏        | 670/5734 [17:50:32<131:43:32, 93.64s/it]                                                         {'loss': 3.1423, 'learning_rate': 9.666886931775212e-05, 'epoch': 0.12}
 12%|█▏        | 670/5734 [17:50:32<131:43:32, 93.64s/it] 12%|█▏        | 671/5734 [17:52:06<131:48:49, 93.72s/it] 12%|█▏        | 672/5734 [17:53:40<131:40:17, 93.64s/it] 12%|█▏        | 673/5734 [17:55:13<131:33:34, 93.58s/it] 12%|█▏        | 674/5734 [17:56:47<131:28:30, 93.54s/it] 12%|█▏        | 675/5734 [17:58:21<131:49:44, 93.81s/it] 12%|█▏        | 676/5734 [17:59:54<131:24:22, 93.53s/it] 12%|█▏        | 677/5734 [18:01:27<131:25:03, 93.55s/it] 12%|█▏        | 678/5734 [18:03:01<131:18:01, 93.49s/it] 12%|█▏        | 679/5734 [18:04:34<131:17:02, 93.50s/it] 12%|█▏        | 680/5734 [18:06:07<130:58:39, 93.30s/it]                                                         {'loss': 3.1376, 'learning_rate': 9.656985173819976e-05, 'epoch': 0.12}
 12%|█▏        | 680/5734 [18:06:07<130:58:39, 93.30s/it] 12%|█▏        | 681/5734 [18:07:41<131:03:47, 93.38s/it] 12%|█▏        | 682/5734 [18:09:15<131:17:04, 93.55s/it] 12%|█▏        | 683/5734 [18:10:48<131:09:50, 93.48s/it] 12%|█▏        | 684/5734 [18:12:22<131:11:23, 93.52s/it] 12%|█▏        | 685/5734 [18:13:55<131:06:25, 93.48s/it] 12%|█▏        | 686/5734 [18:15:29<131:13:09, 93.58s/it] 12%|█▏        | 687/5734 [18:17:02<131:07:34, 93.53s/it] 12%|█▏        | 688/5734 [18:18:36<131:07:16, 93.55s/it] 12%|█▏        | 689/5734 [18:20:10<131:10:14, 93.60s/it] 12%|█▏        | 690/5734 [18:21:43<130:57:47, 93.47s/it]                                                         {'loss': 3.1332, 'learning_rate': 9.646943621991787e-05, 'epoch': 0.12}
 12%|█▏        | 690/5734 [18:21:43<130:57:47, 93.47s/it] 12%|█▏        | 691/5734 [18:23:16<130:58:38, 93.50s/it] 12%|█▏        | 692/5734 [18:24:49<130:47:47, 93.39s/it] 12%|█▏        | 693/5734 [18:26:23<130:47:24, 93.40s/it] 12%|█▏        | 694/5734 [18:27:56<130:50:11, 93.45s/it] 12%|█▏        | 695/5734 [18:29:30<130:48:46, 93.46s/it] 12%|█▏        | 696/5734 [18:31:04<131:01:00, 93.62s/it] 12%|█▏        | 697/5734 [18:32:37<130:52:01, 93.53s/it] 12%|█▏        | 698/5734 [18:34:10<130:41:10, 93.42s/it] 12%|█▏        | 699/5734 [18:35:44<130:34:07, 93.36s/it] 12%|█▏        | 700/5734 [18:37:18<130:47:35, 93.53s/it]                                                         {'loss': 3.1268, 'learning_rate': 9.636762577719007e-05, 'epoch': 0.12}
 12%|█▏        | 700/5734 [18:37:18<130:47:35, 93.53s/it] 12%|█▏        | 701/5734 [18:38:50<130:30:36, 93.35s/it] 12%|█▏        | 702/5734 [18:40:24<130:31:36, 93.38s/it] 12%|█▏        | 703/5734 [18:41:58<130:35:39, 93.45s/it] 12%|█▏        | 704/5734 [18:43:31<130:35:39, 93.47s/it] 12%|█▏        | 705/5734 [18:45:04<130:34:07, 93.47s/it] 12%|█▏        | 706/5734 [18:46:38<130:32:16, 93.46s/it] 12%|█▏        | 707/5734 [18:48:13<131:13:06, 93.97s/it] 12%|█▏        | 708/5734 [18:49:46<130:43:17, 93.63s/it] 12%|█▏        | 709/5734 [18:51:19<130:31:41, 93.51s/it] 12%|█▏        | 710/5734 [18:52:53<130:36:47, 93.59s/it]                                                         {'loss': 3.1177, 'learning_rate': 9.626442346617301e-05, 'epoch': 0.12}
 12%|█▏        | 710/5734 [18:52:53<130:36:47, 93.59s/it] 12%|█▏        | 711/5734 [18:54:26<130:30:16, 93.53s/it] 12%|█▏        | 712/5734 [18:55:59<130:04:57, 93.25s/it] 12%|█▏        | 713/5734 [18:57:32<130:10:23, 93.33s/it] 12%|█▏        | 714/5734 [18:59:07<130:27:23, 93.55s/it] 12%|█▏        | 715/5734 [19:00:40<130:26:53, 93.57s/it] 12%|█▏        | 716/5734 [19:02:13<130:00:03, 93.26s/it] 13%|█▎        | 717/5734 [19:03:46<130:07:38, 93.37s/it] 13%|█▎        | 718/5734 [19:05:20<130:12:49, 93.45s/it] 13%|█▎        | 719/5734 [19:06:54<130:16:38, 93.52s/it] 13%|█▎        | 720/5734 [19:08:27<130:18:32, 93.56s/it]                                                         {'loss': 3.1234, 'learning_rate': 9.615983238480456e-05, 'epoch': 0.13}
 13%|█▎        | 720/5734 [19:08:27<130:18:32, 93.56s/it] 13%|█▎        | 721/5734 [19:10:01<130:28:52, 93.70s/it] 13%|█▎        | 722/5734 [19:11:35<130:20:44, 93.62s/it] 13%|█▎        | 723/5734 [19:13:08<130:15:18, 93.58s/it] 13%|█▎        | 724/5734 [19:14:41<130:00:56, 93.42s/it] 13%|█▎        | 725/5734 [19:16:15<130:05:13, 93.49s/it] 13%|█▎        | 726/5734 [19:17:49<130:05:36, 93.52s/it] 13%|█▎        | 727/5734 [19:19:22<129:57:29, 93.44s/it] 13%|█▎        | 728/5734 [19:20:55<129:56:58, 93.45s/it] 13%|█▎        | 729/5734 [19:22:29<129:58:19, 93.49s/it] 13%|█▎        | 730/5734 [19:24:03<130:03:08, 93.56s/it]                                                         {'loss': 3.1121, 'learning_rate': 9.605385567271084e-05, 'epoch': 0.13}
 13%|█▎        | 730/5734 [19:24:03<130:03:08, 93.56s/it] 13%|█▎        | 731/5734 [19:25:36<129:52:00, 93.45s/it] 13%|█▎        | 732/5734 [19:27:09<129:48:52, 93.43s/it] 13%|█▎        | 733/5734 [19:28:42<129:40:48, 93.35s/it] 13%|█▎        | 734/5734 [19:30:16<129:40:03, 93.36s/it] 13%|█▎        | 735/5734 [19:31:49<129:42:34, 93.41s/it] 13%|█▎        | 736/5734 [19:33:22<129:36:27, 93.35s/it] 13%|█▎        | 737/5734 [19:34:56<129:42:42, 93.45s/it] 13%|█▎        | 738/5734 [19:36:29<129:35:49, 93.38s/it] 13%|█▎        | 739/5734 [19:38:03<129:41:40, 93.47s/it] 13%|█▎        | 740/5734 [19:39:36<129:22:51, 93.27s/it]                                                         {'loss': 3.115, 'learning_rate': 9.594649651111199e-05, 'epoch': 0.13}
 13%|█▎        | 740/5734 [19:39:36<129:22:51, 93.27s/it] 13%|█▎        | 741/5734 [19:41:09<129:18:45, 93.24s/it] 13%|█▎        | 742/5734 [19:42:43<129:27:31, 93.36s/it] 13%|█▎        | 743/5734 [19:44:16<129:26:48, 93.37s/it] 13%|█▎        | 744/5734 [19:45:49<129:06:47, 93.15s/it] 13%|█▎        | 745/5734 [19:47:22<129:04:59, 93.14s/it] 13%|█▎        | 746/5734 [19:48:56<129:27:18, 93.43s/it] 13%|█▎        | 747/5734 [19:50:29<129:18:47, 93.35s/it] 13%|█▎        | 748/5734 [19:52:02<129:06:04, 93.21s/it] 13%|█▎        | 749/5734 [19:53:36<129:18:13, 93.38s/it] 13%|█▎        | 750/5734 [19:55:10<129:32:59, 93.58s/it]                                                         {'loss': 3.1074, 'learning_rate': 9.583775812272666e-05, 'epoch': 0.13}
 13%|█▎        | 750/5734 [19:55:10<129:32:59, 93.58s/it] 13%|█▎        | 751/5734 [19:56:43<129:26:17, 93.51s/it] 13%|█▎        | 752/5734 [19:58:16<129:15:02, 93.40s/it] 13%|█▎        | 753/5734 [19:59:50<129:24:18, 93.53s/it] 13%|█▎        | 754/5734 [20:01:23<129:18:37, 93.48s/it] 13%|█▎        | 755/5734 [20:02:56<129:05:48, 93.34s/it] 13%|█▎        | 756/5734 [20:04:30<128:59:20, 93.28s/it] 13%|█▎        | 757/5734 [20:06:03<129:02:02, 93.33s/it] 13%|█▎        | 758/5734 [20:07:37<129:04:37, 93.38s/it] 13%|█▎        | 759/5734 [20:09:10<128:59:07, 93.34s/it] 13%|█▎        | 760/5734 [20:10:43<128:50:17, 93.25s/it]                                                         {'loss': 3.1009, 'learning_rate': 9.572764377167529e-05, 'epoch': 0.13}
 13%|█▎        | 760/5734 [20:10:43<128:50:17, 93.25s/it] 13%|█▎        | 761/5734 [20:12:16<128:48:35, 93.25s/it] 13%|█▎        | 762/5734 [20:13:51<129:20:21, 93.65s/it] 13%|█▎        | 763/5734 [20:15:24<129:13:45, 93.59s/it] 13%|█▎        | 764/5734 [20:16:57<129:04:19, 93.49s/it] 13%|█▎        | 765/5734 [20:18:30<128:52:35, 93.37s/it] 13%|█▎        | 766/5734 [20:20:04<128:54:31, 93.41s/it] 13%|█▎        | 767/5734 [20:21:37<128:55:59, 93.45s/it] 13%|█▎        | 768/5734 [20:23:11<128:44:19, 93.33s/it] 13%|█▎        | 769/5734 [20:24:44<128:39:44, 93.29s/it] 13%|█▎        | 770/5734 [20:26:18<128:51:55, 93.46s/it]                                                         {'loss': 3.0988, 'learning_rate': 9.561615676338212e-05, 'epoch': 0.13}
 13%|█▎        | 770/5734 [20:26:18<128:51:55, 93.46s/it] 13%|█▎        | 771/5734 [20:27:51<128:55:56, 93.52s/it] 13%|█▎        | 772/5734 [20:29:24<128:38:54, 93.34s/it] 13%|█▎        | 773/5734 [20:30:58<128:39:59, 93.37s/it] 13%|█▎        | 774/5734 [20:32:31<128:44:40, 93.44s/it] 14%|█▎        | 775/5734 [20:34:05<128:58:31, 93.63s/it] 14%|█▎        | 776/5734 [20:35:38<128:40:29, 93.43s/it] 14%|█▎        | 777/5734 [20:37:12<128:42:44, 93.48s/it] 14%|█▎        | 778/5734 [20:38:46<128:50:24, 93.59s/it] 14%|█▎        | 779/5734 [20:40:19<128:44:32, 93.54s/it] 14%|█▎        | 780/5734 [20:41:52<128:36:42, 93.46s/it]                                                         {'loss': 3.0901, 'learning_rate': 9.550330044447591e-05, 'epoch': 0.14}
 14%|█▎        | 780/5734 [20:41:52<128:36:42, 93.46s/it] 14%|█▎        | 781/5734 [20:43:26<128:42:13, 93.55s/it] 14%|█▎        | 782/5734 [20:45:00<128:47:03, 93.62s/it] 14%|█▎        | 783/5734 [20:46:33<128:30:13, 93.44s/it] 14%|█▎        | 784/5734 [20:48:06<128:23:45, 93.38s/it] 14%|█▎        | 785/5734 [20:49:40<128:38:43, 93.58s/it] 14%|█▎        | 786/5734 [20:51:13<128:25:34, 93.44s/it] 14%|█▎        | 787/5734 [20:52:47<128:22:41, 93.42s/it] 14%|█▎        | 788/5734 [20:54:19<128:02:25, 93.20s/it] 14%|█▍        | 789/5734 [20:55:53<128:11:28, 93.32s/it] 14%|█▍        | 790/5734 [20:57:26<128:05:02, 93.27s/it]                                                         {'loss': 3.0831, 'learning_rate': 9.538907820268958e-05, 'epoch': 0.14}
 14%|█▍        | 790/5734 [20:57:26<128:05:02, 93.27s/it] 14%|█▍        | 791/5734 [20:58:59<128:01:05, 93.24s/it] 14%|█▍        | 792/5734 [21:00:33<128:02:54, 93.28s/it] 14%|█▍        | 793/5734 [21:02:06<127:54:42, 93.20s/it] 14%|█▍        | 794/5734 [21:03:39<128:01:53, 93.30s/it] 14%|█▍        | 795/5734 [21:05:12<127:50:29, 93.18s/it] 14%|█▍        | 796/5734 [21:06:46<127:58:30, 93.30s/it] 14%|█▍        | 797/5734 [21:08:18<127:43:19, 93.13s/it] 14%|█▍        | 798/5734 [21:09:52<127:44:31, 93.17s/it] 14%|█▍        | 799/5734 [21:11:25<127:48:39, 93.24s/it] 14%|█▍        | 800/5734 [21:12:59<127:52:26, 93.30s/it]                                                         {'loss': 3.0864, 'learning_rate': 9.527349346675847e-05, 'epoch': 0.14}
 14%|█▍        | 800/5734 [21:12:59<127:52:26, 93.30s/it] 14%|█▍        | 801/5734 [21:14:32<127:43:20, 93.21s/it] 14%|█▍        | 802/5734 [21:16:05<127:41:56, 93.21s/it] 14%|█▍        | 803/5734 [21:17:38<127:51:13, 93.34s/it] 14%|█▍        | 804/5734 [21:19:11<127:35:53, 93.18s/it] 14%|█▍        | 805/5734 [21:20:44<127:30:50, 93.13s/it] 14%|█▍        | 806/5734 [21:22:18<127:42:44, 93.30s/it] 14%|█▍        | 807/5734 [21:23:52<127:57:07, 93.49s/it] 14%|█▍        | 808/5734 [21:25:25<127:43:55, 93.35s/it] 14%|█▍        | 809/5734 [21:26:58<127:47:10, 93.41s/it] 14%|█▍        | 810/5734 [21:28:32<127:55:35, 93.53s/it]                                                         {'loss': 3.0766, 'learning_rate': 9.515654970631735e-05, 'epoch': 0.14}
 14%|█▍        | 810/5734 [21:28:32<127:55:35, 93.53s/it] 14%|█▍        | 811/5734 [21:30:06<127:54:31, 93.53s/it] 14%|█▍        | 812/5734 [21:31:39<127:48:46, 93.48s/it] 14%|█▍        | 813/5734 [21:33:13<127:44:36, 93.45s/it] 14%|█▍        | 814/5734 [21:34:46<127:43:44, 93.46s/it] 14%|█▍        | 815/5734 [21:36:19<127:38:08, 93.41s/it] 14%|█▍        | 816/5734 [21:37:52<127:30:39, 93.34s/it] 14%|█▍        | 817/5734 [21:39:27<127:48:29, 93.58s/it] 14%|█▍        | 818/5734 [21:41:00<127:32:48, 93.40s/it] 14%|█▍        | 819/5734 [21:42:33<127:29:15, 93.38s/it] 14%|█▍        | 820/5734 [21:44:06<127:14:04, 93.21s/it]                                                         {'loss': 3.0666, 'learning_rate': 9.503825043179643e-05, 'epoch': 0.14}
 14%|█▍        | 820/5734 [21:44:06<127:14:04, 93.21s/it] 14%|█▍        | 821/5734 [21:45:39<127:20:07, 93.30s/it] 14%|█▍        | 822/5734 [21:47:12<127:10:45, 93.21s/it] 14%|█▍        | 823/5734 [21:48:45<127:08:32, 93.20s/it] 14%|█▍        | 824/5734 [21:50:19<127:10:29, 93.24s/it] 14%|█▍        | 825/5734 [21:51:52<127:19:25, 93.37s/it] 14%|█▍        | 826/5734 [21:53:26<127:16:14, 93.35s/it] 14%|█▍        | 827/5734 [21:54:59<127:03:49, 93.22s/it] 14%|█▍        | 828/5734 [21:56:32<127:10:34, 93.32s/it] 14%|█▍        | 829/5734 [21:58:05<127:07:43, 93.31s/it] 14%|█▍        | 830/5734 [21:59:39<127:05:15, 93.29s/it]                                                         {'loss': 3.0685, 'learning_rate': 9.491859919431579e-05, 'epoch': 0.14}
 14%|█▍        | 830/5734 [21:59:39<127:05:15, 93.29s/it] 14%|█▍        | 831/5734 [22:01:12<127:10:17, 93.38s/it] 15%|█▍        | 832/5734 [22:02:46<127:18:32, 93.49s/it] 15%|█▍        | 833/5734 [22:04:19<127:08:41, 93.39s/it] 15%|█▍        | 834/5734 [22:05:52<127:02:35, 93.34s/it] 15%|█▍        | 835/5734 [22:07:26<127:11:02, 93.46s/it] 15%|█▍        | 836/5734 [22:08:59<127:04:23, 93.40s/it] 15%|█▍        | 837/5734 [22:10:32<126:46:23, 93.20s/it] 15%|█▍        | 838/5734 [22:12:05<126:44:02, 93.19s/it] 15%|█▍        | 839/5734 [22:13:39<126:58:19, 93.38s/it] 15%|█▍        | 840/5734 [22:15:12<126:52:27, 93.33s/it]                                                         {'loss': 3.0658, 'learning_rate': 9.479759958557893e-05, 'epoch': 0.15}
 15%|█▍        | 840/5734 [22:15:12<126:52:27, 93.33s/it] 15%|█▍        | 841/5734 [22:16:45<126:37:34, 93.16s/it] 15%|█▍        | 842/5734 [22:18:18<126:26:14, 93.04s/it] 15%|█▍        | 843/5734 [22:19:51<126:35:22, 93.18s/it] 15%|█▍        | 844/5734 [22:21:24<126:16:15, 92.96s/it] 15%|█▍        | 845/5734 [22:22:57<126:22:27, 93.06s/it] 15%|█▍        | 846/5734 [22:24:30<126:27:37, 93.14s/it] 15%|█▍        | 847/5734 [22:26:04<126:29:55, 93.19s/it] 15%|█▍        | 848/5734 [22:27:36<126:04:32, 92.89s/it] 15%|█▍        | 849/5734 [22:29:09<126:07:12, 92.94s/it] 15%|█▍        | 850/5734 [22:30:42<126:17:48, 93.09s/it]                                                         {'loss': 3.0711, 'learning_rate': 9.46752552377649e-05, 'epoch': 0.15}
 15%|█▍        | 850/5734 [22:30:42<126:17:48, 93.09s/it] 15%|█▍        | 851/5734 [22:32:16<126:15:48, 93.09s/it] 15%|█▍        | 852/5734 [22:33:48<126:01:48, 92.94s/it] 15%|█▍        | 853/5734 [22:35:22<126:13:39, 93.10s/it] 15%|█▍        | 854/5734 [22:36:55<126:24:01, 93.25s/it] 15%|█▍        | 855/5734 [22:38:29<126:23:46, 93.26s/it] 15%|█▍        | 856/5734 [22:40:01<126:08:25, 93.09s/it] 15%|█▍        | 857/5734 [22:41:35<126:15:46, 93.20s/it] 15%|█▍        | 858/5734 [22:43:08<126:17:54, 93.25s/it] 15%|█▍        | 859/5734 [22:44:41<126:13:05, 93.21s/it] 15%|█▍        | 860/5734 [22:46:14<126:04:20, 93.12s/it]                                                         {'loss': 3.0627, 'learning_rate': 9.455156982341923e-05, 'epoch': 0.15}
 15%|█▍        | 860/5734 [22:46:14<126:04:20, 93.12s/it] 15%|█▌        | 861/5734 [22:47:48<126:22:45, 93.36s/it] 15%|█▌        | 862/5734 [22:49:21<126:13:34, 93.27s/it] 15%|█▌        | 863/5734 [22:50:54<126:01:47, 93.14s/it] 15%|█▌        | 864/5734 [22:52:27<126:02:12, 93.17s/it] 15%|█▌        | 865/5734 [22:54:00<126:03:54, 93.21s/it] 15%|█▌        | 866/5734 [22:55:34<126:05:58, 93.25s/it] 15%|█▌        | 867/5734 [22:57:07<125:52:02, 93.10s/it] 15%|█▌        | 868/5734 [22:58:40<125:58:05, 93.19s/it] 15%|█▌        | 869/5734 [23:00:13<125:52:00, 93.14s/it] 15%|█▌        | 870/5734 [23:01:46<125:53:15, 93.17s/it]                                                         {'loss': 3.0572, 'learning_rate': 9.442654705534377e-05, 'epoch': 0.15}
 15%|█▌        | 870/5734 [23:01:46<125:53:15, 93.17s/it] 15%|█▌        | 871/5734 [23:03:20<125:56:12, 93.23s/it] 15%|█▌        | 872/5734 [23:04:53<126:12:03, 93.44s/it] 15%|█▌        | 873/5734 [23:06:27<126:06:23, 93.39s/it] 15%|█▌        | 874/5734 [23:08:00<126:08:35, 93.44s/it] 15%|█▌        | 875/5734 [23:09:34<126:03:45, 93.40s/it] 15%|█▌        | 876/5734 [23:11:06<125:49:20, 93.24s/it] 15%|█▌        | 877/5734 [23:12:40<126:03:36, 93.44s/it] 15%|█▌        | 878/5734 [23:14:15<126:21:36, 93.68s/it] 15%|█▌        | 879/5734 [23:15:48<126:22:16, 93.70s/it] 15%|█▌        | 880/5734 [23:17:21<126:06:12, 93.53s/it]                                                         {'loss': 3.0492, 'learning_rate': 9.430019068648521e-05, 'epoch': 0.15}
 15%|█▌        | 880/5734 [23:17:21<126:06:12, 93.53s/it] 15%|█▌        | 881/5734 [23:18:55<125:57:59, 93.44s/it] 15%|█▌        | 882/5734 [23:20:28<125:50:47, 93.37s/it] 15%|█▌        | 883/5734 [23:22:02<125:59:04, 93.50s/it] 15%|█▌        | 884/5734 [23:23:34<125:35:30, 93.22s/it] 15%|█▌        | 885/5734 [23:25:08<125:36:23, 93.25s/it] 15%|█▌        | 886/5734 [23:26:41<125:45:40, 93.39s/it] 15%|█▌        | 887/5734 [23:28:15<125:44:07, 93.39s/it] 15%|█▌        | 888/5734 [23:29:47<125:21:36, 93.13s/it] 16%|█▌        | 889/5734 [23:31:20<125:20:29, 93.13s/it] 16%|█▌        | 890/5734 [23:32:54<125:39:09, 93.38s/it]                                                         {'loss': 3.05, 'learning_rate': 9.417250450982234e-05, 'epoch': 0.16}
 16%|█▌        | 890/5734 [23:32:54<125:39:09, 93.38s/it] 16%|█▌        | 891/5734 [23:34:28<125:34:40, 93.35s/it] 16%|█▌        | 892/5734 [23:36:01<125:22:38, 93.22s/it] 16%|█▌        | 893/5734 [23:37:34<125:34:16, 93.38s/it] 16%|█▌        | 894/5734 [23:39:08<125:40:23, 93.48s/it] 16%|█▌        | 895/5734 [23:40:41<125:20:16, 93.25s/it] 16%|█▌        | 896/5734 [23:42:14<125:13:05, 93.18s/it] 16%|█▌        | 897/5734 [23:43:47<125:24:12, 93.33s/it] 16%|█▌        | 898/5734 [23:45:21<125:19:12, 93.29s/it] 16%|█▌        | 899/5734 [23:46:53<124:57:00, 93.03s/it] 16%|█▌        | 900/5734 [23:48:26<124:55:04, 93.03s/it]                                                         {'loss': 3.0397, 'learning_rate': 9.404349235825231e-05, 'epoch': 0.16}
 16%|█▌        | 900/5734 [23:48:26<124:55:04, 93.03s/it] 16%|█▌        | 901/5734 [23:50:00<125:04:02, 93.16s/it] 16%|█▌        | 902/5734 [23:51:33<125:03:34, 93.17s/it] 16%|█▌        | 903/5734 [23:53:06<124:58:34, 93.13s/it] 16%|█▌        | 904/5734 [23:54:40<125:12:08, 93.32s/it] 16%|█▌        | 905/5734 [23:56:13<125:07:00, 93.27s/it] 16%|█▌        | 906/5734 [23:57:46<125:10:52, 93.34s/it] 16%|█▌        | 907/5734 [23:59:19<124:55:55, 93.17s/it] 16%|█▌        | 908/5734 [24:00:52<124:58:59, 93.23s/it] 16%|█▌        | 909/5734 [24:02:25<124:54:16, 93.19s/it] 16%|█▌        | 910/5734 [24:03:59<124:53:27, 93.20s/it]                                                         {'loss': 3.0428, 'learning_rate': 9.391315810447552e-05, 'epoch': 0.16}
 16%|█▌        | 910/5734 [24:03:59<124:53:27, 93.20s/it] 16%|█▌        | 911/5734 [24:05:32<124:45:43, 93.13s/it] 16%|█▌        | 912/5734 [24:07:05<124:47:53, 93.17s/it] 16%|█▌        | 913/5734 [24:08:38<124:49:21, 93.21s/it] 16%|█▌        | 914/5734 [24:10:11<124:43:18, 93.15s/it] 16%|█▌        | 915/5734 [24:11:45<124:55:51, 93.33s/it] 16%|█▌        | 916/5734 [24:13:18<124:55:25, 93.34s/it] 16%|█▌        | 917/5734 [24:14:52<124:51:00, 93.31s/it] 16%|█▌        | 918/5734 [24:16:25<124:47:40, 93.28s/it] 16%|█▌        | 919/5734 [24:17:59<124:56:20, 93.41s/it] 16%|█▌        | 920/5734 [24:19:31<124:38:42, 93.21s/it]                                                         {'loss': 3.0421, 'learning_rate': 9.378150566087938e-05, 'epoch': 0.16}
 16%|█▌        | 920/5734 [24:19:31<124:38:42, 93.21s/it] 16%|█▌        | 921/5734 [24:21:04<124:36:49, 93.21s/it] 16%|█▌        | 922/5734 [24:22:38<124:47:26, 93.36s/it] 16%|█▌        | 923/5734 [24:24:11<124:34:47, 93.22s/it] 16%|█▌        | 924/5734 [24:25:44<124:29:34, 93.18s/it] 16%|█▌        | 925/5734 [24:27:17<124:27:04, 93.16s/it] 16%|█▌        | 926/5734 [24:28:51<124:50:22, 93.47s/it] 16%|█▌        | 927/5734 [24:30:24<124:37:26, 93.33s/it] 16%|█▌        | 928/5734 [24:31:58<124:32:52, 93.29s/it] 16%|█▌        | 929/5734 [24:33:31<124:28:17, 93.26s/it] 16%|█▌        | 930/5734 [24:35:05<124:37:25, 93.39s/it]                                                         {'loss': 3.0358, 'learning_rate': 9.364853897942082e-05, 'epoch': 0.16}
 16%|█▌        | 930/5734 [24:35:05<124:37:25, 93.39s/it] 16%|█▌        | 931/5734 [24:36:37<124:18:28, 93.17s/it] 16%|█▋        | 932/5734 [24:38:10<124:12:21, 93.12s/it] 16%|█▋        | 933/5734 [24:39:44<124:22:38, 93.26s/it] 16%|█▋        | 934/5734 [24:41:17<124:21:02, 93.26s/it] 16%|█▋        | 935/5734 [24:42:50<124:09:11, 93.13s/it] 16%|█▋        | 936/5734 [24:44:23<124:06:18, 93.12s/it] 16%|█▋        | 937/5734 [24:45:57<124:31:15, 93.45s/it] 16%|█▋        | 938/5734 [24:47:32<124:51:43, 93.72s/it] 16%|█▋        | 939/5734 [24:49:04<124:31:18, 93.49s/it] 16%|█▋        | 940/5734 [24:50:38<124:39:57, 93.62s/it]                                                         {'loss': 3.026, 'learning_rate': 9.351426205150777e-05, 'epoch': 0.16}
 16%|█▋        | 940/5734 [24:50:38<124:39:57, 93.62s/it] 16%|█▋        | 941/5734 [24:52:12<124:33:09, 93.55s/it] 16%|█▋        | 942/5734 [24:53:45<124:26:21, 93.49s/it] 16%|█▋        | 943/5734 [24:55:18<124:07:26, 93.27s/it] 16%|█▋        | 944/5734 [24:56:51<124:01:22, 93.21s/it] 16%|█▋        | 945/5734 [24:58:24<124:06:53, 93.30s/it] 16%|█▋        | 946/5734 [24:59:58<123:58:41, 93.22s/it] 17%|█▋        | 947/5734 [25:01:31<123:57:48, 93.23s/it] 17%|█▋        | 948/5734 [25:03:04<123:47:24, 93.11s/it] 17%|█▋        | 949/5734 [25:04:37<123:54:48, 93.23s/it] 17%|█▋        | 950/5734 [25:06:10<123:54:23, 93.24s/it]                                                         {'loss': 3.0277, 'learning_rate': 9.337867890787921e-05, 'epoch': 0.17}
 17%|█▋        | 950/5734 [25:06:10<123:54:23, 93.24s/it] 17%|█▋        | 951/5734 [25:07:44<123:51:21, 93.22s/it] 17%|█▋        | 952/5734 [25:09:16<123:37:56, 93.07s/it] 17%|█▋        | 953/5734 [25:10:49<123:35:15, 93.06s/it] 17%|█▋        | 954/5734 [25:12:23<123:37:38, 93.11s/it] 17%|█▋        | 955/5734 [25:13:56<123:34:56, 93.09s/it] 17%|█▋        | 956/5734 [25:15:28<123:25:53, 93.00s/it] 17%|█▋        | 957/5734 [25:17:02<123:29:42, 93.07s/it] 17%|█▋        | 958/5734 [25:18:35<123:43:14, 93.26s/it] 17%|█▋        | 959/5734 [25:20:08<123:30:46, 93.12s/it] 17%|█▋        | 960/5734 [25:21:41<123:29:19, 93.12s/it]                                                         {'loss': 3.0251, 'learning_rate': 9.324179361848428e-05, 'epoch': 0.17}
 17%|█▋        | 960/5734 [25:21:41<123:29:19, 93.12s/it] 17%|█▋        | 961/5734 [25:23:14<123:26:47, 93.11s/it] 17%|█▋        | 962/5734 [25:24:48<123:36:24, 93.25s/it] 17%|█▋        | 963/5734 [25:26:21<123:20:46, 93.07s/it] 17%|█▋        | 964/5734 [25:27:54<123:21:11, 93.10s/it] 17%|█▋        | 965/5734 [25:29:27<123:20:29, 93.11s/it] 17%|█▋        | 966/5734 [25:31:00<123:21:54, 93.14s/it] 17%|█▋        | 967/5734 [25:32:32<123:00:31, 92.90s/it] 17%|█▋        | 968/5734 [25:34:06<123:05:07, 92.97s/it] 17%|█▋        | 969/5734 [25:35:40<123:33:58, 93.36s/it] 17%|█▋        | 970/5734 [25:37:14<123:45:38, 93.52s/it]                                                         {'loss': 3.0262, 'learning_rate': 9.310361029236007e-05, 'epoch': 0.17}
 17%|█▋        | 970/5734 [25:37:14<123:45:38, 93.52s/it] 17%|█▋        | 971/5734 [25:38:47<123:50:01, 93.60s/it] 17%|█▋        | 972/5734 [25:40:21<123:52:33, 93.65s/it] 17%|█▋        | 973/5734 [25:41:55<123:55:53, 93.71s/it] 17%|█▋        | 974/5734 [25:43:29<123:57:54, 93.76s/it] 17%|█▋        | 975/5734 [25:45:03<123:53:42, 93.72s/it] 17%|█▋        | 976/5734 [25:46:37<124:03:18, 93.86s/it] 17%|█▋        | 977/5734 [25:48:11<124:02:02, 93.87s/it] 17%|█▋        | 978/5734 [25:49:44<123:46:43, 93.69s/it] 17%|█▋        | 979/5734 [25:51:17<123:40:16, 93.63s/it] 17%|█▋        | 980/5734 [25:52:51<123:46:31, 93.73s/it]                                                         {'loss': 3.0199, 'learning_rate': 9.296413307750824e-05, 'epoch': 0.17}
 17%|█▋        | 980/5734 [25:52:51<123:46:31, 93.73s/it] 17%|█▋        | 981/5734 [25:54:25<123:45:10, 93.73s/it] 17%|█▋        | 982/5734 [25:55:59<123:40:57, 93.70s/it] 17%|█▋        | 983/5734 [25:57:33<123:46:53, 93.79s/it] 17%|█▋        | 984/5734 [25:59:06<123:35:49, 93.67s/it] 17%|█▋        | 985/5734 [26:00:40<123:31:16, 93.64s/it] 17%|█▋        | 986/5734 [26:02:13<123:28:57, 93.63s/it] 17%|█▋        | 987/5734 [26:03:47<123:37:37, 93.76s/it] 17%|█▋        | 988/5734 [26:05:21<123:41:06, 93.82s/it] 17%|█▋        | 989/5734 [26:06:54<123:22:43, 93.61s/it] 17%|█▋        | 990/5734 [26:08:29<123:38:17, 93.82s/it]                                                         {'loss': 3.0193, 'learning_rate': 9.282336616077058e-05, 'epoch': 0.17}
 17%|█▋        | 990/5734 [26:08:29<123:38:17, 93.82s/it] 17%|█▋        | 991/5734 [26:10:02<123:29:19, 93.73s/it] 17%|█▋        | 992/5734 [26:11:36<123:22:51, 93.67s/it] 17%|█▋        | 993/5734 [26:13:09<123:19:40, 93.65s/it] 17%|█▋        | 994/5734 [26:14:43<123:22:21, 93.70s/it] 17%|█▋        | 995/5734 [26:16:17<123:27:39, 93.79s/it] 17%|█▋        | 996/5734 [26:17:51<123:28:46, 93.82s/it] 17%|█▋        | 997/5734 [26:19:25<123:29:28, 93.85s/it] 17%|█▋        | 998/5734 [26:20:59<123:24:06, 93.80s/it] 17%|█▋        | 999/5734 [26:22:33<123:28:30, 93.88s/it] 17%|█▋        | 1000/5734 [26:24:07<123:38:05, 94.02s/it]                                                          {'loss': 3.0244, 'learning_rate': 9.268131376770326e-05, 'epoch': 0.17}
 17%|█▋        | 1000/5734 [26:24:07<123:38:05, 94.02s/it] 17%|█▋        | 1001/5734 [26:25:41<123:28:39, 93.92s/it] 17%|█▋        | 1002/5734 [26:27:14<123:17:07, 93.79s/it] 17%|█▋        | 1003/5734 [26:28:48<123:11:56, 93.75s/it] 18%|█▊        | 1004/5734 [26:30:22<123:25:36, 93.94s/it] 18%|█▊        | 1005/5734 [26:31:56<123:24:52, 93.95s/it] 18%|█▊        | 1006/5734 [26:33:29<123:01:18, 93.67s/it] 18%|█▊        | 1007/5734 [26:35:04<123:11:50, 93.82s/it] 18%|█▊        | 1008/5734 [26:36:37<123:04:20, 93.75s/it] 18%|█▊        | 1009/5734 [26:38:11<122:59:49, 93.71s/it] 18%|█▊        | 1010/5734 [26:39:44<122:53:07, 93.65s/it]                                                          {'loss': 3.0253, 'learning_rate': 9.253798016245002e-05, 'epoch': 0.18}
 18%|█▊        | 1010/5734 [26:39:44<122:53:07, 93.65s/it] 18%|█▊        | 1011/5734 [26:41:18<122:57:10, 93.72s/it] 18%|█▊        | 1012/5734 [26:42:52<122:52:55, 93.68s/it] 18%|█▊        | 1013/5734 [26:44:25<122:40:25, 93.54s/it] 18%|█▊        | 1014/5734 [26:45:59<122:48:45, 93.67s/it] 18%|█▊        | 1015/5734 [26:47:32<122:40:12, 93.58s/it] 18%|█▊        | 1016/5734 [26:49:06<122:43:31, 93.64s/it] 18%|█▊        | 1017/5734 [26:50:40<122:42:02, 93.64s/it] 18%|█▊        | 1018/5734 [26:52:14<122:45:25, 93.71s/it] 18%|█▊        | 1019/5734 [26:53:47<122:35:21, 93.60s/it] 18%|█▊        | 1020/5734 [26:55:21<122:35:17, 93.62s/it]                                                          {'loss': 3.0085, 'learning_rate': 9.239336964761418e-05, 'epoch': 0.18}
 18%|█▊        | 1020/5734 [26:55:21<122:35:17, 93.62s/it] 18%|█▊        | 1021/5734 [26:56:55<122:46:56, 93.79s/it] 18%|█▊        | 1022/5734 [26:58:28<122:30:32, 93.60s/it] 18%|█▊        | 1023/5734 [27:00:01<122:28:03, 93.59s/it] 18%|█▊        | 1024/5734 [27:01:35<122:28:48, 93.62s/it] 18%|█▊        | 1025/5734 [27:03:09<122:35:42, 93.72s/it] 18%|█▊        | 1026/5734 [27:04:42<122:25:04, 93.61s/it] 18%|█▊        | 1027/5734 [27:06:16<122:18:15, 93.54s/it] 18%|█▊        | 1028/5734 [27:07:50<122:29:54, 93.71s/it] 18%|█▊        | 1029/5734 [27:09:24<122:32:57, 93.77s/it] 18%|█▊        | 1030/5734 [27:10:57<122:12:10, 93.52s/it]                                                          {'loss': 3.0008, 'learning_rate': 9.224748656412943e-05, 'epoch': 0.18}
 18%|█▊        | 1030/5734 [27:10:57<122:12:10, 93.52s/it] 18%|█▊        | 1031/5734 [27:12:31<122:24:13, 93.70s/it] 18%|█▊        | 1032/5734 [27:14:05<122:21:47, 93.69s/it] 18%|█▊        | 1033/5734 [27:15:38<122:15:13, 93.62s/it] 18%|█▊        | 1034/5734 [27:17:11<121:56:44, 93.41s/it] 18%|█▊        | 1035/5734 [27:18:45<122:04:48, 93.53s/it] 18%|█▊        | 1036/5734 [27:20:18<122:08:12, 93.59s/it] 18%|█▊        | 1037/5734 [27:21:52<122:08:31, 93.62s/it] 18%|█▊        | 1038/5734 [27:23:26<122:11:42, 93.68s/it] 18%|█▊        | 1039/5734 [27:25:00<122:08:37, 93.66s/it] 18%|█▊        | 1040/5734 [27:26:33<122:01:58, 93.59s/it]                                                          {'loss': 3.0026, 'learning_rate': 9.210033529112961e-05, 'epoch': 0.18}
 18%|█▊        | 1040/5734 [27:26:33<122:01:58, 93.59s/it] 18%|█▊        | 1041/5734 [27:28:07<121:58:28, 93.57s/it] 18%|█▊        | 1042/5734 [27:29:41<122:08:26, 93.71s/it] 18%|█▊        | 1043/5734 [27:31:14<121:56:09, 93.58s/it] 18%|█▊        | 1044/5734 [27:32:47<121:50:21, 93.52s/it] 18%|█▊        | 1045/5734 [27:34:21<122:00:44, 93.68s/it] 18%|█▊        | 1046/5734 [27:35:54<121:40:42, 93.44s/it] 18%|█▊        | 1047/5734 [27:37:27<121:29:07, 93.31s/it] 18%|█▊        | 1048/5734 [27:39:00<121:23:28, 93.26s/it] 18%|█▊        | 1049/5734 [27:40:33<121:07:07, 93.07s/it] 18%|█▊        | 1050/5734 [27:42:06<121:16:17, 93.21s/it]                                                          {'loss': 2.9996, 'learning_rate': 9.195192024581713e-05, 'epoch': 0.18}
 18%|█▊        | 1050/5734 [27:42:06<121:16:17, 93.21s/it] 18%|█▊        | 1051/5734 [27:43:40<121:15:13, 93.21s/it] 18%|█▊        | 1052/5734 [27:45:13<121:11:09, 93.18s/it] 18%|█▊        | 1053/5734 [27:46:46<121:00:53, 93.07s/it] 18%|█▊        | 1054/5734 [27:48:18<120:54:37, 93.01s/it] 18%|█▊        | 1055/5734 [27:49:52<120:58:22, 93.08s/it] 18%|█▊        | 1056/5734 [27:51:24<120:46:24, 92.94s/it] 18%|█▊        | 1057/5734 [27:52:57<120:46:01, 92.96s/it] 18%|█▊        | 1058/5734 [27:54:30<120:45:05, 92.97s/it] 18%|█▊        | 1059/5734 [27:56:04<120:55:58, 93.12s/it] 18%|█▊        | 1060/5734 [27:57:38<121:18:43, 93.44s/it]                                                          {'loss': 2.9924, 'learning_rate': 9.180224588333057e-05, 'epoch': 0.18}
 18%|█▊        | 1060/5734 [27:57:38<121:18:43, 93.44s/it] 19%|█▊        | 1061/5734 [27:59:11<121:16:57, 93.43s/it] 19%|█▊        | 1062/5734 [28:00:45<121:09:26, 93.36s/it] 19%|█▊        | 1063/5734 [28:02:18<121:08:59, 93.37s/it] 19%|█▊        | 1064/5734 [28:03:51<120:51:34, 93.17s/it] 19%|█▊        | 1065/5734 [28:05:24<121:01:49, 93.32s/it] 19%|█▊        | 1066/5734 [28:06:57<120:52:27, 93.22s/it] 19%|█▊        | 1067/5734 [28:08:31<120:55:43, 93.28s/it] 19%|█▊        | 1068/5734 [28:10:04<120:52:27, 93.26s/it] 19%|█▊        | 1069/5734 [28:11:37<120:34:54, 93.05s/it] 19%|█▊        | 1070/5734 [28:13:09<120:22:25, 92.91s/it]                                                          {'loss': 2.9947, 'learning_rate': 9.16513166966107e-05, 'epoch': 0.19}
 19%|█▊        | 1070/5734 [28:13:09<120:22:25, 92.91s/it] 19%|█▊        | 1071/5734 [28:14:42<120:18:01, 92.88s/it] 19%|█▊        | 1072/5734 [28:16:15<120:29:31, 93.04s/it] 19%|█▊        | 1073/5734 [28:17:49<120:40:03, 93.20s/it] 19%|█▊        | 1074/5734 [28:19:22<120:40:01, 93.22s/it] 19%|█▊        | 1075/5734 [28:20:56<120:47:28, 93.34s/it] 19%|█▉        | 1076/5734 [28:22:29<120:52:08, 93.42s/it] 19%|█▉        | 1077/5734 [28:24:02<120:38:50, 93.26s/it] 19%|█▉        | 1078/5734 [28:25:35<120:33:08, 93.21s/it] 19%|█▉        | 1079/5734 [28:27:08<120:20:40, 93.07s/it] 19%|█▉        | 1080/5734 [28:28:42<120:42:28, 93.37s/it]                                                          {'loss': 2.9932, 'learning_rate': 9.149913721626581e-05, 'epoch': 0.19}
 19%|█▉        | 1080/5734 [28:28:42<120:42:28, 93.37s/it] 19%|█▉        | 1081/5734 [28:30:15<120:32:51, 93.27s/it] 19%|█▉        | 1082/5734 [28:31:48<120:21:09, 93.14s/it] 19%|█▉        | 1083/5734 [28:33:22<120:28:41, 93.25s/it] 19%|█▉        | 1084/5734 [28:34:55<120:30:14, 93.29s/it] 19%|█▉        | 1085/5734 [28:36:28<120:32:46, 93.35s/it] 19%|█▉        | 1086/5734 [28:38:01<120:24:10, 93.26s/it] 19%|█▉        | 1087/5734 [28:39:35<120:18:26, 93.20s/it] 19%|█▉        | 1088/5734 [28:41:08<120:33:23, 93.41s/it] 19%|█▉        | 1089/5734 [28:42:42<120:37:37, 93.49s/it] 19%|█▉        | 1090/5734 [28:44:15<120:33:12, 93.45s/it]                                                          {'loss': 2.9933, 'learning_rate': 9.134571201043564e-05, 'epoch': 0.19}
 19%|█▉        | 1090/5734 [28:44:15<120:33:12, 93.45s/it] 19%|█▉        | 1091/5734 [28:45:49<120:34:07, 93.48s/it] 19%|█▉        | 1092/5734 [28:47:23<120:43:52, 93.63s/it] 19%|█▉        | 1093/5734 [28:48:56<120:34:16, 93.53s/it] 19%|█▉        | 1094/5734 [28:50:29<120:06:40, 93.19s/it] 19%|█▉        | 1095/5734 [28:52:02<120:18:11, 93.36s/it] 19%|█▉        | 1096/5734 [28:53:36<120:30:58, 93.54s/it] 19%|█▉        | 1097/5734 [28:55:10<120:26:44, 93.51s/it] 19%|█▉        | 1098/5734 [28:56:43<120:27:52, 93.54s/it] 19%|█▉        | 1099/5734 [28:58:17<120:17:24, 93.43s/it] 19%|█▉        | 1100/5734 [28:59:50<120:07:27, 93.32s/it]                                                          {'loss': 2.9868, 'learning_rate': 9.11910456846542e-05, 'epoch': 0.19}
 19%|█▉        | 1100/5734 [28:59:50<120:07:27, 93.32s/it] 19%|█▉        | 1101/5734 [29:01:23<120:10:27, 93.38s/it] 19%|█▉        | 1102/5734 [29:02:56<120:02:33, 93.30s/it] 19%|█▉        | 1103/5734 [29:04:30<120:04:32, 93.34s/it] 19%|█▉        | 1104/5734 [29:06:02<119:46:41, 93.13s/it] 19%|█▉        | 1105/5734 [29:07:36<119:45:30, 93.14s/it] 19%|█▉        | 1106/5734 [29:09:10<120:05:16, 93.41s/it] 19%|█▉        | 1107/5734 [29:10:42<119:45:49, 93.18s/it] 19%|█▉        | 1108/5734 [29:12:16<119:49:09, 93.24s/it] 19%|█▉        | 1109/5734 [29:13:49<119:39:32, 93.14s/it] 19%|█▉        | 1110/5734 [29:15:22<119:40:58, 93.18s/it]                                                          {'loss': 2.9824, 'learning_rate': 9.103514288171159e-05, 'epoch': 0.19}
 19%|█▉        | 1110/5734 [29:15:22<119:40:58, 93.18s/it] 19%|█▉        | 1111/5734 [29:16:56<119:56:05, 93.40s/it] 19%|█▉        | 1112/5734 [29:18:29<119:44:24, 93.26s/it] 19%|█▉        | 1113/5734 [29:20:01<119:24:48, 93.03s/it] 19%|█▉        | 1114/5734 [29:21:35<119:31:55, 93.14s/it] 19%|█▉        | 1115/5734 [29:23:08<119:44:36, 93.33s/it] 19%|█▉        | 1116/5734 [29:24:42<119:40:39, 93.30s/it] 19%|█▉        | 1117/5734 [29:26:14<119:30:06, 93.18s/it] 19%|█▉        | 1118/5734 [29:27:48<119:26:13, 93.15s/it] 20%|█▉        | 1119/5734 [29:29:21<119:37:47, 93.32s/it] 20%|█▉        | 1120/5734 [29:30:54<119:18:09, 93.08s/it]                                                          {'loss': 2.982, 'learning_rate': 9.087800828151465e-05, 'epoch': 0.2}
 20%|█▉        | 1120/5734 [29:30:54<119:18:09, 93.08s/it] 20%|█▉        | 1121/5734 [29:32:26<119:03:44, 92.92s/it] 20%|█▉        | 1122/5734 [29:33:59<118:50:31, 92.76s/it] 20%|█▉        | 1123/5734 [29:35:32<118:55:01, 92.84s/it] 20%|█▉        | 1124/5734 [29:37:05<119:01:05, 92.94s/it] 20%|█▉        | 1125/5734 [29:38:38<119:00:13, 92.95s/it] 20%|█▉        | 1126/5734 [29:40:11<118:58:11, 92.95s/it] 20%|█▉        | 1127/5734 [29:41:44<119:08:04, 93.09s/it] 20%|█▉        | 1128/5734 [29:43:17<118:54:43, 92.94s/it] 20%|█▉        | 1129/5734 [29:44:50<119:01:55, 93.05s/it] 20%|█▉        | 1130/5734 [29:46:23<118:54:26, 92.98s/it]                                                          {'loss': 2.9866, 'learning_rate': 9.071964660094638e-05, 'epoch': 0.2}
 20%|█▉        | 1130/5734 [29:46:23<118:54:26, 92.98s/it] 20%|█▉        | 1131/5734 [29:47:56<118:56:01, 93.02s/it] 20%|█▉        | 1132/5734 [29:49:29<118:55:25, 93.03s/it] 20%|█▉        | 1133/5734 [29:51:02<118:51:49, 93.00s/it] 20%|█▉        | 1134/5734 [29:52:35<118:59:21, 93.12s/it] 20%|█▉        | 1135/5734 [29:54:08<118:50:57, 93.03s/it] 20%|█▉        | 1136/5734 [29:55:41<118:51:33, 93.06s/it] 20%|█▉        | 1137/5734 [29:57:14<118:45:04, 93.00s/it] 20%|█▉        | 1138/5734 [29:58:47<118:41:07, 92.97s/it] 20%|█▉        | 1139/5734 [30:00:20<118:45:08, 93.04s/it] 20%|█▉        | 1140/5734 [30:01:54<118:51:18, 93.14s/it]                                                          {'loss': 2.9803, 'learning_rate': 9.056006259372449e-05, 'epoch': 0.2}
 20%|█▉        | 1140/5734 [30:01:54<118:51:18, 93.14s/it] 20%|█▉        | 1141/5734 [30:03:27<118:40:37, 93.02s/it] 20%|█▉        | 1142/5734 [30:05:00<118:45:44, 93.11s/it] 20%|█▉        | 1143/5734 [30:06:32<118:30:14, 92.92s/it] 20%|█▉        | 1144/5734 [30:08:05<118:24:19, 92.87s/it] 20%|█▉        | 1145/5734 [30:09:38<118:29:11, 92.95s/it] 20%|█▉        | 1146/5734 [30:11:11<118:34:44, 93.04s/it] 20%|██        | 1147/5734 [30:12:45<118:50:01, 93.26s/it] 20%|██        | 1148/5734 [30:14:18<118:35:19, 93.09s/it] 20%|██        | 1149/5734 [30:15:50<118:18:17, 92.89s/it] 20%|██        | 1150/5734 [30:17:23<118:22:15, 92.96s/it]                                                          {'loss': 2.969, 'learning_rate': 9.03992610502585e-05, 'epoch': 0.2}
 20%|██        | 1150/5734 [30:17:24<118:22:15, 92.96s/it] 20%|██        | 1151/5734 [30:18:56<118:06:14, 92.77s/it] 20%|██        | 1152/5734 [30:20:29<118:17:14, 92.94s/it] 20%|██        | 1153/5734 [30:22:02<118:15:13, 92.93s/it] 20%|██        | 1154/5734 [30:23:35<118:19:29, 93.01s/it] 20%|██        | 1155/5734 [30:25:09<118:36:44, 93.25s/it] 20%|██        | 1156/5734 [30:26:42<118:23:12, 93.10s/it] 20%|██        | 1157/5734 [30:28:15<118:20:09, 93.08s/it] 20%|██        | 1158/5734 [30:29:48<118:21:07, 93.11s/it] 20%|██        | 1159/5734 [30:31:21<118:27:36, 93.21s/it] 20%|██        | 1160/5734 [30:32:54<118:14:38, 93.06s/it]                                                          {'loss': 2.9796, 'learning_rate': 9.023724679750618e-05, 'epoch': 0.2}
 20%|██        | 1160/5734 [30:32:54<118:14:38, 93.06s/it] 20%|██        | 1161/5734 [30:34:28<118:21:47, 93.18s/it] 20%|██        | 1162/5734 [30:36:01<118:25:55, 93.25s/it] 20%|██        | 1163/5734 [30:37:34<118:21:53, 93.22s/it] 20%|██        | 1164/5734 [30:39:07<118:04:59, 93.02s/it] 20%|██        | 1165/5734 [30:40:40<118:05:33, 93.05s/it] 20%|██        | 1166/5734 [30:42:14<118:29:46, 93.39s/it] 20%|██        | 1167/5734 [30:43:47<118:19:53, 93.28s/it] 20%|██        | 1168/5734 [30:45:20<117:59:47, 93.03s/it] 20%|██        | 1169/5734 [30:46:53<118:11:16, 93.20s/it] 20%|██        | 1170/5734 [30:48:27<118:16:05, 93.29s/it]                                                          {'loss': 2.9687, 'learning_rate': 9.007402469882845e-05, 'epoch': 0.2}
 20%|██        | 1170/5734 [30:48:27<118:16:05, 93.29s/it] 20%|██        | 1171/5734 [30:49:59<118:05:40, 93.17s/it] 20%|██        | 1172/5734 [30:51:32<117:53:39, 93.03s/it] 20%|██        | 1173/5734 [30:53:06<118:05:41, 93.21s/it] 20%|██        | 1174/5734 [30:54:38<117:50:42, 93.04s/it] 20%|██        | 1175/5734 [30:56:12<117:52:29, 93.08s/it] 21%|██        | 1176/5734 [30:57:44<117:43:56, 92.99s/it] 21%|██        | 1177/5734 [30:59:18<117:51:56, 93.11s/it] 21%|██        | 1178/5734 [31:00:51<117:48:12, 93.08s/it] 21%|██        | 1179/5734 [31:02:24<117:44:55, 93.06s/it] 21%|██        | 1180/5734 [31:03:56<117:29:00, 92.87s/it]                                                          {'loss': 2.9635, 'learning_rate': 8.990959965384354e-05, 'epoch': 0.21}
 21%|██        | 1180/5734 [31:03:56<117:29:00, 92.87s/it] 21%|██        | 1181/5734 [31:05:30<117:38:57, 93.02s/it] 21%|██        | 1182/5734 [31:07:03<117:46:20, 93.14s/it] 21%|██        | 1183/5734 [31:08:36<117:33:36, 92.99s/it] 21%|██        | 1184/5734 [31:10:09<117:37:49, 93.07s/it] 21%|██        | 1185/5734 [31:11:42<117:29:36, 92.98s/it] 21%|██        | 1186/5734 [31:13:15<117:28:06, 92.98s/it] 21%|██        | 1187/5734 [31:14:47<117:21:37, 92.92s/it] 21%|██        | 1188/5734 [31:16:21<117:23:26, 92.96s/it] 21%|██        | 1189/5734 [31:17:54<117:21:51, 92.96s/it] 21%|██        | 1190/5734 [31:19:27<117:24:23, 93.02s/it]                                                          {'loss': 2.9555, 'learning_rate': 8.974397659827982e-05, 'epoch': 0.21}
 21%|██        | 1190/5734 [31:19:27<117:24:23, 93.02s/it] 21%|██        | 1191/5734 [31:21:00<117:29:31, 93.10s/it] 21%|██        | 1192/5734 [31:22:33<117:37:30, 93.23s/it] 21%|██        | 1193/5734 [31:24:08<117:56:37, 93.50s/it] 21%|██        | 1194/5734 [31:25:42<118:06:30, 93.65s/it] 21%|██        | 1195/5734 [31:27:15<117:57:26, 93.56s/it] 21%|██        | 1196/5734 [31:28:48<117:37:15, 93.31s/it] 21%|██        | 1197/5734 [31:30:21<117:30:59, 93.25s/it] 21%|██        | 1198/5734 [31:31:54<117:19:29, 93.12s/it] 21%|██        | 1199/5734 [31:33:27<117:24:30, 93.20s/it] 21%|██        | 1200/5734 [31:35:00<117:07:43, 93.00s/it]                                                          {'loss': 2.9586, 'learning_rate': 8.957716050382765e-05, 'epoch': 0.21}
 21%|██        | 1200/5734 [31:35:00<117:07:43, 93.00s/it] 21%|██        | 1201/5734 [31:36:33<117:19:20, 93.17s/it] 21%|██        | 1202/5734 [31:38:07<117:27:35, 93.30s/it] 21%|██        | 1203/5734 [31:39:40<117:26:30, 93.31s/it] 21%|██        | 1204/5734 [31:41:12<117:02:54, 93.02s/it] 21%|██        | 1205/5734 [31:42:45<117:03:05, 93.04s/it] 21%|██        | 1206/5734 [31:44:19<117:09:43, 93.15s/it] 21%|██        | 1207/5734 [31:45:52<117:14:44, 93.24s/it] 21%|██        | 1208/5734 [31:47:25<116:58:14, 93.04s/it] 21%|██        | 1209/5734 [31:48:58<116:59:43, 93.08s/it] 21%|██        | 1210/5734 [31:50:32<117:11:12, 93.25s/it]                                                          {'loss': 2.9522, 'learning_rate': 8.94091563779902e-05, 'epoch': 0.21}
 21%|██        | 1210/5734 [31:50:32<117:11:12, 93.25s/it] 21%|██        | 1211/5734 [31:52:05<117:02:31, 93.16s/it] 21%|██        | 1212/5734 [31:53:38<117:01:58, 93.17s/it] 21%|██        | 1213/5734 [31:55:11<117:02:49, 93.20s/it] 21%|██        | 1214/5734 [31:56:45<117:04:38, 93.25s/it] 21%|██        | 1215/5734 [31:58:17<116:53:26, 93.12s/it] 21%|██        | 1216/5734 [31:59:51<116:53:59, 93.15s/it] 21%|██        | 1217/5734 [32:01:24<116:56:31, 93.20s/it] 21%|██        | 1218/5734 [32:02:58<117:05:50, 93.35s/it] 21%|██▏       | 1219/5734 [32:04:33<118:01:08, 94.10s/it] 21%|██▏       | 1220/5734 [32:06:07<117:47:57, 93.95s/it]                                                          {'loss': 2.9562, 'learning_rate': 8.923996926393305e-05, 'epoch': 0.21}
 21%|██▏       | 1220/5734 [32:06:07<117:47:57, 93.95s/it] 21%|██▏       | 1221/5734 [32:07:41<117:38:10, 93.84s/it] 21%|██▏       | 1222/5734 [32:09:14<117:18:21, 93.60s/it] 21%|██▏       | 1223/5734 [32:10:46<116:50:40, 93.25s/it] 21%|██▏       | 1224/5734 [32:12:20<116:59:26, 93.39s/it] 21%|██▏       | 1225/5734 [32:13:53<116:54:33, 93.34s/it] 21%|██▏       | 1226/5734 [32:15:27<117:01:06, 93.45s/it] 21%|██▏       | 1227/5734 [32:17:00<116:53:59, 93.37s/it] 21%|██▏       | 1228/5734 [32:18:33<116:40:16, 93.21s/it] 21%|██▏       | 1229/5734 [32:20:06<116:33:41, 93.15s/it] 21%|██▏       | 1230/5734 [32:21:39<116:27:39, 93.09s/it]                                                          {'loss': 2.9505, 'learning_rate': 8.906960424033288e-05, 'epoch': 0.21}
 21%|██▏       | 1230/5734 [32:21:39<116:27:39, 93.09s/it] 21%|██▏       | 1231/5734 [32:23:12<116:33:34, 93.19s/it] 21%|██▏       | 1232/5734 [32:24:45<116:21:36, 93.05s/it] 22%|██▏       | 1233/5734 [32:26:18<116:21:04, 93.06s/it] 22%|██▏       | 1234/5734 [32:27:51<116:19:01, 93.05s/it] 22%|██▏       | 1235/5734 [32:29:24<116:20:36, 93.10s/it] 22%|██▏       | 1236/5734 [32:30:57<116:09:50, 92.97s/it] 22%|██▏       | 1237/5734 [32:32:30<116:08:46, 92.98s/it] 22%|██▏       | 1238/5734 [32:34:03<116:07:08, 92.98s/it] 22%|██▏       | 1239/5734 [32:35:35<115:59:27, 92.90s/it] 22%|██▏       | 1240/5734 [32:37:08<115:50:03, 92.79s/it]                                                          {'loss': 2.9488, 'learning_rate': 8.889806642122495e-05, 'epoch': 0.22}
 22%|██▏       | 1240/5734 [32:37:08<115:50:03, 92.79s/it] 22%|██▏       | 1241/5734 [32:38:41<115:49:49, 92.81s/it] 22%|██▏       | 1242/5734 [32:40:14<115:54:37, 92.89s/it] 22%|██▏       | 1243/5734 [32:41:47<115:47:34, 92.82s/it] 22%|██▏       | 1244/5734 [32:43:19<115:45:44, 92.82s/it] 22%|██▏       | 1245/5734 [32:44:53<115:59:13, 93.02s/it] 22%|██▏       | 1246/5734 [32:46:27<116:13:47, 93.23s/it] 22%|██▏       | 1247/5734 [32:47:59<115:55:29, 93.01s/it] 22%|██▏       | 1248/5734 [32:49:32<115:45:34, 92.90s/it] 22%|██▏       | 1249/5734 [32:51:05<115:50:19, 92.98s/it] 22%|██▏       | 1250/5734 [32:52:38<116:00:01, 93.13s/it]                                                          {'loss': 2.9493, 'learning_rate': 8.872536095584967e-05, 'epoch': 0.22}
 22%|██▏       | 1250/5734 [32:52:38<116:00:01, 93.13s/it] 22%|██▏       | 1251/5734 [32:54:11<115:41:00, 92.90s/it] 22%|██▏       | 1252/5734 [32:55:44<115:41:15, 92.92s/it] 22%|██▏       | 1253/5734 [32:57:17<115:41:58, 92.95s/it] 22%|██▏       | 1254/5734 [32:58:50<115:45:51, 93.02s/it] 22%|██▏       | 1255/5734 [33:00:22<115:31:08, 92.85s/it] 22%|██▏       | 1256/5734 [33:01:56<115:38:03, 92.96s/it] 22%|██▏       | 1257/5734 [33:03:29<115:53:19, 93.19s/it] 22%|██▏       | 1258/5734 [33:05:02<115:43:32, 93.08s/it] 22%|██▏       | 1259/5734 [33:06:35<115:27:48, 92.89s/it] 22%|██▏       | 1260/5734 [33:08:07<115:23:59, 92.86s/it]                                                          {'loss': 2.95, 'learning_rate': 8.85514930284979e-05, 'epoch': 0.22}
 22%|██▏       | 1260/5734 [33:08:07<115:23:59, 92.86s/it] 22%|██▏       | 1261/5734 [33:09:41<115:35:31, 93.03s/it] 22%|██▏       | 1262/5734 [33:11:14<115:36:48, 93.07s/it] 22%|██▏       | 1263/5734 [33:12:46<115:20:38, 92.87s/it] 22%|██▏       | 1264/5734 [33:14:20<115:36:34, 93.11s/it] 22%|██▏       | 1265/5734 [33:15:53<115:34:23, 93.10s/it] 22%|██▏       | 1266/5734 [33:17:26<115:27:14, 93.02s/it] 22%|██▏       | 1267/5734 [33:18:58<115:11:09, 92.83s/it] 22%|██▏       | 1268/5734 [33:20:32<115:22:36, 93.00s/it] 22%|██▏       | 1269/5734 [33:22:05<115:14:24, 92.91s/it] 22%|██▏       | 1270/5734 [33:23:38<115:24:16, 93.07s/it]                                                          {'loss': 2.9546, 'learning_rate': 8.837646785835547e-05, 'epoch': 0.22}
 22%|██▏       | 1270/5734 [33:23:38<115:24:16, 93.07s/it] 22%|██▏       | 1271/5734 [33:25:12<115:37:19, 93.26s/it] 22%|██▏       | 1272/5734 [33:26:45<115:41:09, 93.34s/it] 22%|██▏       | 1273/5734 [33:28:18<115:37:41, 93.31s/it] 22%|██▏       | 1274/5734 [33:29:51<115:24:15, 93.15s/it] 22%|██▏       | 1275/5734 [33:31:24<115:12:36, 93.02s/it] 22%|██▏       | 1276/5734 [33:32:57<115:19:07, 93.12s/it] 22%|██▏       | 1277/5734 [33:34:30<115:13:00, 93.06s/it] 22%|██▏       | 1278/5734 [33:36:03<115:03:26, 92.95s/it] 22%|██▏       | 1279/5734 [33:37:36<115:08:12, 93.04s/it] 22%|██▏       | 1280/5734 [33:39:09<115:06:47, 93.04s/it]                                                          {'loss': 2.9506, 'learning_rate': 8.820029069934646e-05, 'epoch': 0.22}
 22%|██▏       | 1280/5734 [33:39:09<115:06:47, 93.04s/it] 22%|██▏       | 1281/5734 [33:40:42<115:05:10, 93.04s/it] 22%|██▏       | 1282/5734 [33:42:15<114:54:53, 92.92s/it] 22%|██▏       | 1283/5734 [33:43:48<115:01:25, 93.03s/it] 22%|██▏       | 1284/5734 [33:45:21<114:47:55, 92.87s/it] 22%|██▏       | 1285/5734 [33:46:54<114:47:56, 92.89s/it] 22%|██▏       | 1286/5734 [33:48:27<114:50:10, 92.94s/it] 22%|██▏       | 1287/5734 [33:49:59<114:46:10, 92.91s/it] 22%|██▏       | 1288/5734 [33:51:32<114:39:36, 92.84s/it] 22%|██▏       | 1289/5734 [33:53:05<114:41:58, 92.90s/it] 22%|██▏       | 1290/5734 [33:54:38<114:49:04, 93.01s/it]                                                          {'loss': 2.9461, 'learning_rate': 8.802296683997538e-05, 'epoch': 0.22}
 22%|██▏       | 1290/5734 [33:54:38<114:49:04, 93.01s/it] 23%|██▎       | 1291/5734 [33:56:11<114:36:07, 92.86s/it] 23%|██▎       | 1292/5734 [33:57:44<114:48:10, 93.04s/it] 23%|██▎       | 1293/5734 [33:59:17<114:44:32, 93.01s/it] 23%|██▎       | 1294/5734 [34:00:51<114:51:19, 93.13s/it] 23%|██▎       | 1295/5734 [34:02:23<114:32:58, 92.90s/it] 23%|██▎       | 1296/5734 [34:03:56<114:39:19, 93.01s/it] 23%|██▎       | 1297/5734 [34:05:29<114:35:53, 92.98s/it] 23%|██▎       | 1298/5734 [34:07:03<114:43:02, 93.10s/it] 23%|██▎       | 1299/5734 [34:08:35<114:27:05, 92.90s/it] 23%|██▎       | 1300/5734 [34:10:08<114:32:27, 93.00s/it]                                                          {'loss': 2.9444, 'learning_rate': 8.784450160316864e-05, 'epoch': 0.23}
 23%|██▎       | 1300/5734 [34:10:08<114:32:27, 93.00s/it] 23%|██▎       | 1301/5734 [34:11:42<114:48:48, 93.24s/it] 23%|██▎       | 1302/5734 [34:13:15<114:46:06, 93.22s/it]mh_moe.sh: line 34: 1838719 Killed                  deepspeed --num_gpus 4 --master_port=9902 src/train_bash.py --deepspeed ./llama-pt/config/ds_config_cpu.json --stage pt --model_name_or_path /home/nfs04/wangzj/models/Qwen1.5-1.8B --do_train --flash_attn --dataset ar_2b,de_2b,is_2b,hi_2b --mix_strategy concat --preprocessing_num_workers 32 --cache_path /home/nfs03/wangzj/dataset/pretrain/arderu6b --cutoff_len 1024 --finetuning_type moe --moe_every_k_layers 1 --moe_router_type mh_moe --moe_num_experts 4 --moe_num_heads 4 --topk 2 --moe_with_aux --output_dir /home/nfs04/wangzj/checkpoints/moe/1.8b-arderu-mhmoe --overwrite_output_dir --per_device_train_batch_size 16 --gradient_accumulation_steps 16 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 1 --save_only_model --save_steps 10000 --learning_rate 1e-4 --num_train_epochs 1.0 --plot_loss --bf16
