[2024-05-26 21:01:26,835] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 21:01:27,866] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-26 21:01:27,866] [INFO] [runner.py:568:main] cmd = /home/yangdezhao/anaconda3/envs/ydz/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/yangdezhao/ydz_temp/LLaMA-Factory/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/yangdezhao/ydz_temp/models/Qwen1.5-1.8B --do_train --flash_attn --dataset slimpajam_1b --max_samples 500000 --preprocessing_num_workers 16 --cutoff_len 1024 --finetuning_type moe --moe_every_k_layers 1 --moe_router_type top1 --moe_num_experts 2 --save_all_params --output_dir /home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe --overwrite_output_dir --per_device_train_batch_size 8 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 1 --save_only_model --save_steps 1000 --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-05-26 21:01:29,957] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 21:01:30,925] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-26 21:01:30,925] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-26 21:01:30,925] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-26 21:01:30,925] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-26 21:01:30,925] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-26 21:01:34,967] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 21:01:34,999] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 21:01:35,041] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 21:01:35,091] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 21:01:37,049] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-26 21:01:37,078] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-26 21:01:37,272] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-26 21:01:37,339] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-26 21:01:37,339] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
05/26/2024 21:01:38 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/26/2024 21:01:38 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/ydz_temp/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/runs/May26_21-01-37_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
05/26/2024 21:01:38 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/26/2024 21:01:38 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/ydz_temp/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/runs/May26_21-01-37_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
05/26/2024 21:01:38 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/26/2024 21:01:38 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/ydz_temp/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/runs/May26_21-01-37_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-05-26 21:01:38,362 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2027] 2024-05-26 21:01:38,363 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2027] 2024-05-26 21:01:38,363 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-05-26 21:01:38,363 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-05-26 21:01:38,363 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-05-26 21:01:38,363 >> loading file tokenizer.json
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
05/26/2024 21:01:38 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/26/2024 21:01:38 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/ydz_temp/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/runs/May26_21-01-37_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
05/26/2024 21:01:38 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
05/26/2024 21:01:38 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[WARNING|logging.py:314] 2024-05-26 21:01:38,602 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:727] 2024-05-26 21:01:38,602 >> loading configuration file /home/yangdezhao/ydz_temp/models/Qwen1.5-1.8B/config.json
[INFO|configuration_utils.py:792] 2024-05-26 21:01:38,603 >> Model config Qwen2Config {
  "_name_or_path": "/home/yangdezhao/ydz_temp/models/Qwen1.5-1.8B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

05/26/2024 21:01:38 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
[INFO|modeling_utils.py:3334] 2024-05-26 21:01:38,632 >> loading weights file /home/yangdezhao/ydz_temp/models/Qwen1.5-1.8B/model.safetensors
[INFO|modeling_utils.py:1459] 2024-05-26 21:01:38,653 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[WARNING|logging.py:329] 2024-05-26 21:01:38,654 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[WARNING|logging.py:329] 2024-05-26 21:01:38,656 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:827] 2024-05-26 21:01:38,658 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
05/26/2024 21:01:38 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
05/26/2024 21:01:40 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 21:01:40 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/26/2024 21:01:40 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 21:01:40 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/26/2024 21:01:40 - INFO - llmtuner.model.adapter - Moe training and saving all params.
05/26/2024 21:01:40 - INFO - llmtuner.model.loader - trainable params: 2648524800 || all params: 2648524800 || trainable%: 100.0000
05/26/2024 21:01:40 - INFO - llmtuner.model.adapter - Moe training and saving all params.
05/26/2024 21:01:40 - INFO - llmtuner.model.loader - trainable params: 2648524800 || all params: 2648524800 || trainable%: 100.0000
05/26/2024 21:01:41 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 21:01:41 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
[INFO|modeling_utils.py:4070] 2024-05-26 21:01:41,155 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4078] 2024-05-26 21:01:41,155 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/yangdezhao/ydz_temp/models/Qwen1.5-1.8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-05-26 21:01:41,159 >> loading configuration file /home/yangdezhao/ydz_temp/models/Qwen1.5-1.8B/generation_config.json
[INFO|configuration_utils.py:827] 2024-05-26 21:01:41,159 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "max_new_tokens": 2048
}

05/26/2024 21:01:41 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 21:01:41 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/26/2024 21:01:41 - INFO - llmtuner.model.adapter - Moe training and saving all params.
05/26/2024 21:01:41 - INFO - llmtuner.model.loader - trainable params: 2648524800 || all params: 2648524800 || trainable%: 100.0000
05/26/2024 21:01:41 - INFO - llmtuner.model.adapter - Moe training and saving all params.
05/26/2024 21:01:41 - INFO - llmtuner.model.loader - trainable params: 2648524800 || all params: 2648524800 || trainable%: 100.0000
05/26/2024 21:01:47 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Using custom data configuration default-8967e71434917b95
Loading Dataset Infos from /home/yangdezhao/anaconda3/envs/ydz/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Downloading and preparing dataset json/default to /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 7174 examples [00:00, 66263.38 examples/s]Generating train split: 15798 examples [00:00, 45069.47 examples/s]Generating train split: 22470 examples [00:00, 45686.87 examples/s]Generating train split: 31744 examples [00:00, 47201.77 examples/s]Generating train split: 38236 examples [00:00, 48333.36 examples/s]Generating train split: 47390 examples [00:00, 51031.21 examples/s]Generating train split: 54006 examples [00:01, 48213.58 examples/s]Generating train split: 63183 examples [00:01, 50435.56 examples/s]Generating train split: 72499 examples [00:01, 51983.56 examples/s]Generating train split: 79342 examples [00:01, 52007.07 examples/s]Generating train split: 88543 examples [00:01, 51761.31 examples/s]Generating train split: 97257 examples [00:01, 51208.17 examples/s]Generating train split: 106930 examples [00:02, 51731.67 examples/s]Generating train split: 114065 examples [00:02, 50443.42 examples/s]Generating train split: 123627 examples [00:02, 52528.89 examples/s]Generating train split: 130120 examples [00:02, 50929.54 examples/s]Generating train split: 139333 examples [00:02, 52217.96 examples/s]Generating train split: 148487 examples [00:02, 52667.31 examples/s]Generating train split: 158099 examples [00:03, 53848.24 examples/s]Generating train split: 164869 examples [00:03, 52376.53 examples/s]Generating train split: 174668 examples [00:03, 55338.77 examples/s]Generating train split: 181544 examples [00:03, 52513.48 examples/s]Generating train split: 188490 examples [00:03, 52629.58 examples/s]Generating train split: 197796 examples [00:03, 52926.04 examples/s]Generating train split: 207041 examples [00:04, 53610.94 examples/s]Generating train split: 215976 examples [00:04, 53722.24 examples/s]Generating train split: 225535 examples [00:04, 53311.98 examples/s]Generating train split: 232119 examples [00:04, 50918.54 examples/s]Generating train split: 241247 examples [00:04, 52566.55 examples/s]Generating train split: 250163 examples [00:04, 52150.90 examples/s]Generating train split: 259230 examples [00:05, 52623.84 examples/s]Generating train split: 268399 examples [00:05, 54598.03 examples/s]Generating train split: 277541 examples [00:05, 52379.97 examples/s]Generating train split: 286820 examples [00:05, 54062.55 examples/s]Generating train split: 295818 examples [00:05, 53462.76 examples/s]Generating train split: 304569 examples [00:05, 53981.38 examples/s]Generating train split: 313846 examples [00:06, 54814.26 examples/s]Generating train split: 322977 examples [00:06, 52692.16 examples/s]Generating train split: 329335 examples [00:06, 50471.73 examples/s]Generating train split: 338470 examples [00:06, 51694.54 examples/s]Generating train split: 347886 examples [00:06, 53619.77 examples/s]Generating train split: 356882 examples [00:06, 52549.01 examples/s]Generating train split: 366314 examples [00:07, 53541.17 examples/s]Generating train split: 373232 examples [00:07, 52840.55 examples/s]Generating train split: 382452 examples [00:07, 53054.24 examples/s]Generating train split: 391857 examples [00:07, 53669.39 examples/s]Generating train split: 398595 examples [00:07, 52131.75 examples/s]Generating train split: 408824 examples [00:07, 54740.15 examples/s]Generating train split: 415408 examples [00:07, 53386.09 examples/s]Generating train split: 424728 examples [00:08, 54313.41 examples/s]Generating train split: 433805 examples [00:08, 54322.14 examples/s]Generating train split: 443210 examples [00:08, 55427.96 examples/s]Generating train split: 450197 examples [00:08, 54041.86 examples/s]Generating train split: 459664 examples [00:08, 53665.98 examples/s]Generating train split: 466743 examples [00:08, 52974.18 examples/s]Generating train split: 475565 examples [00:09, 51361.52 examples/s]Generating train split: 485235 examples [00:09, 53044.50 examples/s]Generating train split: 494684 examples [00:09, 52885.55 examples/s]Generating train split: 501786 examples [00:09, 52604.22 examples/s]Generating train split: 511201 examples [00:09, 54475.27 examples/s]Generating train split: 520573 examples [00:09, 55795.79 examples/s]Generating train split: 530431 examples [00:10, 57023.26 examples/s]Generating train split: 540193 examples [00:10, 57448.55 examples/s]Generating train split: 549135 examples [00:10, 54419.07 examples/s]Generating train split: 558576 examples [00:10, 54729.63 examples/s]Generating train split: 565986 examples [00:10, 55204.03 examples/s]Generating train split: 575624 examples [00:10, 56863.94 examples/s]Generating train split: 584692 examples [00:11, 51370.40 examples/s]Generating train split: 594291 examples [00:11, 52796.58 examples/s]Generating train split: 604182 examples [00:11, 55203.44 examples/s]Generating train split: 610779 examples [00:11, 53466.46 examples/s]Generating train split: 619982 examples [00:11, 54367.35 examples/s]Generating train split: 629002 examples [00:11, 53759.94 examples/s]Generating train split: 638738 examples [00:12, 54732.76 examples/s]Generating train split: 647661 examples [00:12, 53127.55 examples/s]Generating train split: 656980 examples [00:12, 53835.97 examples/s]Generating train split: 666152 examples [00:12, 54704.39 examples/s]Generating train split: 675569 examples [00:12, 54694.87 examples/s]Generating train split: 685139 examples [00:12, 54614.90 examples/s]Generating train split: 694463 examples [00:13, 55234.29 examples/s]Generating train split: 703782 examples [00:13, 56431.12 examples/s]Generating train split: 713659 examples [00:13, 57122.35 examples/s]Generating train split: 722847 examples [00:13, 56579.25 examples/s]Generating train split: 732059 examples [00:13, 56573.32 examples/s]Generating train split: 741164 examples [00:13, 54424.44 examples/s]Generating train split: 750599 examples [00:14, 54515.76 examples/s]Generating train split: 759234 examples [00:14, 53967.37 examples/s]Generating train split: 768783 examples [00:14, 54017.64 examples/s]Generating train split: 778233 examples [00:14, 55115.49 examples/s]Generating train split: 784587 examples [00:14, 51564.86 examples/s]Generating train split: 791173 examples [00:14, 49603.19 examples/s]Generating train split: 798410 examples [00:15, 50483.75 examples/s]Generating train split: 807768 examples [00:15, 52887.15 examples/s]Generating train split: 817029 examples [00:15, 51855.75 examples/s]Generating train split: 823785 examples [00:15, 51476.05 examples/s]Generating train split: 833181 examples [00:15, 52514.12 examples/s]Generating train split: 841848 examples [00:15, 52095.44 examples/s]Generating train split: 848734 examples [00:15, 52322.32 examples/s]Generating train split: 855529 examples [00:16, 52075.05 examples/s]Generating train split: 855529 examples [00:16, 53147.83 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 500000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Process #0 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00000_of_00016.arrow
Process #1 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00001_of_00016.arrow
Process #2 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00002_of_00016.arrow
Process #3 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00003_of_00016.arrow
Process #4 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00004_of_00016.arrow
Process #5 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00005_of_00016.arrow
Process #6 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00006_of_00016.arrow
Process #7 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00007_of_00016.arrow
Process #8 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00008_of_00016.arrow
Process #9 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00009_of_00016.arrow
Process #10 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00010_of_00016.arrow
Process #11 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00011_of_00016.arrow
Process #12 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00012_of_00016.arrow
Process #13 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00013_of_00016.arrow
Process #14 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00014_of_00016.arrow
Process #15 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00015_of_00016.arrow
Spawning 16 processes
Converting format of dataset (num_proc=16):   0%|          | 0/500000 [00:00<?, ? examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00000_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00002_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00001_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00005_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00004_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00003_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00011_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00012_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00006_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00010_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00009_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00007_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00015_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00008_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00013_of_00016.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d937ccb4b446dd1a_00014_of_00016.arrow
Converting format of dataset (num_proc=16):   0%|          | 2000/500000 [00:00<00:41, 11932.67 examples/s]Converting format of dataset (num_proc=16):  13%|█▎        | 66000/500000 [00:00<00:01, 298277.51 examples/s]Converting format of dataset (num_proc=16):  27%|██▋       | 135000/500000 [00:00<00:00, 452015.47 examples/s]Converting format of dataset (num_proc=16):  43%|████▎     | 214000/500000 [00:00<00:00, 565959.59 examples/s]Converting format of dataset (num_proc=16):  59%|█████▉    | 297000/500000 [00:00<00:00, 654211.00 examples/s]Converting format of dataset (num_proc=16):  75%|███████▌  | 375000/500000 [00:00<00:00, 684599.55 examples/s]Converting format of dataset (num_proc=16):  90%|█████████ | 451000/500000 [00:00<00:00, 702699.81 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 500000/500000 [00:01<00:00, 255768.05 examples/s]
Concatenating 16 shards
Process #0 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00000_of_00016.arrow
Process #1 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00001_of_00016.arrow
Process #2 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00002_of_00016.arrow
Process #3 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00003_of_00016.arrow
Process #4 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00004_of_00016.arrow
Process #5 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00005_of_00016.arrow
Process #6 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00006_of_00016.arrow
Process #7 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00007_of_00016.arrow
Process #8 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00008_of_00016.arrow
Process #9 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00009_of_00016.arrow
Process #10 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00010_of_00016.arrow
Process #11 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00011_of_00016.arrow
Process #12 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00012_of_00016.arrow
Process #13 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00013_of_00016.arrow
Process #14 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00014_of_00016.arrow
Process #15 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00015_of_00016.arrow
Spawning 16 processes
Running tokenizer on dataset (num_proc=16):   0%|          | 0/500000 [00:00<?, ? examples/s]05/26/2024 21:02:14 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
05/26/2024 21:02:14 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
05/26/2024 21:02:15 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 500000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 500000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 500000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00000_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   0%|          | 1000/500000 [00:08<1:12:09, 115.25 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:24,195 >> Token indices sequence length is longer than the specified maximum sequence length for this model (36232 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00001_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   0%|          | 2000/500000 [00:11<44:34, 186.20 examples/s]  [WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:26,253 >> Token indices sequence length is longer than the specified maximum sequence length for this model (34999 > 32768). Running this sequence through the model will result in indexing errors
[WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:26,352 >> Token indices sequence length is longer than the specified maximum sequence length for this model (35370 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00002_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   1%|          | 3000/500000 [00:13<31:32, 262.66 examples/s]Running tokenizer on dataset (num_proc=16):   1%|          | 4000/500000 [00:13<19:36, 421.67 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:29,256 >> Token indices sequence length is longer than the specified maximum sequence length for this model (78760 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00003_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   1%|          | 5000/500000 [00:16<21:12, 389.06 examples/s]Running tokenizer on dataset (num_proc=16):   1%|          | 6000/500000 [00:17<16:57, 485.73 examples/s]Running tokenizer on dataset (num_proc=16):   1%|▏         | 7000/500000 [00:18<13:36, 603.44 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:32,416 >> Token indices sequence length is longer than the specified maximum sequence length for this model (174817 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00004_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   2%|▏         | 9000/500000 [00:19<09:33, 856.02 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00005_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   2%|▏         | 10000/500000 [00:21<09:38, 847.15 examples/s]Running tokenizer on dataset (num_proc=16):   2%|▏         | 11000/500000 [00:21<08:41, 937.11 examples/s]Running tokenizer on dataset (num_proc=16):   2%|▏         | 12000/500000 [00:22<06:37, 1227.24 examples/s]Running tokenizer on dataset (num_proc=16):   3%|▎         | 13000/500000 [00:22<06:37, 1224.44 examples/s]Running tokenizer on dataset (num_proc=16):   3%|▎         | 14000/500000 [00:23<05:49, 1392.16 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:38,135 >> Token indices sequence length is longer than the specified maximum sequence length for this model (93395 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00006_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   3%|▎         | 15000/500000 [00:25<09:37, 839.80 examples/s] [WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:38,789 >> Token indices sequence length is longer than the specified maximum sequence length for this model (46105 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):   3%|▎         | 16000/500000 [00:25<07:16, 1109.76 examples/s]Running tokenizer on dataset (num_proc=16):   3%|▎         | 17000/500000 [00:26<05:48, 1384.33 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:39,190 >> Token indices sequence length is longer than the specified maximum sequence length for this model (57095 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00007_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   4%|▎         | 18000/500000 [00:26<04:51, 1651.83 examples/s]Running tokenizer on dataset (num_proc=16):   4%|▍         | 19000/500000 [00:26<03:40, 2180.47 examples/s]Running tokenizer on dataset (num_proc=16):   4%|▍         | 20000/500000 [00:27<04:23, 1823.30 examples/s]Running tokenizer on dataset (num_proc=16):   4%|▍         | 21000/500000 [00:27<03:25, 2326.78 examples/s]Running tokenizer on dataset (num_proc=16):   4%|▍         | 22000/500000 [00:28<03:36, 2207.13 examples/s]Running tokenizer on dataset (num_proc=16):   5%|▍         | 23000/500000 [00:30<07:32, 1054.91 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:43,741 >> Token indices sequence length is longer than the specified maximum sequence length for this model (143004 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):   5%|▍         | 24000/500000 [00:31<07:12, 1101.12 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00008_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   5%|▌         | 25000/500000 [00:31<05:52, 1348.60 examples/s]Running tokenizer on dataset (num_proc=16):   5%|▌         | 27000/500000 [00:31<03:50, 2052.70 examples/s]Running tokenizer on dataset (num_proc=16):   6%|▌         | 28000/500000 [00:32<04:08, 1903.00 examples/s]Running tokenizer on dataset (num_proc=16):   6%|▌         | 29000/500000 [00:32<03:17, 2383.23 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:45,973 >> Token indices sequence length is longer than the specified maximum sequence length for this model (181881 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):   6%|▌         | 31000/500000 [00:33<02:51, 2727.98 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00009_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   6%|▋         | 32000/500000 [00:33<03:02, 2564.50 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00010_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   7%|▋         | 33000/500000 [00:34<03:26, 2261.70 examples/s]Running tokenizer on dataset (num_proc=16):   7%|▋         | 35000/500000 [00:35<04:28, 1734.97 examples/s]Running tokenizer on dataset (num_proc=16):   7%|▋         | 36000/500000 [00:35<03:43, 2075.71 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:49,089 >> Token indices sequence length is longer than the specified maximum sequence length for this model (38329 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00011_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   7%|▋         | 37000/500000 [00:36<03:41, 2085.70 examples/s]Running tokenizer on dataset (num_proc=16):   8%|▊         | 39000/500000 [00:36<02:30, 3062.59 examples/s]Running tokenizer on dataset (num_proc=16):   8%|▊         | 40000/500000 [00:36<02:30, 3049.11 examples/s]Running tokenizer on dataset (num_proc=16):   8%|▊         | 41000/500000 [00:37<02:33, 2980.81 examples/s]Running tokenizer on dataset (num_proc=16):   8%|▊         | 42000/500000 [00:37<02:26, 3136.40 examples/s]Running tokenizer on dataset (num_proc=16):   9%|▉         | 44000/500000 [00:38<02:33, 2975.75 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:51,149 >> Token indices sequence length is longer than the specified maximum sequence length for this model (52663 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):   9%|▉         | 45000/500000 [00:38<02:31, 3003.71 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:52,256 >> Token indices sequence length is longer than the specified maximum sequence length for this model (80592 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00012_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   9%|▉         | 47000/500000 [00:39<03:19, 2275.93 examples/s]Running tokenizer on dataset (num_proc=16):  10%|▉         | 48000/500000 [00:40<03:59, 1886.69 examples/s]Running tokenizer on dataset (num_proc=16):  10%|▉         | 49000/500000 [00:41<03:54, 1921.22 examples/s]Running tokenizer on dataset (num_proc=16):  10%|█         | 50000/500000 [00:41<03:12, 2334.83 examples/s]Running tokenizer on dataset (num_proc=16):  10%|█         | 52000/500000 [00:41<02:22, 3144.68 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:54,536 >> Token indices sequence length is longer than the specified maximum sequence length for this model (36919 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00013_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  11%|█         | 55000/500000 [00:41<01:31, 4847.58 examples/s]Running tokenizer on dataset (num_proc=16):  11%|█         | 56000/500000 [00:41<01:28, 4997.85 examples/s]Running tokenizer on dataset (num_proc=16):  11%|█▏        | 57000/500000 [00:42<02:23, 3078.21 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▏        | 59000/500000 [00:43<01:56, 3781.98 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▏        | 60000/500000 [00:43<01:55, 3826.07 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:56,855 >> Token indices sequence length is longer than the specified maximum sequence length for this model (73964 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00014_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  12%|█▏        | 61000/500000 [00:44<03:03, 2394.49 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▏        | 62000/500000 [00:44<03:30, 2082.09 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 64000/500000 [00:45<02:30, 2889.87 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 65000/500000 [00:45<02:35, 2791.32 examples/s]Running tokenizer on dataset (num_proc=16):  14%|█▎        | 68000/500000 [00:45<01:30, 4751.94 examples/s]Running tokenizer on dataset (num_proc=16):  14%|█▍        | 69000/500000 [00:46<01:59, 3599.97 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-26 21:02:59,662 >> Token indices sequence length is longer than the specified maximum sequence length for this model (105135 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  14%|█▍        | 71000/500000 [00:46<01:45, 4064.29 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aee5c63566873201_00015_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  14%|█▍        | 72000/500000 [00:47<01:51, 3854.14 examples/s]Running tokenizer on dataset (num_proc=16):  15%|█▍        | 73000/500000 [00:47<02:08, 3313.52 examples/s]Running tokenizer on dataset (num_proc=16):  15%|█▍        | 74000/500000 [00:47<01:53, 3767.91 examples/s]Running tokenizer on dataset (num_proc=16):  15%|█▌        | 75000/500000 [00:47<01:36, 4392.70 examples/s]Running tokenizer on dataset (num_proc=16):  15%|█▌        | 76000/500000 [00:48<02:15, 3124.38 examples/s]Running tokenizer on dataset (num_proc=16):  15%|█▌        | 77000/500000 [00:49<02:55, 2416.22 examples/s]Running tokenizer on dataset (num_proc=16):  16%|█▌        | 78000/500000 [00:49<02:21, 2974.46 examples/s]Running tokenizer on dataset (num_proc=16):  16%|█▌        | 79000/500000 [00:49<02:49, 2480.35 examples/s]Running tokenizer on dataset (num_proc=16):  16%|█▌        | 80000/500000 [00:49<02:25, 2895.62 examples/s]Running tokenizer on dataset (num_proc=16):  16%|█▌        | 81000/500000 [00:50<02:02, 3417.54 examples/s]Running tokenizer on dataset (num_proc=16):  17%|█▋        | 83000/500000 [00:50<01:31, 4579.23 examples/s]Running tokenizer on dataset (num_proc=16):  17%|█▋        | 84000/500000 [00:50<01:44, 3962.14 examples/s]Running tokenizer on dataset (num_proc=16):  17%|█▋        | 85000/500000 [00:50<01:29, 4639.59 examples/s]Running tokenizer on dataset (num_proc=16):  17%|█▋        | 86000/500000 [00:51<01:24, 4896.12 examples/s]Running tokenizer on dataset (num_proc=16):  17%|█▋        | 87000/500000 [00:51<01:23, 4919.64 examples/s]Running tokenizer on dataset (num_proc=16):  18%|█▊        | 88000/500000 [00:51<02:26, 2820.69 examples/s]Running tokenizer on dataset (num_proc=16):  18%|█▊        | 89000/500000 [00:52<02:18, 2976.59 examples/s]Running tokenizer on dataset (num_proc=16):  18%|█▊        | 90000/500000 [00:52<01:53, 3604.53 examples/s]Running tokenizer on dataset (num_proc=16):  18%|█▊        | 91000/500000 [00:52<02:24, 2836.12 examples/s]Running tokenizer on dataset (num_proc=16):  18%|█▊        | 92000/500000 [00:53<03:09, 2148.00 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▊        | 93000/500000 [00:53<02:31, 2693.28 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 94000/500000 [00:53<02:03, 3300.62 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 96000/500000 [00:54<02:26, 2767.06 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 97000/500000 [00:55<02:18, 2906.35 examples/s]Running tokenizer on dataset (num_proc=16):  20%|█▉        | 98000/500000 [00:55<02:04, 3221.82 examples/s]Running tokenizer on dataset (num_proc=16):  20%|██        | 100000/500000 [00:55<01:53, 3532.01 examples/s]Running tokenizer on dataset (num_proc=16):  20%|██        | 101000/500000 [00:55<01:36, 4126.22 examples/s]Running tokenizer on dataset (num_proc=16):  21%|██        | 103000/500000 [00:56<01:51, 3565.21 examples/s]Running tokenizer on dataset (num_proc=16):  21%|██        | 104000/500000 [00:56<01:58, 3330.17 examples/s]Running tokenizer on dataset (num_proc=16):  21%|██        | 105000/500000 [00:57<01:59, 3294.56 examples/s]Running tokenizer on dataset (num_proc=16):  21%|██▏       | 107000/500000 [00:58<02:45, 2368.43 examples/s]Running tokenizer on dataset (num_proc=16):  22%|██▏       | 110000/500000 [00:58<01:35, 4099.35 examples/s]Running tokenizer on dataset (num_proc=16):  22%|██▏       | 111000/500000 [00:59<02:07, 3047.94 examples/s]Running tokenizer on dataset (num_proc=16):  22%|██▏       | 112000/500000 [00:59<01:52, 3461.72 examples/s]Running tokenizer on dataset (num_proc=16):  23%|██▎       | 114000/500000 [00:59<01:33, 4113.65 examples/s]Running tokenizer on dataset (num_proc=16):  23%|██▎       | 116000/500000 [01:00<01:40, 3825.87 examples/s]Running tokenizer on dataset (num_proc=16):  23%|██▎       | 117000/500000 [01:00<01:28, 4331.41 examples/s]Running tokenizer on dataset (num_proc=16):  24%|██▎       | 118000/500000 [01:01<02:24, 2637.50 examples/s]Running tokenizer on dataset (num_proc=16):  24%|██▍       | 120000/500000 [01:02<02:38, 2396.78 examples/s]Running tokenizer on dataset (num_proc=16):  24%|██▍       | 121000/500000 [01:02<02:32, 2492.40 examples/s]Running tokenizer on dataset (num_proc=16):  24%|██▍       | 122000/500000 [01:03<02:25, 2599.58 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▍       | 124000/500000 [01:03<01:38, 3827.91 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 126000/500000 [01:03<01:12, 5183.98 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 127000/500000 [01:03<01:25, 4347.56 examples/s]Running tokenizer on dataset (num_proc=16):  26%|██▌       | 128000/500000 [01:03<01:20, 4626.86 examples/s]Running tokenizer on dataset (num_proc=16):  26%|██▌       | 130000/500000 [01:03<00:56, 6595.68 examples/s]Running tokenizer on dataset (num_proc=16):  26%|██▋       | 132000/500000 [01:04<01:25, 4314.35 examples/s]Running tokenizer on dataset (num_proc=16):  27%|██▋       | 133000/500000 [01:05<01:32, 3966.08 examples/s]Running tokenizer on dataset (num_proc=16):  27%|██▋       | 134000/500000 [01:05<02:22, 2573.22 examples/s]Running tokenizer on dataset (num_proc=16):  27%|██▋       | 135000/500000 [01:06<02:56, 2073.01 examples/s]Running tokenizer on dataset (num_proc=16):  27%|██▋       | 136000/500000 [01:06<02:22, 2551.08 examples/s]Running tokenizer on dataset (num_proc=16):  27%|██▋       | 137000/500000 [01:06<02:01, 2990.60 examples/s]Running tokenizer on dataset (num_proc=16):  28%|██▊       | 139000/500000 [01:07<02:07, 2840.69 examples/s]Running tokenizer on dataset (num_proc=16):  28%|██▊       | 140000/500000 [01:07<01:57, 3064.51 examples/s]Running tokenizer on dataset (num_proc=16):  28%|██▊       | 142000/500000 [01:08<01:24, 4247.05 examples/s]Running tokenizer on dataset (num_proc=16):  29%|██▉       | 144000/500000 [01:08<01:09, 5123.18 examples/s]Running tokenizer on dataset (num_proc=16):  29%|██▉       | 146000/500000 [01:08<01:16, 4607.48 examples/s]Running tokenizer on dataset (num_proc=16):  29%|██▉       | 147000/500000 [01:09<01:12, 4869.84 examples/s]Running tokenizer on dataset (num_proc=16):  30%|██▉       | 148000/500000 [01:09<01:09, 5039.06 examples/s]Running tokenizer on dataset (num_proc=16):  30%|███       | 150000/500000 [01:10<02:34, 2267.26 examples/s]Running tokenizer on dataset (num_proc=16):  30%|███       | 151000/500000 [01:11<02:15, 2580.42 examples/s]Running tokenizer on dataset (num_proc=16):  30%|███       | 152000/500000 [01:11<01:51, 3120.71 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███       | 153000/500000 [01:11<01:32, 3770.25 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███       | 154000/500000 [01:12<02:11, 2630.48 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███       | 156000/500000 [01:12<01:22, 4150.20 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 158000/500000 [01:12<00:57, 5941.58 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 160000/500000 [01:12<01:10, 4802.47 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 162000/500000 [01:13<01:22, 4102.93 examples/s]Running tokenizer on dataset (num_proc=16):  33%|███▎      | 163000/500000 [01:13<01:30, 3718.34 examples/s]Running tokenizer on dataset (num_proc=16):  33%|███▎      | 164000/500000 [01:13<01:20, 4157.33 examples/s]Running tokenizer on dataset (num_proc=16):  33%|███▎      | 165000/500000 [01:14<01:31, 3643.91 examples/s]Running tokenizer on dataset (num_proc=16):  33%|███▎      | 166000/500000 [01:15<03:21, 1661.08 examples/s]Running tokenizer on dataset (num_proc=16):  34%|███▎      | 168000/500000 [01:16<02:05, 2635.84 examples/s]Running tokenizer on dataset (num_proc=16):  34%|███▍      | 170000/500000 [01:16<01:34, 3501.23 examples/s]Running tokenizer on dataset (num_proc=16):  35%|███▍      | 173000/500000 [01:16<01:01, 5315.29 examples/s]Running tokenizer on dataset (num_proc=16):  35%|███▌      | 175000/500000 [01:16<00:56, 5737.14 examples/s]Running tokenizer on dataset (num_proc=16):  35%|███▌      | 176000/500000 [01:17<01:37, 3317.63 examples/s]Running tokenizer on dataset (num_proc=16):  35%|███▌      | 177000/500000 [01:17<01:37, 3300.57 examples/s]Running tokenizer on dataset (num_proc=16):  36%|███▌      | 178000/500000 [01:18<01:36, 3343.23 examples/s]Running tokenizer on dataset (num_proc=16):  36%|███▌      | 180000/500000 [01:18<01:12, 4396.83 examples/s]Running tokenizer on dataset (num_proc=16):  36%|███▌      | 181000/500000 [01:20<03:05, 1718.99 examples/s]Running tokenizer on dataset (num_proc=16):  37%|███▋      | 183000/500000 [01:20<02:21, 2235.96 examples/s]Running tokenizer on dataset (num_proc=16):  37%|███▋      | 185000/500000 [01:20<01:37, 3220.15 examples/s]Running tokenizer on dataset (num_proc=16):  37%|███▋      | 187000/500000 [01:21<01:11, 4355.40 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 188000/500000 [01:21<01:04, 4838.39 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 190000/500000 [01:21<00:58, 5285.22 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 191000/500000 [01:21<01:01, 5033.95 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 192000/500000 [01:21<01:00, 5054.46 examples/s]Running tokenizer on dataset (num_proc=16):  39%|███▊      | 193000/500000 [01:22<01:09, 4433.17 examples/s]Running tokenizer on dataset (num_proc=16):  39%|███▉      | 194000/500000 [01:22<01:13, 4156.57 examples/s]Running tokenizer on dataset (num_proc=16):  39%|███▉      | 195000/500000 [01:22<01:18, 3880.15 examples/s]Running tokenizer on dataset (num_proc=16):  39%|███▉      | 196000/500000 [01:22<01:08, 4466.45 examples/s]Running tokenizer on dataset (num_proc=16):  39%|███▉      | 197000/500000 [01:24<02:42, 1867.03 examples/s]Running tokenizer on dataset (num_proc=16):  40%|███▉      | 198000/500000 [01:25<02:57, 1702.77 examples/s]Running tokenizer on dataset (num_proc=16):  40%|████      | 200000/500000 [01:25<01:49, 2744.45 examples/s]Running tokenizer on dataset (num_proc=16):  41%|████      | 203000/500000 [01:25<01:15, 3951.14 examples/s]Running tokenizer on dataset (num_proc=16):  41%|████      | 205000/500000 [01:26<01:09, 4226.09 examples/s]Running tokenizer on dataset (num_proc=16):  41%|████      | 206000/500000 [01:26<01:17, 3793.97 examples/s]Running tokenizer on dataset (num_proc=16):  41%|████▏     | 207000/500000 [01:26<01:16, 3823.91 examples/s]Running tokenizer on dataset (num_proc=16):  42%|████▏     | 208000/500000 [01:26<01:13, 3997.19 examples/s]Running tokenizer on dataset (num_proc=16):  42%|████▏     | 210000/500000 [01:26<00:50, 5767.99 examples/s]Running tokenizer on dataset (num_proc=16):  42%|████▏     | 211000/500000 [01:27<01:14, 3904.67 examples/s]Running tokenizer on dataset (num_proc=16):  42%|████▏     | 212000/500000 [01:27<01:14, 3886.38 examples/s]Running tokenizer on dataset (num_proc=16):  43%|████▎     | 213000/500000 [01:28<01:47, 2678.26 examples/s]Running tokenizer on dataset (num_proc=16):  43%|████▎     | 214000/500000 [01:28<01:46, 2677.54 examples/s]Running tokenizer on dataset (num_proc=16):  43%|████▎     | 216000/500000 [01:29<01:39, 2841.46 examples/s]Running tokenizer on dataset (num_proc=16):  43%|████▎     | 217000/500000 [01:29<01:41, 2793.51 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 219000/500000 [01:30<01:10, 4010.68 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 221000/500000 [01:30<00:59, 4691.61 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 222000/500000 [01:30<01:09, 4027.19 examples/s]Running tokenizer on dataset (num_proc=16):  45%|████▌     | 225000/500000 [01:31<01:00, 4522.18 examples/s]Running tokenizer on dataset (num_proc=16):  45%|████▌     | 227000/500000 [01:31<00:52, 5243.35 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▌     | 228000/500000 [01:32<01:09, 3933.62 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▌     | 229000/500000 [01:33<01:57, 2313.52 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▌     | 230000/500000 [01:33<01:36, 2801.62 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▌     | 231000/500000 [01:34<02:04, 2157.51 examples/s]Running tokenizer on dataset (num_proc=16):  47%|████▋     | 233000/500000 [01:34<01:35, 2795.55 examples/s]Running tokenizer on dataset (num_proc=16):  47%|████▋     | 235000/500000 [01:34<01:09, 3793.60 examples/s]Running tokenizer on dataset (num_proc=16):  47%|████▋     | 236000/500000 [01:34<01:05, 4001.75 examples/s]Running tokenizer on dataset (num_proc=16):  48%|████▊     | 239000/500000 [01:35<00:40, 6475.49 examples/s]Running tokenizer on dataset (num_proc=16):  48%|████▊     | 241000/500000 [01:35<00:59, 4331.20 examples/s]Running tokenizer on dataset (num_proc=16):  49%|████▊     | 243000/500000 [01:36<00:55, 4666.05 examples/s]Running tokenizer on dataset (num_proc=16):  49%|████▉     | 245000/500000 [01:37<01:18, 3236.07 examples/s]Running tokenizer on dataset (num_proc=16):  49%|████▉     | 246000/500000 [01:38<01:38, 2589.69 examples/s]Running tokenizer on dataset (num_proc=16):  49%|████▉     | 247000/500000 [01:38<01:31, 2774.74 examples/s]Running tokenizer on dataset (num_proc=16):  50%|████▉     | 248000/500000 [01:38<01:24, 2982.06 examples/s]Running tokenizer on dataset (num_proc=16):  50%|████▉     | 249000/500000 [01:38<01:20, 3115.24 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 251000/500000 [01:39<01:09, 3578.40 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 252000/500000 [01:39<01:02, 3944.50 examples/s]Running tokenizer on dataset (num_proc=16):  51%|█████     | 253000/500000 [01:39<01:16, 3236.82 examples/s]Running tokenizer on dataset (num_proc=16):  51%|█████     | 254000/500000 [01:40<01:04, 3841.07 examples/s]Running tokenizer on dataset (num_proc=16):  51%|█████     | 255000/500000 [01:40<01:02, 3928.32 examples/s]Running tokenizer on dataset (num_proc=16):  51%|█████     | 256000/500000 [01:40<01:06, 3675.31 examples/s]Running tokenizer on dataset (num_proc=16):  51%|█████▏    | 257000/500000 [01:40<00:54, 4428.15 examples/s]Running tokenizer on dataset (num_proc=16):  52%|█████▏    | 258000/500000 [01:40<00:53, 4536.23 examples/s]Running tokenizer on dataset (num_proc=16):  52%|█████▏    | 260000/500000 [01:41<00:48, 4948.30 examples/s]Running tokenizer on dataset (num_proc=16):  52%|█████▏    | 261000/500000 [01:42<01:28, 2709.54 examples/s]Running tokenizer on dataset (num_proc=16):  52%|█████▏    | 262000/500000 [01:42<01:41, 2333.42 examples/s]Running tokenizer on dataset (num_proc=16):  53%|█████▎    | 263000/500000 [01:42<01:25, 2765.47 examples/s]Running tokenizer on dataset (num_proc=16):  53%|█████▎    | 264000/500000 [01:43<01:23, 2834.50 examples/s]Running tokenizer on dataset (num_proc=16):  53%|█████▎    | 265000/500000 [01:43<01:09, 3363.42 examples/s]Running tokenizer on dataset (num_proc=16):  53%|█████▎    | 267000/500000 [01:43<01:04, 3620.82 examples/s]Running tokenizer on dataset (num_proc=16):  54%|█████▎    | 268000/500000 [01:44<01:04, 3608.31 examples/s]Running tokenizer on dataset (num_proc=16):  54%|█████▍    | 270000/500000 [01:44<00:50, 4575.20 examples/s]Running tokenizer on dataset (num_proc=16):  54%|█████▍    | 271000/500000 [01:44<00:47, 4871.96 examples/s]Running tokenizer on dataset (num_proc=16):  54%|█████▍    | 272000/500000 [01:44<00:53, 4284.16 examples/s]Running tokenizer on dataset (num_proc=16):  55%|█████▍    | 274000/500000 [01:45<00:58, 3872.03 examples/s]Running tokenizer on dataset (num_proc=16):  55%|█████▌    | 275000/500000 [01:45<00:50, 4446.11 examples/s]Running tokenizer on dataset (num_proc=16):  55%|█████▌    | 276000/500000 [01:46<00:57, 3922.42 examples/s]Running tokenizer on dataset (num_proc=16):  55%|█████▌    | 277000/500000 [01:46<01:16, 2920.35 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▌    | 278000/500000 [01:46<01:12, 3056.68 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▌    | 279000/500000 [01:47<01:44, 2114.04 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▌    | 281000/500000 [01:47<01:03, 3446.26 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 282000/500000 [01:48<01:04, 3386.56 examples/s]Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 283000/500000 [01:48<01:02, 3499.40 examples/s]Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 285000/500000 [01:48<00:50, 4281.99 examples/s]Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 286000/500000 [01:48<00:48, 4455.65 examples/s]Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 288000/500000 [01:49<01:00, 3493.39 examples/s]Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 290000/500000 [01:50<00:58, 3582.16 examples/s]Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 291000/500000 [01:50<00:57, 3648.56 examples/s]Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 292000/500000 [01:50<00:49, 4203.82 examples/s]Running tokenizer on dataset (num_proc=16):  59%|█████▊    | 293000/500000 [01:50<00:46, 4473.88 examples/s]Running tokenizer on dataset (num_proc=16):  59%|█████▉    | 294000/500000 [01:51<01:31, 2257.74 examples/s]Running tokenizer on dataset (num_proc=16):  59%|█████▉    | 295000/500000 [01:52<01:15, 2724.92 examples/s]Running tokenizer on dataset (num_proc=16):  59%|█████▉    | 296000/500000 [01:52<01:13, 2776.91 examples/s]Running tokenizer on dataset (num_proc=16):  60%|█████▉    | 299000/500000 [01:52<00:51, 3867.45 examples/s]Running tokenizer on dataset (num_proc=16):  60%|██████    | 300000/500000 [01:53<00:50, 3933.94 examples/s]Running tokenizer on dataset (num_proc=16):  60%|██████    | 301000/500000 [01:53<00:44, 4425.52 examples/s]Running tokenizer on dataset (num_proc=16):  60%|██████    | 302000/500000 [01:53<00:41, 4728.20 examples/s]Running tokenizer on dataset (num_proc=16):  61%|██████    | 303000/500000 [01:53<00:55, 3543.12 examples/s]Running tokenizer on dataset (num_proc=16):  61%|██████    | 304000/500000 [01:54<00:49, 3924.11 examples/s]Running tokenizer on dataset (num_proc=16):  61%|██████    | 305000/500000 [01:54<00:49, 3974.90 examples/s]Running tokenizer on dataset (num_proc=16):  61%|██████    | 306000/500000 [01:54<01:06, 2909.78 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 308000/500000 [01:55<00:49, 3903.34 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 309000/500000 [01:55<00:52, 3604.90 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 310000/500000 [01:56<01:15, 2533.31 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 312000/500000 [01:56<00:53, 3510.91 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 314000/500000 [01:56<00:37, 5023.31 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 315000/500000 [01:56<00:41, 4495.92 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 316000/500000 [01:57<01:10, 2594.71 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 317000/500000 [01:58<01:08, 2677.34 examples/s]Running tokenizer on dataset (num_proc=16):  64%|██████▎   | 318000/500000 [01:58<00:56, 3249.08 examples/s]Running tokenizer on dataset (num_proc=16):  64%|██████▍   | 319000/500000 [01:58<00:49, 3658.20 examples/s]Running tokenizer on dataset (num_proc=16):  64%|██████▍   | 320000/500000 [01:58<00:47, 3767.00 examples/s]Running tokenizer on dataset (num_proc=16):  64%|██████▍   | 321000/500000 [01:59<00:58, 3068.32 examples/s]Running tokenizer on dataset (num_proc=16):  64%|██████▍   | 322000/500000 [01:59<00:56, 3134.32 examples/s]Running tokenizer on dataset (num_proc=16):  65%|██████▍   | 323000/500000 [01:59<00:49, 3582.83 examples/s]Running tokenizer on dataset (num_proc=16):  65%|██████▍   | 324000/500000 [02:00<00:51, 3446.29 examples/s]Running tokenizer on dataset (num_proc=16):  65%|██████▌   | 325000/500000 [02:00<00:53, 3264.92 examples/s]Running tokenizer on dataset (num_proc=16):  65%|██████▌   | 326000/500000 [02:00<00:56, 3078.85 examples/s]Running tokenizer on dataset (num_proc=16):  65%|██████▌   | 327000/500000 [02:00<00:49, 3493.57 examples/s]Running tokenizer on dataset (num_proc=16):  66%|██████▌   | 328000/500000 [02:01<00:49, 3453.60 examples/s]Running tokenizer on dataset (num_proc=16):  66%|██████▌   | 331000/500000 [02:01<00:28, 6011.41 examples/s]Running tokenizer on dataset (num_proc=16):  66%|██████▋   | 332000/500000 [02:01<00:40, 4131.56 examples/s]Running tokenizer on dataset (num_proc=16):  67%|██████▋   | 333000/500000 [02:02<00:51, 3266.70 examples/s]Running tokenizer on dataset (num_proc=16):  67%|██████▋   | 336000/500000 [02:03<00:47, 3443.85 examples/s]Running tokenizer on dataset (num_proc=16):  67%|██████▋   | 337000/500000 [02:03<00:43, 3730.78 examples/s]Running tokenizer on dataset (num_proc=16):  68%|██████▊   | 338000/500000 [02:03<00:50, 3200.39 examples/s]Running tokenizer on dataset (num_proc=16):  68%|██████▊   | 340000/500000 [02:04<00:41, 3870.07 examples/s]Running tokenizer on dataset (num_proc=16):  68%|██████▊   | 341000/500000 [02:05<01:02, 2544.67 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 344000/500000 [02:05<00:36, 4318.90 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 346000/500000 [02:05<00:28, 5350.16 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 347000/500000 [02:05<00:35, 4341.42 examples/s]Running tokenizer on dataset (num_proc=16):  70%|██████▉   | 349000/500000 [02:06<00:36, 4094.66 examples/s]Running tokenizer on dataset (num_proc=16):  70%|███████   | 350000/500000 [02:06<00:35, 4174.88 examples/s]Running tokenizer on dataset (num_proc=16):  70%|███████   | 351000/500000 [02:06<00:35, 4192.80 examples/s]Running tokenizer on dataset (num_proc=16):  70%|███████   | 352000/500000 [02:07<00:43, 3391.85 examples/s]Running tokenizer on dataset (num_proc=16):  71%|███████   | 353000/500000 [02:07<00:39, 3724.25 examples/s]Running tokenizer on dataset (num_proc=16):  71%|███████   | 354000/500000 [02:08<00:57, 2529.63 examples/s]Running tokenizer on dataset (num_proc=16):  71%|███████   | 355000/500000 [02:08<01:03, 2274.21 examples/s]Running tokenizer on dataset (num_proc=16):  71%|███████   | 356000/500000 [02:09<00:52, 2719.68 examples/s]Running tokenizer on dataset (num_proc=16):  71%|███████▏  | 357000/500000 [02:09<00:46, 3068.05 examples/s]Running tokenizer on dataset (num_proc=16):  72%|███████▏  | 358000/500000 [02:09<00:44, 3208.15 examples/s]Running tokenizer on dataset (num_proc=16):  72%|███████▏  | 360000/500000 [02:09<00:32, 4246.41 examples/s]Running tokenizer on dataset (num_proc=16):  72%|███████▏  | 361000/500000 [02:10<00:33, 4137.06 examples/s]Running tokenizer on dataset (num_proc=16):  73%|███████▎  | 363000/500000 [02:10<00:28, 4730.16 examples/s]Running tokenizer on dataset (num_proc=16):  73%|███████▎  | 365000/500000 [02:10<00:24, 5480.16 examples/s]Running tokenizer on dataset (num_proc=16):  73%|███████▎  | 366000/500000 [02:11<00:27, 4810.63 examples/s]Running tokenizer on dataset (num_proc=16):  73%|███████▎  | 367000/500000 [02:11<00:41, 3194.92 examples/s]Running tokenizer on dataset (num_proc=16):  74%|███████▎  | 368000/500000 [02:12<00:39, 3330.64 examples/s]Running tokenizer on dataset (num_proc=16):  74%|███████▍  | 369000/500000 [02:12<00:36, 3563.07 examples/s]Running tokenizer on dataset (num_proc=16):  74%|███████▍  | 370000/500000 [02:12<00:44, 2944.52 examples/s]Running tokenizer on dataset (num_proc=16):  74%|███████▍  | 372000/500000 [02:13<00:37, 3395.11 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▍  | 373000/500000 [02:13<00:34, 3730.79 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 375000/500000 [02:13<00:29, 4185.86 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 376000/500000 [02:14<00:42, 2886.99 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 377000/500000 [02:14<00:39, 3103.23 examples/s]Running tokenizer on dataset (num_proc=16):  76%|███████▌  | 378000/500000 [02:14<00:34, 3568.32 examples/s]Running tokenizer on dataset (num_proc=16):  76%|███████▌  | 379000/500000 [02:15<00:33, 3633.82 examples/s]Running tokenizer on dataset (num_proc=16):  76%|███████▌  | 381000/500000 [02:15<00:24, 4911.83 examples/s]Running tokenizer on dataset (num_proc=16):  76%|███████▋  | 382000/500000 [02:15<00:21, 5421.13 examples/s]Running tokenizer on dataset (num_proc=16):  77%|███████▋  | 383000/500000 [02:16<00:38, 3074.38 examples/s]Running tokenizer on dataset (num_proc=16):  77%|███████▋  | 384000/500000 [02:16<00:35, 3275.31 examples/s]Running tokenizer on dataset (num_proc=16):  77%|███████▋  | 385000/500000 [02:17<00:47, 2439.53 examples/s]Running tokenizer on dataset (num_proc=16):  77%|███████▋  | 386000/500000 [02:17<00:42, 2654.61 examples/s]Running tokenizer on dataset (num_proc=16):  77%|███████▋  | 387000/500000 [02:17<00:37, 3041.67 examples/s]Running tokenizer on dataset (num_proc=16):  78%|███████▊  | 388000/500000 [02:17<00:34, 3230.89 examples/s]Running tokenizer on dataset (num_proc=16):  78%|███████▊  | 391000/500000 [02:18<00:25, 4245.33 examples/s]Running tokenizer on dataset (num_proc=16):  78%|███████▊  | 392000/500000 [02:18<00:24, 4347.48 examples/s]Running tokenizer on dataset (num_proc=16):  79%|███████▉  | 394000/500000 [02:19<00:22, 4759.82 examples/s]Running tokenizer on dataset (num_proc=16):  79%|███████▉  | 395000/500000 [02:19<00:28, 3667.89 examples/s]Running tokenizer on dataset (num_proc=16):  79%|███████▉  | 397000/500000 [02:19<00:20, 4928.83 examples/s]Running tokenizer on dataset (num_proc=16):  80%|███████▉  | 398000/500000 [02:19<00:22, 4587.57 examples/s]Running tokenizer on dataset (num_proc=16):  80%|████████  | 400000/500000 [02:21<00:35, 2850.27 examples/s]Running tokenizer on dataset (num_proc=16):  80%|████████  | 401000/500000 [02:21<00:43, 2262.73 examples/s]Running tokenizer on dataset (num_proc=16):  80%|████████  | 402000/500000 [02:22<00:36, 2663.02 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████  | 404000/500000 [02:22<00:25, 3694.48 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████  | 405000/500000 [02:22<00:26, 3640.88 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████  | 406000/500000 [02:22<00:23, 4018.67 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 407000/500000 [02:22<00:19, 4659.39 examples/s]Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 408000/500000 [02:22<00:16, 5434.93 examples/s]Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 409000/500000 [02:23<00:16, 5369.51 examples/s]Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 410000/500000 [02:23<00:19, 4732.22 examples/s]Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 412000/500000 [02:23<00:13, 6657.24 examples/s]Running tokenizer on dataset (num_proc=16):  83%|████████▎ | 414000/500000 [02:24<00:16, 5217.19 examples/s]Running tokenizer on dataset (num_proc=16):  83%|████████▎ | 416000/500000 [02:25<00:26, 3122.42 examples/s]Running tokenizer on dataset (num_proc=16):  83%|████████▎ | 417000/500000 [02:26<00:36, 2300.65 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▎ | 418000/500000 [02:26<00:29, 2753.20 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▍ | 419000/500000 [02:26<00:30, 2653.75 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▍ | 421000/500000 [02:26<00:19, 3965.37 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▍ | 422000/500000 [02:27<00:29, 2616.38 examples/s]Running tokenizer on dataset (num_proc=16):  85%|████████▍ | 424000/500000 [02:27<00:19, 3966.00 examples/s]Running tokenizer on dataset (num_proc=16):  85%|████████▌ | 426000/500000 [02:27<00:14, 5048.54 examples/s]Running tokenizer on dataset (num_proc=16):  86%|████████▌ | 429000/500000 [02:28<00:13, 5289.18 examples/s]Running tokenizer on dataset (num_proc=16):  86%|████████▌ | 430000/500000 [02:28<00:12, 5444.32 examples/s]Running tokenizer on dataset (num_proc=16):  86%|████████▋ | 431250/500000 [02:28<00:12, 5404.21 examples/s]Running tokenizer on dataset (num_proc=16):  86%|████████▋ | 432250/500000 [02:29<00:14, 4558.87 examples/s]Running tokenizer on dataset (num_proc=16):  87%|████████▋ | 433500/500000 [02:30<00:23, 2777.17 examples/s]Running tokenizer on dataset (num_proc=16):  87%|████████▋ | 434500/500000 [02:30<00:23, 2729.38 examples/s]Running tokenizer on dataset (num_proc=16):  87%|████████▋ | 435500/500000 [02:30<00:23, 2776.67 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 437500/500000 [02:31<00:15, 4029.50 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 438500/500000 [02:31<00:15, 3877.73 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 439500/500000 [02:31<00:21, 2815.72 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 441750/500000 [02:32<00:13, 4217.10 examples/s]Running tokenizer on dataset (num_proc=16):  89%|████████▉ | 443750/500000 [02:32<00:12, 4600.61 examples/s]Running tokenizer on dataset (num_proc=16):  89%|████████▉ | 444750/500000 [02:32<00:12, 4524.48 examples/s]Running tokenizer on dataset (num_proc=16):  89%|████████▉ | 445750/500000 [02:32<00:10, 5041.63 examples/s]Running tokenizer on dataset (num_proc=16):  89%|████████▉ | 446750/500000 [02:33<00:18, 2935.57 examples/s]Running tokenizer on dataset (num_proc=16):  90%|████████▉ | 447750/500000 [02:34<00:26, 1993.12 examples/s]Running tokenizer on dataset (num_proc=16):  90%|████████▉ | 448750/500000 [02:34<00:22, 2266.14 examples/s]Running tokenizer on dataset (num_proc=16):  90%|████████▉ | 449750/500000 [02:35<00:17, 2864.84 examples/s]Running tokenizer on dataset (num_proc=16):  90%|█████████ | 450750/500000 [02:35<00:14, 3495.06 examples/s]Running tokenizer on dataset (num_proc=16):  90%|█████████ | 451750/500000 [02:35<00:18, 2626.37 examples/s]Running tokenizer on dataset (num_proc=16):  91%|█████████ | 453750/500000 [02:35<00:12, 3805.60 examples/s]Running tokenizer on dataset (num_proc=16):  91%|█████████ | 454750/500000 [02:36<00:12, 3639.62 examples/s]Running tokenizer on dataset (num_proc=16):  91%|█████████ | 455750/500000 [02:36<00:11, 3898.63 examples/s]Running tokenizer on dataset (num_proc=16):  91%|█████████▏| 457000/500000 [02:36<00:11, 3831.56 examples/s]Running tokenizer on dataset (num_proc=16):  92%|█████████▏| 459000/500000 [02:36<00:07, 5722.88 examples/s]Running tokenizer on dataset (num_proc=16):  92%|█████████▏| 460250/500000 [02:37<00:12, 3073.07 examples/s]Running tokenizer on dataset (num_proc=16):  92%|█████████▏| 461500/500000 [02:38<00:11, 3380.85 examples/s]Running tokenizer on dataset (num_proc=16):  92%|█████████▎| 462500/500000 [02:39<00:17, 2097.85 examples/s]Running tokenizer on dataset (num_proc=16):  93%|█████████▎| 464500/500000 [02:39<00:12, 2875.62 examples/s]Running tokenizer on dataset (num_proc=16):  93%|█████████▎| 465500/500000 [02:40<00:14, 2399.76 examples/s]Running tokenizer on dataset (num_proc=16):  93%|█████████▎| 466500/500000 [02:40<00:12, 2639.06 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▎| 467500/500000 [02:40<00:11, 2910.17 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▎| 468500/500000 [02:40<00:09, 3388.98 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 469500/500000 [02:41<00:14, 2167.27 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 470750/500000 [02:42<00:12, 2391.57 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 471750/500000 [02:43<00:19, 1434.33 examples/s]Running tokenizer on dataset (num_proc=16):  95%|█████████▍| 472750/500000 [02:43<00:14, 1873.64 examples/s]Running tokenizer on dataset (num_proc=16):  95%|█████████▍| 473750/500000 [02:43<00:11, 2196.00 examples/s]Running tokenizer on dataset (num_proc=16):  95%|█████████▍| 474750/500000 [02:44<00:12, 2045.75 examples/s]Running tokenizer on dataset (num_proc=16):  95%|█████████▌| 476750/500000 [02:45<00:09, 2462.21 examples/s]Running tokenizer on dataset (num_proc=16):  96%|█████████▌| 478750/500000 [02:45<00:07, 2943.26 examples/s]Running tokenizer on dataset (num_proc=16):  96%|█████████▌| 480000/500000 [02:46<00:07, 2606.39 examples/s]Running tokenizer on dataset (num_proc=16):  96%|█████████▋| 481250/500000 [02:47<00:09, 1885.15 examples/s]Running tokenizer on dataset (num_proc=16):  97%|█████████▋| 483250/500000 [02:48<00:08, 1946.94 examples/s]Running tokenizer on dataset (num_proc=16):  97%|█████████▋| 484250/500000 [02:48<00:07, 1991.35 examples/s]Running tokenizer on dataset (num_proc=16):  97%|█████████▋| 485250/500000 [02:49<00:08, 1741.81 examples/s]Running tokenizer on dataset (num_proc=16):  97%|█████████▋| 487250/500000 [02:49<00:04, 2577.36 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 488250/500000 [02:51<00:07, 1541.15 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 489250/500000 [02:51<00:06, 1640.18 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 490250/500000 [02:52<00:06, 1467.73 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 491500/500000 [02:53<00:05, 1564.99 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 492500/500000 [02:53<00:03, 1924.21 examples/s]Running tokenizer on dataset (num_proc=16):  99%|█████████▉| 494750/500000 [02:54<00:02, 2199.33 examples/s]Running tokenizer on dataset (num_proc=16):  99%|█████████▉| 495250/500000 [02:54<00:02, 2113.82 examples/s]Running tokenizer on dataset (num_proc=16):  99%|█████████▉| 496250/500000 [02:55<00:02, 1607.54 examples/s]Running tokenizer on dataset (num_proc=16):  99%|█████████▉| 496500/500000 [02:57<00:03, 1027.02 examples/s]Running tokenizer on dataset (num_proc=16): 100%|█████████▉| 498500/500000 [02:58<00:01, 1328.53 examples/s]Running tokenizer on dataset (num_proc=16): 100%|█████████▉| 498750/500000 [02:58<00:00, 1376.53 examples/s]Running tokenizer on dataset (num_proc=16): 100%|█████████▉| 499750/500000 [03:02<00:00, 531.29 examples/s] Running tokenizer on dataset (num_proc=16): 100%|██████████| 500000/500000 [03:03<00:00, 489.67 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 500000/500000 [03:03<00:00, 2723.61 examples/s]
Concatenating 16 shards
input_ids:
[12093, 70290, 6859, 6484, 43, 890, 27134, 2448, 7395, 30400, 55, 549, 26209, 198, 6622, 198, 16578, 70290, 6859, 6484, 43, 890, 27134, 2448, 7395, 30400, 55, 198, 6622, 198, 151643, 7039, 33976, 432, 646, 387, 264, 2699, 312, 2015, 1388, 311, 3270, 458, 4549, 911, 279, 10990, 7070, 14429, 3840, 438, 1052, 374, 825, 304, 1449, 1614, 14418, 504, 882, 311, 882, 11, 323, 279, 3482, 374, 49485, 448, 1105, 304, 1449, 4128, 369, 5019, 879, 30997, 311, 1349, 1105, 13, 2055, 358, 686, 36355, 304, 11629, 1246, 358, 5798, 323, 23983, 279, 1614, 624, 34762, 1635, 4134, 16145, 6635, 311, 1281, 1549, 862, 10990, 7070, 14429, 31496, 438, 807, 1030, 264, 501, 3093, 315, 50270, 82, 429, 1410, 15551, 279, 12188, 504, 2155, 11067, 323, 25941, 323, 8450, 1281, 64255, 323, 803, 35201, 9666, 13, 2379, 3381, 429, 3432, 419, 501, 5440, 432, 1035, 387, 2664, 311, 8193, 1549, 279, 330, 1040, 1211, 1, 1091, 894, 1008, 25546, 6174, 323, 432, 4977, 429, 807, 1033, 1290, 438, 1449, 1463, 7073, 10788, 825, 52163, 269, 803, 624, 9485, 501, 50270, 82, 1033, 537, 279, 1172, 501, 3166, 304, 1493, 4119, 11, 807, 1030, 264, 738, 315, 13918, 7746, 2669, 18663, 504, 279, 8151, 1137, 5527, 311, 41740, 11, 1045, 6548, 36780, 9317, 5479, 323, 501, 97685, 624, 2121, 5135, 438, 358, 8930, 279, 3745, 358, 1030, 264, 1602, 2797, 2168, 315, 279, 3093, 315, 1614, 358, 4829, 311, 1281, 11, 358, 1030, 3884, 10077, 315, 24248, 304, 279, 6467, 11, 1602, 76873, 12645, 98732, 448, 25386, 424, 429, 95758, 2310, 7218, 76024, 624, 2461, 419, 2390, 358, 6635, 311, 990, 279, 5235, 18457, 53514, 22293, 738, 369, 279, 10990, 7070, 14429, 429, 374, 2167, 14452, 323, 18304, 13942, 624, 2132, 4436, 944, 16965, 438, 279, 5479, 4946, 1602, 1632, 323, 279, 11221, 525, 1602, 2797, 11, 2337, 279, 1882, 582, 686, 1172, 614, 311, 1896, 2453, 979, 11589, 279, 2632, 5479, 311, 5648, 14719, 1105, 476, 11785, 11, 775, 14421, 1105, 13, 1634, 847, 4522, 504, 279, 7167, 572, 311, 4009, 264, 12896, 448, 279, 23603, 86968, 18824, 24569, 23704, 700, 11, 358, 23983, 279, 1614, 448, 5938, 12258, 25685, 12463, 438, 358, 5798, 432, 311, 614, 419, 12463, 2331, 369, 279, 2937, 330, 35012, 18824, 1, 14762, 358, 1035, 3796, 389, 279, 86968, 13, 2055, 358, 5798, 279, 44909, 323, 279, 64386, 9380, 15663, 279, 305, 9118, 11, 22696, 11, 39932, 13569, 11, 4992, 13, 14576, 311, 6707, 6177, 279, 65739, 448, 279, 15625, 476, 279, 68348, 13, 151643, 641, 3213, 1635, 358, 614, 65806, 3807, 52269, 14697, 304, 847, 2205, 3082, 13, 20035, 537, 3884, 894, 32319, 15570, 4730, 5926, 358, 572, 18442, 311, 1490, 458, 52269, 16262, 825, 315, 847, 22791, 14697, 264, 5625, 315, 2849, 4134, 13, 60033, 358, 9099, 847, 8849, 6249, 14046, 311, 1490, 421, 279, 52269, 374, 264, 5792, 20181, 13, 3197, 358, 10067, 279, 21852, 419, 6556, 358, 572, 33972, 311, 1490, 358, 614, 264, 6716, 315, 32319, 15570, 4730, 304, 21682, 13, 358, 2776, 10282, 4297, 27031, 1431, 304, 279, 3900, 429, 807, 686, 27775, 13, 10357, 419, 3550, 0, 151643, 38112, 28909, 874, 82, 9679, 18905, 572, 9555, 304, 1042, 220, 16, 24, 24, 20, 438, 44365, 315, 18495, 11, 47885, 11, 90884, 323, 30491, 323, 2114, 4563, 3020, 13, 5542, 429, 882, 11, 582, 3003, 8570, 311, 3063, 71323, 553, 21080, 389, 8241, 4271, 12506, 518, 48039, 3347, 7576, 13, 6065, 279, 1635, 11, 582, 3003, 7790, 17183, 1039, 1985, 1555, 311, 2924, 264, 6884, 8045, 315, 16916, 47674, 6741, 829, 11502, 30491, 429, 646, 944, 6707, 387, 1730, 1526, 1008, 7982, 11744, 323, 419, 23132, 13, 11201, 14102, 28909, 874, 374, 4221, 279, 61853, 594, 7772, 25211, 13111, 315, 6366, 11, 90884, 323, 11502, 30491, 7096, 624, 7981, 30112, 323, 25042, 525, 7407, 304, 40669, 27347, 11, 61853, 11, 6747, 13, 1205, 5591, 220, 24, 24, 4, 315, 4297, 582, 4559, 323, 3010, 1852, 1899, 11601, 389, 1429, 10163, 624, 4431, 287, 504, 1039, 3910, 374, 3974, 323, 4285, 13, 46093, 1039, 3910, 3545, 686, 2968, 498, 279, 9423, 315, 41587, 279, 5535, 12506, 1573, 807, 4559, 700, 13, 151643, 48, 25, 17780, 7818, 804, 6147, 12621, 358, 1079, 68672, 264, 1614, 1667, 264, 20180, 510, 1040, 2657, 366, 17780, 486, 3978, 198, 220, 51708, 56196, 284, 23536, 32, 26056, 86, 41715, 14523, 7574, 40160, 64, 9141, 34487, 59, 14523, 66327, 7873, 64, 9141, 66327, 89, 16438, 198, 220, 13872, 56196, 284, 608, 7609, 59, 86, 9769, 72, 271, 220, 26257, 549, 606, 11, 220, 9362, 25, 830, 11, 3084, 25, 314, 7192, 25, 220, 16, 21, 2470, 3561, 25, 314, 448, 25, 13872, 56196, 456, 220, 26257, 549, 2332, 11, 9362, 25, 830, 11, 59057, 25, 830, 11, 3561, 25, 314, 448, 25, 51708, 56196, 456, 408, 271, 11209, 11, 279, 549, 606, 19130, 374, 537, 12440, 32332, 13, 1416, 600, 728, 1119, 2139, 112, 37345, 2339, 28111, 323, 1855, 264, 501, 2867, 315, 279, 2657, 536, 11, 600, 646, 738, 549, 606, 311, 264, 914, 8482, 12621, 323, 659, 1891, 30, 2058, 4675, 830, 13, 1416, 358, 1273, 279, 20180, 304, 6216, 65, 11, 432, 1558, 537, 2432, 894, 12621, 624, 32, 29716, 510, 3, 42600, 272, 1177, 76756, 198, 9338, 17, 13, 15, 13, 15, 2268, 19, 20, 16, 549, 15, 15, 16, 861, 575, 284, 2657, 4618, 829, 25, 330, 14990, 1879, 497, 2551, 25, 330, 1944, 35487, 905, 698, 17, 13, 15, 13, 15, 2268, 19, 20, 16, 549, 15, 15, 17, 861, 575, 20586, 5267, 220, 2657, 72426, 320, 15, 13, 17, 1011, 8, 220, 19094, 220, 16, 5752, 825, 4295, 330, 4218, 1, 5288, 330, 4218, 65720, 2332, 1, 284, 364, 1944, 35487, 905, 6, 10376, 220, 16, 198, 589, 830, 271, 40, 1079, 1667, 23726, 220, 17, 13, 15, 13, 15, 2268, 19, 20, 16, 389, 14340, 11, 448, 42600, 220, 19, 13, 15, 13, 19, 382, 32, 25, 4615, 5792, 7493, 4436, 944, 77119, 11, 323, 702, 902, 10272, 3049, 13, 608, 7609, 59, 86, 9769, 72, 9071, 894, 914, 429, 702, 894, 3668, 12579, 429, 20180, 624, 785, 4396, 20180, 1035]
inputs:
@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject
@end
@implementation PodsDummy_XCDLumberjackNSLogger_OSX
@end
<|endoftext|>Nowadays it can be a bit reiterative to write an article about the Panzer III history as there is one in every model magazine from time to time, and the web is saturated with them in every language for everyone who desires to read them. So I will concentrate in telling how I built and painted the model.
Several years ago Dragon decided to make again their Panzer III kits as they had a new kind of moulds that could inject the plastic from different sides and angles and thus make thinner and more delicate pieces. They thought that having this new technology it would be better to produce again the "classics" than any other newer ones and it seems that they were right as every modeller bought one…..or more.
These new moulds were not the only new thing in these models, they had a set of tracks links already separated from the sprues ready to assemble, some photoetched metal parts and new decals.
As soon as I opened the box I had a very clear image of the kind of model I wanted to make, I had seen lots of photographs in the books, very dusty machines cramped with equipage that resembled old moving vans.
For this project I decided to use the Blackdog resin accessories set for the Panzer III that is really suitable and fits perfectly.
It isn't complicated as the parts fit very well and the instructions are very clear, during the process we will only have to take care when handling the little parts to avoid breaking them or worst, loosing them. As my idea from the beginning was to represent a tank with the desert camouflage painting badly worn out, I painted the model with German Dark Grey colour as I built it to have this colour base for the later "soap painting" technique I would apply on the camouflage. So I built the chassis and the turret leaving aside the hatches, wheels, antenna rail, etc. mainly to easily paint the scratches with the brush or the sponge.<|endoftext|>In recent years I have erected several owl boxes in my local area. Having not seen any barn owls recently I was pleased to see an owl entering one of my nest boxes a couple of days ago. Yesterday I placed my trail camera nearby to see if the owl is a regular visitor. When I checked the footage this morning I was delighted to see I have a pair of barn owls in residence. I'm keeping everything crossed now in the hope that they will breed. Watch this space!<|endoftext|>Digital Syscoms Private Limited was established in year 1995 as trader of computers, laptops, peripherals and electronics and home applicances. From that time, we've continued to grow tremendously by focusing on providing quality deals at aggressively low prices. Over the years, we've successfully expanded our product line to include a wide variety of heavily discounted brand name consumer electronics that can't easily be found through other distribution channels and this portal. Today Digital Syscom is among the Gujarat's largest wholesaler of computer, peripherals and consumer electronics equipment.
Our warehouse and headquarters are located in Ahmedabad, Gujarat, India. We stock 99% of everything we sell and offer same day shipping on most orders.
Ordering from our website is quick and simple. Checking our website often will give you the advantage of discovering the latest deals before they sell out.<|endoftext|>Q: ActiveRecord Validations allows spaces I am validating a model using a regex:
class User < ActiveRecord::Base
  EMAIL_REGEX = /\A[\w+\-.]+@[a-z\d\-.]+\.[a-z]+\z/i
  USER_REGEX = /[-\w]/i

  validates :name,  presence: true, length: { maximum: 16 }, format: { with: USER_REGEX }
  validates :email, presence: true, uniqueness: true, format: { with: EMAIL_REGEX }
end

However, the :name-field is not correctly validated. If i go into ´rails console´ and create a new instance of the User class, i can set :name to a string containing spaces and .valid? still returns true. If I test the regex in irb, it does not match any spaces.
A demonstration:
$ rails c --sandbox
...
2.0.0-p451 :001 > u = User.new name: "hello world", email: "test@example.com"
2.0.0-p451 :002 > u.valid?
  User Exists (0.2ms)  SELECT 1 AS one FROM "users" WHERE "users"."email" = 'test@example.com' LIMIT 1
 => true

I am using Ruby 2.0.0-p451 on Linux, with rails 4.0.4.

A: Your regular expression isn't anchored, and has no quantifier. /[-\w]/i matches any string that has any character matching that regex.
The correct regex would
Caching indices mapping at /home/yangdezhao/.cache/huggingface/datasets/json/default-8967e71434917b95/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2ae0b52a42f5b44.arrow
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 496681
})
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): Qwen2ForCausalLM(
      (model): Qwen2Model(
        (embed_tokens): Embedding(151936, 2048)
        (layers): ModuleList(
          (0-23): 24 x Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
        )
        (norm): Qwen2RMSNorm()
      )
      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
    )
  )
)
[INFO|trainer.py:586] 2024-05-26 21:05:16,785 >> Using auto half precision backend
[2024-05-26 21:05:17,069] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 496681
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 496681
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 496681
})
[2024-05-26 21:05:23,024] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-26 21:05:23,026] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-05-26 21:05:23,026] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-26 21:05:23,046] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-05-26 21:05:23,046] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-05-26 21:05:23,047] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-05-26 21:05:23,047] [INFO] [stage_1_and_2.py:143:__init__] Reduce bucket size 500000000
[2024-05-26 21:05:23,047] [INFO] [stage_1_and_2.py:144:__init__] Allgather bucket size 500000000
[2024-05-26 21:05:23,047] [INFO] [stage_1_and_2.py:145:__init__] CPU Offload: False
[2024-05-26 21:05:23,047] [INFO] [stage_1_and_2.py:146:__init__] Round robin gradient partitioning: False
[2024-05-26 21:05:32,018] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
[2024-05-26 21:05:32,019] [INFO] [utils.py:792:see_memory_usage] MA 7.78 GB         Max_MA 7.78 GB         CA 7.87 GB         Max_CA 8 GB 
[2024-05-26 21:05:32,019] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 68.09 GB, percent = 27.1%
[2024-05-26 21:05:32,338] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
[2024-05-26 21:05:32,339] [INFO] [utils.py:792:see_memory_usage] MA 12.71 GB         Max_MA 17.64 GB         CA 17.74 GB         Max_CA 18 GB 
[2024-05-26 21:05:32,339] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 68.1 GB, percent = 27.1%
[2024-05-26 21:05:32,339] [INFO] [stage_1_and_2.py:533:__init__] optimizer state initialized
[2024-05-26 21:05:32,548] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
[2024-05-26 21:05:32,549] [INFO] [utils.py:792:see_memory_usage] MA 12.71 GB         Max_MA 12.71 GB         CA 17.74 GB         Max_CA 18 GB 
[2024-05-26 21:05:32,549] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 68.1 GB, percent = 27.1%
[2024-05-26 21:05:32,552] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-05-26 21:05:32,552] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-05-26 21:05:32,552] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-05-26 21:05:32,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
[2024-05-26 21:05:32,554] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2024-05-26 21:05:32,554] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-05-26 21:05:32,554] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-05-26 21:05:32,554] [INFO] [config.py:988:print]   amp_enabled .................. False
[2024-05-26 21:05:32,554] [INFO] [config.py:988:print]   amp_params ................... False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   bfloat16_enabled ............. True
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbd45647460>
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   communication_data_type ...... None
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   disable_allgather ............ False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   dump_state ................... False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   fp16_enabled ................. False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   global_rank .................. 0
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 8
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-05-26 21:05:32,555] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 1
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   loss_scale ................... 1.0
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   optimizer_name ............... None
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   optimizer_params ............. None
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   pld_enabled .................. False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   pld_params ................... False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   scheduler_name ............... None
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   scheduler_params ............. None
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   sparse_attention ............. None
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   steps_per_print .............. inf
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   train_batch_size ............. 256
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  8
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   world_size ................... 4
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  True
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   zero_enabled ................. True
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2024-05-26 21:05:32,556] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2
[2024-05-26 21:05:32,556] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1747] 2024-05-26 21:05:32,556 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-05-26 21:05:32,557 >>   Num examples = 496,681
[INFO|trainer.py:1749] 2024-05-26 21:05:32,557 >>   Num Epochs = 1
[INFO|trainer.py:1750] 2024-05-26 21:05:32,557 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-05-26 21:05:32,557 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:1754] 2024-05-26 21:05:32,557 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1755] 2024-05-26 21:05:32,557 >>   Total optimization steps = 1,940
[INFO|trainer.py:1756] 2024-05-26 21:05:32,558 >>   Number of trainable parameters = 2,648,524,800
05/26/2024 21:05:32 - WARNING - llmtuner.extras.callbacks - Previous log file in this folder will be deleted.
  0%|          | 0/1940 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/ydz/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1673: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/ydz/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1673: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/ydz/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1673: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/ydz/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1673: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/1940 [00:24<13:06:44, 24.34s/it]  0%|          | 2/1940 [00:46<12:33:01, 23.31s/it]  0%|          | 3/1940 [01:09<12:24:12, 23.05s/it]  0%|          | 4/1940 [01:32<12:23:28, 23.04s/it]  0%|          | 5/1940 [01:54<12:13:59, 22.76s/it]  0%|          | 6/1940 [02:16<12:05:13, 22.50s/it]  0%|          | 7/1940 [02:39<12:02:27, 22.42s/it]  0%|          | 8/1940 [03:01<11:57:26, 22.28s/it]  0%|          | 9/1940 [03:23<11:52:49, 22.15s/it]  1%|          | 10/1940 [03:44<11:43:31, 21.87s/it]                                                    {'loss': 2.6711, 'learning_rate': 4.999672209164081e-05, 'epoch': 0.01}
  1%|          | 10/1940 [03:44<11:43:31, 21.87s/it]  1%|          | 11/1940 [04:02<11:02:58, 20.62s/it]  1%|          | 12/1940 [04:19<10:31:30, 19.65s/it]  1%|          | 13/1940 [04:36<10:04:23, 18.82s/it]  1%|          | 14/1940 [04:53<9:48:02, 18.32s/it]   1%|          | 15/1940 [05:10<9:33:44, 17.88s/it]  1%|          | 16/1940 [05:27<9:28:50, 17.74s/it]  1%|          | 17/1940 [05:44<9:22:10, 17.54s/it]  1%|          | 18/1940 [06:01<9:12:21, 17.24s/it]  1%|          | 19/1940 [06:18<9:04:53, 17.02s/it]  1%|          | 20/1940 [06:34<9:01:19, 16.92s/it]                                                   {'loss': 2.6022, 'learning_rate': 4.998688922613788e-05, 'epoch': 0.01}
  1%|          | 20/1940 [06:34<9:01:19, 16.92s/it]  1%|          | 21/1940 [06:51<8:57:58, 16.82s/it]  1%|          | 22/1940 [07:07<8:53:05, 16.68s/it]  1%|          | 23/1940 [07:23<8:49:22, 16.57s/it]  1%|          | 24/1940 [07:40<8:48:50, 16.56s/it]  1%|▏         | 25/1940 [07:56<8:46:54, 16.51s/it]  1%|▏         | 26/1940 [08:13<8:45:20, 16.47s/it]  1%|▏         | 27/1940 [08:30<8:47:54, 16.56s/it]  1%|▏         | 28/1940 [08:46<8:46:47, 16.53s/it]  1%|▏         | 29/1940 [09:02<8:44:52, 16.48s/it]  2%|▏         | 30/1940 [09:19<8:44:55, 16.49s/it]                                                   {'loss': 2.6086, 'learning_rate': 4.997050398198977e-05, 'epoch': 0.02}
  2%|▏         | 30/1940 [09:19<8:44:55, 16.49s/it]  2%|▏         | 31/1940 [09:35<8:45:04, 16.50s/it]  2%|▏         | 32/1940 [09:52<8:43:03, 16.45s/it]  2%|▏         | 33/1940 [10:08<8:41:45, 16.42s/it]  2%|▏         | 34/1940 [10:25<8:47:04, 16.59s/it]  2%|▏         | 35/1940 [10:45<9:14:47, 17.47s/it]  2%|▏         | 36/1940 [11:06<9:55:29, 18.77s/it]  2%|▏         | 37/1940 [11:29<10:28:33, 19.82s/it]  2%|▏         | 38/1940 [11:51<10:50:34, 20.52s/it]  2%|▏         | 39/1940 [12:13<11:07:14, 21.06s/it]  2%|▏         | 40/1940 [12:36<11:22:37, 21.56s/it]                                                    {'loss': 2.5841, 'learning_rate': 4.9947570655942796e-05, 'epoch': 0.02}
  2%|▏         | 40/1940 [12:36<11:22:37, 21.56s/it]  2%|▏         | 41/1940 [12:58<11:32:25, 21.88s/it]  2%|▏         | 42/1940 [13:21<11:40:01, 22.13s/it]  2%|▏         | 43/1940 [13:44<11:44:54, 22.30s/it]  2%|▏         | 44/1940 [14:07<11:50:31, 22.49s/it]  2%|▏         | 45/1940 [14:29<11:51:37, 22.53s/it]  2%|▏         | 46/1940 [14:52<11:52:04, 22.56s/it]  2%|▏         | 47/1940 [15:15<11:51:58, 22.57s/it]  2%|▏         | 48/1940 [15:36<11:41:42, 22.25s/it]  3%|▎         | 49/1940 [15:58<11:37:09, 22.12s/it]  3%|▎         | 50/1940 [16:20<11:31:25, 21.95s/it]                                                    {'loss': 2.5985, 'learning_rate': 4.991809526186424e-05, 'epoch': 0.03}
  3%|▎         | 50/1940 [16:20<11:31:25, 21.95s/it]  3%|▎         | 51/1940 [16:41<11:28:00, 21.85s/it]  3%|▎         | 52/1940 [17:03<11:25:17, 21.78s/it]  3%|▎         | 53/1940 [17:24<11:23:07, 21.72s/it]  3%|▎         | 54/1940 [17:46<11:23:46, 21.75s/it]  3%|▎         | 55/1940 [18:08<11:21:54, 21.71s/it]  3%|▎         | 56/1940 [18:29<11:20:02, 21.66s/it]  3%|▎         | 57/1940 [18:51<11:21:01, 21.70s/it]  3%|▎         | 58/1940 [19:13<11:19:17, 21.66s/it]  3%|▎         | 59/1940 [19:34<11:18:06, 21.63s/it]  3%|▎         | 60/1940 [19:56<11:16:32, 21.59s/it]                                                    {'loss': 2.5666, 'learning_rate': 4.988208552916535e-05, 'epoch': 0.03}
  3%|▎         | 60/1940 [19:56<11:16:32, 21.59s/it]  3%|▎         | 61/1940 [20:17<11:17:30, 21.63s/it]  3%|▎         | 62/1940 [20:39<11:16:19, 21.61s/it]  3%|▎         | 63/1940 [21:01<11:15:26, 21.59s/it]  3%|▎         | 64/1940 [21:21<11:08:57, 21.40s/it]  3%|▎         | 65/1940 [21:42<11:00:40, 21.14s/it]  3%|▎         | 66/1940 [22:03<10:57:45, 21.06s/it]  3%|▎         | 67/1940 [22:24<10:54:44, 20.97s/it]  4%|▎         | 68/1940 [22:44<10:50:38, 20.85s/it]  4%|▎         | 69/1940 [23:05<10:50:08, 20.85s/it]  4%|▎         | 70/1940 [23:27<11:04:12, 21.31s/it]                                                    {'loss': 2.555, 'learning_rate': 4.983955090077444e-05, 'epoch': 0.04}
  4%|▎         | 70/1940 [23:27<11:04:12, 21.31s/it]  4%|▎         | 71/1940 [23:50<11:16:40, 21.72s/it]  4%|▎         | 72/1940 [24:12<11:17:05, 21.75s/it]  4%|▍         | 73/1940 [24:34<11:16:20, 21.74s/it]  4%|▍         | 74/1940 [24:55<11:15:27, 21.72s/it]  4%|▍         | 75/1940 [25:17<11:12:32, 21.64s/it]  4%|▍         | 76/1940 [25:38<11:10:22, 21.58s/it]  4%|▍         | 77/1940 [26:00<11:09:13, 21.55s/it]  4%|▍         | 78/1940 [26:21<11:07:57, 21.52s/it]  4%|▍         | 79/1940 [26:43<11:06:56, 21.50s/it]  4%|▍         | 80/1940 [27:04<11:07:32, 21.53s/it]                                                    {'loss': 2.5734, 'learning_rate': 4.9790502530660635e-05, 'epoch': 0.04}
  4%|▍         | 80/1940 [27:04<11:07:32, 21.53s/it]  4%|▍         | 81/1940 [27:25<10:57:15, 21.21s/it]  4%|▍         | 82/1940 [27:46<10:52:54, 21.08s/it]  4%|▍         | 83/1940 [28:06<10:47:05, 20.91s/it]  4%|▍         | 84/1940 [28:27<10:45:03, 20.85s/it]  4%|▍         | 85/1940 [28:47<10:39:06, 20.67s/it]  4%|▍         | 86/1940 [29:04<10:01:15, 19.46s/it]  4%|▍         | 87/1940 [29:21<9:37:25, 18.70s/it]   5%|▍         | 88/1940 [29:37<9:18:39, 18.10s/it]  5%|▍         | 89/1940 [29:54<9:05:24, 17.68s/it]  5%|▍         | 90/1940 [30:11<8:58:40, 17.47s/it]                                                   {'loss': 2.5766, 'learning_rate': 4.9734953280908904e-05, 'epoch': 0.05}
  5%|▍         | 90/1940 [30:11<8:58:40, 17.47s/it]  5%|▍         | 91/1940 [30:28<8:52:43, 17.29s/it]  5%|▍         | 92/1940 [30:45<8:51:14, 17.25s/it]  5%|▍         | 93/1940 [31:02<8:46:38, 17.11s/it]  5%|▍         | 94/1940 [31:24<9:34:57, 18.69s/it]  5%|▍         | 95/1940 [31:41<9:14:56, 18.05s/it]  5%|▍         | 96/1940 [31:58<9:04:11, 17.71s/it]  5%|▌         | 97/1940 [32:14<8:55:32, 17.44s/it]  5%|▌         | 98/1940 [32:33<9:07:50, 17.84s/it]  5%|▌         | 99/1940 [32:56<9:56:39, 19.45s/it]  5%|▌         | 100/1940 [33:20<10:35:59, 20.74s/it]                                                     {'loss': 2.5787, 'learning_rate': 4.967291771834727e-05, 'epoch': 0.05}
  5%|▌         | 100/1940 [33:20<10:35:59, 20.74s/it]  5%|▌         | 101/1940 [33:40<10:29:49, 20.55s/it]  5%|▌         | 102/1940 [33:57<9:56:12, 19.46s/it]   5%|▌         | 103/1940 [34:20<10:26:55, 20.48s/it]  5%|▌         | 104/1940 [34:44<10:56:43, 21.46s/it]  5%|▌         | 105/1940 [35:03<10:35:32, 20.78s/it]  5%|▌         | 106/1940 [35:20<9:59:34, 19.62s/it]   6%|▌         | 107/1940 [35:37<9:40:25, 19.00s/it]  6%|▌         | 108/1940 [35:54<9:18:43, 18.30s/it]  6%|▌         | 109/1940 [36:11<9:03:36, 17.81s/it]  6%|▌         | 110/1940 [36:27<8:53:04, 17.48s/it]                                                    {'loss': 2.5745, 'learning_rate': 4.960441211072686e-05, 'epoch': 0.06}
  6%|▌         | 110/1940 [36:27<8:53:04, 17.48s/it]  6%|▌         | 111/1940 [36:44<8:49:11, 17.36s/it]  6%|▌         | 112/1940 [37:02<8:54:36, 17.55s/it]  6%|▌         | 113/1940 [37:25<9:41:07, 19.08s/it]  6%|▌         | 114/1940 [37:49<10:27:45, 20.63s/it]  6%|▌         | 115/1940 [38:14<11:03:08, 21.80s/it]  6%|▌         | 116/1940 [38:30<10:14:50, 20.23s/it]  6%|▌         | 117/1940 [38:48<9:49:46, 19.41s/it]   6%|▌         | 118/1940 [39:06<9:32:42, 18.86s/it]  6%|▌         | 119/1940 [39:22<9:12:01, 18.19s/it]  6%|▌         | 120/1940 [39:39<8:57:41, 17.73s/it]                                                    {'loss': 2.5559, 'learning_rate': 4.9529454422455976e-05, 'epoch': 0.06}
  6%|▌         | 120/1940 [39:39<8:57:41, 17.73s/it]  6%|▌         | 121/1940 [39:56<8:50:17, 17.49s/it]  6%|▋         | 122/1940 [40:14<8:53:04, 17.59s/it]  6%|▋         | 123/1940 [40:41<10:23:54, 20.60s/it]  6%|▋         | 124/1940 [41:00<10:07:15, 20.06s/it]  6%|▋         | 125/1940 [41:17<9:43:15, 19.28s/it]   6%|▋         | 126/1940 [41:35<9:26:55, 18.75s/it]  7%|▋         | 127/1940 [41:52<9:12:04, 18.27s/it]  7%|▋         | 128/1940 [42:09<8:58:30, 17.83s/it]  7%|▋         | 129/1940 [42:26<8:47:10, 17.47s/it]  7%|▋         | 130/1940 [42:42<8:39:00, 17.20s/it]                                                    {'loss': 2.5743, 'learning_rate': 4.944806430988927e-05, 'epoch': 0.07}
  7%|▋         | 130/1940 [42:42<8:39:00, 17.20s/it]  7%|▋         | 131/1940 [42:59<8:34:20, 17.06s/it]  7%|▋         | 132/1940 [43:15<8:28:25, 16.87s/it]  7%|▋         | 133/1940 [43:37<9:09:38, 18.25s/it]  7%|▋         | 134/1940 [44:03<10:20:39, 20.62s/it]  7%|▋         | 135/1940 [44:20<9:44:03, 19.41s/it]   7%|▋         | 136/1940 [44:37<9:22:37, 18.71s/it]  7%|▋         | 137/1940 [44:54<9:11:01, 18.34s/it]  7%|▋         | 138/1940 [45:12<9:05:20, 18.16s/it]  7%|▋         | 139/1940 [45:29<8:56:07, 17.86s/it]  7%|▋         | 140/1940 [45:53<9:53:51, 19.80s/it]                                                    {'loss': 2.5567, 'learning_rate': 4.936026311617316e-05, 'epoch': 0.07}
  7%|▋         | 140/1940 [45:53<9:53:51, 19.80s/it]  7%|▋         | 141/1940 [46:17<10:26:40, 20.90s/it]  7%|▋         | 142/1940 [46:40<10:50:33, 21.71s/it]  7%|▋         | 143/1940 [46:58<10:09:12, 20.34s/it]  7%|▋         | 144/1940 [47:15<9:45:45, 19.57s/it]   7%|▋         | 145/1940 [47:32<9:19:03, 18.69s/it]  8%|▊         | 146/1940 [47:49<9:00:21, 18.07s/it]  8%|▊         | 147/1940 [48:05<8:49:08, 17.71s/it]  8%|▊         | 148/1940 [48:22<8:39:56, 17.41s/it]  8%|▊         | 149/1940 [48:40<8:45:04, 17.59s/it]  8%|▊         | 150/1940 [49:03<9:33:59, 19.24s/it]                                                    {'loss': 2.5859, 'learning_rate': 4.926607386564898e-05, 'epoch': 0.08}
  8%|▊         | 150/1940 [49:03<9:33:59, 19.24s/it]  8%|▊         | 151/1940 [49:21<9:18:22, 18.73s/it]  8%|▊         | 152/1940 [49:38<9:02:37, 18.21s/it]  8%|▊         | 153/1940 [49:55<8:53:46, 17.92s/it]  8%|▊         | 154/1940 [50:21<10:08:29, 20.44s/it]  8%|▊         | 155/1940 [50:38<9:37:22, 19.41s/it]   8%|▊         | 156/1940 [50:55<9:16:53, 18.73s/it]  8%|▊         | 157/1940 [51:12<8:57:02, 18.07s/it]  8%|▊         | 158/1940 [51:35<9:36:40, 19.42s/it]  8%|▊         | 159/1940 [51:59<10:25:23, 21.07s/it]  8%|▊         | 160/1940 [52:16<9:47:13, 19.79s/it]                                                     {'loss': 2.5829, 'learning_rate': 4.916552125781528e-05, 'epoch': 0.08}
  8%|▊         | 160/1940 [52:16<9:47:13, 19.79s/it]  8%|▊         | 161/1940 [52:34<9:23:49, 19.02s/it]  8%|▊         | 162/1940 [52:51<9:08:01, 18.49s/it]  8%|▊         | 163/1940 [53:07<8:51:45, 17.95s/it]  8%|▊         | 164/1940 [53:29<9:22:06, 18.99s/it]  9%|▊         | 165/1940 [53:47<9:12:34, 18.68s/it]  9%|▊         | 166/1940 [54:03<8:53:29, 18.04s/it]  9%|▊         | 167/1940 [54:20<8:40:04, 17.60s/it]  9%|▊         | 168/1940 [54:37<8:32:14, 17.34s/it]  9%|▊         | 169/1940 [54:53<8:24:30, 17.09s/it]  9%|▉         | 170/1940 [55:10<8:18:29, 16.90s/it]                                                    {'loss': 2.5832, 'learning_rate': 4.9058631660850765e-05, 'epoch': 0.09}
  9%|▉         | 170/1940 [55:10<8:18:29, 16.90s/it]  9%|▉         | 171/1940 [55:26<8:17:19, 16.87s/it]  9%|▉         | 172/1940 [55:43<8:12:18, 16.71s/it]  9%|▉         | 173/1940 [55:59<8:08:12, 16.58s/it]  9%|▉         | 174/1940 [56:16<8:07:37, 16.57s/it]  9%|▉         | 175/1940 [56:32<8:04:12, 16.46s/it]  9%|▉         | 176/1940 [56:48<8:01:34, 16.38s/it]  9%|▉         | 177/1940 [57:04<8:01:33, 16.39s/it]  9%|▉         | 178/1940 [57:21<8:01:41, 16.40s/it]  9%|▉         | 179/1940 [57:37<7:59:56, 16.35s/it]  9%|▉         | 180/1940 [57:53<7:58:53, 16.33s/it]                                                    {'loss': 2.5645, 'learning_rate': 4.894543310469968e-05, 'epoch': 0.09}
  9%|▉         | 180/1940 [57:53<7:58:53, 16.33s/it]  9%|▉         | 181/1940 [58:10<8:01:22, 16.42s/it]  9%|▉         | 182/1940 [58:26<7:58:56, 16.35s/it]  9%|▉         | 183/1940 [58:42<7:57:01, 16.29s/it]  9%|▉         | 184/1940 [58:59<7:57:41, 16.32s/it] 10%|▉         | 185/1940 [59:15<7:56:01, 16.27s/it] 10%|▉         | 186/1940 [59:31<7:55:00, 16.25s/it] 10%|▉         | 187/1940 [59:47<7:56:09, 16.30s/it] 10%|▉         | 188/1940 [1:00:04<7:56:33, 16.32s/it] 10%|▉         | 189/1940 [1:00:20<7:55:06, 16.28s/it] 10%|▉         | 190/1940 [1:00:36<7:54:04, 16.25s/it]                                                      {'loss': 2.5564, 'learning_rate': 4.882595527372152e-05, 'epoch': 0.1}
 10%|▉         | 190/1940 [1:00:36<7:54:04, 16.25s/it] 10%|▉         | 191/1940 [1:00:53<7:57:34, 16.38s/it] 10%|▉         | 192/1940 [1:01:09<7:56:33, 16.36s/it] 10%|▉         | 193/1940 [1:01:26<7:55:42, 16.34s/it] 10%|█         | 194/1940 [1:01:42<7:56:45, 16.38s/it] 10%|█         | 195/1940 [1:01:58<7:54:39, 16.32s/it] 10%|█         | 196/1940 [1:02:14<7:52:59, 16.27s/it] 10%|█         | 197/1940 [1:02:31<7:54:03, 16.32s/it] 10%|█         | 198/1940 [1:02:47<7:54:18, 16.34s/it] 10%|█         | 199/1940 [1:03:03<7:52:38, 16.29s/it] 10%|█         | 200/1940 [1:03:20<7:51:29, 16.26s/it]                                                      {'loss': 2.5677, 'learning_rate': 4.870022949890676e-05, 'epoch': 0.1}
 10%|█         | 200/1940 [1:03:20<7:51:29, 16.26s/it] 10%|█         | 201/1940 [1:03:36<7:53:15, 16.33s/it] 10%|█         | 202/1940 [1:03:52<7:51:35, 16.28s/it] 10%|█         | 203/1940 [1:04:08<7:50:30, 16.25s/it] 11%|█         | 204/1940 [1:04:25<7:53:52, 16.38s/it] 11%|█         | 205/1940 [1:04:41<7:52:22, 16.34s/it] 11%|█         | 206/1940 [1:04:58<7:51:38, 16.32s/it] 11%|█         | 207/1940 [1:05:14<7:53:18, 16.39s/it] 11%|█         | 208/1940 [1:05:31<7:53:25, 16.40s/it] 11%|█         | 209/1940 [1:05:47<7:51:22, 16.34s/it] 11%|█         | 210/1940 [1:06:03<7:49:39, 16.29s/it]                                                      {'loss': 2.5554, 'learning_rate': 4.856828874966086e-05, 'epoch': 0.11}
 11%|█         | 210/1940 [1:06:03<7:49:39, 16.29s/it] 11%|█         | 211/1940 [1:06:19<7:50:14, 16.32s/it] 11%|█         | 212/1940 [1:06:35<7:48:38, 16.27s/it] 11%|█         | 213/1940 [1:06:52<7:47:34, 16.24s/it] 11%|█         | 214/1940 [1:07:08<7:50:40, 16.36s/it] 11%|█         | 215/1940 [1:07:24<7:48:46, 16.31s/it] 11%|█         | 216/1940 [1:07:41<7:47:31, 16.27s/it] 11%|█         | 217/1940 [1:07:57<7:47:06, 16.27s/it] 11%|█         | 218/1940 [1:08:14<7:50:49, 16.41s/it] 11%|█▏        | 219/1940 [1:08:30<7:49:24, 16.36s/it] 11%|█▏        | 220/1940 [1:08:46<7:47:59, 16.33s/it]                                                      {'loss': 2.5662, 'learning_rate': 4.8430167625158595e-05, 'epoch': 0.11}
 11%|█▏        | 220/1940 [1:08:46<7:47:59, 16.33s/it] 11%|█▏        | 221/1940 [1:09:03<7:48:44, 16.36s/it] 11%|█▏        | 222/1940 [1:09:19<7:47:03, 16.31s/it] 11%|█▏        | 223/1940 [1:09:35<7:45:44, 16.27s/it] 12%|█▏        | 224/1940 [1:09:52<7:48:41, 16.39s/it] 12%|█▏        | 225/1940 [1:10:08<7:46:28, 16.32s/it] 12%|█▏        | 226/1940 [1:10:24<7:44:47, 16.27s/it] 12%|█▏        | 227/1940 [1:10:40<7:43:47, 16.24s/it] 12%|█▏        | 228/1940 [1:10:57<7:45:52, 16.33s/it] 12%|█▏        | 229/1940 [1:11:13<7:44:44, 16.30s/it] 12%|█▏        | 230/1940 [1:11:29<7:44:19, 16.29s/it]                                                      {'loss': 2.583, 'learning_rate': 4.828590234527106e-05, 'epoch': 0.12}
 12%|█▏        | 230/1940 [1:11:29<7:44:19, 16.29s/it] 12%|█▏        | 231/1940 [1:11:46<7:47:25, 16.41s/it] 12%|█▏        | 232/1940 [1:12:02<7:45:19, 16.35s/it] 12%|█▏        | 233/1940 [1:12:18<7:43:40, 16.30s/it] 12%|█▏        | 234/1940 [1:12:35<7:44:15, 16.33s/it] 12%|█▏        | 235/1940 [1:12:51<7:44:37, 16.35s/it] 12%|█▏        | 236/1940 [1:13:07<7:42:44, 16.29s/it] 12%|█▏        | 237/1940 [1:13:23<7:41:41, 16.27s/it] 12%|█▏        | 238/1940 [1:13:40<7:42:30, 16.30s/it] 12%|█▏        | 239/1940 [1:13:56<7:41:08, 16.27s/it] 12%|█▏        | 240/1940 [1:14:12<7:40:09, 16.24s/it]                                                      {'loss': 2.5669, 'learning_rate': 4.813553074106761e-05, 'epoch': 0.12}
 12%|█▏        | 240/1940 [1:14:12<7:40:09, 16.24s/it] 12%|█▏        | 241/1940 [1:14:29<7:43:34, 16.37s/it] 12%|█▏        | 242/1940 [1:14:45<7:42:09, 16.33s/it] 13%|█▎        | 243/1940 [1:15:01<7:41:24, 16.31s/it] 13%|█▎        | 244/1940 [1:15:18<7:42:44, 16.37s/it] 13%|█▎        | 245/1940 [1:15:34<7:43:06, 16.39s/it] 13%|█▎        | 246/1940 [1:15:50<7:41:12, 16.34s/it] 13%|█▎        | 247/1940 [1:16:07<7:39:41, 16.29s/it] 13%|█▎        | 248/1940 [1:16:23<7:40:40, 16.34s/it] 13%|█▎        | 249/1940 [1:16:39<7:39:05, 16.29s/it] 13%|█▎        | 250/1940 [1:16:55<7:38:00, 16.26s/it]                                                      {'loss': 2.564, 'learning_rate': 4.7979092244895305e-05, 'epoch': 0.13}
 13%|█▎        | 250/1940 [1:16:55<7:38:00, 16.26s/it] 13%|█▎        | 251/1940 [1:17:12<7:40:43, 16.37s/it] 13%|█▎        | 252/1940 [1:17:28<7:38:52, 16.31s/it] 13%|█▎        | 253/1940 [1:17:44<7:37:40, 16.28s/it] 13%|█▎        | 254/1940 [1:18:01<7:39:00, 16.33s/it] 13%|█▎        | 255/1940 [1:18:17<7:38:31, 16.33s/it] 13%|█▎        | 256/1940 [1:18:33<7:37:53, 16.31s/it] 13%|█▎        | 257/1940 [1:18:50<7:36:58, 16.29s/it] 13%|█▎        | 258/1940 [1:19:06<7:39:30, 16.39s/it] 13%|█▎        | 259/1940 [1:19:23<7:37:23, 16.33s/it] 13%|█▎        | 260/1940 [1:19:39<7:35:48, 16.28s/it]                                                      {'loss': 2.5668, 'learning_rate': 4.781662788003851e-05, 'epoch': 0.13}
 13%|█▎        | 260/1940 [1:19:39<7:35:48, 16.28s/it] 13%|█▎        | 261/1940 [1:19:55<7:38:26, 16.38s/it] 14%|█▎        | 262/1940 [1:20:11<7:36:18, 16.32s/it] 14%|█▎        | 263/1940 [1:20:28<7:34:44, 16.27s/it] 14%|█▎        | 264/1940 [1:20:44<7:33:49, 16.25s/it] 14%|█▎        | 265/1940 [1:21:00<7:34:57, 16.30s/it] 14%|█▎        | 266/1940 [1:21:16<7:33:58, 16.27s/it] 14%|█▍        | 267/1940 [1:21:33<7:33:39, 16.27s/it] 14%|█▍        | 268/1940 [1:21:49<7:37:01, 16.40s/it] 14%|█▍        | 269/1940 [1:22:06<7:35:36, 16.36s/it] 14%|█▍        | 270/1940 [1:22:22<7:33:57, 16.31s/it]                                                      {'loss': 2.5787, 'learning_rate': 4.764818024996117e-05, 'epoch': 0.14}
 14%|█▍        | 270/1940 [1:22:22<7:33:57, 16.31s/it] 14%|█▍        | 271/1940 [1:22:39<7:36:28, 16.41s/it] 14%|█▍        | 272/1940 [1:22:55<7:34:27, 16.35s/it] 14%|█▍        | 273/1940 [1:23:11<7:32:48, 16.30s/it] 14%|█▍        | 274/1940 [1:23:27<7:31:42, 16.27s/it] 14%|█▍        | 275/1940 [1:23:44<7:32:50, 16.32s/it] 14%|█▍        | 276/1940 [1:24:00<7:31:34, 16.28s/it] 14%|█▍        | 277/1940 [1:24:16<7:30:50, 16.27s/it] 14%|█▍        | 278/1940 [1:24:33<7:34:08, 16.39s/it] 14%|█▍        | 279/1940 [1:24:49<7:32:39, 16.35s/it] 14%|█▍        | 280/1940 [1:25:05<7:31:45, 16.33s/it]                                                      {'loss': 2.5496, 'learning_rate': 4.747379352713489e-05, 'epoch': 0.14}
 14%|█▍        | 280/1940 [1:25:05<7:31:45, 16.33s/it] 14%|█▍        | 281/1940 [1:25:22<7:33:03, 16.39s/it] 15%|█▍        | 282/1940 [1:25:38<7:30:59, 16.32s/it] 15%|█▍        | 283/1940 [1:25:55<7:33:19, 16.41s/it] 15%|█▍        | 284/1940 [1:26:23<9:12:09, 20.01s/it] 15%|█▍        | 285/1940 [1:26:39<8:41:39, 18.91s/it] 15%|█▍        | 286/1940 [1:26:56<8:24:42, 18.31s/it] 15%|█▍        | 287/1940 [1:27:13<8:11:35, 17.84s/it] 15%|█▍        | 288/1940 [1:27:30<8:07:24, 17.70s/it] 15%|█▍        | 289/1940 [1:27:47<7:59:45, 17.44s/it] 15%|█▍        | 290/1940 [1:28:04<7:52:25, 17.18s/it]                                                      {'loss': 2.5738, 'learning_rate': 4.7293513441455364e-05, 'epoch': 0.15}
 15%|█▍        | 290/1940 [1:28:04<7:52:25, 17.18s/it] 15%|█▌        | 291/1940 [1:28:25<8:25:34, 18.40s/it] 15%|█▌        | 292/1940 [1:28:42<8:15:07, 18.03s/it] 15%|█▌        | 293/1940 [1:28:59<8:07:21, 17.75s/it] 15%|█▌        | 294/1940 [1:29:16<7:58:42, 17.45s/it] 15%|█▌        | 295/1940 [1:29:33<7:52:32, 17.24s/it] 15%|█▌        | 296/1940 [1:29:49<7:45:59, 17.01s/it] 15%|█▌        | 297/1940 [1:30:10<8:15:08, 18.08s/it] 15%|█▌        | 298/1940 [1:30:30<8:31:18, 18.68s/it] 15%|█▌        | 299/1940 [1:30:46<8:10:59, 17.95s/it] 15%|█▌        | 300/1940 [1:31:03<8:01:03, 17.60s/it]                                                      {'loss': 2.5525, 'learning_rate': 4.710738726825059e-05, 'epoch': 0.15}
 15%|█▌        | 300/1940 [1:31:03<8:01:03, 17.60s/it] 16%|█▌        | 301/1940 [1:31:20<7:59:13, 17.54s/it] 16%|█▌        | 302/1940 [1:31:37<7:52:41, 17.31s/it] 16%|█▌        | 303/1940 [1:31:53<7:45:05, 17.05s/it] 16%|█▌        | 304/1940 [1:32:10<7:41:40, 16.93s/it] 16%|█▌        | 305/1940 [1:32:27<7:38:29, 16.83s/it] 16%|█▌        | 306/1940 [1:32:43<7:34:20, 16.68s/it] 16%|█▌        | 307/1940 [1:32:59<7:30:53, 16.57s/it] 16%|█▌        | 308/1940 [1:33:16<7:30:30, 16.56s/it] 16%|█▌        | 309/1940 [1:33:32<7:27:39, 16.47s/it] 16%|█▌        | 310/1940 [1:33:48<7:25:20, 16.39s/it]                                                      {'loss': 2.5694, 'learning_rate': 4.69154638158837e-05, 'epoch': 0.16}
 16%|█▌        | 310/1940 [1:33:48<7:25:20, 16.39s/it] 16%|█▌        | 311/1940 [1:34:05<7:27:27, 16.48s/it] 16%|█▌        | 312/1940 [1:34:21<7:24:56, 16.40s/it] 16%|█▌        | 313/1940 [1:34:37<7:22:51, 16.33s/it] 16%|█▌        | 314/1940 [1:34:54<7:23:30, 16.37s/it] 16%|█▌        | 315/1940 [1:35:10<7:23:46, 16.39s/it] 16%|█▋        | 316/1940 [1:35:27<7:22:15, 16.34s/it] 16%|█▋        | 317/1940 [1:35:43<7:20:48, 16.30s/it] 16%|█▋        | 318/1940 [1:35:59<7:21:32, 16.33s/it] 16%|█▋        | 319/1940 [1:36:15<7:19:54, 16.28s/it] 16%|█▋        | 320/1940 [1:36:32<7:18:46, 16.25s/it]                                                      {'loss': 2.5483, 'learning_rate': 4.671779341295378e-05, 'epoch': 0.16}
 16%|█▋        | 320/1940 [1:36:32<7:18:46, 16.25s/it] 17%|█▋        | 321/1940 [1:36:48<7:21:37, 16.37s/it] 17%|█▋        | 322/1940 [1:37:04<7:19:46, 16.31s/it] 17%|█▋        | 323/1940 [1:37:20<7:18:22, 16.27s/it] 17%|█▋        | 324/1940 [1:37:37<7:17:35, 16.25s/it] 17%|█▋        | 325/1940 [1:37:53<7:20:47, 16.38s/it] 17%|█▋        | 326/1940 [1:38:10<7:19:22, 16.33s/it] 17%|█▋        | 327/1940 [1:38:26<7:18:19, 16.30s/it] 17%|█▋        | 328/1940 [1:38:42<7:19:30, 16.36s/it] 17%|█▋        | 329/1940 [1:38:59<7:17:59, 16.31s/it] 17%|█▋        | 330/1940 [1:39:15<7:16:57, 16.28s/it]                                                      {'loss': 2.5624, 'learning_rate': 4.6514427895098134e-05, 'epoch': 0.17}
 17%|█▋        | 330/1940 [1:39:15<7:16:57, 16.28s/it] 17%|█▋        | 331/1940 [1:39:31<7:20:01, 16.41s/it] 17%|█▋        | 332/1940 [1:39:48<7:17:59, 16.34s/it] 17%|█▋        | 333/1940 [1:40:04<7:16:41, 16.30s/it] 17%|█▋        | 334/1940 [1:40:20<7:16:01, 16.29s/it] 17%|█▋        | 335/1940 [1:40:37<7:17:36, 16.36s/it] 17%|█▋        | 336/1940 [1:40:53<7:16:10, 16.32s/it] 17%|█▋        | 337/1940 [1:41:09<7:15:08, 16.29s/it] 17%|█▋        | 338/1940 [1:41:26<7:17:57, 16.40s/it] 17%|█▋        | 339/1940 [1:41:42<7:16:29, 16.36s/it] 18%|█▊        | 340/1940 [1:41:58<7:15:25, 16.33s/it]                                                      {'loss': 2.5618, 'learning_rate': 4.630542059139924e-05, 'epoch': 0.18}
 18%|█▊        | 340/1940 [1:41:58<7:15:25, 16.33s/it] 18%|█▊        | 341/1940 [1:42:15<7:16:13, 16.37s/it] 18%|█▊        | 342/1940 [1:42:31<7:16:24, 16.39s/it] 18%|█▊        | 343/1940 [1:42:47<7:14:31, 16.33s/it] 18%|█▊        | 344/1940 [1:43:04<7:13:23, 16.29s/it] 18%|█▊        | 345/1940 [1:43:20<7:14:17, 16.34s/it] 18%|█▊        | 346/1940 [1:43:36<7:12:54, 16.30s/it] 18%|█▊        | 347/1940 [1:43:52<7:11:58, 16.27s/it] 18%|█▊        | 348/1940 [1:44:09<7:14:57, 16.39s/it] 18%|█▊        | 349/1940 [1:44:25<7:13:05, 16.33s/it] 18%|█▊        | 350/1940 [1:44:41<7:11:57, 16.30s/it]                                                      {'loss': 2.5419, 'learning_rate': 4.6090826310400116e-05, 'epoch': 0.18}
 18%|█▊        | 350/1940 [1:44:41<7:11:57, 16.30s/it] 18%|█▊        | 351/1940 [1:44:58<7:13:15, 16.36s/it] 18%|█▊        | 352/1940 [1:45:14<7:13:46, 16.39s/it] 18%|█▊        | 353/1940 [1:45:31<7:12:08, 16.34s/it] 18%|█▊        | 354/1940 [1:45:47<7:10:54, 16.30s/it] 18%|█▊        | 355/1940 [1:46:03<7:11:38, 16.34s/it] 18%|█▊        | 356/1940 [1:46:20<7:10:18, 16.30s/it] 18%|█▊        | 357/1940 [1:46:36<7:09:11, 16.27s/it] 18%|█▊        | 358/1940 [1:46:52<7:12:09, 16.39s/it] 19%|█▊        | 359/1940 [1:47:09<7:10:15, 16.33s/it] 19%|█▊        | 360/1940 [1:47:25<7:08:51, 16.29s/it]                                                      {'loss': 2.5475, 'learning_rate': 4.587070132573178e-05, 'epoch': 0.19}
 19%|█▊        | 360/1940 [1:47:25<7:08:51, 16.29s/it] 19%|█▊        | 361/1940 [1:47:41<7:09:52, 16.33s/it] 19%|█▊        | 362/1940 [1:47:57<7:09:15, 16.32s/it] 19%|█▊        | 363/1940 [1:48:14<7:08:36, 16.31s/it] 19%|█▉        | 364/1940 [1:48:30<7:07:51, 16.29s/it] 19%|█▉        | 365/1940 [1:48:47<7:10:47, 16.41s/it] 19%|█▉        | 366/1940 [1:49:03<7:08:52, 16.35s/it] 19%|█▉        | 367/1940 [1:49:19<7:07:23, 16.30s/it] 19%|█▉        | 368/1940 [1:49:36<7:10:14, 16.42s/it] 19%|█▉        | 369/1940 [1:49:52<7:08:18, 16.36s/it] 19%|█▉        | 370/1940 [1:50:08<7:07:03, 16.32s/it]                                                      {'loss': 2.5555, 'learning_rate': 4.5645103361356415e-05, 'epoch': 0.19}
 19%|█▉        | 370/1940 [1:50:08<7:07:03, 16.32s/it] 19%|█▉        | 371/1940 [1:50:24<7:06:03, 16.29s/it] 19%|█▉        | 372/1940 [1:50:41<7:07:10, 16.35s/it] 19%|█▉        | 373/1940 [1:50:57<7:05:48, 16.30s/it] 19%|█▉        | 374/1940 [1:51:13<7:05:02, 16.29s/it] 19%|█▉        | 375/1940 [1:51:30<7:07:37, 16.39s/it] 19%|█▉        | 376/1940 [1:51:46<7:05:55, 16.34s/it] 19%|█▉        | 377/1940 [1:52:02<7:04:38, 16.30s/it] 19%|█▉        | 378/1940 [1:52:19<7:07:03, 16.40s/it] 20%|█▉        | 379/1940 [1:52:35<7:05:00, 16.34s/it] 20%|█▉        | 380/1940 [1:52:52<7:03:47, 16.30s/it]                                                      {'loss': 2.5775, 'learning_rate': 4.541409157643027e-05, 'epoch': 0.2}
 20%|█▉        | 380/1940 [1:52:52<7:03:47, 16.30s/it] 20%|█▉        | 381/1940 [1:53:08<7:02:56, 16.28s/it] 20%|█▉        | 382/1940 [1:53:24<7:04:08, 16.33s/it] 20%|█▉        | 383/1940 [1:53:40<7:02:49, 16.29s/it] 20%|█▉        | 384/1940 [1:53:57<7:02:06, 16.28s/it] 20%|█▉        | 385/1940 [1:54:13<7:05:12, 16.41s/it] 20%|█▉        | 386/1940 [1:54:30<7:03:33, 16.35s/it] 20%|█▉        | 387/1940 [1:54:46<7:02:08, 16.31s/it] 20%|██        | 388/1940 [1:55:02<7:03:25, 16.37s/it] 20%|██        | 389/1940 [1:55:18<7:01:50, 16.32s/it] 20%|██        | 390/1940 [1:55:35<7:00:43, 16.29s/it]                                                      {'loss': 2.5555, 'learning_rate': 4.517772654979023e-05, 'epoch': 0.2}
 20%|██        | 390/1940 [1:55:35<7:00:43, 16.29s/it] 20%|██        | 391/1940 [1:55:51<7:01:47, 16.34s/it] 20%|██        | 392/1940 [1:56:08<7:02:04, 16.36s/it] 20%|██        | 393/1940 [1:56:24<7:00:25, 16.31s/it] 20%|██        | 394/1940 [1:56:40<6:59:23, 16.28s/it] 20%|██        | 395/1940 [1:56:57<7:02:02, 16.39s/it] 20%|██        | 396/1940 [1:57:13<7:00:30, 16.34s/it] 20%|██        | 397/1940 [1:57:29<6:59:35, 16.32s/it] 21%|██        | 398/1940 [1:57:46<7:00:46, 16.37s/it] 21%|██        | 399/1940 [1:58:02<6:59:27, 16.33s/it] 21%|██        | 400/1940 [1:58:18<6:58:11, 16.29s/it]                                                      {'loss': 2.5722, 'learning_rate': 4.493607026406802e-05, 'epoch': 0.21}
 21%|██        | 400/1940 [1:58:18<6:58:11, 16.29s/it] 21%|██        | 401/1940 [1:58:34<6:59:08, 16.34s/it] 21%|██        | 402/1940 [1:58:51<6:59:40, 16.37s/it] 21%|██        | 403/1940 [1:59:07<6:58:08, 16.32s/it] 21%|██        | 404/1940 [1:59:23<6:57:09, 16.30s/it] 21%|██        | 405/1940 [1:59:40<6:59:48, 16.41s/it] 21%|██        | 406/1940 [1:59:56<6:58:05, 16.35s/it] 21%|██        | 407/1940 [2:00:12<6:56:37, 16.31s/it] 21%|██        | 408/1940 [2:00:29<6:57:25, 16.35s/it] 21%|██        | 409/1940 [2:00:45<6:56:21, 16.32s/it] 21%|██        | 410/1940 [2:01:01<6:55:51, 16.31s/it]                                                      {'loss': 2.5451, 'learning_rate': 4.4689186089436366e-05, 'epoch': 0.21}
 21%|██        | 410/1940 [2:01:01<6:55:51, 16.31s/it] 21%|██        | 411/1940 [2:01:18<6:56:55, 16.36s/it] 21%|██        | 412/1940 [2:01:34<6:57:30, 16.39s/it] 21%|██▏       | 413/1940 [2:01:51<6:55:37, 16.33s/it] 21%|██▏       | 414/1940 [2:02:07<6:54:26, 16.30s/it] 21%|██▏       | 415/1940 [2:02:23<6:55:55, 16.36s/it] 21%|██▏       | 416/1940 [2:02:40<6:54:41, 16.33s/it] 21%|██▏       | 417/1940 [2:02:56<6:53:39, 16.30s/it] 22%|██▏       | 418/1940 [2:03:12<6:56:10, 16.41s/it] 22%|██▏       | 419/1940 [2:03:29<6:54:17, 16.34s/it] 22%|██▏       | 420/1940 [2:03:45<6:53:07, 16.31s/it]                                                      {'loss': 2.5436, 'learning_rate': 4.443713876699124e-05, 'epoch': 0.22}
 22%|██▏       | 420/1940 [2:03:45<6:53:07, 16.31s/it] 22%|██▏       | 421/1940 [2:04:01<6:54:04, 16.36s/it] 22%|██▏       | 422/1940 [2:04:18<6:54:45, 16.39s/it] 22%|██▏       | 423/1940 [2:04:34<6:53:28, 16.35s/it] 22%|██▏       | 424/1940 [2:04:50<6:52:26, 16.32s/it] 22%|██▏       | 425/1940 [2:05:07<6:53:09, 16.36s/it] 22%|██▏       | 426/1940 [2:05:23<6:51:38, 16.31s/it] 22%|██▏       | 427/1940 [2:05:39<6:50:26, 16.28s/it] 22%|██▏       | 428/1940 [2:05:56<6:53:10, 16.40s/it] 22%|██▏       | 429/1940 [2:06:12<6:51:18, 16.33s/it] 22%|██▏       | 430/1940 [2:06:28<6:50:05, 16.29s/it]                                                      {'loss': 2.5564, 'learning_rate': 4.417999439177466e-05, 'epoch': 0.22}
 22%|██▏       | 430/1940 [2:06:28<6:50:05, 16.29s/it] 22%|██▏       | 431/1940 [2:06:44<6:49:08, 16.27s/it] 22%|██▏       | 432/1940 [2:07:01<6:51:57, 16.39s/it] 22%|██▏       | 433/1940 [2:07:17<6:50:23, 16.34s/it] 22%|██▏       | 434/1940 [2:07:34<6:49:22, 16.31s/it] 22%|██▏       | 435/1940 [2:07:50<6:50:26, 16.36s/it] 22%|██▏       | 436/1940 [2:08:06<6:49:23, 16.33s/it] 23%|██▎       | 437/1940 [2:08:23<6:48:22, 16.30s/it] 23%|██▎       | 438/1940 [2:08:39<6:51:05, 16.42s/it] 23%|██▎       | 439/1940 [2:08:55<6:49:10, 16.36s/it] 23%|██▎       | 440/1940 [2:09:12<6:47:53, 16.32s/it]                                                      {'loss': 2.5744, 'learning_rate': 4.391782039544238e-05, 'epoch': 0.23}
 23%|██▎       | 440/1940 [2:09:12<6:47:53, 16.32s/it] 23%|██▎       | 441/1940 [2:09:28<6:46:49, 16.28s/it] 23%|██▎       | 442/1940 [2:09:44<6:48:09, 16.35s/it] 23%|██▎       | 443/1940 [2:10:01<6:46:51, 16.31s/it] 23%|██▎       | 444/1940 [2:10:17<6:45:52, 16.28s/it] 23%|██▎       | 445/1940 [2:10:34<6:48:41, 16.40s/it] 23%|██▎       | 446/1940 [2:10:50<6:47:12, 16.35s/it] 23%|██▎       | 447/1940 [2:11:06<6:46:05, 16.32s/it] 23%|██▎       | 448/1940 [2:11:23<6:47:11, 16.38s/it] 23%|██▎       | 449/1940 [2:11:39<6:47:36, 16.40s/it] 23%|██▎       | 450/1940 [2:11:55<6:46:06, 16.35s/it]                                                      {'loss': 2.5508, 'learning_rate': 4.365068552858115e-05, 'epoch': 0.23}
 23%|██▎       | 450/1940 [2:11:55<6:46:06, 16.35s/it] 23%|██▎       | 451/1940 [2:12:11<6:44:51, 16.31s/it] 23%|██▎       | 452/1940 [2:12:28<6:45:36, 16.36s/it] 23%|██▎       | 453/1940 [2:12:44<6:44:24, 16.32s/it] 23%|██▎       | 454/1940 [2:13:00<6:43:33, 16.29s/it] 23%|██▎       | 455/1940 [2:13:17<6:45:58, 16.40s/it] 24%|██▎       | 456/1940 [2:13:33<6:44:20, 16.35s/it] 24%|██▎       | 457/1940 [2:13:49<6:43:09, 16.31s/it] 24%|██▎       | 458/1940 [2:14:06<6:44:15, 16.37s/it] 24%|██▎       | 459/1940 [2:14:22<6:44:58, 16.41s/it] 24%|██▎       | 460/1940 [2:14:39<6:43:40, 16.37s/it]                                                      {'loss': 2.5503, 'learning_rate': 4.337865984268001e-05, 'epoch': 0.24}
 24%|██▎       | 460/1940 [2:14:39<6:43:40, 16.37s/it] 24%|██▍       | 461/1940 [2:14:55<6:42:30, 16.33s/it] 24%|██▍       | 462/1940 [2:15:11<6:43:12, 16.37s/it] 24%|██▍       | 463/1940 [2:15:28<6:42:00, 16.33s/it] 24%|██▍       | 464/1940 [2:15:44<6:41:02, 16.30s/it] 24%|██▍       | 465/1940 [2:16:01<6:43:27, 16.41s/it] 24%|██▍       | 466/1940 [2:16:17<6:41:45, 16.35s/it] 24%|██▍       | 467/1940 [2:16:33<6:40:36, 16.32s/it] 24%|██▍       | 468/1940 [2:16:49<6:41:27, 16.36s/it] 24%|██▍       | 469/1940 [2:17:06<6:40:19, 16.33s/it] 24%|██▍       | 470/1940 [2:17:22<6:39:20, 16.30s/it]                                                      {'loss': 2.5339, 'learning_rate': 4.3101814671760546e-05, 'epoch': 0.24}
 24%|██▍       | 470/1940 [2:17:22<6:39:20, 16.30s/it] 24%|██▍       | 471/1940 [2:17:38<6:38:42, 16.28s/it] 24%|██▍       | 472/1940 [2:17:55<6:41:31, 16.41s/it] 24%|██▍       | 473/1940 [2:18:11<6:39:51, 16.35s/it] 24%|██▍       | 474/1940 [2:18:27<6:38:43, 16.32s/it] 24%|██▍       | 475/1940 [2:18:44<6:41:12, 16.43s/it] 25%|██▍       | 476/1940 [2:19:00<6:39:12, 16.36s/it] 25%|██▍       | 477/1940 [2:19:16<6:37:50, 16.32s/it] 25%|██▍       | 478/1940 [2:19:33<6:36:50, 16.29s/it] 25%|██▍       | 479/1940 [2:19:49<6:37:35, 16.33s/it] 25%|██▍       | 480/1940 [2:20:05<6:36:27, 16.29s/it]                                                      {'loss': 2.5518, 'learning_rate': 4.2820222613670736e-05, 'epoch': 0.25}
 25%|██▍       | 480/1940 [2:20:05<6:36:27, 16.29s/it] 25%|██▍       | 481/1940 [2:20:22<6:35:44, 16.27s/it] 25%|██▍       | 482/1940 [2:20:38<6:38:08, 16.38s/it] 25%|██▍       | 483/1940 [2:20:54<6:36:39, 16.33s/it] 25%|██▍       | 484/1940 [2:21:11<6:35:34, 16.30s/it] 25%|██▌       | 485/1940 [2:21:27<6:37:56, 16.41s/it] 25%|██▌       | 486/1940 [2:21:44<6:36:08, 16.35s/it] 25%|██▌       | 487/1940 [2:22:00<6:34:55, 16.31s/it] 25%|██▌       | 488/1940 [2:22:16<6:34:04, 16.28s/it] 25%|██▌       | 489/1940 [2:22:32<6:35:12, 16.34s/it] 25%|██▌       | 490/1940 [2:22:49<6:34:13, 16.31s/it]                                                      {'loss': 2.5485, 'learning_rate': 4.253395751104748e-05, 'epoch': 0.25}
 25%|██▌       | 490/1940 [2:22:49<6:34:13, 16.31s/it] 25%|██▌       | 491/1940 [2:23:05<6:33:14, 16.28s/it] 25%|██▌       | 492/1940 [2:23:22<6:35:52, 16.40s/it] 25%|██▌       | 493/1940 [2:23:38<6:34:25, 16.35s/it] 25%|██▌       | 494/1940 [2:23:54<6:33:17, 16.32s/it] 26%|██▌       | 495/1940 [2:24:11<6:34:35, 16.38s/it] 26%|██▌       | 496/1940 [2:24:27<6:33:19, 16.34s/it] 26%|██▌       | 497/1940 [2:24:43<6:32:04, 16.30s/it] 26%|██▌       | 498/1940 [2:25:00<6:32:59, 16.35s/it] 26%|██▌       | 499/1940 [2:25:16<6:33:06, 16.37s/it] 26%|██▌       | 500/1940 [2:25:32<6:31:32, 16.31s/it]                                                      {'loss': 2.5499, 'learning_rate': 4.224309443195261e-05, 'epoch': 0.26}
 26%|██▌       | 500/1940 [2:25:32<6:31:32, 16.31s/it] 26%|██▌       | 501/1940 [2:25:48<6:30:28, 16.28s/it] 26%|██▌       | 502/1940 [2:26:05<6:32:58, 16.40s/it] 26%|██▌       | 503/1940 [2:26:21<6:31:03, 16.33s/it] 26%|██▌       | 504/1940 [2:26:37<6:29:49, 16.29s/it] 26%|██▌       | 505/1940 [2:26:54<6:30:38, 16.33s/it] 26%|██▌       | 506/1940 [2:27:10<6:29:42, 16.31s/it] 26%|██▌       | 507/1940 [2:27:26<6:29:04, 16.29s/it] 26%|██▌       | 508/1940 [2:27:43<6:30:13, 16.35s/it] 26%|██▌       | 509/1940 [2:27:59<6:30:25, 16.37s/it] 26%|██▋       | 510/1940 [2:28:15<6:28:53, 16.32s/it]                                                      {'loss': 2.5362, 'learning_rate': 4.194770965018758e-05, 'epoch': 0.26}
 26%|██▋       | 510/1940 [2:28:15<6:28:53, 16.32s/it] 26%|██▋       | 511/1940 [2:28:32<6:27:53, 16.29s/it] 26%|██▋       | 512/1940 [2:28:48<6:30:29, 16.41s/it] 26%|██▋       | 513/1940 [2:29:04<6:28:35, 16.34s/it] 26%|██▋       | 514/1940 [2:29:21<6:27:34, 16.31s/it] 27%|██▋       | 515/1940 [2:29:37<6:28:32, 16.36s/it] 27%|██▋       | 516/1940 [2:29:53<6:27:18, 16.32s/it] 27%|██▋       | 517/1940 [2:30:10<6:26:32, 16.30s/it] 27%|██▋       | 518/1940 [2:30:26<6:27:38, 16.36s/it] 27%|██▋       | 519/1940 [2:30:43<6:27:51, 16.38s/it] 27%|██▋       | 520/1940 [2:30:59<6:26:20, 16.32s/it]                                                      {'loss': 2.5461, 'learning_rate': 4.164788062529203e-05, 'epoch': 0.27}
 27%|██▋       | 520/1940 [2:30:59<6:26:20, 16.32s/it] 27%|██▋       | 521/1940 [2:31:15<6:25:00, 16.28s/it] 27%|██▋       | 522/1940 [2:31:31<6:26:15, 16.34s/it] 27%|██▋       | 523/1940 [2:31:48<6:24:58, 16.30s/it] 27%|██▋       | 524/1940 [2:32:04<6:24:15, 16.28s/it] 27%|██▋       | 525/1940 [2:32:21<6:26:43, 16.40s/it] 27%|██▋       | 526/1940 [2:32:37<6:24:55, 16.33s/it] 27%|██▋       | 527/1940 [2:32:53<6:23:40, 16.29s/it] 27%|██▋       | 528/1940 [2:33:09<6:24:32, 16.34s/it] 27%|██▋       | 529/1940 [2:33:26<6:24:51, 16.37s/it] 27%|██▋       | 530/1940 [2:33:42<6:23:29, 16.32s/it]                                                      {'loss': 2.5651, 'learning_rate': 4.134368598223132e-05, 'epoch': 0.27}
 27%|██▋       | 530/1940 [2:33:42<6:23:29, 16.32s/it] 27%|██▋       | 531/1940 [2:33:58<6:22:24, 16.28s/it] 27%|██▋       | 532/1940 [2:34:15<6:23:20, 16.34s/it] 27%|██▋       | 533/1940 [2:34:31<6:22:14, 16.30s/it] 28%|██▊       | 534/1940 [2:34:47<6:21:28, 16.28s/it] 28%|██▊       | 535/1940 [2:35:04<6:24:17, 16.41s/it] 28%|██▊       | 536/1940 [2:35:20<6:22:33, 16.35s/it] 28%|██▊       | 537/1940 [2:35:36<6:21:10, 16.30s/it] 28%|██▊       | 538/1940 [2:35:52<6:20:16, 16.27s/it] 28%|██▊       | 539/1940 [2:36:09<6:22:41, 16.39s/it] 28%|██▊       | 540/1940 [2:36:25<6:21:11, 16.34s/it]                                                      {'loss': 2.5445, 'learning_rate': 4.10352054907785e-05, 'epoch': 0.28}
 28%|██▊       | 540/1940 [2:36:25<6:21:11, 16.34s/it] 28%|██▊       | 541/1940 [2:36:42<6:20:06, 16.30s/it] 28%|██▊       | 542/1940 [2:36:58<6:21:02, 16.35s/it] 28%|██▊       | 543/1940 [2:37:14<6:19:44, 16.31s/it] 28%|██▊       | 544/1940 [2:37:30<6:19:02, 16.29s/it] 28%|██▊       | 545/1940 [2:37:47<6:21:26, 16.41s/it] 28%|██▊       | 546/1940 [2:38:03<6:19:49, 16.35s/it] 28%|██▊       | 547/1940 [2:38:20<6:18:31, 16.30s/it] 28%|██▊       | 548/1940 [2:38:36<6:17:43, 16.28s/it] 28%|██▊       | 549/1940 [2:38:52<6:19:00, 16.35s/it] 28%|██▊       | 550/1940 [2:39:09<6:17:52, 16.31s/it]                                                      {'loss': 2.538, 'learning_rate': 4.072252004459611e-05, 'epoch': 0.28}
 28%|██▊       | 550/1940 [2:39:09<6:17:52, 16.31s/it] 28%|██▊       | 551/1940 [2:39:25<6:16:52, 16.28s/it] 28%|██▊       | 552/1940 [2:39:41<6:19:11, 16.39s/it] 29%|██▊       | 553/1940 [2:39:58<6:17:30, 16.33s/it] 29%|██▊       | 554/1940 [2:40:14<6:16:33, 16.30s/it] 29%|██▊       | 555/1940 [2:40:30<6:17:24, 16.35s/it] 29%|██▊       | 556/1940 [2:40:47<6:18:04, 16.39s/it] 29%|██▊       | 557/1940 [2:41:03<6:17:01, 16.36s/it] 29%|██▉       | 558/1940 [2:41:19<6:16:09, 16.33s/it] 29%|██▉       | 559/1940 [2:41:36<6:16:39, 16.36s/it] 29%|██▉       | 560/1940 [2:41:52<6:15:21, 16.32s/it]                                                      {'loss': 2.5555, 'learning_rate': 4.0405711640023186e-05, 'epoch': 0.29}
 29%|██▉       | 560/1940 [2:41:52<6:15:21, 16.32s/it] 29%|██▉       | 561/1940 [2:42:08<6:14:14, 16.28s/it] 29%|██▉       | 562/1940 [2:42:25<6:16:40, 16.40s/it] 29%|██▉       | 563/1940 [2:42:41<6:15:01, 16.34s/it] 29%|██▉       | 564/1940 [2:42:57<6:13:48, 16.30s/it] 29%|██▉       | 565/1940 [2:43:14<6:14:19, 16.33s/it] 29%|██▉       | 566/1940 [2:43:30<6:14:38, 16.36s/it] 29%|██▉       | 567/1940 [2:43:46<6:13:10, 16.31s/it] 29%|██▉       | 568/1940 [2:44:02<6:12:21, 16.28s/it] 29%|██▉       | 569/1940 [2:44:19<6:13:26, 16.34s/it] 29%|██▉       | 570/1940 [2:44:35<6:12:50, 16.33s/it]                                                      {'loss': 2.5553, 'learning_rate': 4.008486335457312e-05, 'epoch': 0.29}
 29%|██▉       | 570/1940 [2:44:35<6:12:50, 16.33s/it] 29%|██▉       | 571/1940 [2:44:52<6:12:17, 16.32s/it] 29%|██▉       | 572/1940 [2:45:08<6:14:49, 16.44s/it] 30%|██▉       | 573/1940 [2:45:24<6:13:00, 16.37s/it] 30%|██▉       | 574/1940 [2:45:41<6:11:39, 16.32s/it] 30%|██▉       | 575/1940 [2:45:57<6:12:17, 16.36s/it] 30%|██▉       | 576/1940 [2:46:13<6:11:28, 16.34s/it] 30%|██▉       | 577/1940 [2:46:30<6:10:26, 16.31s/it] 30%|██▉       | 578/1940 [2:46:46<6:09:40, 16.29s/it] 30%|██▉       | 579/1940 [2:47:03<6:12:00, 16.40s/it] 30%|██▉       | 580/1940 [2:47:19<6:10:35, 16.35s/it]                                                      {'loss': 2.5331, 'learning_rate': 3.976005932514807e-05, 'epoch': 0.3}
 30%|██▉       | 580/1940 [2:47:19<6:10:35, 16.35s/it] 30%|██▉       | 581/1940 [2:47:35<6:09:25, 16.31s/it] 30%|███       | 582/1940 [2:47:52<6:11:59, 16.44s/it] 30%|███       | 583/1940 [2:48:08<6:10:32, 16.38s/it] 30%|███       | 584/1940 [2:48:24<6:09:31, 16.35s/it] 30%|███       | 585/1940 [2:48:41<6:08:23, 16.31s/it] 30%|███       | 586/1940 [2:48:57<6:09:05, 16.36s/it] 30%|███       | 587/1940 [2:49:13<6:07:51, 16.31s/it] 30%|███       | 588/1940 [2:49:29<6:06:46, 16.28s/it] 30%|███       | 589/1940 [2:49:46<6:09:06, 16.39s/it] 30%|███       | 590/1940 [2:50:02<6:07:26, 16.33s/it]                                                      {'loss': 2.5623, 'learning_rate': 3.943138472597549e-05, 'epoch': 0.3}
 30%|███       | 590/1940 [2:50:02<6:07:26, 16.33s/it] 30%|███       | 591/1940 [2:50:18<6:06:15, 16.29s/it] 31%|███       | 592/1940 [2:50:35<6:08:37, 16.41s/it] 31%|███       | 593/1940 [2:50:51<6:06:56, 16.35s/it] 31%|███       | 594/1940 [2:51:08<6:05:51, 16.31s/it] 31%|███       | 595/1940 [2:51:24<6:05:18, 16.30s/it] 31%|███       | 596/1940 [2:51:40<6:06:21, 16.36s/it] 31%|███       | 597/1940 [2:51:57<6:05:13, 16.32s/it] 31%|███       | 598/1940 [2:52:13<6:04:28, 16.30s/it] 31%|███       | 599/1940 [2:52:29<6:06:50, 16.41s/it] 31%|███       | 600/1940 [2:52:46<6:05:21, 16.36s/it]                                                      {'loss': 2.555, 'learning_rate': 3.909892574627266e-05, 'epoch': 0.31}
 31%|███       | 600/1940 [2:52:46<6:05:21, 16.36s/it] 31%|███       | 601/1940 [2:53:02<6:04:15, 16.32s/it] 31%|███       | 602/1940 [2:53:18<6:05:37, 16.40s/it] 31%|███       | 603/1940 [2:53:35<6:04:07, 16.34s/it] 31%|███       | 604/1940 [2:53:51<6:03:15, 16.31s/it] 31%|███       | 605/1940 [2:54:07<6:03:53, 16.35s/it] 31%|███       | 606/1940 [2:54:24<6:04:17, 16.39s/it] 31%|███▏      | 607/1940 [2:54:40<6:02:56, 16.34s/it] 31%|███▏      | 608/1940 [2:54:56<6:01:52, 16.30s/it] 31%|███▏      | 609/1940 [2:55:13<6:03:58, 16.41s/it] 31%|███▏      | 610/1940 [2:55:29<6:02:10, 16.34s/it]                                                      {'loss': 2.5302, 'learning_rate': 3.876276956764509e-05, 'epoch': 0.31}
 31%|███▏      | 610/1940 [2:55:29<6:02:10, 16.34s/it] 31%|███▏      | 611/1940 [2:55:45<6:01:01, 16.30s/it] 32%|███▏      | 612/1940 [2:56:02<6:01:37, 16.34s/it] 32%|███▏      | 613/1940 [2:56:18<6:00:21, 16.29s/it] 32%|███▏      | 614/1940 [2:56:34<5:59:29, 16.27s/it] 32%|███▏      | 615/1940 [2:56:51<6:00:22, 16.32s/it] 32%|███▏      | 616/1940 [2:57:07<6:00:48, 16.35s/it] 32%|███▏      | 617/1940 [2:57:23<5:59:33, 16.31s/it] 32%|███▏      | 618/1940 [2:57:39<5:58:46, 16.28s/it] 32%|███▏      | 619/1940 [2:57:56<6:00:56, 16.39s/it] 32%|███▏      | 620/1940 [2:58:12<5:59:16, 16.33s/it]                                                      {'loss': 2.5393, 'learning_rate': 3.84230043412246e-05, 'epoch': 0.32}
 32%|███▏      | 620/1940 [2:58:12<5:59:16, 16.33s/it] 32%|███▏      | 621/1940 [2:58:28<5:58:09, 16.29s/it] 32%|███▏      | 622/1940 [2:58:45<5:58:54, 16.34s/it] 32%|███▏      | 623/1940 [2:59:01<5:57:33, 16.29s/it] 32%|███▏      | 624/1940 [2:59:17<5:56:47, 16.27s/it] 32%|███▏      | 625/1940 [2:59:34<5:57:42, 16.32s/it] 32%|███▏      | 626/1940 [2:59:50<5:58:04, 16.35s/it] 32%|███▏      | 627/1940 [3:00:06<5:56:44, 16.30s/it] 32%|███▏      | 628/1940 [3:00:23<5:56:03, 16.28s/it] 32%|███▏      | 629/1940 [3:00:39<5:57:22, 16.36s/it] 32%|███▏      | 630/1940 [3:00:55<5:56:11, 16.31s/it]                                                      {'loss': 2.5408, 'learning_rate': 3.807971916455325e-05, 'epoch': 0.32}
 32%|███▏      | 630/1940 [3:00:55<5:56:11, 16.31s/it] 33%|███▎      | 631/1940 [3:01:12<5:55:04, 16.28s/it] 33%|███▎      | 632/1940 [3:01:28<5:57:21, 16.39s/it] 33%|███▎      | 633/1940 [3:01:44<5:55:50, 16.34s/it] 33%|███▎      | 634/1940 [3:02:01<5:54:46, 16.30s/it] 33%|███▎      | 635/1940 [3:02:17<5:55:36, 16.35s/it] 33%|███▎      | 636/1940 [3:02:34<5:55:52, 16.37s/it] 33%|███▎      | 637/1940 [3:02:50<5:54:32, 16.33s/it] 33%|███▎      | 638/1940 [3:03:06<5:53:38, 16.30s/it] 33%|███▎      | 639/1940 [3:03:22<5:54:15, 16.34s/it] 33%|███▎      | 640/1940 [3:03:39<5:53:11, 16.30s/it]                                                      {'loss': 2.5344, 'learning_rate': 3.773300405821908e-05, 'epoch': 0.33}
 33%|███▎      | 640/1940 [3:03:39<5:53:11, 16.30s/it] 33%|███▎      | 641/1940 [3:03:55<5:52:19, 16.27s/it] 33%|███▎      | 642/1940 [3:04:12<5:54:40, 16.39s/it] 33%|███▎      | 643/1940 [3:04:28<5:53:07, 16.34s/it] 33%|███▎      | 644/1940 [3:04:44<5:52:00, 16.30s/it] 33%|███▎      | 645/1940 [3:05:00<5:51:11, 16.27s/it] 33%|███▎      | 646/1940 [3:05:17<5:53:40, 16.40s/it] 33%|███▎      | 647/1940 [3:05:33<5:52:17, 16.35s/it] 33%|███▎      | 648/1940 [3:05:49<5:51:27, 16.32s/it] 33%|███▎      | 649/1940 [3:06:06<5:52:28, 16.38s/it] 34%|███▎      | 650/1940 [3:06:22<5:51:29, 16.35s/it]                                                      {'loss': 2.5447, 'learning_rate': 3.7382949942249694e-05, 'epoch': 0.34}
 34%|███▎      | 650/1940 [3:06:22<5:51:29, 16.35s/it] 34%|███▎      | 651/1940 [3:06:38<5:50:28, 16.31s/it] 34%|███▎      | 652/1940 [3:06:55<5:52:49, 16.44s/it] 34%|███▎      | 653/1940 [3:07:11<5:51:04, 16.37s/it] 34%|███▎      | 654/1940 [3:07:28<5:50:00, 16.33s/it] 34%|███▍      | 655/1940 [3:07:44<5:49:00, 16.30s/it] 34%|███▍      | 656/1940 [3:08:00<5:50:04, 16.36s/it] 34%|███▍      | 657/1940 [3:08:16<5:48:42, 16.31s/it] 34%|███▍      | 658/1940 [3:08:33<5:47:50, 16.28s/it] 34%|███▍      | 659/1940 [3:08:49<5:50:04, 16.40s/it] 34%|███▍      | 660/1940 [3:09:06<5:48:43, 16.35s/it]                                                      {'loss': 2.5367, 'learning_rate': 3.702964861227013e-05, 'epoch': 0.34}
 34%|███▍      | 660/1940 [3:09:06<5:48:43, 16.35s/it] 34%|███▍      | 661/1940 [3:09:22<5:48:04, 16.33s/it] 34%|███▍      | 662/1940 [3:09:38<5:48:42, 16.37s/it] 34%|███▍      | 663/1940 [3:09:55<5:48:59, 16.40s/it] 34%|███▍      | 664/1940 [3:10:11<5:47:30, 16.34s/it] 34%|███▍      | 665/1940 [3:10:27<5:46:16, 16.30s/it] 34%|███▍      | 666/1940 [3:10:44<5:46:54, 16.34s/it] 34%|███▍      | 667/1940 [3:11:00<5:45:51, 16.30s/it] 34%|███▍      | 668/1940 [3:11:16<5:45:06, 16.28s/it] 34%|███▍      | 669/1940 [3:11:33<5:47:13, 16.39s/it] 35%|███▍      | 670/1940 [3:11:49<5:45:49, 16.34s/it]                                                      {'loss': 2.5324, 'learning_rate': 3.6673192715431015e-05, 'epoch': 0.35}
 35%|███▍      | 670/1940 [3:11:49<5:45:49, 16.34s/it] 35%|███▍      | 671/1940 [3:12:05<5:44:42, 16.30s/it] 35%|███▍      | 672/1940 [3:12:22<5:45:25, 16.35s/it] 35%|███▍      | 673/1940 [3:12:38<5:46:04, 16.39s/it] 35%|███▍      | 674/1940 [3:12:54<5:45:04, 16.35s/it] 35%|███▍      | 675/1940 [3:13:11<5:44:08, 16.32s/it] 35%|███▍      | 676/1940 [3:13:27<5:44:36, 16.36s/it] 35%|███▍      | 677/1940 [3:13:43<5:43:21, 16.31s/it] 35%|███▍      | 678/1940 [3:13:59<5:42:27, 16.28s/it] 35%|███▌      | 679/1940 [3:14:16<5:44:35, 16.40s/it] 35%|███▌      | 680/1940 [3:14:32<5:43:06, 16.34s/it]                                                      {'loss': 2.544, 'learning_rate': 3.631367572611348e-05, 'epoch': 0.35}
 35%|███▌      | 680/1940 [3:14:32<5:43:06, 16.34s/it] 35%|███▌      | 681/1940 [3:14:49<5:42:04, 16.30s/it] 35%|███▌      | 682/1940 [3:15:05<5:42:48, 16.35s/it] 35%|███▌      | 683/1940 [3:15:21<5:42:00, 16.33s/it] 35%|███▌      | 684/1940 [3:15:37<5:41:07, 16.30s/it] 35%|███▌      | 685/1940 [3:15:54<5:40:40, 16.29s/it] 35%|███▌      | 686/1940 [3:16:10<5:43:02, 16.41s/it] 35%|███▌      | 687/1940 [3:16:27<5:41:52, 16.37s/it] 35%|███▌      | 688/1940 [3:16:43<5:40:46, 16.33s/it] 36%|███▌      | 689/1940 [3:17:00<5:42:47, 16.44s/it] 36%|███▌      | 690/1940 [3:17:16<5:40:59, 16.37s/it]                                                      {'loss': 2.5316, 'learning_rate': 3.595119192141706e-05, 'epoch': 0.36}
 36%|███▌      | 690/1940 [3:17:16<5:40:59, 16.37s/it] 36%|███▌      | 691/1940 [3:17:32<5:39:34, 16.31s/it] 36%|███▌      | 692/1940 [3:17:48<5:38:42, 16.28s/it] 36%|███▌      | 693/1940 [3:18:05<5:39:12, 16.32s/it] 36%|███▌      | 694/1940 [3:18:21<5:37:57, 16.27s/it] 36%|███▌      | 695/1940 [3:18:37<5:37:16, 16.25s/it] 36%|███▌      | 696/1940 [3:18:54<5:39:32, 16.38s/it] 36%|███▌      | 697/1940 [3:19:10<5:38:17, 16.33s/it] 36%|███▌      | 698/1940 [3:19:26<5:37:38, 16.31s/it] 36%|███▌      | 699/1940 [3:19:43<5:39:51, 16.43s/it] 36%|███▌      | 700/1940 [3:19:59<5:38:20, 16.37s/it]                                                      {'loss': 2.5367, 'learning_rate': 3.5585836356437264e-05, 'epoch': 0.36}
 36%|███▌      | 700/1940 [3:19:59<5:38:20, 16.37s/it] 36%|███▌      | 701/1940 [3:20:15<5:37:03, 16.32s/it] 36%|███▌      | 702/1940 [3:20:32<5:35:56, 16.28s/it] 36%|███▌      | 703/1940 [3:20:48<5:36:36, 16.33s/it] 36%|███▋      | 704/1940 [3:21:04<5:35:37, 16.29s/it] 36%|███▋      | 705/1940 [3:21:20<5:34:36, 16.26s/it] 36%|███▋      | 706/1940 [3:21:37<5:36:31, 16.36s/it] 36%|███▋      | 707/1940 [3:21:53<5:35:08, 16.31s/it] 36%|███▋      | 708/1940 [3:22:09<5:34:08, 16.27s/it] 37%|███▋      | 709/1940 [3:22:26<5:35:23, 16.35s/it] 37%|███▋      | 710/1940 [3:22:42<5:34:20, 16.31s/it]                                                      {'loss': 2.5592, 'learning_rate': 3.521770483933891e-05, 'epoch': 0.37}
 37%|███▋      | 710/1940 [3:22:42<5:34:20, 16.31s/it] 37%|███▋      | 711/1940 [3:22:58<5:33:42, 16.29s/it] 37%|███▋      | 712/1940 [3:23:15<5:34:28, 16.34s/it] 37%|███▋      | 713/1940 [3:23:31<5:34:57, 16.38s/it] 37%|███▋      | 714/1940 [3:23:47<5:33:28, 16.32s/it] 37%|███▋      | 715/1940 [3:24:04<5:32:32, 16.29s/it] 37%|███▋      | 716/1940 [3:24:20<5:34:30, 16.40s/it] 37%|███▋      | 717/1940 [3:24:36<5:32:55, 16.33s/it] 37%|███▋      | 718/1940 [3:24:53<5:31:46, 16.29s/it] 37%|███▋      | 719/1940 [3:25:09<5:32:21, 16.33s/it] 37%|███▋      | 720/1940 [3:25:25<5:31:19, 16.29s/it]                                                      {'loss': 2.5219, 'learning_rate': 3.484689390623218e-05, 'epoch': 0.37}
 37%|███▋      | 720/1940 [3:25:25<5:31:19, 16.29s/it] 37%|███▋      | 721/1940 [3:25:41<5:30:30, 16.27s/it] 37%|███▋      | 722/1940 [3:25:58<5:31:33, 16.33s/it] 37%|███▋      | 723/1940 [3:26:14<5:32:08, 16.37s/it] 37%|███▋      | 724/1940 [3:26:31<5:31:20, 16.35s/it] 37%|███▋      | 725/1940 [3:26:47<5:30:27, 16.32s/it] 37%|███▋      | 726/1940 [3:27:04<5:32:23, 16.43s/it] 37%|███▋      | 727/1940 [3:27:20<5:30:41, 16.36s/it] 38%|███▊      | 728/1940 [3:27:36<5:29:32, 16.31s/it] 38%|███▊      | 729/1940 [3:27:53<5:30:06, 16.36s/it] 38%|███▊      | 730/1940 [3:28:09<5:28:54, 16.31s/it]                                                      {'loss': 2.5301, 'learning_rate': 3.447350079585767e-05, 'epoch': 0.38}
 38%|███▊      | 730/1940 [3:28:09<5:28:54, 16.31s/it] 38%|███▊      | 731/1940 [3:28:25<5:28:03, 16.28s/it] 38%|███▊      | 732/1940 [3:28:41<5:28:53, 16.34s/it] 38%|███▊      | 733/1940 [3:28:58<5:29:02, 16.36s/it] 38%|███▊      | 734/1940 [3:29:14<5:27:53, 16.31s/it] 38%|███▊      | 735/1940 [3:29:30<5:27:12, 16.29s/it] 38%|███▊      | 736/1940 [3:29:47<5:28:32, 16.37s/it] 38%|███▊      | 737/1940 [3:30:03<5:27:28, 16.33s/it] 38%|███▊      | 738/1940 [3:30:19<5:26:22, 16.29s/it] 38%|███▊      | 739/1940 [3:30:36<5:28:20, 16.40s/it] 38%|███▊      | 740/1940 [3:30:52<5:26:51, 16.34s/it]                                                      {'loss': 2.5328, 'learning_rate': 3.409762342408719e-05, 'epoch': 0.38}
 38%|███▊      | 740/1940 [3:30:52<5:26:51, 16.34s/it] 38%|███▊      | 741/1940 [3:31:08<5:25:43, 16.30s/it] 38%|███▊      | 742/1940 [3:31:25<5:26:12, 16.34s/it] 38%|███▊      | 743/1940 [3:31:41<5:26:34, 16.37s/it] 38%|███▊      | 744/1940 [3:31:57<5:25:21, 16.32s/it] 38%|███▊      | 745/1940 [3:32:14<5:24:30, 16.29s/it] 38%|███▊      | 746/1940 [3:32:30<5:25:24, 16.35s/it] 39%|███▊      | 747/1940 [3:32:46<5:24:39, 16.33s/it] 39%|███▊      | 748/1940 [3:33:03<5:24:09, 16.32s/it] 39%|███▊      | 749/1940 [3:33:19<5:26:07, 16.43s/it] 39%|███▊      | 750/1940 [3:33:36<5:24:27, 16.36s/it]                                                      {'loss': 2.5333, 'learning_rate': 3.3719360358247054e-05, 'epoch': 0.39}
 39%|███▊      | 750/1940 [3:33:36<5:24:27, 16.36s/it] 39%|███▊      | 751/1940 [3:33:52<5:23:07, 16.31s/it] 39%|███▉      | 752/1940 [3:34:08<5:22:22, 16.28s/it] 39%|███▉      | 753/1940 [3:34:25<5:24:19, 16.39s/it] 39%|███▉      | 754/1940 [3:34:41<5:22:51, 16.33s/it] 39%|███▉      | 755/1940 [3:34:57<5:21:44, 16.29s/it] 39%|███▉      | 756/1940 [3:35:13<5:22:24, 16.34s/it] 39%|███▉      | 757/1940 [3:35:30<5:21:16, 16.29s/it] 39%|███▉      | 758/1940 [3:35:46<5:20:31, 16.27s/it] 39%|███▉      | 759/1940 [3:36:03<5:22:44, 16.40s/it] 39%|███▉      | 760/1940 [3:36:19<5:21:33, 16.35s/it]                                                      {'loss': 2.5438, 'learning_rate': 3.333881079127052e-05, 'epoch': 0.39}
 39%|███▉      | 760/1940 [3:36:19<5:21:33, 16.35s/it] 39%|███▉      | 761/1940 [3:36:35<5:20:37, 16.32s/it] 39%|███▉      | 762/1940 [3:36:51<5:19:53, 16.29s/it] 39%|███▉      | 763/1940 [3:37:08<5:20:51, 16.36s/it] 39%|███▉      | 764/1940 [3:37:24<5:19:45, 16.31s/it] 39%|███▉      | 765/1940 [3:37:40<5:18:56, 16.29s/it] 39%|███▉      | 766/1940 [3:37:57<5:21:01, 16.41s/it] 40%|███▉      | 767/1940 [3:38:13<5:19:33, 16.35s/it] 40%|███▉      | 768/1940 [3:38:29<5:18:32, 16.31s/it] 40%|███▉      | 769/1940 [3:38:46<5:19:10, 16.35s/it] 40%|███▉      | 770/1940 [3:39:02<5:19:29, 16.38s/it]                                                      {'loss': 2.543, 'learning_rate': 3.29560745156861e-05, 'epoch': 0.4}
 40%|███▉      | 770/1940 [3:39:02<5:19:29, 16.38s/it] 40%|███▉      | 771/1940 [3:39:18<5:18:13, 16.33s/it] 40%|███▉      | 772/1940 [3:39:35<5:17:21, 16.30s/it] 40%|███▉      | 773/1940 [3:39:51<5:18:15, 16.36s/it] 40%|███▉      | 774/1940 [3:40:08<5:17:38, 16.34s/it] 40%|███▉      | 775/1940 [3:40:24<5:17:00, 16.33s/it] 40%|████      | 776/1940 [3:40:41<5:19:01, 16.44s/it] 40%|████      | 777/1940 [3:40:57<5:17:19, 16.37s/it] 40%|████      | 778/1940 [3:41:13<5:16:12, 16.33s/it] 40%|████      | 779/1940 [3:41:29<5:16:40, 16.37s/it] 40%|████      | 780/1940 [3:41:46<5:17:02, 16.40s/it]                                                      {'loss': 2.5301, 'learning_rate': 3.2571251897448765e-05, 'epoch': 0.4}
 40%|████      | 780/1940 [3:41:46<5:17:02, 16.40s/it] 40%|████      | 781/1940 [3:42:02<5:15:46, 16.35s/it] 40%|████      | 782/1940 [3:42:18<5:14:39, 16.30s/it] 40%|████      | 783/1940 [3:42:35<5:15:06, 16.34s/it] 40%|████      | 784/1940 [3:42:51<5:13:57, 16.30s/it] 40%|████      | 785/1940 [3:43:07<5:13:08, 16.27s/it] 41%|████      | 786/1940 [3:43:24<5:15:26, 16.40s/it] 41%|████      | 787/1940 [3:43:40<5:14:26, 16.36s/it] 41%|████      | 788/1940 [3:43:56<5:13:49, 16.34s/it] 41%|████      | 789/1940 [3:44:13<5:14:09, 16.38s/it] 41%|████      | 790/1940 [3:44:29<5:13:11, 16.34s/it]                                                      {'loss': 2.5216, 'learning_rate': 3.218444384962071e-05, 'epoch': 0.41}
 41%|████      | 790/1940 [3:44:29<5:13:11, 16.34s/it] 41%|████      | 791/1940 [3:44:45<5:12:04, 16.30s/it] 41%|████      | 792/1940 [3:45:02<5:11:23, 16.28s/it] 41%|████      | 793/1940 [3:45:18<5:13:27, 16.40s/it] 41%|████      | 794/1940 [3:45:34<5:12:07, 16.34s/it] 41%|████      | 795/1940 [3:45:51<5:11:06, 16.30s/it] 41%|████      | 796/1940 [3:46:07<5:12:57, 16.41s/it] 41%|████      | 797/1940 [3:46:23<5:11:24, 16.35s/it] 41%|████      | 798/1940 [3:46:40<5:10:27, 16.31s/it] 41%|████      | 799/1940 [3:46:56<5:09:37, 16.28s/it] 41%|████      | 800/1940 [3:47:12<5:10:30, 16.34s/it]                                                      {'loss': 2.5239, 'learning_rate': 3.1795751805908573e-05, 'epoch': 0.41}
 41%|████      | 800/1940 [3:47:12<5:10:30, 16.34s/it] 41%|████▏     | 801/1940 [3:47:29<5:09:47, 16.32s/it] 41%|████▏     | 802/1940 [3:47:45<5:09:06, 16.30s/it] 41%|████▏     | 803/1940 [3:48:02<5:10:52, 16.40s/it] 41%|████▏     | 804/1940 [3:48:18<5:09:25, 16.34s/it] 41%|████▏     | 805/1940 [3:48:34<5:08:17, 16.30s/it] 42%|████▏     | 806/1940 [3:48:51<5:10:10, 16.41s/it] 42%|████▏     | 807/1940 [3:49:07<5:08:37, 16.34s/it] 42%|████▏     | 808/1940 [3:49:23<5:07:31, 16.30s/it] 42%|████▏     | 809/1940 [3:49:39<5:06:35, 16.26s/it] 42%|████▏     | 810/1940 [3:49:56<5:07:19, 16.32s/it]                                                      {'loss': 2.5423, 'learning_rate': 3.1405277694064305e-05, 'epoch': 0.42}
 42%|████▏     | 810/1940 [3:49:56<5:07:19, 16.32s/it] 42%|████▏     | 811/1940 [3:50:12<5:06:21, 16.28s/it] 42%|████▏     | 812/1940 [3:50:28<5:05:51, 16.27s/it] 42%|████▏     | 813/1940 [3:50:45<5:08:04, 16.40s/it] 42%|████▏     | 814/1940 [3:51:01<5:06:58, 16.36s/it] 42%|████▏     | 815/1940 [3:51:17<5:06:01, 16.32s/it] 42%|████▏     | 816/1940 [3:51:34<5:06:40, 16.37s/it] 42%|████▏     | 817/1940 [3:51:50<5:05:17, 16.31s/it] 42%|████▏     | 818/1940 [3:52:06<5:04:24, 16.28s/it] 42%|████▏     | 819/1940 [3:52:23<5:05:01, 16.33s/it] 42%|████▏     | 820/1940 [3:52:39<5:05:12, 16.35s/it]                                                      {'loss': 2.5544, 'learning_rate': 3.101312390915634e-05, 'epoch': 0.42}
 42%|████▏     | 820/1940 [3:52:39<5:05:12, 16.35s/it] 42%|████▏     | 821/1940 [3:52:55<5:04:04, 16.30s/it] 42%|████▏     | 822/1940 [3:53:11<5:03:05, 16.27s/it] 42%|████▏     | 823/1940 [3:53:28<5:05:02, 16.39s/it] 42%|████▏     | 824/1940 [3:53:44<5:03:43, 16.33s/it] 43%|████▎     | 825/1940 [3:54:00<5:02:57, 16.30s/it] 43%|████▎     | 826/1940 [3:54:17<5:03:44, 16.36s/it] 43%|████▎     | 827/1940 [3:54:33<5:02:49, 16.32s/it] 43%|████▎     | 828/1940 [3:54:49<5:02:01, 16.30s/it] 43%|████▎     | 829/1940 [3:55:06<5:02:38, 16.34s/it] 43%|████▎     | 830/1940 [3:55:22<5:03:00, 16.38s/it]                                                      {'loss': 2.5125, 'learning_rate': 3.061939328671824e-05, 'epoch': 0.43}
 43%|████▎     | 830/1940 [3:55:22<5:03:00, 16.38s/it] 43%|████▎     | 831/1940 [3:55:39<5:01:47, 16.33s/it] 43%|████▎     | 832/1940 [3:55:55<5:00:56, 16.30s/it] 43%|████▎     | 833/1940 [3:56:11<5:02:51, 16.41s/it] 43%|████▎     | 834/1940 [3:56:28<5:01:30, 16.36s/it] 43%|████▎     | 835/1940 [3:56:44<5:00:26, 16.31s/it] 43%|████▎     | 836/1940 [3:57:00<5:00:59, 16.36s/it] 43%|████▎     | 837/1940 [3:57:17<5:00:06, 16.32s/it] 43%|████▎     | 838/1940 [3:57:33<4:59:33, 16.31s/it] 43%|████▎     | 839/1940 [3:57:49<5:00:26, 16.37s/it] 43%|████▎     | 840/1940 [3:58:06<5:00:38, 16.40s/it]                                                      {'loss': 2.5436, 'learning_rate': 3.0224189075781884e-05, 'epoch': 0.43}
 43%|████▎     | 840/1940 [3:58:06<5:00:38, 16.40s/it] 43%|████▎     | 841/1940 [3:58:22<4:59:12, 16.34s/it] 43%|████▎     | 842/1940 [3:58:38<4:58:13, 16.30s/it] 43%|████▎     | 843/1940 [3:58:55<4:59:05, 16.36s/it] 44%|████▎     | 844/1940 [3:59:11<4:57:56, 16.31s/it] 44%|████▎     | 845/1940 [3:59:27<4:57:03, 16.28s/it] 44%|████▎     | 846/1940 [3:59:44<4:59:07, 16.41s/it] 44%|████▎     | 847/1940 [4:00:00<4:57:36, 16.34s/it] 44%|████▎     | 848/1940 [4:00:16<4:56:35, 16.30s/it] 44%|████▍     | 849/1940 [4:00:33<4:57:08, 16.34s/it] 44%|████▍     | 850/1940 [4:00:49<4:57:37, 16.38s/it]                                                      {'loss': 2.5265, 'learning_rate': 2.9827614911802203e-05, 'epoch': 0.44}
 44%|████▍     | 850/1940 [4:00:49<4:57:37, 16.38s/it] 44%|████▍     | 851/1940 [4:01:05<4:56:45, 16.35s/it] 44%|████▍     | 852/1940 [4:01:22<4:55:56, 16.32s/it] 44%|████▍     | 853/1940 [4:01:38<4:56:20, 16.36s/it] 44%|████▍     | 854/1940 [4:01:54<4:55:12, 16.31s/it] 44%|████▍     | 855/1940 [4:02:11<4:54:11, 16.27s/it] 44%|████▍     | 856/1940 [4:02:27<4:56:02, 16.39s/it] 44%|████▍     | 857/1940 [4:02:43<4:54:33, 16.32s/it] 44%|████▍     | 858/1940 [4:03:00<4:53:36, 16.28s/it] 44%|████▍     | 859/1940 [4:03:16<4:52:55, 16.26s/it] 44%|████▍     | 860/1940 [4:03:32<4:54:55, 16.39s/it]                                                      {'loss': 2.5274, 'learning_rate': 2.9429774789480575e-05, 'epoch': 0.44}
 44%|████▍     | 860/1940 [4:03:32<4:54:55, 16.39s/it] 44%|████▍     | 861/1940 [4:03:49<4:53:49, 16.34s/it] 44%|████▍     | 862/1940 [4:04:05<4:53:05, 16.31s/it] 44%|████▍     | 863/1940 [4:04:21<4:53:47, 16.37s/it] 45%|████▍     | 864/1940 [4:04:38<4:52:45, 16.32s/it] 45%|████▍     | 865/1940 [4:04:54<4:51:48, 16.29s/it] 45%|████▍     | 866/1940 [4:05:11<4:53:36, 16.40s/it] 45%|████▍     | 867/1940 [4:05:27<4:52:16, 16.34s/it] 45%|████▍     | 868/1940 [4:05:43<4:51:20, 16.31s/it] 45%|████▍     | 869/1940 [4:05:59<4:50:38, 16.28s/it] 45%|████▍     | 870/1940 [4:06:16<4:51:45, 16.36s/it]                                                      {'loss': 2.5141, 'learning_rate': 2.9030773035493997e-05, 'epoch': 0.45}
 45%|████▍     | 870/1940 [4:06:16<4:51:45, 16.36s/it] 45%|████▍     | 871/1940 [4:06:32<4:50:45, 16.32s/it] 45%|████▍     | 872/1940 [4:06:48<4:49:58, 16.29s/it] 45%|████▌     | 873/1940 [4:07:05<4:51:54, 16.41s/it] 45%|████▌     | 874/1940 [4:07:21<4:50:41, 16.36s/it] 45%|████▌     | 875/1940 [4:07:37<4:49:51, 16.33s/it] 45%|████▌     | 876/1940 [4:07:54<4:50:20, 16.37s/it] 45%|████▌     | 877/1940 [4:08:10<4:50:19, 16.39s/it] 45%|████▌     | 878/1940 [4:08:26<4:49:04, 16.33s/it] 45%|████▌     | 879/1940 [4:08:43<4:47:59, 16.29s/it] 45%|████▌     | 880/1940 [4:08:59<4:48:31, 16.33s/it]                                                      {'loss': 2.5228, 'learning_rate': 2.863071428113726e-05, 'epoch': 0.45}
 45%|████▌     | 880/1940 [4:08:59<4:48:31, 16.33s/it] 45%|████▌     | 881/1940 [4:09:15<4:47:30, 16.29s/it] 45%|████▌     | 882/1940 [4:09:31<4:46:56, 16.27s/it] 46%|████▌     | 883/1940 [4:09:48<4:48:44, 16.39s/it] 46%|████▌     | 884/1940 [4:10:04<4:47:28, 16.33s/it] 46%|████▌     | 885/1940 [4:10:21<4:46:31, 16.30s/it] 46%|████▌     | 886/1940 [4:10:37<4:47:12, 16.35s/it] 46%|████▌     | 887/1940 [4:10:54<4:47:45, 16.40s/it] 46%|████▌     | 888/1940 [4:11:10<4:46:49, 16.36s/it] 46%|████▌     | 889/1940 [4:11:26<4:45:48, 16.32s/it] 46%|████▌     | 890/1940 [4:11:42<4:46:10, 16.35s/it]                                                      {'loss': 2.5144, 'learning_rate': 2.8229703434885163e-05, 'epoch': 0.46}
 46%|████▌     | 890/1940 [4:11:42<4:46:10, 16.35s/it] 46%|████▌     | 891/1940 [4:11:59<4:45:03, 16.31s/it] 46%|████▌     | 892/1940 [4:12:15<4:44:19, 16.28s/it] 46%|████▌     | 893/1940 [4:12:32<4:46:09, 16.40s/it] 46%|████▌     | 894/1940 [4:12:48<4:44:48, 16.34s/it] 46%|████▌     | 895/1940 [4:13:04<4:43:44, 16.29s/it] 46%|████▌     | 896/1940 [4:13:20<4:44:24, 16.34s/it] 46%|████▌     | 897/1940 [4:13:37<4:43:50, 16.33s/it] 46%|████▋     | 898/1940 [4:13:53<4:43:04, 16.30s/it] 46%|████▋     | 899/1940 [4:14:09<4:42:35, 16.29s/it] 46%|████▋     | 900/1940 [4:14:26<4:44:30, 16.41s/it]                                                      {'loss': 2.5165, 'learning_rate': 2.782784565488211e-05, 'epoch': 0.46}
 46%|████▋     | 900/1940 [4:14:26<4:44:30, 16.41s/it] 46%|████▋     | 901/1940 [4:14:42<4:43:14, 16.36s/it] 46%|████▋     | 902/1940 [4:14:58<4:42:10, 16.31s/it] 47%|████▋     | 903/1940 [4:15:15<4:43:39, 16.41s/it] 47%|████▋     | 904/1940 [4:15:31<4:42:11, 16.34s/it] 47%|████▋     | 905/1940 [4:15:47<4:41:08, 16.30s/it] 47%|████▋     | 906/1940 [4:16:04<4:40:26, 16.27s/it] 47%|████▋     | 907/1940 [4:16:20<4:40:56, 16.32s/it] 47%|████▋     | 908/1940 [4:16:36<4:40:07, 16.29s/it] 47%|████▋     | 909/1940 [4:16:52<4:39:35, 16.27s/it] 47%|████▋     | 910/1940 [4:17:09<4:41:23, 16.39s/it]                                                      {'loss': 2.5148, 'learning_rate': 2.7425246321366203e-05, 'epoch': 0.47}
 47%|████▋     | 910/1940 [4:17:09<4:41:23, 16.39s/it] 47%|████▋     | 911/1940 [4:17:25<4:40:05, 16.33s/it] 47%|████▋     | 912/1940 [4:17:42<4:39:18, 16.30s/it] 47%|████▋     | 913/1940 [4:17:58<4:40:54, 16.41s/it] 47%|████▋     | 914/1940 [4:18:14<4:39:31, 16.35s/it] 47%|████▋     | 915/1940 [4:18:31<4:38:25, 16.30s/it] 47%|████▋     | 916/1940 [4:18:47<4:37:39, 16.27s/it] 47%|████▋     | 917/1940 [4:19:03<4:38:14, 16.32s/it] 47%|████▋     | 918/1940 [4:19:19<4:37:19, 16.28s/it] 47%|████▋     | 919/1940 [4:19:36<4:36:38, 16.26s/it] 47%|████▋     | 920/1940 [4:19:52<4:38:21, 16.37s/it]                                                      {'loss': 2.5352, 'learning_rate': 2.7022011009035107e-05, 'epoch': 0.47}
 47%|████▋     | 920/1940 [4:19:52<4:38:21, 16.37s/it] 47%|████▋     | 921/1940 [4:20:08<4:37:13, 16.32s/it] 48%|████▊     | 922/1940 [4:20:25<4:36:18, 16.29s/it] 48%|████▊     | 923/1940 [4:20:41<4:37:04, 16.35s/it] 48%|████▊     | 924/1940 [4:20:57<4:35:54, 16.29s/it] 48%|████▊     | 925/1940 [4:21:13<4:34:59, 16.26s/it] 48%|████▊     | 926/1940 [4:21:30<4:35:33, 16.31s/it] 48%|████▊     | 927/1940 [4:21:46<4:35:51, 16.34s/it] 48%|████▊     | 928/1940 [4:22:03<4:34:49, 16.29s/it] 48%|████▊     | 929/1940 [4:22:19<4:34:09, 16.27s/it] 48%|████▊     | 930/1940 [4:22:35<4:35:48, 16.38s/it]                                                      {'loss': 2.5226, 'learning_rate': 2.6618245459360897e-05, 'epoch': 0.48}
 48%|████▊     | 930/1940 [4:22:35<4:35:48, 16.38s/it] 48%|████▊     | 931/1940 [4:22:52<4:34:39, 16.33s/it] 48%|████▊     | 932/1940 [4:23:08<4:33:51, 16.30s/it] 48%|████▊     | 933/1940 [4:23:24<4:34:30, 16.36s/it] 48%|████▊     | 934/1940 [4:23:41<4:33:36, 16.32s/it] 48%|████▊     | 935/1940 [4:23:57<4:32:52, 16.29s/it] 48%|████▊     | 936/1940 [4:24:13<4:33:23, 16.34s/it] 48%|████▊     | 937/1940 [4:24:30<4:33:29, 16.36s/it] 48%|████▊     | 938/1940 [4:24:46<4:32:24, 16.31s/it] 48%|████▊     | 939/1940 [4:25:02<4:31:30, 16.27s/it] 48%|████▊     | 940/1940 [4:25:19<4:33:17, 16.40s/it]                                                      {'loss': 2.5237, 'learning_rate': 2.621405555286121e-05, 'epoch': 0.48}
 48%|████▊     | 940/1940 [4:25:19<4:33:17, 16.40s/it] 49%|████▊     | 941/1940 [4:25:35<4:32:03, 16.34s/it] 49%|████▊     | 942/1940 [4:25:51<4:31:03, 16.30s/it] 49%|████▊     | 943/1940 [4:26:08<4:31:44, 16.35s/it] 49%|████▊     | 944/1940 [4:26:24<4:30:55, 16.32s/it] 49%|████▊     | 945/1940 [4:26:40<4:30:18, 16.30s/it] 49%|████▉     | 946/1940 [4:26:57<4:30:58, 16.36s/it] 49%|████▉     | 947/1940 [4:27:13<4:31:11, 16.39s/it] 49%|████▉     | 948/1940 [4:27:29<4:30:02, 16.33s/it] 49%|████▉     | 949/1940 [4:27:45<4:29:14, 16.30s/it] 49%|████▉     | 950/1940 [4:28:02<4:30:03, 16.37s/it]                                                      {'loss': 2.5151, 'learning_rate': 2.5809547281333902e-05, 'epoch': 0.49}
 49%|████▉     | 950/1940 [4:28:02<4:30:03, 16.37s/it] 49%|████▉     | 951/1940 [4:28:18<4:29:03, 16.32s/it] 49%|████▉     | 952/1940 [4:28:34<4:28:19, 16.29s/it] 49%|████▉     | 953/1940 [4:28:51<4:29:50, 16.40s/it] 49%|████▉     | 954/1940 [4:29:07<4:28:36, 16.35s/it] 49%|████▉     | 955/1940 [4:29:24<4:27:52, 16.32s/it] 49%|████▉     | 956/1940 [4:29:40<4:28:36, 16.38s/it] 49%|████▉     | 957/1940 [4:29:57<4:28:51, 16.41s/it] 49%|████▉     | 958/1940 [4:30:13<4:27:42, 16.36s/it] 49%|████▉     | 959/1940 [4:30:29<4:26:44, 16.31s/it] 49%|████▉     | 960/1940 [4:30:45<4:27:10, 16.36s/it]                                                      {'loss': 2.5077, 'learning_rate': 2.540482672006254e-05, 'epoch': 0.49}
 49%|████▉     | 960/1940 [4:30:45<4:27:10, 16.36s/it] 50%|████▉     | 961/1940 [4:31:02<4:26:11, 16.31s/it] 50%|████▉     | 962/1940 [4:31:18<4:25:21, 16.28s/it] 50%|████▉     | 963/1940 [4:31:35<4:27:04, 16.40s/it] 50%|████▉     | 964/1940 [4:31:51<4:25:49, 16.34s/it] 50%|████▉     | 965/1940 [4:32:07<4:24:48, 16.30s/it] 50%|████▉     | 966/1940 [4:32:23<4:24:07, 16.27s/it] 50%|████▉     | 967/1940 [4:32:40<4:25:51, 16.39s/it] 50%|████▉     | 968/1940 [4:32:56<4:24:55, 16.35s/it] 50%|████▉     | 969/1940 [4:33:12<4:24:20, 16.33s/it] 50%|█████     | 970/1940 [4:33:29<4:24:53, 16.39s/it]                                                      {'loss': 2.5248, 'learning_rate': 2.5e-05, 'epoch': 0.5}
 50%|█████     | 970/1940 [4:33:29<4:24:53, 16.39s/it] 50%|█████     | 971/1940 [4:33:45<4:23:52, 16.34s/it] 50%|█████     | 972/1940 [4:34:01<4:23:04, 16.31s/it] 50%|█████     | 973/1940 [4:34:18<4:24:35, 16.42s/it] 50%|█████     | 974/1940 [4:34:34<4:23:26, 16.36s/it] 50%|█████     | 975/1940 [4:34:50<4:22:34, 16.33s/it] 50%|█████     | 976/1940 [4:35:07<4:21:54, 16.30s/it] 50%|█████     | 977/1940 [4:35:23<4:22:36, 16.36s/it] 50%|█████     | 978/1940 [4:35:39<4:21:36, 16.32s/it] 50%|█████     | 979/1940 [4:35:56<4:20:46, 16.28s/it] 51%|█████     | 980/1940 [4:36:12<4:22:29, 16.41s/it]                                                      {'loss': 2.5174, 'learning_rate': 2.4595173279937464e-05, 'epoch': 0.51}
 51%|█████     | 980/1940 [4:36:12<4:22:29, 16.41s/it] 51%|█████     | 981/1940 [4:36:29<4:21:30, 16.36s/it] 51%|█████     | 982/1940 [4:36:45<4:20:54, 16.34s/it] 51%|█████     | 983/1940 [4:37:01<4:21:22, 16.39s/it] 51%|█████     | 984/1940 [4:37:18<4:21:17, 16.40s/it] 51%|█████     | 985/1940 [4:37:34<4:20:00, 16.34s/it] 51%|█████     | 986/1940 [4:37:50<4:19:03, 16.29s/it] 51%|█████     | 987/1940 [4:38:07<4:19:24, 16.33s/it] 51%|█████     | 988/1940 [4:38:23<4:18:29, 16.29s/it] 51%|█████     | 989/1940 [4:38:39<4:17:43, 16.26s/it] 51%|█████     | 990/1940 [4:38:56<4:19:23, 16.38s/it]                                                      {'loss': 2.4952, 'learning_rate': 2.419045271866611e-05, 'epoch': 0.51}
 51%|█████     | 990/1940 [4:38:56<4:19:23, 16.38s/it] 51%|█████     | 991/1940 [4:39:12<4:18:17, 16.33s/it] 51%|█████     | 992/1940 [4:39:28<4:17:33, 16.30s/it] 51%|█████     | 993/1940 [4:39:45<4:18:06, 16.35s/it] 51%|█████     | 994/1940 [4:40:01<4:18:19, 16.38s/it] 51%|█████▏    | 995/1940 [4:40:17<4:17:10, 16.33s/it] 51%|█████▏    | 996/1940 [4:40:33<4:16:24, 16.30s/it] 51%|█████▏    | 997/1940 [4:40:50<4:16:45, 16.34s/it] 51%|█████▏    | 998/1940 [4:41:06<4:15:51, 16.30s/it] 51%|█████▏    | 999/1940 [4:41:22<4:15:03, 16.26s/it] 52%|█████▏    | 1000/1940 [4:41:39<4:16:32, 16.38s/it]                                                       {'loss': 2.517, 'learning_rate': 2.3785944447138802e-05, 'epoch': 0.52}
 52%|█████▏    | 1000/1940 [4:41:39<4:16:32, 16.38s/it][INFO|trainer.py:2979] 2024-05-27 01:47:15,001 >> Saving model checkpoint to /home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/tmp-checkpoint-1000
[INFO|tokenization_utils_base.py:2435] 2024-05-27 01:47:21,369 >> tokenizer config file saved in /home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-05-27 01:47:21,369 >> Special tokens file saved in /home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/tmp-checkpoint-1000/special_tokens_map.json
[INFO|tokenization_utils_base.py:2495] 2024-05-27 01:47:21,369 >> added tokens file saved in /home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/tmp-checkpoint-1000/added_tokens.json
 52%|█████▏    | 1001/1940 [4:42:05<5:01:12, 19.25s/it] 52%|█████▏    | 1002/1940 [4:42:21<4:46:23, 18.32s/it] 52%|█████▏    | 1003/1940 [4:42:37<4:37:00, 17.74s/it] 52%|█████▏    | 1004/1940 [4:42:54<4:32:28, 17.47s/it] 52%|█████▏    | 1005/1940 [4:43:11<4:28:12, 17.21s/it] 52%|█████▏    | 1006/1940 [4:43:27<4:25:12, 17.04s/it] 52%|█████▏    | 1007/1940 [4:43:45<4:24:56, 17.04s/it] 52%|█████▏    | 1008/1940 [4:44:01<4:22:15, 16.88s/it] 52%|█████▏    | 1009/1940 [4:44:18<4:19:58, 16.75s/it] 52%|█████▏    | 1010/1940 [4:44:34<4:19:26, 16.74s/it]                                                       {'loss': 2.5007, 'learning_rate': 2.338175454063911e-05, 'epoch': 0.52}
 52%|█████▏    | 1010/1940 [4:44:34<4:19:26, 16.74s/it] 52%|█████▏    | 1011/1940 [4:44:51<4:17:24, 16.63s/it] 52%|█████▏    | 1012/1940 [4:45:07<4:15:49, 16.54s/it] 52%|█████▏    | 1013/1940 [4:45:23<4:15:26, 16.53s/it] 52%|█████▏    | 1014/1940 [4:45:40<4:14:48, 16.51s/it] 52%|█████▏    | 1015/1940 [4:45:56<4:13:08, 16.42s/it] 52%|█████▏    | 1016/1940 [4:46:12<4:11:49, 16.35s/it] 52%|█████▏    | 1017/1940 [4:46:29<4:13:07, 16.45s/it] 52%|█████▏    | 1018/1940 [4:46:45<4:11:57, 16.40s/it] 53%|█████▎    | 1019/1940 [4:47:02<4:11:11, 16.36s/it] 53%|█████▎    | 1020/1940 [4:47:18<4:11:33, 16.41s/it]                                                       {'loss': 2.5019, 'learning_rate': 2.29779889909649e-05, 'epoch': 0.53}
 53%|█████▎    | 1020/1940 [4:47:18<4:11:33, 16.41s/it] 53%|█████▎    | 1021/1940 [4:47:34<4:10:25, 16.35s/it] 53%|█████▎    | 1022/1940 [4:47:50<4:09:19, 16.30s/it] 53%|█████▎    | 1023/1940 [4:48:07<4:09:31, 16.33s/it] 53%|█████▎    | 1024/1940 [4:48:23<4:09:34, 16.35s/it] 53%|█████▎    | 1025/1940 [4:48:39<4:08:27, 16.29s/it] 53%|█████▎    | 1026/1940 [4:48:56<4:07:42, 16.26s/it] 53%|█████▎    | 1027/1940 [4:49:12<4:09:14, 16.38s/it] 53%|█████▎    | 1028/1940 [4:49:28<4:08:07, 16.32s/it] 53%|█████▎    | 1029/1940 [4:49:45<4:07:17, 16.29s/it] 53%|█████▎    | 1030/1940 [4:50:01<4:07:58, 16.35s/it]                                                       {'loss': 2.5321, 'learning_rate': 2.25747536786338e-05, 'epoch': 0.53}
 53%|█████▎    | 1030/1940 [4:50:01<4:07:58, 16.35s/it] 53%|█████▎    | 1031/1940 [4:50:17<4:07:13, 16.32s/it] 53%|█████▎    | 1032/1940 [4:50:34<4:06:40, 16.30s/it] 53%|█████▎    | 1033/1940 [4:50:50<4:07:06, 16.35s/it] 53%|█████▎    | 1034/1940 [4:51:07<4:07:16, 16.38s/it] 53%|█████▎    | 1035/1940 [4:51:23<4:06:12, 16.32s/it] 53%|█████▎    | 1036/1940 [4:51:39<4:05:28, 16.29s/it] 53%|█████▎    | 1037/1940 [4:51:55<4:06:10, 16.36s/it] 54%|█████▎    | 1038/1940 [4:52:12<4:05:17, 16.32s/it] 54%|█████▎    | 1039/1940 [4:52:28<4:04:29, 16.28s/it] 54%|█████▎    | 1040/1940 [4:52:45<4:06:08, 16.41s/it]                                                       {'loss': 2.5212, 'learning_rate': 2.2172154345117894e-05, 'epoch': 0.54}
 54%|█████▎    | 1040/1940 [4:52:45<4:06:08, 16.41s/it] 54%|█████▎    | 1041/1940 [4:53:01<4:05:02, 16.35s/it] 54%|█████▎    | 1042/1940 [4:53:17<4:04:23, 16.33s/it] 54%|█████▍    | 1043/1940 [4:53:34<4:04:57, 16.38s/it] 54%|█████▍    | 1044/1940 [4:53:50<4:05:05, 16.41s/it] 54%|█████▍    | 1045/1940 [4:54:06<4:03:56, 16.35s/it] 54%|█████▍    | 1046/1940 [4:54:23<4:03:10, 16.32s/it] 54%|█████▍    | 1047/1940 [4:54:39<4:03:36, 16.37s/it] 54%|█████▍    | 1048/1940 [4:54:55<4:02:41, 16.32s/it] 54%|█████▍    | 1049/1940 [4:55:11<4:02:00, 16.30s/it] 54%|█████▍    | 1050/1940 [4:55:28<4:03:32, 16.42s/it]                                                       {'loss': 2.525, 'learning_rate': 2.177029656511485e-05, 'epoch': 0.54}
 54%|█████▍    | 1050/1940 [4:55:28<4:03:32, 16.42s/it] 54%|█████▍    | 1051/1940 [4:55:44<4:02:17, 16.35s/it] 54%|█████▍    | 1052/1940 [4:56:01<4:01:26, 16.31s/it] 54%|█████▍    | 1053/1940 [4:56:17<4:00:49, 16.29s/it] 54%|█████▍    | 1054/1940 [4:56:34<4:02:31, 16.42s/it] 54%|█████▍    | 1055/1940 [4:56:50<4:01:25, 16.37s/it] 54%|█████▍    | 1056/1940 [4:57:06<4:00:35, 16.33s/it] 54%|█████▍    | 1057/1940 [4:57:22<4:00:50, 16.37s/it] 55%|█████▍    | 1058/1940 [4:57:39<3:59:54, 16.32s/it] 55%|█████▍    | 1059/1940 [4:57:55<3:59:10, 16.29s/it] 55%|█████▍    | 1060/1940 [4:58:12<4:00:41, 16.41s/it]                                                       {'loss': 2.5047, 'learning_rate': 2.136928571886275e-05, 'epoch': 0.55}
 55%|█████▍    | 1060/1940 [4:58:12<4:00:41, 16.41s/it] 55%|█████▍    | 1061/1940 [4:58:28<3:59:27, 16.35s/it] 55%|█████▍    | 1062/1940 [4:58:44<3:58:30, 16.30s/it] 55%|█████▍    | 1063/1940 [4:59:00<3:57:45, 16.27s/it] 55%|█████▍    | 1064/1940 [4:59:17<3:58:33, 16.34s/it] 55%|█████▍    | 1065/1940 [4:59:33<3:57:43, 16.30s/it] 55%|█████▍    | 1066/1940 [4:59:49<3:57:17, 16.29s/it] 55%|█████▌    | 1067/1940 [5:00:06<3:59:01, 16.43s/it] 55%|█████▌    | 1068/1940 [5:00:22<3:58:04, 16.38s/it] 55%|█████▌    | 1069/1940 [5:00:38<3:57:13, 16.34s/it] 55%|█████▌    | 1070/1940 [5:00:55<3:57:26, 16.38s/it]                                                       {'loss': 2.4954, 'learning_rate': 2.0969226964506006e-05, 'epoch': 0.55}
 55%|█████▌    | 1070/1940 [5:00:55<3:57:26, 16.38s/it] 55%|█████▌    | 1071/1940 [5:01:11<3:57:28, 16.40s/it] 55%|█████▌    | 1072/1940 [5:01:28<3:56:17, 16.33s/it] 55%|█████▌    | 1073/1940 [5:01:44<3:55:22, 16.29s/it] 55%|█████▌    | 1074/1940 [5:02:00<3:55:40, 16.33s/it] 55%|█████▌    | 1075/1940 [5:02:16<3:54:49, 16.29s/it] 55%|█████▌    | 1076/1940 [5:02:33<3:54:06, 16.26s/it] 56%|█████▌    | 1077/1940 [5:02:49<3:55:41, 16.39s/it] 56%|█████▌    | 1078/1940 [5:03:05<3:54:50, 16.35s/it] 56%|█████▌    | 1079/1940 [5:03:22<3:53:58, 16.31s/it] 56%|█████▌    | 1080/1940 [5:03:38<3:54:21, 16.35s/it]                                                       {'loss': 2.4827, 'learning_rate': 2.0570225210519434e-05, 'epoch': 0.56}
 56%|█████▌    | 1080/1940 [5:03:38<3:54:21, 16.35s/it] 56%|█████▌    | 1081/1940 [5:03:55<3:54:23, 16.37s/it] 56%|█████▌    | 1082/1940 [5:04:11<3:53:21, 16.32s/it] 56%|█████▌    | 1083/1940 [5:04:27<3:52:37, 16.29s/it] 56%|█████▌    | 1084/1940 [5:04:43<3:53:02, 16.33s/it] 56%|█████▌    | 1085/1940 [5:05:00<3:52:18, 16.30s/it] 56%|█████▌    | 1086/1940 [5:05:16<3:51:39, 16.28s/it] 56%|█████▌    | 1087/1940 [5:05:33<3:53:04, 16.39s/it] 56%|█████▌    | 1088/1940 [5:05:49<3:52:00, 16.34s/it] 56%|█████▌    | 1089/1940 [5:06:05<3:51:19, 16.31s/it] 56%|█████▌    | 1090/1940 [5:06:21<3:51:51, 16.37s/it]                                                       {'loss': 2.517, 'learning_rate': 2.0172385088197803e-05, 'epoch': 0.56}
 56%|█████▌    | 1090/1940 [5:06:21<3:51:51, 16.37s/it] 56%|█████▌    | 1091/1940 [5:06:38<3:51:11, 16.34s/it] 56%|█████▋    | 1092/1940 [5:06:54<3:50:16, 16.29s/it] 56%|█████▋    | 1093/1940 [5:07:10<3:49:30, 16.26s/it] 56%|█████▋    | 1094/1940 [5:07:27<3:50:57, 16.38s/it] 56%|█████▋    | 1095/1940 [5:07:43<3:49:49, 16.32s/it] 56%|█████▋    | 1096/1940 [5:07:59<3:48:55, 16.27s/it] 57%|█████▋    | 1097/1940 [5:08:16<3:50:13, 16.39s/it] 57%|█████▋    | 1098/1940 [5:08:32<3:49:03, 16.32s/it] 57%|█████▋    | 1099/1940 [5:08:48<3:48:11, 16.28s/it] 57%|█████▋    | 1100/1940 [5:09:04<3:47:36, 16.26s/it]                                                       {'loss': 2.494, 'learning_rate': 1.9775810924218125e-05, 'epoch': 0.57}
 57%|█████▋    | 1100/1940 [5:09:04<3:47:36, 16.26s/it] 57%|█████▋    | 1101/1940 [5:09:21<3:48:20, 16.33s/it] 57%|█████▋    | 1102/1940 [5:09:37<3:47:52, 16.32s/it] 57%|█████▋    | 1103/1940 [5:09:53<3:47:20, 16.30s/it] 57%|█████▋    | 1104/1940 [5:10:10<3:48:45, 16.42s/it] 57%|█████▋    | 1105/1940 [5:10:26<3:47:30, 16.35s/it] 57%|█████▋    | 1106/1940 [5:10:42<3:46:34, 16.30s/it] 57%|█████▋    | 1107/1940 [5:10:59<3:47:45, 16.40s/it] 57%|█████▋    | 1108/1940 [5:11:15<3:46:28, 16.33s/it] 57%|█████▋    | 1109/1940 [5:11:31<3:45:32, 16.29s/it] 57%|█████▋    | 1110/1940 [5:11:48<3:44:50, 16.25s/it]                                                       {'loss': 2.503, 'learning_rate': 1.9380606713281775e-05, 'epoch': 0.57}
 57%|█████▋    | 1110/1940 [5:11:48<3:44:50, 16.25s/it] 57%|█████▋    | 1111/1940 [5:12:04<3:45:17, 16.31s/it] 57%|█████▋    | 1112/1940 [5:12:20<3:44:45, 16.29s/it] 57%|█████▋    | 1113/1940 [5:12:37<3:44:21, 16.28s/it] 57%|█████▋    | 1114/1940 [5:12:53<3:45:55, 16.41s/it] 57%|█████▋    | 1115/1940 [5:13:09<3:44:51, 16.35s/it] 58%|█████▊    | 1116/1940 [5:13:26<3:43:59, 16.31s/it] 58%|█████▊    | 1117/1940 [5:13:42<3:44:32, 16.37s/it] 58%|█████▊    | 1118/1940 [5:13:58<3:43:32, 16.32s/it] 58%|█████▊    | 1119/1940 [5:14:15<3:42:47, 16.28s/it] 58%|█████▊    | 1120/1940 [5:14:31<3:43:12, 16.33s/it]                                                       {'loss': 2.4834, 'learning_rate': 1.8986876090843667e-05, 'epoch': 0.58}
 58%|█████▊    | 1120/1940 [5:14:31<3:43:12, 16.33s/it] 58%|█████▊    | 1121/1940 [5:14:47<3:43:17, 16.36s/it] 58%|█████▊    | 1122/1940 [5:15:04<3:42:23, 16.31s/it] 58%|█████▊    | 1123/1940 [5:15:20<3:41:46, 16.29s/it] 58%|█████▊    | 1124/1940 [5:15:37<3:43:15, 16.42s/it] 58%|█████▊    | 1125/1940 [5:15:53<3:42:18, 16.37s/it] 58%|█████▊    | 1126/1940 [5:16:09<3:41:36, 16.34s/it] 58%|█████▊    | 1127/1940 [5:16:26<3:41:52, 16.37s/it] 58%|█████▊    | 1128/1940 [5:16:42<3:40:54, 16.32s/it] 58%|█████▊    | 1129/1940 [5:16:58<3:40:07, 16.29s/it] 58%|█████▊    | 1130/1940 [5:17:14<3:40:24, 16.33s/it]                                                       {'loss': 2.5119, 'learning_rate': 1.859472230593569e-05, 'epoch': 0.58}
 58%|█████▊    | 1130/1940 [5:17:14<3:40:24, 16.33s/it] 58%|█████▊    | 1131/1940 [5:17:31<3:40:30, 16.35s/it] 58%|█████▊    | 1132/1940 [5:17:47<3:39:27, 16.30s/it] 58%|█████▊    | 1133/1940 [5:18:03<3:38:45, 16.26s/it] 58%|█████▊    | 1134/1940 [5:18:20<3:40:05, 16.38s/it] 59%|█████▊    | 1135/1940 [5:18:36<3:39:09, 16.33s/it] 59%|█████▊    | 1136/1940 [5:18:52<3:38:38, 16.32s/it] 59%|█████▊    | 1137/1940 [5:19:09<3:39:03, 16.37s/it] 59%|█████▊    | 1138/1940 [5:19:25<3:38:22, 16.34s/it] 59%|█████▊    | 1139/1940 [5:19:41<3:37:44, 16.31s/it] 59%|█████▉    | 1140/1940 [5:19:58<3:38:04, 16.36s/it]                                                       {'loss': 2.5045, 'learning_rate': 1.820424819409143e-05, 'epoch': 0.59}
 59%|█████▉    | 1140/1940 [5:19:58<3:38:04, 16.36s/it] 59%|█████▉    | 1141/1940 [5:20:14<3:37:59, 16.37s/it] 59%|█████▉    | 1142/1940 [5:20:30<3:37:00, 16.32s/it] 59%|█████▉    | 1143/1940 [5:20:47<3:36:15, 16.28s/it] 59%|█████▉    | 1144/1940 [5:21:03<3:36:51, 16.35s/it] 59%|█████▉    | 1145/1940 [5:21:19<3:36:03, 16.31s/it] 59%|█████▉    | 1146/1940 [5:21:36<3:35:26, 16.28s/it] 59%|█████▉    | 1147/1940 [5:21:52<3:36:50, 16.41s/it] 59%|█████▉    | 1148/1940 [5:22:08<3:35:54, 16.36s/it] 59%|█████▉    | 1149/1940 [5:22:25<3:35:23, 16.34s/it] 59%|█████▉    | 1150/1940 [5:22:41<3:35:51, 16.39s/it]                                                       {'loss': 2.4998, 'learning_rate': 1.7815556150379298e-05, 'epoch': 0.59}
 59%|█████▉    | 1150/1940 [5:22:41<3:35:51, 16.39s/it] 59%|█████▉    | 1151/1940 [5:22:58<3:35:56, 16.42s/it] 59%|█████▉    | 1152/1940 [5:23:14<3:34:53, 16.36s/it] 59%|█████▉    | 1153/1940 [5:23:30<3:34:04, 16.32s/it] 59%|█████▉    | 1154/1940 [5:23:47<3:34:22, 16.36s/it] 60%|█████▉    | 1155/1940 [5:24:03<3:33:31, 16.32s/it] 60%|█████▉    | 1156/1940 [5:24:19<3:32:46, 16.28s/it] 60%|█████▉    | 1157/1940 [5:24:36<3:33:58, 16.40s/it] 60%|█████▉    | 1158/1940 [5:24:52<3:32:54, 16.34s/it] 60%|█████▉    | 1159/1940 [5:25:08<3:32:07, 16.30s/it] 60%|█████▉    | 1160/1940 [5:25:24<3:31:37, 16.28s/it]                                                       {'loss': 2.51, 'learning_rate': 1.7428748102551237e-05, 'epoch': 0.6}
 60%|█████▉    | 1160/1940 [5:25:24<3:31:37, 16.28s/it] 60%|█████▉    | 1161/1940 [5:25:41<3:33:04, 16.41s/it] 60%|█████▉    | 1162/1940 [5:25:57<3:32:10, 16.36s/it] 60%|█████▉    | 1163/1940 [5:26:14<3:31:24, 16.32s/it] 60%|██████    | 1164/1940 [5:26:30<3:31:42, 16.37s/it] 60%|██████    | 1165/1940 [5:26:46<3:30:50, 16.32s/it] 60%|██████    | 1166/1940 [5:27:03<3:30:13, 16.30s/it] 60%|██████    | 1167/1940 [5:27:19<3:31:28, 16.41s/it] 60%|██████    | 1168/1940 [5:27:35<3:30:17, 16.34s/it] 60%|██████    | 1169/1940 [5:27:52<3:29:23, 16.30s/it] 60%|██████    | 1170/1940 [5:28:08<3:28:41, 16.26s/it]                                                       {'loss': 2.512, 'learning_rate': 1.704392548431391e-05, 'epoch': 0.6}
 60%|██████    | 1170/1940 [5:28:08<3:28:41, 16.26s/it] 60%|██████    | 1171/1940 [5:28:24<3:29:18, 16.33s/it] 60%|██████    | 1172/1940 [5:28:40<3:28:37, 16.30s/it] 60%|██████    | 1173/1940 [5:28:57<3:28:10, 16.29s/it] 61%|██████    | 1174/1940 [5:29:13<3:29:41, 16.42s/it] 61%|██████    | 1175/1940 [5:29:30<3:28:55, 16.39s/it] 61%|██████    | 1176/1940 [5:29:46<3:28:08, 16.35s/it] 61%|██████    | 1177/1940 [5:30:02<3:28:19, 16.38s/it] 61%|██████    | 1178/1940 [5:30:19<3:28:21, 16.41s/it] 61%|██████    | 1179/1940 [5:30:35<3:27:22, 16.35s/it] 61%|██████    | 1180/1940 [5:30:51<3:26:41, 16.32s/it]                                                       {'loss': 2.5088, 'learning_rate': 1.666118920872949e-05, 'epoch': 0.61}
 61%|██████    | 1180/1940 [5:30:51<3:26:41, 16.32s/it] 61%|██████    | 1181/1940 [5:31:08<3:26:52, 16.35s/it] 61%|██████    | 1182/1940 [5:31:24<3:26:05, 16.31s/it] 61%|██████    | 1183/1940 [5:31:40<3:25:25, 16.28s/it] 61%|██████    | 1184/1940 [5:31:57<3:26:41, 16.40s/it] 61%|██████    | 1185/1940 [5:32:13<3:25:46, 16.35s/it] 61%|██████    | 1186/1940 [5:32:29<3:25:10, 16.33s/it] 61%|██████    | 1187/1940 [5:32:46<3:25:30, 16.37s/it] 61%|██████    | 1188/1940 [5:33:02<3:25:32, 16.40s/it] 61%|██████▏   | 1189/1940 [5:33:19<3:24:28, 16.34s/it] 61%|██████▏   | 1190/1940 [5:33:35<3:23:50, 16.31s/it]                                                       {'loss': 2.5081, 'learning_rate': 1.6280639641752942e-05, 'epoch': 0.61}
 61%|██████▏   | 1190/1940 [5:33:35<3:23:50, 16.31s/it] 61%|██████▏   | 1191/1940 [5:33:51<3:24:06, 16.35s/it] 61%|██████▏   | 1192/1940 [5:34:07<3:23:16, 16.31s/it] 61%|██████▏   | 1193/1940 [5:34:24<3:22:41, 16.28s/it] 62%|██████▏   | 1194/1940 [5:34:40<3:23:55, 16.40s/it] 62%|██████▏   | 1195/1940 [5:34:57<3:22:54, 16.34s/it] 62%|██████▏   | 1196/1940 [5:35:13<3:22:17, 16.31s/it] 62%|██████▏   | 1197/1940 [5:35:29<3:22:35, 16.36s/it] 62%|██████▏   | 1198/1940 [5:35:46<3:22:07, 16.34s/it] 62%|██████▏   | 1199/1940 [5:36:02<3:21:21, 16.30s/it] 62%|██████▏   | 1200/1940 [5:36:18<3:20:42, 16.27s/it]                                                       {'loss': 2.491, 'learning_rate': 1.5902376575912815e-05, 'epoch': 0.62}
 62%|██████▏   | 1200/1940 [5:36:18<3:20:42, 16.27s/it] 62%|██████▏   | 1201/1940 [5:36:35<3:21:51, 16.39s/it] 62%|██████▏   | 1202/1940 [5:36:51<3:20:46, 16.32s/it] 62%|██████▏   | 1203/1940 [5:37:07<3:20:04, 16.29s/it] 62%|██████▏   | 1204/1940 [5:37:24<3:21:11, 16.40s/it] 62%|██████▏   | 1205/1940 [5:37:40<3:20:07, 16.34s/it] 62%|██████▏   | 1206/1940 [5:37:56<3:19:27, 16.30s/it] 62%|██████▏   | 1207/1940 [5:38:12<3:18:56, 16.28s/it] 62%|██████▏   | 1208/1940 [5:38:29<3:19:23, 16.34s/it] 62%|██████▏   | 1209/1940 [5:38:45<3:18:51, 16.32s/it] 62%|██████▏   | 1210/1940 [5:39:01<3:18:24, 16.31s/it]                                                       {'loss': 2.4999, 'learning_rate': 1.552649920414233e-05, 'epoch': 0.62}
 62%|██████▏   | 1210/1940 [5:39:01<3:18:24, 16.31s/it] 62%|██████▏   | 1211/1940 [5:39:18<3:19:26, 16.42s/it] 62%|██████▏   | 1212/1940 [5:39:34<3:18:24, 16.35s/it] 63%|██████▎   | 1213/1940 [5:39:50<3:17:36, 16.31s/it] 63%|██████▎   | 1214/1940 [5:40:07<3:18:37, 16.41s/it] 63%|██████▎   | 1215/1940 [5:40:23<3:17:24, 16.34s/it] 63%|██████▎   | 1216/1940 [5:40:39<3:16:36, 16.29s/it] 63%|██████▎   | 1217/1940 [5:40:56<3:15:59, 16.26s/it] 63%|██████▎   | 1218/1940 [5:41:12<3:16:24, 16.32s/it] 63%|██████▎   | 1219/1940 [5:41:28<3:15:49, 16.30s/it] 63%|██████▎   | 1220/1940 [5:41:45<3:15:28, 16.29s/it]                                                       {'loss': 2.4929, 'learning_rate': 1.5153106093767827e-05, 'epoch': 0.63}
 63%|██████▎   | 1220/1940 [5:41:45<3:15:28, 16.29s/it] 63%|██████▎   | 1221/1940 [5:42:01<3:16:46, 16.42s/it] 63%|██████▎   | 1222/1940 [5:42:18<3:15:47, 16.36s/it] 63%|██████▎   | 1223/1940 [5:42:34<3:14:56, 16.31s/it] 63%|██████▎   | 1224/1940 [5:42:50<3:15:19, 16.37s/it] 63%|██████▎   | 1225/1940 [5:43:06<3:14:21, 16.31s/it] 63%|██████▎   | 1226/1940 [5:43:23<3:13:42, 16.28s/it] 63%|██████▎   | 1227/1940 [5:43:39<3:13:55, 16.32s/it] 63%|██████▎   | 1228/1940 [5:43:55<3:13:50, 16.33s/it] 63%|██████▎   | 1229/1940 [5:44:12<3:13:02, 16.29s/it] 63%|██████▎   | 1230/1940 [5:44:28<3:12:32, 16.27s/it]                                                       {'loss': 2.5161, 'learning_rate': 1.4782295160661103e-05, 'epoch': 0.63}
 63%|██████▎   | 1230/1940 [5:44:28<3:12:32, 16.27s/it] 63%|██████▎   | 1231/1940 [5:44:45<3:13:49, 16.40s/it] 64%|██████▎   | 1232/1940 [5:45:01<3:13:00, 16.36s/it] 64%|██████▎   | 1233/1940 [5:45:17<3:12:26, 16.33s/it] 64%|██████▎   | 1234/1940 [5:45:34<3:12:45, 16.38s/it] 64%|██████▎   | 1235/1940 [5:45:50<3:11:55, 16.33s/it] 64%|██████▎   | 1236/1940 [5:46:06<3:11:15, 16.30s/it] 64%|██████▍   | 1237/1940 [5:46:22<3:11:26, 16.34s/it] 64%|██████▍   | 1238/1940 [5:46:39<3:11:28, 16.37s/it] 64%|██████▍   | 1239/1940 [5:46:55<3:10:34, 16.31s/it] 64%|██████▍   | 1240/1940 [5:47:11<3:09:50, 16.27s/it]                                                       {'loss': 2.5057, 'learning_rate': 1.4414163643562755e-05, 'epoch': 0.64}
 64%|██████▍   | 1240/1940 [5:47:11<3:09:50, 16.27s/it] 64%|██████▍   | 1241/1940 [5:47:28<3:10:55, 16.39s/it] 64%|██████▍   | 1242/1940 [5:47:44<3:09:57, 16.33s/it] 64%|██████▍   | 1243/1940 [5:48:00<3:09:15, 16.29s/it] 64%|██████▍   | 1244/1940 [5:48:17<3:09:44, 16.36s/it] 64%|██████▍   | 1245/1940 [5:48:33<3:09:10, 16.33s/it] 64%|██████▍   | 1246/1940 [5:48:49<3:08:42, 16.32s/it] 64%|██████▍   | 1247/1940 [5:49:06<3:08:59, 16.36s/it] 64%|██████▍   | 1248/1940 [5:49:22<3:08:56, 16.38s/it] 64%|██████▍   | 1249/1940 [5:49:39<3:08:02, 16.33s/it] 64%|██████▍   | 1250/1940 [5:49:55<3:07:17, 16.29s/it]                                                       {'loss': 2.4938, 'learning_rate': 1.4048808078582942e-05, 'epoch': 0.64}
 64%|██████▍   | 1250/1940 [5:49:55<3:07:17, 16.29s/it] 64%|██████▍   | 1251/1940 [5:50:11<3:07:42, 16.35s/it] 65%|██████▍   | 1252/1940 [5:50:27<3:06:55, 16.30s/it] 65%|██████▍   | 1253/1940 [5:50:44<3:06:15, 16.27s/it] 65%|██████▍   | 1254/1940 [5:51:00<3:07:21, 16.39s/it] 65%|██████▍   | 1255/1940 [5:51:16<3:06:19, 16.32s/it] 65%|██████▍   | 1256/1940 [5:51:33<3:05:44, 16.29s/it] 65%|██████▍   | 1257/1940 [5:51:49<3:06:08, 16.35s/it] 65%|██████▍   | 1258/1940 [5:52:06<3:06:19, 16.39s/it] 65%|██████▍   | 1259/1940 [5:52:22<3:05:25, 16.34s/it] 65%|██████▍   | 1260/1940 [5:52:38<3:04:39, 16.29s/it]                                                       {'loss': 2.4835, 'learning_rate': 1.368632427388653e-05, 'epoch': 0.65}
 65%|██████▍   | 1260/1940 [5:52:38<3:04:39, 16.29s/it] 65%|██████▌   | 1261/1940 [5:52:54<3:04:52, 16.34s/it] 65%|██████▌   | 1262/1940 [5:53:11<3:04:07, 16.29s/it] 65%|██████▌   | 1263/1940 [5:53:27<3:03:27, 16.26s/it] 65%|██████▌   | 1264/1940 [5:53:43<3:04:35, 16.38s/it] 65%|██████▌   | 1265/1940 [5:54:00<3:03:40, 16.33s/it] 65%|██████▌   | 1266/1940 [5:54:16<3:03:03, 16.30s/it] 65%|██████▌   | 1267/1940 [5:54:32<3:02:34, 16.28s/it] 65%|██████▌   | 1268/1940 [5:54:49<3:03:46, 16.41s/it] 65%|██████▌   | 1269/1940 [5:55:05<3:02:57, 16.36s/it] 65%|██████▌   | 1270/1940 [5:55:21<3:02:18, 16.33s/it]                                                       {'loss': 2.4954, 'learning_rate': 1.3326807284568984e-05, 'epoch': 0.65}
 65%|██████▌   | 1270/1940 [5:55:21<3:02:18, 16.33s/it] 66%|██████▌   | 1271/1940 [5:55:38<3:02:22, 16.36s/it] 66%|██████▌   | 1272/1940 [5:55:54<3:01:32, 16.31s/it] 66%|██████▌   | 1273/1940 [5:56:10<3:00:51, 16.27s/it] 66%|██████▌   | 1274/1940 [5:56:27<3:01:49, 16.38s/it] 66%|██████▌   | 1275/1940 [5:56:43<3:00:44, 16.31s/it] 66%|██████▌   | 1276/1940 [5:56:59<3:00:00, 16.27s/it] 66%|██████▌   | 1277/1940 [5:57:15<2:59:25, 16.24s/it] 66%|██████▌   | 1278/1940 [5:57:32<3:00:02, 16.32s/it] 66%|██████▌   | 1279/1940 [5:57:48<2:59:28, 16.29s/it] 66%|██████▌   | 1280/1940 [5:58:04<2:59:09, 16.29s/it]                                                       {'loss': 2.4841, 'learning_rate': 1.2970351387729873e-05, 'epoch': 0.66}
 66%|██████▌   | 1280/1940 [5:58:04<2:59:09, 16.29s/it] 66%|██████▌   | 1281/1940 [5:58:21<3:00:22, 16.42s/it] 66%|██████▌   | 1282/1940 [5:58:37<2:59:29, 16.37s/it] 66%|██████▌   | 1283/1940 [5:58:53<2:58:42, 16.32s/it] 66%|██████▌   | 1284/1940 [5:59:10<2:58:53, 16.36s/it] 66%|██████▌   | 1285/1940 [5:59:26<2:58:50, 16.38s/it] 66%|██████▋   | 1286/1940 [5:59:43<2:57:59, 16.33s/it] 66%|██████▋   | 1287/1940 [5:59:59<2:57:18, 16.29s/it] 66%|██████▋   | 1288/1940 [6:00:15<2:57:25, 16.33s/it] 66%|██████▋   | 1289/1940 [6:00:31<2:56:42, 16.29s/it] 66%|██████▋   | 1290/1940 [6:00:48<2:56:12, 16.27s/it]                                                       {'loss': 2.5059, 'learning_rate': 1.2617050057750322e-05, 'epoch': 0.66}
 66%|██████▋   | 1290/1940 [6:00:48<2:56:12, 16.27s/it] 67%|██████▋   | 1291/1940 [6:01:04<2:57:24, 16.40s/it] 67%|██████▋   | 1292/1940 [6:01:21<2:56:35, 16.35s/it] 67%|██████▋   | 1293/1940 [6:01:37<2:55:55, 16.31s/it] 67%|██████▋   | 1294/1940 [6:01:53<2:56:02, 16.35s/it] 67%|██████▋   | 1295/1940 [6:02:10<2:55:56, 16.37s/it] 67%|██████▋   | 1296/1940 [6:02:26<2:55:04, 16.31s/it] 67%|██████▋   | 1297/1940 [6:02:42<2:54:25, 16.28s/it] 67%|██████▋   | 1298/1940 [6:02:58<2:54:38, 16.32s/it] 67%|██████▋   | 1299/1940 [6:03:15<2:54:00, 16.29s/it] 67%|██████▋   | 1300/1940 [6:03:31<2:53:27, 16.26s/it]                                                       {'loss': 2.4926, 'learning_rate': 1.2266995941780934e-05, 'epoch': 0.67}
 67%|██████▋   | 1300/1940 [6:03:31<2:53:27, 16.26s/it] 67%|██████▋   | 1301/1940 [6:03:47<2:54:32, 16.39s/it] 67%|██████▋   | 1302/1940 [6:04:04<2:53:44, 16.34s/it] 67%|██████▋   | 1303/1940 [6:04:20<2:53:12, 16.31s/it] 67%|██████▋   | 1304/1940 [6:04:36<2:53:30, 16.37s/it] 67%|██████▋   | 1305/1940 [6:04:53<2:52:53, 16.34s/it] 67%|██████▋   | 1306/1940 [6:05:09<2:52:11, 16.30s/it] 67%|██████▋   | 1307/1940 [6:05:25<2:51:32, 16.26s/it] 67%|██████▋   | 1308/1940 [6:05:42<2:52:37, 16.39s/it] 67%|██████▋   | 1309/1940 [6:05:58<2:51:43, 16.33s/it] 68%|██████▊   | 1310/1940 [6:06:14<2:51:00, 16.29s/it]                                                       {'loss': 2.5034, 'learning_rate': 1.1920280835446748e-05, 'epoch': 0.68}
 68%|██████▊   | 1310/1940 [6:06:14<2:51:00, 16.29s/it] 68%|██████▊   | 1311/1940 [6:06:31<2:51:58, 16.40s/it] 68%|██████▊   | 1312/1940 [6:06:47<2:51:04, 16.35s/it] 68%|██████▊   | 1313/1940 [6:07:03<2:50:30, 16.32s/it] 68%|██████▊   | 1314/1940 [6:07:20<2:50:06, 16.30s/it] 68%|██████▊   | 1315/1940 [6:07:36<2:50:29, 16.37s/it] 68%|██████▊   | 1316/1940 [6:07:52<2:49:51, 16.33s/it] 68%|██████▊   | 1317/1940 [6:08:09<2:49:12, 16.30s/it] 68%|██████▊   | 1318/1940 [6:08:25<2:50:05, 16.41s/it] 68%|██████▊   | 1319/1940 [6:08:41<2:49:09, 16.34s/it] 68%|██████▊   | 1320/1940 [6:08:58<2:48:24, 16.30s/it]                                                       {'loss': 2.4853, 'learning_rate': 1.1576995658775405e-05, 'epoch': 0.68}
 68%|██████▊   | 1320/1940 [6:08:58<2:48:24, 16.30s/it] 68%|██████▊   | 1321/1940 [6:09:14<2:49:15, 16.41s/it] 68%|██████▊   | 1322/1940 [6:09:30<2:48:18, 16.34s/it] 68%|██████▊   | 1323/1940 [6:09:47<2:47:38, 16.30s/it] 68%|██████▊   | 1324/1940 [6:10:03<2:47:10, 16.28s/it] 68%|██████▊   | 1325/1940 [6:10:19<2:47:27, 16.34s/it] 68%|██████▊   | 1326/1940 [6:10:36<2:46:52, 16.31s/it] 68%|██████▊   | 1327/1940 [6:10:52<2:46:17, 16.28s/it] 68%|██████▊   | 1328/1940 [6:11:09<2:47:16, 16.40s/it] 69%|██████▊   | 1329/1940 [6:11:25<2:46:22, 16.34s/it] 69%|██████▊   | 1330/1940 [6:11:41<2:45:42, 16.30s/it]                                                       {'loss': 2.5186, 'learning_rate': 1.1237230432354912e-05, 'epoch': 0.69}
 69%|██████▊   | 1330/1940 [6:11:41<2:45:42, 16.30s/it] 69%|██████▊   | 1331/1940 [6:11:57<2:46:01, 16.36s/it] 69%|██████▊   | 1332/1940 [6:12:14<2:45:16, 16.31s/it] 69%|██████▊   | 1333/1940 [6:12:30<2:44:42, 16.28s/it] 69%|██████▉   | 1334/1940 [6:12:46<2:44:59, 16.34s/it] 69%|██████▉   | 1335/1940 [6:13:03<2:45:01, 16.37s/it] 69%|██████▉   | 1336/1940 [6:13:19<2:44:20, 16.32s/it] 69%|██████▉   | 1337/1940 [6:13:35<2:43:44, 16.29s/it] 69%|██████▉   | 1338/1940 [6:13:52<2:44:33, 16.40s/it] 69%|██████▉   | 1339/1940 [6:14:08<2:43:41, 16.34s/it] 69%|██████▉   | 1340/1940 [6:14:24<2:42:57, 16.30s/it]                                                       {'loss': 2.4998, 'learning_rate': 1.0901074253727336e-05, 'epoch': 0.69}
 69%|██████▉   | 1340/1940 [6:14:24<2:42:57, 16.30s/it] 69%|██████▉   | 1341/1940 [6:14:41<2:43:02, 16.33s/it] 69%|██████▉   | 1342/1940 [6:14:57<2:42:23, 16.29s/it] 69%|██████▉   | 1343/1940 [6:15:13<2:41:49, 16.26s/it] 69%|██████▉   | 1344/1940 [6:15:29<2:42:08, 16.32s/it] 69%|██████▉   | 1345/1940 [6:15:46<2:42:13, 16.36s/it] 69%|██████▉   | 1346/1940 [6:16:02<2:41:35, 16.32s/it] 69%|██████▉   | 1347/1940 [6:16:18<2:41:07, 16.30s/it] 69%|██████▉   | 1348/1940 [6:16:35<2:41:59, 16.42s/it] 70%|██████▉   | 1349/1940 [6:16:51<2:41:07, 16.36s/it] 70%|██████▉   | 1350/1940 [6:17:08<2:40:26, 16.32s/it]                                                       {'loss': 2.4858, 'learning_rate': 1.0568615274024522e-05, 'epoch': 0.7}
 70%|██████▉   | 1350/1940 [6:17:08<2:40:26, 16.32s/it] 70%|██████▉   | 1351/1940 [6:17:24<2:40:35, 16.36s/it] 70%|██████▉   | 1352/1940 [6:17:40<2:39:57, 16.32s/it] 70%|██████▉   | 1353/1940 [6:17:56<2:39:22, 16.29s/it] 70%|██████▉   | 1354/1940 [6:18:13<2:39:36, 16.34s/it] 70%|██████▉   | 1355/1940 [6:18:29<2:39:39, 16.38s/it] 70%|██████▉   | 1356/1940 [6:18:46<2:39:00, 16.34s/it] 70%|██████▉   | 1357/1940 [6:19:02<2:38:31, 16.31s/it] 70%|███████   | 1358/1940 [6:19:18<2:38:55, 16.38s/it] 70%|███████   | 1359/1940 [6:19:35<2:38:06, 16.33s/it] 70%|███████   | 1360/1940 [6:19:51<2:37:28, 16.29s/it]                                                       {'loss': 2.4935, 'learning_rate': 1.0239940674851941e-05, 'epoch': 0.7}
 70%|███████   | 1360/1940 [6:19:51<2:37:28, 16.29s/it] 70%|███████   | 1361/1940 [6:20:08<2:38:21, 16.41s/it] 70%|███████   | 1362/1940 [6:20:24<2:37:29, 16.35s/it] 70%|███████   | 1363/1940 [6:20:40<2:36:46, 16.30s/it] 70%|███████   | 1364/1940 [6:20:56<2:36:52, 16.34s/it] 70%|███████   | 1365/1940 [6:21:13<2:36:50, 16.37s/it] 70%|███████   | 1366/1940 [6:21:29<2:36:10, 16.32s/it] 70%|███████   | 1367/1940 [6:21:45<2:35:40, 16.30s/it] 71%|███████   | 1368/1940 [6:22:02<2:35:54, 16.35s/it] 71%|███████   | 1369/1940 [6:22:18<2:35:23, 16.33s/it] 71%|███████   | 1370/1940 [6:22:34<2:34:54, 16.31s/it]                                                       {'loss': 2.4883, 'learning_rate': 9.915136645426884e-06, 'epoch': 0.71}
 71%|███████   | 1370/1940 [6:22:34<2:34:54, 16.31s/it] 71%|███████   | 1371/1940 [6:22:51<2:35:44, 16.42s/it] 71%|███████   | 1372/1940 [6:23:07<2:34:46, 16.35s/it] 71%|███████   | 1373/1940 [6:23:23<2:33:59, 16.29s/it] 71%|███████   | 1374/1940 [6:23:39<2:33:21, 16.26s/it] 71%|███████   | 1375/1940 [6:23:56<2:34:08, 16.37s/it] 71%|███████   | 1376/1940 [6:24:12<2:33:16, 16.31s/it] 71%|███████   | 1377/1940 [6:24:28<2:32:37, 16.27s/it] 71%|███████   | 1378/1940 [6:24:45<2:32:47, 16.31s/it] 71%|███████   | 1379/1940 [6:25:01<2:32:10, 16.28s/it] 71%|███████   | 1380/1940 [6:25:17<2:31:43, 16.26s/it]                                                       {'loss': 2.5132, 'learning_rate': 9.594288359976817e-06, 'epoch': 0.71}
 71%|███████   | 1380/1940 [6:25:17<2:31:43, 16.26s/it] 71%|███████   | 1381/1940 [6:25:34<2:32:44, 16.39s/it] 71%|███████   | 1382/1940 [6:25:50<2:32:03, 16.35s/it] 71%|███████▏  | 1383/1940 [6:26:06<2:31:30, 16.32s/it] 71%|███████▏  | 1384/1940 [6:26:23<2:31:00, 16.30s/it] 71%|███████▏  | 1385/1940 [6:26:39<2:31:23, 16.37s/it] 71%|███████▏  | 1386/1940 [6:26:55<2:30:38, 16.32s/it] 71%|███████▏  | 1387/1940 [6:27:12<2:30:04, 16.28s/it] 72%|███████▏  | 1388/1940 [6:27:28<2:30:54, 16.40s/it] 72%|███████▏  | 1389/1940 [6:27:44<2:29:59, 16.33s/it] 72%|███████▏  | 1390/1940 [6:28:01<2:29:22, 16.30s/it]                                                       {'loss': 2.4832, 'learning_rate': 9.277479955403887e-06, 'epoch': 0.72}
 72%|███████▏  | 1390/1940 [6:28:01<2:29:22, 16.30s/it] 72%|███████▏  | 1391/1940 [6:28:17<2:29:32, 16.34s/it] 72%|███████▏  | 1392/1940 [6:28:34<2:29:33, 16.38s/it] 72%|███████▏  | 1393/1940 [6:28:50<2:28:56, 16.34s/it] 72%|███████▏  | 1394/1940 [6:29:06<2:28:30, 16.32s/it] 72%|███████▏  | 1395/1940 [6:29:23<2:28:43, 16.37s/it] 72%|███████▏  | 1396/1940 [6:29:39<2:28:03, 16.33s/it] 72%|███████▏  | 1397/1940 [6:29:55<2:27:23, 16.29s/it] 72%|███████▏  | 1398/1940 [6:30:12<2:28:03, 16.39s/it] 72%|███████▏  | 1399/1940 [6:30:28<2:27:08, 16.32s/it] 72%|███████▏  | 1400/1940 [6:30:44<2:26:32, 16.28s/it]                                                       {'loss': 2.4885, 'learning_rate': 8.964794509221508e-06, 'epoch': 0.72}
 72%|███████▏  | 1400/1940 [6:30:44<2:26:32, 16.28s/it] 72%|███████▏  | 1401/1940 [6:31:00<2:26:36, 16.32s/it] 72%|███████▏  | 1402/1940 [6:31:17<2:26:35, 16.35s/it] 72%|███████▏  | 1403/1940 [6:31:33<2:25:55, 16.30s/it] 72%|███████▏  | 1404/1940 [6:31:49<2:25:29, 16.29s/it] 72%|███████▏  | 1405/1940 [6:32:06<2:25:45, 16.35s/it] 72%|███████▏  | 1406/1940 [6:32:22<2:25:15, 16.32s/it] 73%|███████▎  | 1407/1940 [6:32:38<2:24:48, 16.30s/it] 73%|███████▎  | 1408/1940 [6:32:55<2:25:35, 16.42s/it] 73%|███████▎  | 1409/1940 [6:33:11<2:24:42, 16.35s/it] 73%|███████▎  | 1410/1940 [6:33:27<2:24:08, 16.32s/it]                                                       {'loss': 2.4933, 'learning_rate': 8.656314017768693e-06, 'epoch': 0.73}
 73%|███████▎  | 1410/1940 [6:33:27<2:24:08, 16.32s/it] 73%|███████▎  | 1411/1940 [6:33:44<2:24:10, 16.35s/it] 73%|███████▎  | 1412/1940 [6:34:00<2:23:38, 16.32s/it] 73%|███████▎  | 1413/1940 [6:34:16<2:23:01, 16.28s/it] 73%|███████▎  | 1414/1940 [6:34:32<2:22:29, 16.25s/it] 73%|███████▎  | 1415/1940 [6:34:49<2:23:21, 16.38s/it] 73%|███████▎  | 1416/1940 [6:35:05<2:22:42, 16.34s/it] 73%|███████▎  | 1417/1940 [6:35:22<2:22:15, 16.32s/it] 73%|███████▎  | 1418/1940 [6:35:38<2:23:04, 16.45s/it] 73%|███████▎  | 1419/1940 [6:35:55<2:22:14, 16.38s/it] 73%|███████▎  | 1420/1940 [6:36:11<2:21:30, 16.33s/it]                                                       {'loss': 2.4776, 'learning_rate': 8.352119374707978e-06, 'epoch': 0.73}
 73%|███████▎  | 1420/1940 [6:36:11<2:21:30, 16.33s/it] 73%|███████▎  | 1421/1940 [6:36:27<2:20:55, 16.29s/it] 73%|███████▎  | 1422/1940 [6:36:44<2:21:04, 16.34s/it] 73%|███████▎  | 1423/1940 [6:37:00<2:20:32, 16.31s/it] 73%|███████▎  | 1424/1940 [6:37:16<2:20:06, 16.29s/it] 73%|███████▎  | 1425/1940 [6:37:33<2:20:48, 16.41s/it] 74%|███████▎  | 1426/1940 [6:37:49<2:20:03, 16.35s/it] 74%|███████▎  | 1427/1940 [6:38:05<2:19:25, 16.31s/it] 74%|███████▎  | 1428/1940 [6:38:22<2:20:10, 16.43s/it] 74%|███████▎  | 1429/1940 [6:38:38<2:19:27, 16.38s/it] 74%|███████▎  | 1430/1940 [6:38:54<2:18:58, 16.35s/it]                                                       {'loss': 2.491, 'learning_rate': 8.052290349812419e-06, 'epoch': 0.74}
 74%|███████▎  | 1430/1940 [6:38:54<2:18:58, 16.35s/it] 74%|███████▍  | 1431/1940 [6:39:11<2:18:30, 16.33s/it] 74%|███████▍  | 1432/1940 [6:39:27<2:18:31, 16.36s/it] 74%|███████▍  | 1433/1940 [6:39:43<2:17:47, 16.31s/it] 74%|███████▍  | 1434/1940 [6:39:59<2:17:13, 16.27s/it] 74%|███████▍  | 1435/1940 [6:40:16<2:17:52, 16.38s/it] 74%|███████▍  | 1436/1940 [6:40:32<2:17:06, 16.32s/it] 74%|███████▍  | 1437/1940 [6:40:48<2:16:31, 16.29s/it] 74%|███████▍  | 1438/1940 [6:41:05<2:16:45, 16.35s/it] 74%|███████▍  | 1439/1940 [6:41:21<2:16:06, 16.30s/it] 74%|███████▍  | 1440/1940 [6:41:37<2:15:34, 16.27s/it]                                                       {'loss': 2.5126, 'learning_rate': 7.756905568047393e-06, 'epoch': 0.74}
 74%|███████▍  | 1440/1940 [6:41:37<2:15:34, 16.27s/it] 74%|███████▍  | 1441/1940 [6:41:54<2:15:49, 16.33s/it] 74%|███████▍  | 1442/1940 [6:42:10<2:15:58, 16.38s/it] 74%|███████▍  | 1443/1940 [6:42:27<2:15:27, 16.35s/it] 74%|███████▍  | 1444/1940 [6:42:43<2:14:58, 16.33s/it] 74%|███████▍  | 1445/1940 [6:43:00<2:15:35, 16.43s/it] 75%|███████▍  | 1446/1940 [6:43:16<2:14:47, 16.37s/it] 75%|███████▍  | 1447/1940 [6:43:32<2:14:09, 16.33s/it] 75%|███████▍  | 1448/1940 [6:43:48<2:14:11, 16.36s/it] 75%|███████▍  | 1449/1940 [6:44:05<2:13:29, 16.31s/it] 75%|███████▍  | 1450/1940 [6:44:21<2:12:56, 16.28s/it]                                                       {'loss': 2.4705, 'learning_rate': 7.466042488952521e-06, 'epoch': 0.75}
 75%|███████▍  | 1450/1940 [6:44:21<2:12:56, 16.28s/it] 75%|███████▍  | 1451/1940 [6:44:37<2:13:08, 16.34s/it] 75%|███████▍  | 1452/1940 [6:44:54<2:13:06, 16.36s/it] 75%|███████▍  | 1453/1940 [6:45:10<2:12:26, 16.32s/it] 75%|███████▍  | 1454/1940 [6:45:26<2:12:01, 16.30s/it] 75%|███████▌  | 1455/1940 [6:45:43<2:12:49, 16.43s/it] 75%|███████▌  | 1456/1940 [6:45:59<2:12:13, 16.39s/it] 75%|███████▌  | 1457/1940 [6:46:16<2:11:40, 16.36s/it] 75%|███████▌  | 1458/1940 [6:46:32<2:11:37, 16.39s/it] 75%|███████▌  | 1459/1940 [6:46:48<2:10:51, 16.32s/it] 75%|███████▌  | 1460/1940 [6:47:04<2:10:15, 16.28s/it]                                                       {'loss': 2.4897, 'learning_rate': 7.179777386329276e-06, 'epoch': 0.75}
 75%|███████▌  | 1460/1940 [6:47:04<2:10:15, 16.28s/it] 75%|███████▌  | 1461/1940 [6:47:21<2:10:18, 16.32s/it] 75%|███████▌  | 1462/1940 [6:47:37<2:10:17, 16.36s/it] 75%|███████▌  | 1463/1940 [6:47:53<2:09:40, 16.31s/it] 75%|███████▌  | 1464/1940 [6:48:10<2:09:11, 16.28s/it] 76%|███████▌  | 1465/1940 [6:48:26<2:09:26, 16.35s/it] 76%|███████▌  | 1466/1940 [6:48:42<2:08:51, 16.31s/it] 76%|███████▌  | 1467/1940 [6:48:59<2:08:27, 16.30s/it] 76%|███████▌  | 1468/1940 [6:49:15<2:09:14, 16.43s/it] 76%|███████▌  | 1469/1940 [6:49:32<2:08:30, 16.37s/it] 76%|███████▌  | 1470/1940 [6:49:48<2:07:53, 16.33s/it]                                                       {'loss': 2.4829, 'learning_rate': 6.898185328239468e-06, 'epoch': 0.76}
 76%|███████▌  | 1470/1940 [6:49:48<2:07:53, 16.33s/it] 76%|███████▌  | 1471/1940 [6:50:04<2:07:52, 16.36s/it] 76%|███████▌  | 1472/1940 [6:50:21<2:07:42, 16.37s/it] 76%|███████▌  | 1473/1940 [6:50:37<2:06:58, 16.31s/it] 76%|███████▌  | 1474/1940 [6:50:53<2:06:26, 16.28s/it] 76%|███████▌  | 1475/1940 [6:51:09<2:06:29, 16.32s/it] 76%|███████▌  | 1476/1940 [6:51:26<2:05:56, 16.28s/it] 76%|███████▌  | 1477/1940 [6:51:42<2:05:28, 16.26s/it] 76%|███████▌  | 1478/1940 [6:51:59<2:06:13, 16.39s/it] 76%|███████▌  | 1479/1940 [6:52:15<2:05:33, 16.34s/it] 76%|███████▋  | 1480/1940 [6:52:31<2:05:00, 16.31s/it]                                                       {'loss': 2.4958, 'learning_rate': 6.621340157319997e-06, 'epoch': 0.76}
 76%|███████▋  | 1480/1940 [6:52:31<2:05:00, 16.31s/it] 76%|███████▋  | 1481/1940 [6:52:47<2:04:29, 16.27s/it] 76%|███████▋  | 1482/1940 [6:53:04<2:05:07, 16.39s/it] 76%|███████▋  | 1483/1940 [6:53:20<2:04:23, 16.33s/it] 76%|███████▋  | 1484/1940 [6:53:36<2:03:48, 16.29s/it] 77%|███████▋  | 1485/1940 [6:53:53<2:03:51, 16.33s/it] 77%|███████▋  | 1486/1940 [6:54:09<2:03:17, 16.29s/it] 77%|███████▋  | 1487/1940 [6:54:25<2:02:51, 16.27s/it] 77%|███████▋  | 1488/1940 [6:54:42<2:03:34, 16.40s/it] 77%|███████▋  | 1489/1940 [6:54:58<2:02:56, 16.35s/it] 77%|███████▋  | 1490/1940 [6:55:14<2:02:27, 16.33s/it]                                                       {'loss': 2.468, 'learning_rate': 6.349314471418849e-06, 'epoch': 0.77}
 77%|███████▋  | 1490/1940 [6:55:14<2:02:27, 16.33s/it] 77%|███████▋  | 1491/1940 [6:55:31<2:02:00, 16.30s/it] 77%|███████▋  | 1492/1940 [6:55:47<2:02:14, 16.37s/it] 77%|███████▋  | 1493/1940 [6:56:03<2:01:32, 16.31s/it] 77%|███████▋  | 1494/1940 [6:56:19<2:00:56, 16.27s/it] 77%|███████▋  | 1495/1940 [6:56:36<2:01:30, 16.38s/it] 77%|███████▋  | 1496/1940 [6:56:52<2:00:44, 16.32s/it] 77%|███████▋  | 1497/1940 [6:57:08<2:00:08, 16.27s/it] 77%|███████▋  | 1498/1940 [6:57:25<2:00:12, 16.32s/it] 77%|███████▋  | 1499/1940 [6:57:41<2:00:08, 16.35s/it] 77%|███████▋  | 1500/1940 [6:57:57<1:59:34, 16.31s/it]                                                       {'loss': 2.4762, 'learning_rate': 6.082179604557617e-06, 'epoch': 0.77}
 77%|███████▋  | 1500/1940 [6:57:57<1:59:34, 16.31s/it] 77%|███████▋  | 1501/1940 [6:58:14<1:59:08, 16.28s/it] 77%|███████▋  | 1502/1940 [6:58:30<1:59:18, 16.34s/it] 77%|███████▋  | 1503/1940 [6:58:46<1:58:44, 16.30s/it] 78%|███████▊  | 1504/1940 [6:59:03<1:58:16, 16.28s/it] 78%|███████▊  | 1505/1940 [6:59:19<1:58:49, 16.39s/it] 78%|███████▊  | 1506/1940 [6:59:35<1:58:03, 16.32s/it] 78%|███████▊  | 1507/1940 [6:59:52<1:57:26, 16.27s/it] 78%|███████▊  | 1508/1940 [7:00:08<1:57:27, 16.31s/it] 78%|███████▊  | 1509/1940 [7:00:24<1:57:23, 16.34s/it] 78%|███████▊  | 1510/1940 [7:00:41<1:56:44, 16.29s/it]                                                       {'loss': 2.4802, 'learning_rate': 5.820005608225346e-06, 'epoch': 0.78}
 78%|███████▊  | 1510/1940 [7:00:41<1:56:44, 16.29s/it] 78%|███████▊  | 1511/1940 [7:00:57<1:56:15, 16.26s/it] 78%|███████▊  | 1512/1940 [7:01:13<1:56:23, 16.32s/it] 78%|███████▊  | 1513/1940 [7:01:29<1:55:56, 16.29s/it] 78%|███████▊  | 1514/1940 [7:01:46<1:55:37, 16.29s/it] 78%|███████▊  | 1515/1940 [7:02:02<1:56:15, 16.41s/it] 78%|███████▊  | 1516/1940 [7:02:19<1:55:34, 16.36s/it] 78%|███████▊  | 1517/1940 [7:02:35<1:54:57, 16.31s/it] 78%|███████▊  | 1518/1940 [7:02:51<1:54:58, 16.35s/it] 78%|███████▊  | 1519/1940 [7:03:08<1:54:32, 16.32s/it] 78%|███████▊  | 1520/1940 [7:03:24<1:54:03, 16.29s/it]                                                       {'loss': 2.4969, 'learning_rate': 5.562861233008774e-06, 'epoch': 0.78}
 78%|███████▊  | 1520/1940 [7:03:24<1:54:03, 16.29s/it] 78%|███████▊  | 1521/1940 [7:03:40<1:53:33, 16.26s/it] 78%|███████▊  | 1522/1940 [7:03:57<1:54:08, 16.38s/it] 79%|███████▊  | 1523/1940 [7:04:13<1:53:29, 16.33s/it] 79%|███████▊  | 1524/1940 [7:04:29<1:53:02, 16.31s/it] 79%|███████▊  | 1525/1940 [7:04:46<1:53:39, 16.43s/it] 79%|███████▊  | 1526/1940 [7:05:02<1:53:03, 16.39s/it] 79%|███████▊  | 1527/1940 [7:05:18<1:52:31, 16.35s/it] 79%|███████▉  | 1528/1940 [7:05:35<1:52:01, 16.31s/it] 79%|███████▉  | 1529/1940 [7:05:51<1:51:59, 16.35s/it] 79%|███████▉  | 1530/1940 [7:06:07<1:51:26, 16.31s/it]                                                       {'loss': 2.4784, 'learning_rate': 5.310813910563644e-06, 'epoch': 0.79}
 79%|███████▉  | 1530/1940 [7:06:07<1:51:26, 16.31s/it] 79%|███████▉  | 1531/1940 [7:06:23<1:50:56, 16.27s/it] 79%|███████▉  | 1532/1940 [7:06:40<1:51:28, 16.39s/it] 79%|███████▉  | 1533/1940 [7:06:56<1:50:48, 16.34s/it] 79%|███████▉  | 1534/1940 [7:07:12<1:50:17, 16.30s/it] 79%|███████▉  | 1535/1940 [7:07:29<1:50:48, 16.42s/it] 79%|███████▉  | 1536/1940 [7:07:45<1:50:07, 16.36s/it] 79%|███████▉  | 1537/1940 [7:08:02<1:49:37, 16.32s/it] 79%|███████▉  | 1538/1940 [7:08:18<1:49:14, 16.30s/it] 79%|███████▉  | 1539/1940 [7:08:34<1:49:18, 16.36s/it] 79%|███████▉  | 1540/1940 [7:08:51<1:48:45, 16.31s/it]                                                       {'loss': 2.476, 'learning_rate': 5.063929735931985e-06, 'epoch': 0.79}
 79%|███████▉  | 1540/1940 [7:08:51<1:48:45, 16.31s/it] 79%|███████▉  | 1541/1940 [7:09:07<1:48:15, 16.28s/it] 79%|███████▉  | 1542/1940 [7:09:23<1:48:45, 16.39s/it] 80%|███████▉  | 1543/1940 [7:09:40<1:48:07, 16.34s/it] 80%|███████▉  | 1544/1940 [7:09:56<1:47:33, 16.30s/it] 80%|███████▉  | 1545/1940 [7:10:12<1:47:39, 16.35s/it] 80%|███████▉  | 1546/1940 [7:10:29<1:47:05, 16.31s/it] 80%|███████▉  | 1547/1940 [7:10:45<1:46:36, 16.28s/it] 80%|███████▉  | 1548/1940 [7:11:01<1:46:41, 16.33s/it] 80%|███████▉  | 1549/1940 [7:11:18<1:46:42, 16.37s/it] 80%|███████▉  | 1550/1940 [7:11:34<1:46:13, 16.34s/it]                                                       {'loss': 2.4689, 'learning_rate': 4.8222734502097665e-06, 'epoch': 0.8}
 80%|███████▉  | 1550/1940 [7:11:34<1:46:13, 16.34s/it] 80%|███████▉  | 1551/1940 [7:11:50<1:45:50, 16.33s/it] 80%|████████  | 1552/1940 [7:12:07<1:46:20, 16.45s/it] 80%|████████  | 1553/1940 [7:12:23<1:45:37, 16.38s/it] 80%|████████  | 1554/1940 [7:12:39<1:45:03, 16.33s/it] 80%|████████  | 1555/1940 [7:12:56<1:45:00, 16.37s/it] 80%|████████  | 1556/1940 [7:13:12<1:44:27, 16.32s/it] 80%|████████  | 1557/1940 [7:13:28<1:43:58, 16.29s/it] 80%|████████  | 1558/1940 [7:13:45<1:43:57, 16.33s/it] 80%|████████  | 1559/1940 [7:14:01<1:43:49, 16.35s/it] 80%|████████  | 1560/1940 [7:14:17<1:43:15, 16.30s/it]                                                       {'loss': 2.4821, 'learning_rate': 4.585908423569724e-06, 'epoch': 0.8}
 80%|████████  | 1560/1940 [7:14:17<1:43:15, 16.30s/it] 80%|████████  | 1561/1940 [7:14:33<1:42:45, 16.27s/it] 81%|████████  | 1562/1940 [7:14:50<1:43:20, 16.40s/it] 81%|████████  | 1563/1940 [7:15:06<1:42:51, 16.37s/it] 81%|████████  | 1564/1940 [7:15:23<1:42:28, 16.35s/it] 81%|████████  | 1565/1940 [7:15:39<1:42:33, 16.41s/it] 81%|████████  | 1566/1940 [7:15:56<1:41:57, 16.36s/it] 81%|████████  | 1567/1940 [7:16:12<1:41:23, 16.31s/it] 81%|████████  | 1568/1940 [7:16:28<1:41:23, 16.35s/it] 81%|████████  | 1569/1940 [7:16:45<1:41:15, 16.38s/it] 81%|████████  | 1570/1940 [7:17:01<1:40:40, 16.33s/it]                                                       {'loss': 2.4731, 'learning_rate': 4.35489663864359e-06, 'epoch': 0.81}
 81%|████████  | 1570/1940 [7:17:01<1:40:40, 16.33s/it] 81%|████████  | 1571/1940 [7:17:17<1:40:12, 16.29s/it] 81%|████████  | 1572/1940 [7:17:34<1:40:18, 16.35s/it] 81%|████████  | 1573/1940 [7:17:50<1:39:43, 16.30s/it] 81%|████████  | 1574/1940 [7:18:06<1:39:17, 16.28s/it] 81%|████████  | 1575/1940 [7:18:23<1:39:44, 16.40s/it] 81%|████████  | 1576/1940 [7:18:39<1:39:12, 16.35s/it] 81%|████████▏ | 1577/1940 [7:18:55<1:38:50, 16.34s/it] 81%|████████▏ | 1578/1940 [7:19:12<1:38:57, 16.40s/it] 81%|████████▏ | 1579/1940 [7:19:28<1:38:51, 16.43s/it] 81%|████████▏ | 1580/1940 [7:19:44<1:38:11, 16.37s/it]                                                       {'loss': 2.4798, 'learning_rate': 4.129298674268225e-06, 'epoch': 0.81}
 81%|████████▏ | 1580/1940 [7:19:44<1:38:11, 16.37s/it] 81%|████████▏ | 1581/1940 [7:20:01<1:37:37, 16.32s/it] 82%|████████▏ | 1582/1940 [7:20:17<1:37:34, 16.35s/it] 82%|████████▏ | 1583/1940 [7:20:33<1:36:59, 16.30s/it] 82%|████████▏ | 1584/1940 [7:20:50<1:36:33, 16.27s/it] 82%|████████▏ | 1585/1940 [7:21:06<1:36:58, 16.39s/it] 82%|████████▏ | 1586/1940 [7:21:22<1:36:18, 16.32s/it] 82%|████████▏ | 1587/1940 [7:21:39<1:35:51, 16.29s/it] 82%|████████▏ | 1588/1940 [7:21:55<1:35:30, 16.28s/it] 82%|████████▏ | 1589/1940 [7:22:12<1:36:02, 16.42s/it] 82%|████████▏ | 1590/1940 [7:22:28<1:35:31, 16.38s/it]                                                       {'loss': 2.4736, 'learning_rate': 3.90917368959989e-06, 'epoch': 0.82}
 82%|████████▏ | 1590/1940 [7:22:28<1:35:31, 16.38s/it] 82%|████████▏ | 1591/1940 [7:22:44<1:35:02, 16.34s/it] 82%|████████▏ | 1592/1940 [7:23:01<1:34:58, 16.38s/it] 82%|████████▏ | 1593/1940 [7:23:17<1:34:21, 16.31s/it] 82%|████████▏ | 1594/1940 [7:23:33<1:33:51, 16.28s/it] 82%|████████▏ | 1595/1940 [7:23:50<1:34:12, 16.38s/it] 82%|████████▏ | 1596/1940 [7:24:06<1:33:35, 16.32s/it] 82%|████████▏ | 1597/1940 [7:24:22<1:33:03, 16.28s/it] 82%|████████▏ | 1598/1940 [7:24:38<1:32:40, 16.26s/it] 82%|████████▏ | 1599/1940 [7:24:55<1:32:47, 16.33s/it] 82%|████████▏ | 1600/1940 [7:25:11<1:32:16, 16.28s/it]                                                       {'loss': 2.4562, 'learning_rate': 3.694579408600771e-06, 'epoch': 0.82}
 82%|████████▏ | 1600/1940 [7:25:11<1:32:16, 16.28s/it] 83%|████████▎ | 1601/1940 [7:25:27<1:31:56, 16.27s/it] 83%|████████▎ | 1602/1940 [7:25:44<1:32:24, 16.40s/it] 83%|████████▎ | 1603/1940 [7:26:00<1:31:50, 16.35s/it] 83%|████████▎ | 1604/1940 [7:26:16<1:31:22, 16.32s/it] 83%|████████▎ | 1605/1940 [7:26:33<1:31:18, 16.35s/it] 83%|████████▎ | 1606/1940 [7:26:49<1:31:07, 16.37s/it] 83%|████████▎ | 1607/1940 [7:27:05<1:30:33, 16.32s/it] 83%|████████▎ | 1608/1940 [7:27:21<1:30:03, 16.28s/it] 83%|████████▎ | 1609/1940 [7:27:38<1:30:01, 16.32s/it] 83%|████████▎ | 1610/1940 [7:27:54<1:29:31, 16.28s/it]                                                       {'loss': 2.4711, 'learning_rate': 3.4855721049018688e-06, 'epoch': 0.83}
 83%|████████▎ | 1610/1940 [7:27:54<1:29:31, 16.28s/it] 83%|████████▎ | 1611/1940 [7:28:10<1:29:07, 16.25s/it] 83%|████████▎ | 1612/1940 [7:28:27<1:29:35, 16.39s/it] 83%|████████▎ | 1613/1940 [7:28:43<1:29:06, 16.35s/it] 83%|████████▎ | 1614/1940 [7:28:59<1:28:42, 16.33s/it] 83%|████████▎ | 1615/1940 [7:29:16<1:28:39, 16.37s/it] 83%|████████▎ | 1616/1940 [7:29:32<1:28:28, 16.39s/it] 83%|████████▎ | 1617/1940 [7:29:49<1:27:52, 16.32s/it] 83%|████████▎ | 1618/1940 [7:30:05<1:27:21, 16.28s/it] 83%|████████▎ | 1619/1940 [7:30:21<1:27:19, 16.32s/it] 84%|████████▎ | 1620/1940 [7:30:37<1:26:49, 16.28s/it]                                                       {'loss': 2.4796, 'learning_rate': 3.2822065870462217e-06, 'epoch': 0.83}
 84%|████████▎ | 1620/1940 [7:30:37<1:26:49, 16.28s/it] 84%|████████▎ | 1621/1940 [7:30:54<1:26:26, 16.26s/it] 84%|████████▎ | 1622/1940 [7:31:10<1:26:50, 16.39s/it] 84%|████████▎ | 1623/1940 [7:31:26<1:26:18, 16.34s/it] 84%|████████▎ | 1624/1940 [7:31:43<1:25:56, 16.32s/it] 84%|████████▍ | 1625/1940 [7:31:59<1:25:58, 16.38s/it] 84%|████████▍ | 1626/1940 [7:32:16<1:25:35, 16.36s/it] 84%|████████▍ | 1627/1940 [7:32:32<1:25:05, 16.31s/it] 84%|████████▍ | 1628/1940 [7:32:48<1:24:36, 16.27s/it] 84%|████████▍ | 1629/1940 [7:33:05<1:24:55, 16.39s/it] 84%|████████▍ | 1630/1940 [7:33:21<1:24:19, 16.32s/it]                                                       {'loss': 2.4774, 'learning_rate': 3.08453618411631e-06, 'epoch': 0.84}
 84%|████████▍ | 1630/1940 [7:33:21<1:24:19, 16.32s/it] 84%|████████▍ | 1631/1940 [7:33:37<1:23:50, 16.28s/it] 84%|████████▍ | 1632/1940 [7:33:54<1:24:09, 16.40s/it] 84%|████████▍ | 1633/1940 [7:34:10<1:23:32, 16.33s/it] 84%|████████▍ | 1634/1940 [7:34:26<1:23:05, 16.29s/it] 84%|████████▍ | 1635/1940 [7:34:42<1:22:44, 16.28s/it] 84%|████████▍ | 1636/1940 [7:34:59<1:22:49, 16.35s/it] 84%|████████▍ | 1637/1940 [7:35:15<1:22:26, 16.33s/it] 84%|████████▍ | 1638/1940 [7:35:31<1:22:05, 16.31s/it] 84%|████████▍ | 1639/1940 [7:35:48<1:22:22, 16.42s/it] 85%|████████▍ | 1640/1940 [7:36:04<1:21:46, 16.36s/it]                                                       {'loss': 2.4662, 'learning_rate': 2.892612731749414e-06, 'epoch': 0.85}
 85%|████████▍ | 1640/1940 [7:36:04<1:21:46, 16.36s/it] 85%|████████▍ | 1641/1940 [7:36:20<1:21:16, 16.31s/it] 85%|████████▍ | 1642/1940 [7:36:37<1:21:31, 16.41s/it] 85%|████████▍ | 1643/1940 [7:36:53<1:20:54, 16.34s/it] 85%|████████▍ | 1644/1940 [7:37:09<1:20:22, 16.29s/it] 85%|████████▍ | 1645/1940 [7:37:26<1:19:59, 16.27s/it] 85%|████████▍ | 1646/1940 [7:37:42<1:19:59, 16.32s/it] 85%|████████▍ | 1647/1940 [7:37:58<1:19:32, 16.29s/it] 85%|████████▍ | 1648/1940 [7:38:14<1:19:12, 16.27s/it] 85%|████████▌ | 1649/1940 [7:38:31<1:19:34, 16.41s/it] 85%|████████▌ | 1650/1940 [7:38:47<1:19:02, 16.35s/it]                                                       {'loss': 2.4805, 'learning_rate': 2.7064865585446434e-06, 'epoch': 0.85}
 85%|████████▌ | 1650/1940 [7:38:47<1:19:02, 16.35s/it] 85%|████████▌ | 1651/1940 [7:39:04<1:18:36, 16.32s/it] 85%|████████▌ | 1652/1940 [7:39:20<1:18:37, 16.38s/it] 85%|████████▌ | 1653/1940 [7:39:36<1:18:05, 16.33s/it] 85%|████████▌ | 1654/1940 [7:39:53<1:17:36, 16.28s/it] 85%|████████▌ | 1655/1940 [7:40:09<1:17:32, 16.33s/it] 85%|████████▌ | 1656/1940 [7:40:25<1:17:23, 16.35s/it] 85%|████████▌ | 1657/1940 [7:40:42<1:16:50, 16.29s/it] 85%|████████▌ | 1658/1940 [7:40:58<1:16:26, 16.27s/it] 86%|████████▌ | 1659/1940 [7:41:14<1:16:44, 16.38s/it] 86%|████████▌ | 1660/1940 [7:41:31<1:16:14, 16.34s/it]                                                       {'loss': 2.4851, 'learning_rate': 2.52620647286512e-06, 'epoch': 0.86}
 86%|████████▌ | 1660/1940 [7:41:31<1:16:14, 16.34s/it] 86%|████████▌ | 1661/1940 [7:41:47<1:15:50, 16.31s/it] 86%|████████▌ | 1662/1940 [7:42:03<1:15:51, 16.37s/it] 86%|████████▌ | 1663/1940 [7:42:20<1:15:23, 16.33s/it] 86%|████████▌ | 1664/1940 [7:42:36<1:15:01, 16.31s/it] 86%|████████▌ | 1665/1940 [7:42:52<1:14:57, 16.35s/it] 86%|████████▌ | 1666/1940 [7:43:09<1:14:47, 16.38s/it] 86%|████████▌ | 1667/1940 [7:43:25<1:14:13, 16.31s/it] 86%|████████▌ | 1668/1940 [7:43:41<1:13:48, 16.28s/it] 86%|████████▌ | 1669/1940 [7:43:58<1:14:03, 16.40s/it] 86%|████████▌ | 1670/1940 [7:44:14<1:13:29, 16.33s/it]                                                       {'loss': 2.4841, 'learning_rate': 2.351819750038828e-06, 'epoch': 0.86}
 86%|████████▌ | 1670/1940 [7:44:14<1:13:29, 16.33s/it] 86%|████████▌ | 1671/1940 [7:44:30<1:13:02, 16.29s/it] 86%|████████▌ | 1672/1940 [7:44:47<1:13:00, 16.34s/it] 86%|████████▌ | 1673/1940 [7:45:03<1:12:36, 16.32s/it] 86%|████████▋ | 1674/1940 [7:45:19<1:12:15, 16.30s/it] 86%|████████▋ | 1675/1940 [7:45:36<1:12:13, 16.35s/it] 86%|████████▋ | 1676/1940 [7:45:52<1:12:04, 16.38s/it] 86%|████████▋ | 1677/1940 [7:46:08<1:11:33, 16.33s/it] 86%|████████▋ | 1678/1940 [7:46:25<1:11:09, 16.30s/it] 87%|████████▋ | 1679/1940 [7:46:41<1:11:11, 16.37s/it] 87%|████████▋ | 1680/1940 [7:46:57<1:10:43, 16.32s/it]                                                       {'loss': 2.4726, 'learning_rate': 2.183372119961499e-06, 'epoch': 0.87}
 87%|████████▋ | 1680/1940 [7:46:57<1:10:43, 16.32s/it] 87%|████████▋ | 1681/1940 [7:47:13<1:10:20, 16.29s/it] 87%|████████▋ | 1682/1940 [7:47:30<1:10:36, 16.42s/it] 87%|████████▋ | 1683/1940 [7:47:46<1:10:02, 16.35s/it] 87%|████████▋ | 1684/1940 [7:48:03<1:09:38, 16.32s/it] 87%|████████▋ | 1685/1940 [7:48:19<1:09:35, 16.37s/it] 87%|████████▋ | 1686/1940 [7:48:36<1:09:27, 16.41s/it] 87%|████████▋ | 1687/1940 [7:48:52<1:08:56, 16.35s/it] 87%|████████▋ | 1688/1940 [7:49:08<1:08:31, 16.32s/it] 87%|████████▋ | 1689/1940 [7:49:25<1:08:27, 16.36s/it] 87%|████████▋ | 1690/1940 [7:49:41<1:07:59, 16.32s/it]                                                       {'loss': 2.4866, 'learning_rate': 2.020907755104698e-06, 'epoch': 0.87}
 87%|████████▋ | 1690/1940 [7:49:41<1:07:59, 16.32s/it] 87%|████████▋ | 1691/1940 [7:49:57<1:07:35, 16.29s/it] 87%|████████▋ | 1692/1940 [7:50:14<1:07:48, 16.40s/it] 87%|████████▋ | 1693/1940 [7:50:30<1:07:14, 16.34s/it] 87%|████████▋ | 1694/1940 [7:50:46<1:06:47, 16.29s/it] 87%|████████▋ | 1695/1940 [7:51:02<1:06:23, 16.26s/it] 87%|████████▋ | 1696/1940 [7:51:19<1:06:39, 16.39s/it] 87%|████████▋ | 1697/1940 [7:51:35<1:06:12, 16.35s/it] 88%|████████▊ | 1698/1940 [7:51:51<1:05:50, 16.33s/it] 88%|████████▊ | 1699/1940 [7:52:08<1:05:46, 16.38s/it] 88%|████████▊ | 1700/1940 [7:52:24<1:05:21, 16.34s/it]                                                       {'loss': 2.4532, 'learning_rate': 1.864469258932397e-06, 'epoch': 0.88}
 88%|████████▊ | 1700/1940 [7:52:24<1:05:21, 16.34s/it] 88%|████████▊ | 1701/1940 [7:52:40<1:04:56, 16.30s/it] 88%|████████▊ | 1702/1940 [7:52:57<1:05:07, 16.42s/it] 88%|████████▊ | 1703/1940 [7:53:13<1:04:35, 16.35s/it] 88%|████████▊ | 1704/1940 [7:53:29<1:04:09, 16.31s/it] 88%|████████▊ | 1705/1940 [7:53:46<1:03:44, 16.28s/it] 88%|████████▊ | 1706/1940 [7:54:02<1:03:46, 16.35s/it] 88%|████████▊ | 1707/1940 [7:54:18<1:03:20, 16.31s/it] 88%|████████▊ | 1708/1940 [7:54:35<1:02:58, 16.29s/it] 88%|████████▊ | 1709/1940 [7:54:51<1:03:11, 16.41s/it] 88%|████████▊ | 1710/1940 [7:55:08<1:02:44, 16.37s/it]                                                       {'loss': 2.482, 'learning_rate': 1.7140976547289438e-06, 'epoch': 0.88}
 88%|████████▊ | 1710/1940 [7:55:08<1:02:44, 16.37s/it] 88%|████████▊ | 1711/1940 [7:55:24<1:02:22, 16.34s/it] 88%|████████▊ | 1712/1940 [7:55:40<1:02:16, 16.39s/it] 88%|████████▊ | 1713/1940 [7:55:57<1:02:03, 16.40s/it] 88%|████████▊ | 1714/1940 [7:56:13<1:01:33, 16.34s/it] 88%|████████▊ | 1715/1940 [7:56:29<1:01:07, 16.30s/it] 88%|████████▊ | 1716/1940 [7:56:46<1:01:01, 16.35s/it] 89%|████████▊ | 1717/1940 [7:57:02<1:00:35, 16.30s/it] 89%|████████▊ | 1718/1940 [7:57:18<1:00:15, 16.29s/it] 89%|████████▊ | 1719/1940 [7:57:35<1:00:25, 16.41s/it] 89%|████████▊ | 1720/1940 [7:57:51<59:57, 16.35s/it]                                                       {'loss': 2.4661, 'learning_rate': 1.5698323748414124e-06, 'epoch': 0.89}
 89%|████████▊ | 1720/1940 [7:57:51<59:57, 16.35s/it] 89%|████████▊ | 1721/1940 [7:58:07<59:33, 16.32s/it] 89%|████████▉ | 1722/1940 [7:58:24<59:27, 16.36s/it] 89%|████████▉ | 1723/1940 [7:58:40<59:16, 16.39s/it] 89%|████████▉ | 1724/1940 [7:58:56<58:48, 16.33s/it] 89%|████████▉ | 1725/1940 [7:59:13<58:25, 16.30s/it] 89%|████████▉ | 1726/1940 [7:59:29<58:18, 16.35s/it] 89%|████████▉ | 1727/1940 [7:59:45<57:53, 16.31s/it] 89%|████████▉ | 1728/1940 [8:00:02<57:31, 16.28s/it] 89%|████████▉ | 1729/1940 [8:00:18<57:40, 16.40s/it] 89%|████████▉ | 1730/1940 [8:00:34<57:13, 16.35s/it]                                                     {'loss': 2.4794, 'learning_rate': 1.4317112503391432e-06, 'epoch': 0.89}
 89%|████████▉ | 1730/1940 [8:00:34<57:13, 16.35s/it] 89%|████████▉ | 1731/1940 [8:00:51<56:49, 16.31s/it] 89%|████████▉ | 1732/1940 [8:01:07<56:47, 16.38s/it] 89%|████████▉ | 1733/1940 [8:01:24<56:27, 16.36s/it] 89%|████████▉ | 1734/1940 [8:01:40<56:02, 16.32s/it] 89%|████████▉ | 1735/1940 [8:01:56<55:39, 16.29s/it] 89%|████████▉ | 1736/1940 [8:02:13<55:45, 16.40s/it] 90%|████████▉ | 1737/1940 [8:02:29<55:15, 16.33s/it] 90%|████████▉ | 1738/1940 [8:02:45<54:52, 16.30s/it] 90%|████████▉ | 1739/1940 [8:03:02<54:56, 16.40s/it] 90%|████████▉ | 1740/1940 [8:03:18<54:26, 16.33s/it]                                                     {'loss': 2.4796, 'learning_rate': 1.2997705010932393e-06, 'epoch': 0.9}
 90%|████████▉ | 1740/1940 [8:03:18<54:26, 16.33s/it] 90%|████████▉ | 1741/1940 [8:03:34<54:01, 16.29s/it] 90%|████████▉ | 1742/1940 [8:03:50<53:41, 16.27s/it] 90%|████████▉ | 1743/1940 [8:04:07<53:37, 16.33s/it] 90%|████████▉ | 1744/1940 [8:04:23<53:16, 16.31s/it] 90%|████████▉ | 1745/1940 [8:04:39<52:56, 16.29s/it] 90%|█████████ | 1746/1940 [8:04:56<53:04, 16.41s/it] 90%|█████████ | 1747/1940 [8:05:12<52:36, 16.35s/it] 90%|█████████ | 1748/1940 [8:05:28<52:12, 16.31s/it] 90%|█████████ | 1749/1940 [8:05:45<52:16, 16.42s/it] 90%|█████████ | 1750/1940 [8:06:01<51:47, 16.36s/it]                                                     {'loss': 2.4803, 'learning_rate': 1.1740447262784781e-06, 'epoch': 0.9}
 90%|█████████ | 1750/1940 [8:06:01<51:47, 16.36s/it] 90%|█████████ | 1751/1940 [8:06:17<51:23, 16.31s/it] 90%|█████████ | 1752/1940 [8:06:34<51:02, 16.29s/it] 90%|█████████ | 1753/1940 [8:06:50<50:53, 16.33s/it] 90%|█████████ | 1754/1940 [8:07:06<50:31, 16.30s/it] 90%|█████████ | 1755/1940 [8:07:23<50:11, 16.28s/it] 91%|█████████ | 1756/1940 [8:07:39<50:19, 16.41s/it] 91%|█████████ | 1757/1940 [8:07:56<49:53, 16.36s/it] 91%|█████████ | 1758/1940 [8:08:12<49:32, 16.33s/it] 91%|█████████ | 1759/1940 [8:08:28<49:26, 16.39s/it] 91%|█████████ | 1760/1940 [8:08:45<49:00, 16.34s/it]                                                     {'loss': 2.4738, 'learning_rate': 1.0545668953003241e-06, 'epoch': 0.91}
 91%|█████████ | 1760/1940 [8:08:45<49:00, 16.34s/it] 91%|█████████ | 1761/1940 [8:09:01<48:36, 16.29s/it] 91%|█████████ | 1762/1940 [8:09:17<48:27, 16.33s/it] 91%|█████████ | 1763/1940 [8:09:34<48:17, 16.37s/it] 91%|█████████ | 1764/1940 [8:09:50<47:52, 16.32s/it] 91%|█████████ | 1765/1940 [8:10:06<47:29, 16.28s/it] 91%|█████████ | 1766/1940 [8:10:23<47:35, 16.41s/it] 91%|█████████ | 1767/1940 [8:10:39<47:09, 16.36s/it] 91%|█████████ | 1768/1940 [8:10:55<46:49, 16.33s/it] 91%|█████████ | 1769/1940 [8:11:12<46:41, 16.38s/it] 91%|█████████ | 1770/1940 [8:11:28<46:17, 16.34s/it]                                                     {'loss': 2.4919, 'learning_rate': 9.413683391492456e-07, 'epoch': 0.91}
 91%|█████████ | 1770/1940 [8:11:28<46:17, 16.34s/it] 91%|█████████▏| 1771/1940 [8:11:44<45:54, 16.30s/it] 91%|█████████▏| 1772/1940 [8:12:01<45:46, 16.35s/it] 91%|█████████▏| 1773/1940 [8:12:17<45:33, 16.37s/it] 91%|█████████▏| 1774/1940 [8:12:33<45:08, 16.32s/it] 91%|█████████▏| 1775/1940 [8:12:49<44:46, 16.28s/it] 92%|█████████▏| 1776/1940 [8:13:06<44:49, 16.40s/it] 92%|█████████▏| 1777/1940 [8:13:22<44:23, 16.34s/it] 92%|█████████▏| 1778/1940 [8:13:39<44:02, 16.31s/it] 92%|█████████▏| 1779/1940 [8:13:55<43:55, 16.37s/it] 92%|█████████▏| 1780/1940 [8:14:11<43:33, 16.33s/it]                                                     {'loss': 2.4746, 'learning_rate': 8.344787421847217e-07, 'epoch': 0.92}
 92%|█████████▏| 1780/1940 [8:14:11<43:33, 16.33s/it] 92%|█████████▏| 1781/1940 [8:14:28<43:13, 16.31s/it] 92%|█████████▏| 1782/1940 [8:14:44<43:05, 16.36s/it] 92%|█████████▏| 1783/1940 [8:15:00<42:52, 16.38s/it] 92%|█████████▏| 1784/1940 [8:15:17<42:27, 16.33s/it] 92%|█████████▏| 1785/1940 [8:15:33<42:04, 16.29s/it] 92%|█████████▏| 1786/1940 [8:15:49<41:58, 16.36s/it] 92%|█████████▏| 1787/1940 [8:16:06<41:36, 16.31s/it] 92%|█████████▏| 1788/1940 [8:16:22<41:14, 16.28s/it] 92%|█████████▏| 1789/1940 [8:16:39<41:16, 16.40s/it] 92%|█████████▏| 1790/1940 [8:16:55<40:52, 16.35s/it]                                                     {'loss': 2.468, 'learning_rate': 7.339261343510206e-07, 'epoch': 0.92}
 92%|█████████▏| 1790/1940 [8:16:55<40:52, 16.35s/it] 92%|█████████▏| 1791/1940 [8:17:11<40:33, 16.33s/it] 92%|█████████▏| 1792/1940 [8:17:28<40:24, 16.38s/it] 92%|█████████▏| 1793/1940 [8:17:44<40:11, 16.40s/it] 92%|█████████▏| 1794/1940 [8:18:00<39:47, 16.35s/it] 93%|█████████▎| 1795/1940 [8:18:16<39:23, 16.30s/it] 93%|█████████▎| 1796/1940 [8:18:33<39:13, 16.35s/it] 93%|█████████▎| 1797/1940 [8:18:49<38:50, 16.30s/it] 93%|█████████▎| 1798/1940 [8:19:05<38:31, 16.28s/it] 93%|█████████▎| 1799/1940 [8:19:22<38:32, 16.40s/it] 93%|█████████▎| 1800/1940 [8:19:38<38:07, 16.34s/it]                                                     {'loss': 2.4904, 'learning_rate': 6.397368838268497e-07, 'epoch': 0.93}
 93%|█████████▎| 1800/1940 [8:19:38<38:07, 16.34s/it] 93%|█████████▎| 1801/1940 [8:19:54<37:46, 16.30s/it] 93%|█████████▎| 1802/1940 [8:20:11<37:27, 16.29s/it] 93%|█████████▎| 1803/1940 [8:20:27<37:28, 16.41s/it] 93%|█████████▎| 1804/1940 [8:20:44<37:05, 16.36s/it] 93%|█████████▎| 1805/1940 [8:21:00<36:43, 16.32s/it] 93%|█████████▎| 1806/1940 [8:21:16<36:32, 16.36s/it] 93%|█████████▎| 1807/1940 [8:21:32<36:09, 16.31s/it] 93%|█████████▎| 1808/1940 [8:21:49<35:49, 16.29s/it] 93%|█████████▎| 1809/1940 [8:22:05<35:48, 16.40s/it] 93%|█████████▎| 1810/1940 [8:22:22<35:23, 16.34s/it]                                                     {'loss': 2.5017, 'learning_rate': 5.519356901107358e-07, 'epoch': 0.93}
 93%|█████████▎| 1810/1940 [8:22:22<35:23, 16.34s/it] 93%|█████████▎| 1811/1940 [8:22:38<35:02, 16.29s/it] 93%|█████████▎| 1812/1940 [8:22:54<34:42, 16.27s/it] 93%|█████████▎| 1813/1940 [8:23:10<34:35, 16.35s/it] 94%|█████████▎| 1814/1940 [8:23:27<34:15, 16.32s/it] 94%|█████████▎| 1815/1940 [8:23:43<33:58, 16.30s/it] 94%|█████████▎| 1816/1940 [8:24:00<33:56, 16.42s/it] 94%|█████████▎| 1817/1940 [8:24:16<33:32, 16.36s/it] 94%|█████████▎| 1818/1940 [8:24:32<33:11, 16.32s/it] 94%|█████████▍| 1819/1940 [8:24:49<33:00, 16.37s/it] 94%|█████████▍| 1820/1940 [8:25:05<32:47, 16.39s/it]                                                     {'loss': 2.4988, 'learning_rate': 4.7054557754402373e-07, 'epoch': 0.94}
 94%|█████████▍| 1820/1940 [8:25:05<32:47, 16.39s/it] 94%|█████████▍| 1821/1940 [8:25:21<32:24, 16.34s/it] 94%|█████████▍| 1822/1940 [8:25:38<32:04, 16.31s/it] 94%|█████████▍| 1823/1940 [8:25:54<31:53, 16.36s/it] 94%|█████████▍| 1824/1940 [8:26:10<31:34, 16.33s/it] 94%|█████████▍| 1825/1940 [8:26:27<31:15, 16.31s/it] 94%|█████████▍| 1826/1940 [8:26:43<31:12, 16.43s/it] 94%|█████████▍| 1827/1940 [8:26:59<30:48, 16.36s/it] 94%|█████████▍| 1828/1940 [8:27:16<30:26, 16.31s/it] 94%|█████████▍| 1829/1940 [8:27:32<30:14, 16.35s/it] 94%|█████████▍| 1830/1940 [8:27:48<30:01, 16.37s/it]                                                     {'loss': 2.4753, 'learning_rate': 3.9558788927314407e-07, 'epoch': 0.94}
 94%|█████████▍| 1830/1940 [8:27:48<30:01, 16.37s/it] 94%|█████████▍| 1831/1940 [8:28:05<29:39, 16.32s/it] 94%|█████████▍| 1832/1940 [8:28:21<29:19, 16.29s/it] 94%|█████████▍| 1833/1940 [8:28:37<29:07, 16.33s/it] 95%|█████████▍| 1834/1940 [8:28:54<28:47, 16.30s/it] 95%|█████████▍| 1835/1940 [8:29:10<28:28, 16.27s/it] 95%|█████████▍| 1836/1940 [8:29:27<28:26, 16.41s/it] 95%|█████████▍| 1837/1940 [8:29:43<28:05, 16.37s/it] 95%|█████████▍| 1838/1940 [8:29:59<27:45, 16.33s/it] 95%|█████████▍| 1839/1940 [8:30:15<27:33, 16.37s/it] 95%|█████████▍| 1840/1940 [8:30:32<27:14, 16.34s/it]                                                     {'loss': 2.4825, 'learning_rate': 3.270822816527325e-07, 'epoch': 0.95}
 95%|█████████▍| 1840/1940 [8:30:32<27:14, 16.34s/it] 95%|█████████▍| 1841/1940 [8:30:48<26:53, 16.30s/it] 95%|█████████▍| 1842/1940 [8:31:04<26:35, 16.28s/it] 95%|█████████▌| 1843/1940 [8:31:21<26:29, 16.39s/it] 95%|█████████▌| 1844/1940 [8:31:37<26:08, 16.33s/it] 95%|█████████▌| 1845/1940 [8:31:53<25:47, 16.29s/it] 95%|█████████▌| 1846/1940 [8:32:10<25:42, 16.41s/it] 95%|█████████▌| 1847/1940 [8:32:26<25:19, 16.34s/it] 95%|█████████▌| 1848/1940 [8:32:42<24:59, 16.30s/it] 95%|█████████▌| 1849/1940 [8:32:59<24:42, 16.29s/it] 95%|█████████▌| 1850/1940 [8:33:15<24:31, 16.35s/it]                                                     {'loss': 2.487, 'learning_rate': 2.650467190910999e-07, 'epoch': 0.95}
 95%|█████████▌| 1850/1940 [8:33:15<24:31, 16.35s/it] 95%|█████████▌| 1851/1940 [8:33:31<24:12, 16.32s/it] 95%|█████████▌| 1852/1940 [8:33:48<23:53, 16.29s/it] 96%|█████████▌| 1853/1940 [8:34:04<23:46, 16.40s/it] 96%|█████████▌| 1854/1940 [8:34:20<23:24, 16.33s/it] 96%|█████████▌| 1855/1940 [8:34:37<23:04, 16.29s/it] 96%|█████████▌| 1856/1940 [8:34:53<22:58, 16.41s/it] 96%|█████████▌| 1857/1940 [8:35:09<22:36, 16.35s/it] 96%|█████████▌| 1858/1940 [8:35:26<22:17, 16.31s/it] 96%|█████████▌| 1859/1940 [8:35:42<21:59, 16.29s/it] 96%|█████████▌| 1860/1940 [8:35:58<21:48, 16.35s/it]                                                     {'loss': 2.473, 'learning_rate': 2.094974693393731e-07, 'epoch': 0.96}
 96%|█████████▌| 1860/1940 [8:35:58<21:48, 16.35s/it] 96%|█████████▌| 1861/1940 [8:36:15<21:29, 16.32s/it] 96%|█████████▌| 1862/1940 [8:36:31<21:10, 16.29s/it] 96%|█████████▌| 1863/1940 [8:36:48<21:03, 16.41s/it] 96%|█████████▌| 1864/1940 [8:37:04<20:41, 16.34s/it] 96%|█████████▌| 1865/1940 [8:37:20<20:22, 16.30s/it] 96%|█████████▌| 1866/1940 [8:37:36<20:10, 16.36s/it] 96%|█████████▌| 1867/1940 [8:37:53<19:50, 16.31s/it] 96%|█████████▋| 1868/1940 [8:38:09<19:31, 16.28s/it] 96%|█████████▋| 1869/1940 [8:38:25<19:19, 16.33s/it] 96%|█████████▋| 1870/1940 [8:38:42<19:06, 16.38s/it]                                                     {'loss': 2.4721, 'learning_rate': 1.6044909922555974e-07, 'epoch': 0.96}
 96%|█████████▋| 1870/1940 [8:38:42<19:06, 16.38s/it] 96%|█████████▋| 1871/1940 [8:38:58<18:47, 16.34s/it] 96%|█████████▋| 1872/1940 [8:39:14<18:29, 16.31s/it] 97%|█████████▋| 1873/1940 [8:39:31<18:20, 16.42s/it] 97%|█████████▋| 1874/1940 [8:39:47<18:00, 16.37s/it] 97%|█████████▋| 1875/1940 [8:40:03<17:41, 16.32s/it] 97%|█████████▋| 1876/1940 [8:40:20<17:28, 16.38s/it] 97%|█████████▋| 1877/1940 [8:40:36<17:08, 16.33s/it] 97%|█████████▋| 1878/1940 [8:40:52<16:50, 16.30s/it] 97%|█████████▋| 1879/1940 [8:41:09<16:37, 16.35s/it] 97%|█████████▋| 1880/1940 [8:41:25<16:23, 16.39s/it]                                                     {'loss': 2.4971, 'learning_rate': 1.1791447083465134e-07, 'epoch': 0.97}
 97%|█████████▋| 1880/1940 [8:41:25<16:23, 16.39s/it] 97%|█████████▋| 1881/1940 [8:41:42<16:04, 16.34s/it] 97%|█████████▋| 1882/1940 [8:41:58<15:46, 16.32s/it] 97%|█████████▋| 1883/1940 [8:42:15<15:36, 16.44s/it] 97%|█████████▋| 1884/1940 [8:42:31<15:16, 16.36s/it] 97%|█████████▋| 1885/1940 [8:42:47<14:57, 16.32s/it] 97%|█████████▋| 1886/1940 [8:43:03<14:43, 16.36s/it] 97%|█████████▋| 1887/1940 [8:43:20<14:24, 16.31s/it] 97%|█████████▋| 1888/1940 [8:43:36<14:06, 16.28s/it] 97%|█████████▋| 1889/1940 [8:43:52<13:52, 16.33s/it] 97%|█████████▋| 1890/1940 [8:44:09<13:38, 16.36s/it]                                                     {'loss': 2.461, 'learning_rate': 8.190473813576572e-08, 'epoch': 0.97}
 97%|█████████▋| 1890/1940 [8:44:09<13:38, 16.36s/it] 97%|█████████▋| 1891/1940 [8:44:25<13:19, 16.32s/it] 98%|█████████▊| 1892/1940 [8:44:41<13:02, 16.30s/it] 98%|█████████▊| 1893/1940 [8:44:58<12:49, 16.37s/it] 98%|█████████▊| 1894/1940 [8:45:14<12:30, 16.32s/it] 98%|█████████▊| 1895/1940 [8:45:30<12:12, 16.29s/it] 98%|█████████▊| 1896/1940 [8:45:47<12:01, 16.40s/it] 98%|█████████▊| 1897/1940 [8:46:03<11:42, 16.34s/it] 98%|█████████▊| 1898/1940 [8:46:19<11:24, 16.30s/it] 98%|█████████▊| 1899/1940 [8:46:36<11:10, 16.34s/it] 98%|█████████▊| 1900/1940 [8:46:52<10:54, 16.37s/it]                                                     {'loss': 2.4933, 'learning_rate': 5.242934405720879e-08, 'epoch': 0.98}
 98%|█████████▊| 1900/1940 [8:46:52<10:54, 16.37s/it] 98%|█████████▊| 1901/1940 [8:47:08<10:36, 16.31s/it] 98%|█████████▊| 1902/1940 [8:47:24<10:18, 16.28s/it] 98%|█████████▊| 1903/1940 [8:47:41<10:04, 16.34s/it] 98%|█████████▊| 1904/1940 [8:47:57<09:47, 16.32s/it] 98%|█████████▊| 1905/1940 [8:48:13<09:30, 16.31s/it] 98%|█████████▊| 1906/1940 [8:48:30<09:18, 16.42s/it] 98%|█████████▊| 1907/1940 [8:48:46<08:59, 16.36s/it] 98%|█████████▊| 1908/1940 [8:49:03<08:42, 16.31s/it] 98%|█████████▊| 1909/1940 [8:49:19<08:24, 16.28s/it] 98%|█████████▊| 1910/1940 [8:49:36<08:12, 16.41s/it]                                                     {'loss': 2.483, 'learning_rate': 2.9496018010233274e-08, 'epoch': 0.98}
 98%|█████████▊| 1910/1940 [8:49:36<08:12, 16.41s/it] 99%|█████████▊| 1911/1940 [8:49:52<07:53, 16.34s/it] 99%|█████████▊| 1912/1940 [8:50:08<07:36, 16.30s/it] 99%|█████████▊| 1913/1940 [8:50:24<07:21, 16.34s/it] 99%|█████████▊| 1914/1940 [8:50:41<07:04, 16.31s/it] 99%|█████████▊| 1915/1940 [8:50:57<06:47, 16.29s/it] 99%|█████████▉| 1916/1940 [8:51:14<06:34, 16.43s/it] 99%|█████████▉| 1917/1940 [8:51:30<06:16, 16.36s/it] 99%|█████████▉| 1918/1940 [8:51:46<05:59, 16.32s/it] 99%|█████████▉| 1919/1940 [8:52:02<05:42, 16.29s/it] 99%|█████████▉| 1920/1940 [8:52:19<05:27, 16.35s/it]                                                     {'loss': 2.4817, 'learning_rate': 1.3110773862126669e-08, 'epoch': 0.99}
 99%|█████████▉| 1920/1940 [8:52:19<05:27, 16.35s/it] 99%|█████████▉| 1921/1940 [8:52:35<05:09, 16.30s/it] 99%|█████████▉| 1922/1940 [8:52:51<04:53, 16.28s/it] 99%|█████████▉| 1923/1940 [8:53:08<04:38, 16.40s/it] 99%|█████████▉| 1924/1940 [8:53:24<04:21, 16.34s/it] 99%|█████████▉| 1925/1940 [8:53:40<04:04, 16.30s/it] 99%|█████████▉| 1926/1940 [8:53:57<03:48, 16.35s/it] 99%|█████████▉| 1927/1940 [8:54:13<03:32, 16.38s/it] 99%|█████████▉| 1928/1940 [8:54:29<03:16, 16.35s/it] 99%|█████████▉| 1929/1940 [8:54:46<02:59, 16.33s/it] 99%|█████████▉| 1930/1940 [8:55:02<02:43, 16.38s/it]                                                     {'loss': 2.4691, 'learning_rate': 3.2779083591949478e-09, 'epoch': 0.99}
 99%|█████████▉| 1930/1940 [8:55:02<02:43, 16.38s/it]100%|█████████▉| 1931/1940 [8:55:18<02:26, 16.33s/it]100%|█████████▉| 1932/1940 [8:55:35<02:10, 16.30s/it]100%|█████████▉| 1933/1940 [8:55:51<01:54, 16.41s/it]100%|█████████▉| 1934/1940 [8:56:07<01:38, 16.35s/it]100%|█████████▉| 1935/1940 [8:56:24<01:21, 16.31s/it]100%|█████████▉| 1936/1940 [8:56:40<01:05, 16.35s/it]100%|█████████▉| 1937/1940 [8:56:57<00:49, 16.38s/it]100%|█████████▉| 1938/1940 [8:57:13<00:32, 16.32s/it]100%|█████████▉| 1939/1940 [8:57:29<00:16, 16.30s/it]100%|██████████| 1940/1940 [8:57:46<00:00, 16.36s/it]                                                     {'loss': 2.4778, 'learning_rate': 0.0, 'epoch': 1.0}
100%|██████████| 1940/1940 [8:57:46<00:00, 16.36s/it][INFO|trainer.py:1988] 2024-05-27 06:03:18,610 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 32266.0515, 'train_samples_per_second': 15.393, 'train_steps_per_second': 0.06, 'train_loss': 2.5204884411133444, 'epoch': 1.0}
100%|██████████| 1940/1940 [8:57:46<00:00, 16.36s/it]100%|██████████| 1940/1940 [8:57:46<00:00, 16.63s/it]
[INFO|trainer.py:2979] 2024-05-27 06:03:21,095 >> Saving model checkpoint to /home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe
[2024-05-27 06:03:23,377] [INFO] [launch.py:347:main] Process 1911093 exits successfully.
[2024-05-27 06:03:23,378] [INFO] [launch.py:347:main] Process 1911094 exits successfully.
[2024-05-27 06:03:23,378] [INFO] [launch.py:347:main] Process 1911095 exits successfully.
[INFO|tokenization_utils_base.py:2435] 2024-05-27 06:03:27,640 >> tokenizer config file saved in /home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-05-27 06:03:27,640 >> Special tokens file saved in /home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/special_tokens_map.json
[INFO|tokenization_utils_base.py:2495] 2024-05-27 06:03:27,640 >> added tokens file saved in /home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/added_tokens.json
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     2.5205
  train_runtime            = 8:57:46.05
  train_samples_per_second =     15.393
  train_steps_per_second   =       0.06
Figure saved: /home/yangdezhao/ydz_temp/checkpoints/moe/qwen/qwen-moe/training_loss.png
05/27/2024 06:03:28 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-05-27 06:03:28,251 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-05-27 06:03:30,387] [INFO] [launch.py:347:main] Process 1911092 exits successfully.
