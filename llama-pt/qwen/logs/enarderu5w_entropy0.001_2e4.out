[2024-05-30 10:30:51,131] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-30 10:31:00,857] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-30 10:31:00,882] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/wzjsz/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/wangzj/LLaMA-Factory-hx/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/nfs04/wangzj/models/Qwen1.5-1.8B --adapter_name_or_path /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe --train_only_router --moe_router_type top2 --polarization_func entropy --polarization_coef 0.001 --use_polarization_loss --do_train --cache_dir /home/nfs04/wangzj/dataset/cache --dataset slimpajam_1b,ar_2b,de_2b,ru_2b --max_samples 50000 --preprocessing_num_workers 16 --cutoff_len 512 --finetuning_type moe --output_dir /home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w --overwrite_output_dir --per_device_train_batch_size 16 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 100 --save_steps 10000 --save_only_model --learning_rate 2e-4 --num_train_epochs 1.0 --plot_loss --fp16
[2024-05-30 10:31:05,590] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-30 10:31:11,871] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-30 10:31:11,871] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-30 10:31:11,871] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-30 10:31:11,871] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-30 10:31:11,871] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-30 10:31:57,432] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-30 10:31:57,432] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-30 10:31:57,432] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-30 10:31:57,432] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-30 10:32:13,185] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-30 10:32:13,185] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-30 10:32:13,185] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-30 10:32:13,185] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-30 10:32:13,185] [INFO] [comm.py:637:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
05/30/2024 10:32:13 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/30/2024 10:32:13 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/30/2024 10:32:13 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory-hx/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w/runs/May30_10-32-13_v100-23,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/30/2024 10:32:13 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory-hx/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w/runs/May30_10-32-13_v100-23,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/30/2024 10:32:13 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/30/2024 10:32:13 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/30/2024 10:32:13 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory-hx/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w/runs/May30_10-32-13_v100-23,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/30/2024 10:32:13 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory-hx/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w/runs/May30_10-32-13_v100-23,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-05-30 10:32:13,215 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2027] 2024-05-30 10:32:13,215 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2027] 2024-05-30 10:32:13,215 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-05-30 10:32:13,215 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-05-30 10:32:13,215 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-05-30 10:32:13,215 >> loading file tokenizer.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-05-30 10:32:13,503 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:727] 2024-05-30 10:32:13,505 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/config.json
[INFO|configuration_utils.py:792] 2024-05-30 10:32:13,506 >> Model config Qwen2Config {
  "_name_or_path": "/home/nfs04/wangzj/models/Qwen1.5-1.8B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3334] 2024-05-30 10:32:13,798 >> loading weights file /home/nfs04/wangzj/models/Qwen1.5-1.8B/model.safetensors
[INFO|modeling_utils.py:1459] 2024-05-30 10:32:31,649 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:827] 2024-05-30 10:32:31,655 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}

[INFO|modeling_utils.py:4070] 2024-05-30 10:32:51,590 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4078] 2024-05-30 10:32:51,590 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/nfs04/wangzj/models/Qwen1.5-1.8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-05-30 10:32:51,595 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/generation_config.json
[INFO|configuration_utils.py:827] 2024-05-30 10:32:51,595 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "max_new_tokens": 2048
}

05/30/2024 10:32:51 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/30/2024 10:32:51 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/30/2024 10:32:51 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
05/30/2024 10:32:51 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/30/2024 10:32:51 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/30/2024 10:32:51 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
05/30/2024 10:32:51 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/30/2024 10:32:51 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/30/2024 10:32:51 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
05/30/2024 10:32:51 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/30/2024 10:32:51 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/30/2024 10:32:51 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top2', save_router_logits=True, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=0.001, use_polarization_loss=True, polarization_func='entropy')
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
05/30/2024 10:33:18 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top2', save_router_logits=True, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=0.001, use_polarization_loss=True, polarization_func='entropy')
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
05/30/2024 10:33:18 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top2', save_router_logits=True, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=0.001, use_polarization_loss=True, polarization_func='entropy')
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
05/30/2024 10:33:18 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top2', save_router_logits=True, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=0.001, use_polarization_loss=True, polarization_func='entropy')
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
05/30/2024 10:33:18 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
05/30/2024 10:33:18 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
05/30/2024 10:33:58 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Using custom data configuration default-5f559cb30c539bf4
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 50000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Process #0 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00000_of_00016.arrow
Process #1 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00001_of_00016.arrow
Process #2 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00002_of_00016.arrow
Process #3 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00003_of_00016.arrow
Process #4 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00004_of_00016.arrow
Process #5 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00005_of_00016.arrow
Process #6 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00006_of_00016.arrow
Process #7 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00007_of_00016.arrow
Process #8 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00008_of_00016.arrow
Process #9 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00009_of_00016.arrow
Process #10 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00010_of_00016.arrow
Process #11 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00011_of_00016.arrow
Process #12 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00012_of_00016.arrow
Process #13 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00013_of_00016.arrow
Process #14 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00014_of_00016.arrow
Process #15 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_00015_of_00016.arrow
Loading cached processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-95d8a670eef2051d_*_of_00016.arrow
Concatenating 16 shards
05/30/2024 10:36:20 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
Using custom data configuration default-51c779b67edea835
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Downloading and preparing dataset json/default to /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 5801 examples [00:00, 39112.21 examples/s]Generating train split: 13346 examples [00:00, 49427.54 examples/s]Generating train split: 21010 examples [00:00, 55796.01 examples/s]Generating train split: 28588 examples [00:00, 58432.62 examples/s]Generating train split: 36089 examples [00:00, 57325.98 examples/s]Generating train split: 43862 examples [00:00, 61955.66 examples/s]Generating train split: 51423 examples [00:00, 62220.26 examples/s]Generating train split: 59110 examples [00:01, 62733.40 examples/s]Generating train split: 66635 examples [00:01, 56310.96 examples/s]Generating train split: 74541 examples [00:01, 58523.52 examples/s]Generating train split: 82188 examples [00:01, 59331.34 examples/s]Generating train split: 89914 examples [00:01, 58374.38 examples/s]Generating train split: 97658 examples [00:06, 5292.37 examples/s] Generating train split: 105317 examples [00:06, 7338.91 examples/s]Generating train split: 112952 examples [00:06, 9927.37 examples/s]Generating train split: 120781 examples [00:06, 13365.50 examples/s]Generating train split: 128585 examples [00:06, 17422.39 examples/s]Generating train split: 136292 examples [00:06, 22032.27 examples/s]Generating train split: 143821 examples [00:06, 27157.67 examples/s]Generating train split: 151666 examples [00:06, 32784.60 examples/s]Generating train split: 159196 examples [00:07, 38195.01 examples/s]Generating train split: 166789 examples [00:07, 43966.99 examples/s]Generating train split: 174399 examples [00:07, 48385.99 examples/s]Generating train split: 181953 examples [00:07, 50962.58 examples/s]Generating train split: 189753 examples [00:12, 4674.17 examples/s] Generating train split: 197479 examples [00:12, 6483.84 examples/s]Generating train split: 205227 examples [00:12, 8930.52 examples/s]Generating train split: 213063 examples [00:12, 12031.63 examples/s]Generating train split: 220695 examples [00:13, 15878.69 examples/s]Generating train split: 228317 examples [00:13, 20246.92 examples/s]Generating train split: 236248 examples [00:13, 25485.70 examples/s]Generating train split: 244086 examples [00:13, 31355.05 examples/s]Generating train split: 252100 examples [00:13, 37380.83 examples/s]Generating train split: 259825 examples [00:13, 41753.79 examples/s]Generating train split: 267381 examples [00:13, 46044.26 examples/s]Generating train split: 275027 examples [00:13, 49876.40 examples/s]Generating train split: 282674 examples [00:18, 5153.55 examples/s] Generating train split: 290024 examples [00:18, 6969.85 examples/s]Generating train split: 297667 examples [00:18, 9505.34 examples/s]Generating train split: 304994 examples [00:18, 12475.12 examples/s]Generating train split: 312585 examples [00:19, 16329.28 examples/s]Generating train split: 320310 examples [00:19, 21242.62 examples/s]Generating train split: 327878 examples [00:19, 26728.53 examples/s]Generating train split: 335334 examples [00:19, 32352.29 examples/s]Generating train split: 343110 examples [00:19, 37745.46 examples/s]Generating train split: 350873 examples [00:19, 43196.57 examples/s]Generating train split: 358685 examples [00:19, 49085.97 examples/s]Generating train split: 366380 examples [00:19, 54071.23 examples/s]Generating train split: 373928 examples [00:25, 4099.79 examples/s] Generating train split: 381522 examples [00:25, 5650.14 examples/s]Generating train split: 389107 examples [00:26, 7775.16 examples/s]Generating train split: 396682 examples [00:26, 10504.69 examples/s]Generating train split: 404345 examples [00:26, 13883.89 examples/s]Generating train split: 412160 examples [00:26, 18212.93 examples/s]Generating train split: 419787 examples [00:26, 22883.89 examples/s]Generating train split: 427334 examples [00:26, 28024.84 examples/s]Generating train split: 434951 examples [00:26, 33497.82 examples/s]Generating train split: 442547 examples [00:26, 39718.63 examples/s]Generating train split: 450408 examples [00:27, 46493.70 examples/s]Generating train split: 458213 examples [00:27, 51333.22 examples/s]Generating train split: 465916 examples [00:31, 5297.17 examples/s] Generating train split: 473550 examples [00:31, 7286.55 examples/s]Generating train split: 481050 examples [00:31, 9902.35 examples/s]Generating train split: 488559 examples [00:31, 13263.58 examples/s]Generating train split: 496084 examples [00:32, 17384.46 examples/s]Generating train split: 503751 examples [00:32, 22009.37 examples/s]Generating train split: 511408 examples [00:32, 27469.12 examples/s]Generating train split: 519010 examples [00:32, 33760.90 examples/s]Generating train split: 526747 examples [00:32, 39414.93 examples/s]Generating train split: 534466 examples [00:32, 43042.47 examples/s]Generating train split: 542170 examples [00:32, 47210.62 examples/s]Generating train split: 549741 examples [00:32, 48646.28 examples/s]Generating train split: 557602 examples [00:37, 5270.62 examples/s] Generating train split: 565349 examples [00:37, 7259.83 examples/s]Generating train split: 572869 examples [00:37, 9790.30 examples/s]Generating train split: 580497 examples [00:37, 13049.45 examples/s]Generating train split: 588218 examples [00:38, 17021.29 examples/s]Generating train split: 595756 examples [00:38, 21784.05 examples/s]Generating train split: 603358 examples [00:38, 27341.16 examples/s]Generating train split: 610955 examples [00:38, 32773.08 examples/s]Generating train split: 618800 examples [00:38, 38783.98 examples/s]Generating train split: 626587 examples [00:38, 44926.69 examples/s]Generating train split: 634460 examples [00:38, 48972.06 examples/s]Generating train split: 642032 examples [00:38, 51593.09 examples/s]Generating train split: 649789 examples [00:43, 5286.62 examples/s] Generating train split: 657500 examples [00:43, 7289.58 examples/s]Generating train split: 665029 examples [00:43, 9844.19 examples/s]Generating train split: 672758 examples [00:43, 13090.23 examples/s]Generating train split: 680288 examples [00:43, 16974.62 examples/s]Generating train split: 687865 examples [00:43, 21763.99 examples/s]Generating train split: 695626 examples [00:44, 26912.48 examples/s]Generating train split: 703173 examples [00:44, 32030.23 examples/s]Generating train split: 711079 examples [00:44, 37587.72 examples/s]Generating train split: 718823 examples [00:44, 43499.53 examples/s]Generating train split: 726312 examples [00:44, 46596.15 examples/s]Generating train split: 733913 examples [00:44, 50139.91 examples/s]Generating train split: 743793 examples [00:51, 4120.36 examples/s] Generating train split: 751468 examples [00:51, 5595.93 examples/s]Generating train split: 759035 examples [00:51, 7521.32 examples/s]Generating train split: 766846 examples [00:51, 10216.77 examples/s]Generating train split: 774500 examples [00:51, 13609.89 examples/s]Generating train split: 782300 examples [00:51, 17942.89 examples/s]Generating train split: 789908 examples [00:51, 22833.52 examples/s]Generating train split: 797601 examples [00:51, 27970.75 examples/s]Generating train split: 805265 examples [00:52, 33089.96 examples/s]Generating train split: 812995 examples [00:52, 38145.97 examples/s]Generating train split: 820808 examples [00:52, 44242.08 examples/s]Generating train split: 828546 examples [00:52, 47762.37 examples/s]Generating train split: 838235 examples [00:57, 5642.79 examples/s] Generating train split: 845991 examples [00:57, 7625.53 examples/s]Generating train split: 853558 examples [00:57, 10121.93 examples/s]Generating train split: 861294 examples [00:57, 13436.72 examples/s]Generating train split: 868807 examples [00:57, 17287.25 examples/s]Generating train split: 876325 examples [00:57, 21573.21 examples/s]Generating train split: 883854 examples [00:57, 26374.83 examples/s]Generating train split: 891468 examples [00:57, 31653.75 examples/s]Generating train split: 899070 examples [00:58, 36002.76 examples/s]Generating train split: 906843 examples [00:58, 40899.26 examples/s]Generating train split: 914534 examples [00:58, 45131.54 examples/s]Generating train split: 922181 examples [01:02, 5211.35 examples/s] Generating train split: 929798 examples [01:02, 7166.82 examples/s]Generating train split: 937486 examples [01:03, 9771.90 examples/s]Generating train split: 945037 examples [01:03, 13038.59 examples/s]Generating train split: 952522 examples [01:03, 16862.98 examples/s]Generating train split: 960252 examples [01:03, 21694.18 examples/s]Generating train split: 968093 examples [01:03, 26989.08 examples/s]Generating train split: 975597 examples [01:03, 31969.21 examples/s]Generating train split: 983458 examples [01:03, 37140.61 examples/s]Generating train split: 991297 examples [01:03, 42011.40 examples/s]Generating train split: 998758 examples [01:04, 46558.38 examples/s]Generating train split: 1006304 examples [01:04, 48046.15 examples/s]Generating train split: 1013783 examples [01:08, 5190.77 examples/s] Generating train split: 1021208 examples [01:08, 7118.03 examples/s]Generating train split: 1028773 examples [01:08, 9701.04 examples/s]Generating train split: 1036439 examples [01:09, 13028.78 examples/s]Generating train split: 1043956 examples [01:09, 17006.51 examples/s]Generating train split: 1051560 examples [01:09, 21813.70 examples/s]Generating train split: 1059194 examples [01:09, 27295.76 examples/s]Generating train split: 1066771 examples [01:09, 32757.50 examples/s]Generating train split: 1074316 examples [01:09, 38270.62 examples/s]Generating train split: 1082106 examples [01:09, 43796.54 examples/s]Generating train split: 1089821 examples [01:09, 47687.72 examples/s]Generating train split: 1097710 examples [01:10, 51852.03 examples/s]Generating train split: 1105185 examples [01:14, 5298.07 examples/s] Generating train split: 1112864 examples [01:14, 7295.97 examples/s]Generating train split: 1120473 examples [01:14, 9918.25 examples/s]Generating train split: 1128130 examples [01:14, 13220.45 examples/s]Generating train split: 1135795 examples [01:15, 17251.22 examples/s]Generating train split: 1143285 examples [01:15, 21930.63 examples/s]Generating train split: 1150837 examples [01:15, 26988.05 examples/s]Generating train split: 1158588 examples [01:15, 32911.53 examples/s]Generating train split: 1166168 examples [01:15, 37764.36 examples/s]Generating train split: 1173662 examples [01:15, 41558.79 examples/s]Generating train split: 1181426 examples [01:15, 46519.61 examples/s]Generating train split: 1189130 examples [01:15, 50014.71 examples/s]Generating train split: 1196720 examples [01:20, 4824.66 examples/s] Generating train split: 1204363 examples [01:20, 6664.76 examples/s]Generating train split: 1211984 examples [01:21, 9099.96 examples/s]Generating train split: 1219453 examples [01:21, 12085.13 examples/s]Generating train split: 1227125 examples [01:21, 15968.15 examples/s]Generating train split: 1234654 examples [01:21, 20416.91 examples/s]Generating train split: 1242324 examples [01:21, 25231.35 examples/s]Generating train split: 1249817 examples [01:21, 30182.74 examples/s]Generating train split: 1257392 examples [01:21, 35276.93 examples/s]Generating train split: 1265291 examples [01:22, 41228.02 examples/s]Generating train split: 1272867 examples [01:22, 43856.34 examples/s]Generating train split: 1280610 examples [01:22, 49216.34 examples/s]Generating train split: 1288397 examples [01:26, 5290.88 examples/s] Generating train split: 1296036 examples [01:26, 7269.63 examples/s]Generating train split: 1303866 examples [01:27, 9904.82 examples/s]Generating train split: 1311425 examples [01:27, 13233.58 examples/s]Generating train split: 1319282 examples [01:27, 17516.05 examples/s]Generating train split: 1326662 examples [01:27, 22313.53 examples/s]Generating train split: 1334215 examples [01:27, 27918.88 examples/s]Generating train split: 1341607 examples [01:27, 33277.10 examples/s]Generating train split: 1349129 examples [01:27, 38805.20 examples/s]Generating train split: 1356740 examples [01:27, 44373.26 examples/s]Generating train split: 1364409 examples [01:27, 48255.08 examples/s]Generating train split: 1371892 examples [01:28, 52673.88 examples/s]Generating train split: 1379657 examples [01:32, 5246.44 examples/s] Generating train split: 1387353 examples [01:32, 7261.23 examples/s]Generating train split: 1395021 examples [01:32, 9936.19 examples/s]Generating train split: 1402582 examples [01:32, 13312.01 examples/s]Generating train split: 1410230 examples [01:33, 17489.13 examples/s]Generating train split: 1417937 examples [01:33, 22635.14 examples/s]Generating train split: 1425509 examples [01:33, 28119.84 examples/s]Generating train split: 1433212 examples [01:33, 34321.72 examples/s]Generating train split: 1440741 examples [01:33, 39445.32 examples/s]Generating train split: 1448311 examples [01:33, 44713.83 examples/s]Generating train split: 1455762 examples [01:33, 48943.98 examples/s]Generating train split: 1463369 examples [01:33, 53053.29 examples/s]Generating train split: 1470915 examples [01:38, 5236.80 examples/s] Generating train split: 1478548 examples [01:38, 7222.90 examples/s]Generating train split: 1486367 examples [01:38, 9919.84 examples/s]Generating train split: 1493939 examples [01:38, 13195.22 examples/s]Generating train split: 1501402 examples [01:38, 17050.26 examples/s]Generating train split: 1509026 examples [01:38, 21662.25 examples/s]Generating train split: 1516514 examples [01:39, 25944.93 examples/s]Generating train split: 1524182 examples [01:39, 30602.94 examples/s]Generating train split: 1531931 examples [01:39, 36603.25 examples/s]Generating train split: 1539557 examples [01:39, 41842.41 examples/s]Generating train split: 1547122 examples [01:39, 45352.83 examples/s]Generating train split: 1554850 examples [01:39, 48038.69 examples/s]Generating train split: 1562586 examples [01:44, 5249.25 examples/s] Generating train split: 1570286 examples [01:44, 7209.70 examples/s]Generating train split: 1578233 examples [01:44, 9930.89 examples/s]Generating train split: 1585802 examples [01:44, 13171.52 examples/s]Generating train split: 1593443 examples [01:44, 17106.14 examples/s]Generating train split: 1601198 examples [01:44, 21891.62 examples/s]Generating train split: 1608868 examples [01:45, 27191.98 examples/s]Generating train split: 1616377 examples [01:45, 32576.95 examples/s]Generating train split: 1623952 examples [01:45, 37916.66 examples/s]Generating train split: 1631621 examples [01:45, 42247.03 examples/s]Generating train split: 1639436 examples [01:45, 46471.31 examples/s]Generating train split: 1646981 examples [01:45, 50084.75 examples/s]Generating train split: 1654513 examples [01:50, 4882.32 examples/s] Generating train split: 1661943 examples [01:50, 6695.68 examples/s]Generating train split: 1669563 examples [01:50, 9176.43 examples/s]Generating train split: 1677505 examples [01:50, 12545.31 examples/s]Generating train split: 1685218 examples [01:50, 16616.16 examples/s]Generating train split: 1692823 examples [01:51, 21343.60 examples/s]Generating train split: 1700419 examples [01:51, 26936.43 examples/s]Generating train split: 1700419 examples [01:53, 14982.37 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.
Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
Process #0 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00000_of_00016.arrow
Process #1 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00001_of_00016.arrow
Process #2 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00002_of_00016.arrow
Process #3 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00003_of_00016.arrow
Process #4 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00004_of_00016.arrow
Process #5 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00005_of_00016.arrow
Process #6 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00006_of_00016.arrow
Process #7 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00007_of_00016.arrow
Process #8 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00008_of_00016.arrow
Process #9 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00009_of_00016.arrow
Process #10 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00010_of_00016.arrow
Process #11 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00011_of_00016.arrow
Process #12 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00012_of_00016.arrow
Process #13 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00013_of_00016.arrow
Process #14 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00014_of_00016.arrow
Process #15 will write at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00015_of_00016.arrow
Spawning 16 processes
Converting format of dataset (num_proc=16):   0%|          | 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00004_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00002_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00001_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00000_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00006_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00003_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00007_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00012_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00011_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00008_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00009_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00014_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00010_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00005_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00015_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-51c779b67edea835/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e603e5c2737b929_00013_of_00016.arrow
Converting format of dataset (num_proc=16):   2%|▏         | 1000/50000 [00:00<00:11, 4092.01 examples/s]Converting format of dataset (num_proc=16):  54%|█████▍    | 27000/50000 [00:00<00:00, 98065.78 examples/s]Converting format of dataset (num_proc=16):  86%|████████▌ | 42875/50000 [00:02<00:00, 13496.01 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 50000/50000 [00:05<00:00, 9327.99 examples/s] 
Concatenating 16 shards
05/30/2024 10:40:58 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Using custom data configuration default-e190469ed47188cb
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Downloading and preparing dataset json/default to /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 8649 examples [00:00, 55757.13 examples/s]Generating train split: 20001 examples [00:00, 73251.32 examples/s]Generating train split: 31420 examples [00:00, 84931.03 examples/s]Generating train split: 42910 examples [00:00, 86680.23 examples/s]Generating train split: 54536 examples [00:00, 89153.63 examples/s]Generating train split: 66086 examples [00:00, 91365.23 examples/s]Generating train split: 77663 examples [00:00, 96956.84 examples/s]Generating train split: 89346 examples [00:00, 98805.77 examples/s]Generating train split: 100573 examples [00:01, 98944.14 examples/s]Generating train split: 111617 examples [00:01, 98571.73 examples/s]Generating train split: 123293 examples [00:01, 102879.89 examples/s]Generating train split: 134844 examples [00:01, 105034.81 examples/s]Generating train split: 148983 examples [00:06, 8080.54 examples/s]  Generating train split: 160278 examples [00:06, 10923.35 examples/s]Generating train split: 171657 examples [00:06, 14711.59 examples/s]Generating train split: 183031 examples [00:06, 19427.37 examples/s]Generating train split: 194326 examples [00:06, 25326.02 examples/s]Generating train split: 205552 examples [00:06, 32116.24 examples/s]Generating train split: 217199 examples [00:06, 39993.23 examples/s]Generating train split: 228370 examples [00:07, 46610.10 examples/s]Generating train split: 239892 examples [00:07, 55471.35 examples/s]Generating train split: 251402 examples [00:07, 63848.57 examples/s]Generating train split: 263037 examples [00:07, 72073.32 examples/s]Generating train split: 274584 examples [00:07, 77488.53 examples/s]Generating train split: 289051 examples [00:12, 8321.89 examples/s] Generating train split: 300731 examples [00:12, 11274.47 examples/s]Generating train split: 312269 examples [00:12, 15175.38 examples/s]Generating train split: 323931 examples [00:12, 20087.59 examples/s]Generating train split: 335151 examples [00:12, 25810.68 examples/s]Generating train split: 346777 examples [00:12, 33175.13 examples/s]Generating train split: 358361 examples [00:12, 41318.94 examples/s]Generating train split: 370155 examples [00:13, 49716.70 examples/s]Generating train split: 381564 examples [00:13, 57766.42 examples/s]Generating train split: 393258 examples [00:13, 66527.35 examples/s]Generating train split: 404686 examples [00:13, 71577.51 examples/s]Generating train split: 416498 examples [00:13, 77597.17 examples/s]Generating train split: 430879 examples [00:18, 8340.28 examples/s] Generating train split: 442445 examples [00:18, 11259.40 examples/s]Generating train split: 454002 examples [00:18, 15121.16 examples/s]Generating train split: 465330 examples [00:18, 19893.65 examples/s]Generating train split: 476710 examples [00:18, 26032.89 examples/s]Generating train split: 488043 examples [00:18, 32845.36 examples/s]Generating train split: 499666 examples [00:18, 41118.14 examples/s]Generating train split: 511615 examples [00:18, 49282.48 examples/s]Generating train split: 523350 examples [00:19, 58769.32 examples/s]Generating train split: 535170 examples [00:19, 66211.78 examples/s]Generating train split: 546687 examples [00:19, 74859.60 examples/s]Generating train split: 558118 examples [00:19, 81050.87 examples/s]Generating train split: 572558 examples [00:24, 8388.37 examples/s] Generating train split: 583701 examples [00:24, 11190.07 examples/s]Generating train split: 595144 examples [00:24, 14965.74 examples/s]Generating train split: 607066 examples [00:24, 20090.04 examples/s]Generating train split: 618345 examples [00:24, 26144.97 examples/s]Generating train split: 629926 examples [00:24, 33399.60 examples/s]Generating train split: 641344 examples [00:24, 41076.60 examples/s]Generating train split: 652927 examples [00:24, 47674.08 examples/s]Generating train split: 664317 examples [00:25, 55246.62 examples/s]Generating train split: 675953 examples [00:25, 63392.10 examples/s]Generating train split: 687762 examples [00:25, 68731.11 examples/s]Generating train split: 698939 examples [00:25, 75879.94 examples/s]Generating train split: 713368 examples [00:30, 8338.93 examples/s] Generating train split: 724988 examples [00:30, 11289.28 examples/s]Generating train split: 736581 examples [00:30, 15143.07 examples/s]Generating train split: 748417 examples [00:30, 20092.80 examples/s]Generating train split: 760135 examples [00:30, 26378.62 examples/s]Generating train split: 771539 examples [00:30, 33735.97 examples/s]Generating train split: 783112 examples [00:30, 41510.95 examples/s]Generating train split: 794504 examples [00:30, 49374.20 examples/s]Generating train split: 805886 examples [00:31, 57191.51 examples/s]Generating train split: 817520 examples [00:31, 65284.23 examples/s]Generating train split: 829356 examples [00:31, 74082.99 examples/s]Generating train split: 840718 examples [00:31, 81443.25 examples/s]Generating train split: 855031 examples [00:36, 8047.41 examples/s] Generating train split: 866489 examples [00:36, 10860.59 examples/s]Generating train split: 877562 examples [00:36, 14418.53 examples/s]Generating train split: 889094 examples [00:36, 19133.78 examples/s]Generating train split: 900460 examples [00:36, 25057.82 examples/s]Generating train split: 912003 examples [00:36, 32202.64 examples/s]Generating train split: 923211 examples [00:36, 39943.02 examples/s]Generating train split: 934592 examples [00:37, 48607.93 examples/s]Generating train split: 946266 examples [00:37, 57929.16 examples/s]Generating train split: 957998 examples [00:37, 67550.57 examples/s]Generating train split: 969619 examples [00:37, 72229.26 examples/s]Generating train split: 980830 examples [00:37, 79267.12 examples/s]Generating train split: 995171 examples [00:42, 8296.94 examples/s] Generating train split: 1006542 examples [00:42, 11201.49 examples/s]Generating train split: 1018134 examples [00:42, 15186.25 examples/s]Generating train split: 1029972 examples [00:42, 20367.55 examples/s]Generating train split: 1041754 examples [00:42, 26966.15 examples/s]Generating train split: 1053249 examples [00:42, 34624.36 examples/s]Generating train split: 1065118 examples [00:42, 43934.65 examples/s]Generating train split: 1076477 examples [00:42, 52948.09 examples/s]Generating train split: 1088076 examples [00:42, 60194.70 examples/s]Generating train split: 1099498 examples [00:43, 69402.55 examples/s]Generating train split: 1111118 examples [00:43, 77331.76 examples/s]Generating train split: 1122546 examples [00:43, 78640.33 examples/s]Generating train split: 1137062 examples [00:48, 8063.36 examples/s] Generating train split: 1148880 examples [00:48, 10947.72 examples/s]Generating train split: 1160372 examples [00:48, 14743.47 examples/s]Generating train split: 1171632 examples [00:48, 19453.21 examples/s]Generating train split: 1183058 examples [00:48, 25561.31 examples/s]Generating train split: 1194503 examples [00:48, 32791.40 examples/s]Generating train split: 1206317 examples [00:48, 41419.14 examples/s]Generating train split: 1217932 examples [00:48, 49472.56 examples/s]Generating train split: 1229673 examples [00:49, 59184.70 examples/s]Generating train split: 1241160 examples [00:49, 66340.62 examples/s]Generating train split: 1252934 examples [00:49, 72228.95 examples/s]Generating train split: 1264487 examples [00:49, 80578.60 examples/s]Generating train split: 1278932 examples [00:54, 8385.78 examples/s] Generating train split: 1290564 examples [00:54, 11319.98 examples/s]Generating train split: 1302025 examples [00:54, 15059.07 examples/s]Generating train split: 1313409 examples [00:54, 19814.59 examples/s]Generating train split: 1330837 examples [00:54, 29182.73 examples/s]Generating train split: 1345000 examples [00:54, 37700.42 examples/s]Generating train split: 1356488 examples [00:54, 44661.92 examples/s]Generating train split: 1368052 examples [00:54, 50615.88 examples/s]Generating train split: 1379759 examples [00:55, 58784.07 examples/s]Generating train split: 1390843 examples [00:55, 65433.61 examples/s]Generating train split: 1402099 examples [00:55, 69700.18 examples/s]Generating train split: 1413698 examples [00:59, 7842.70 examples/s] Generating train split: 1425414 examples [01:00, 10833.88 examples/s]Generating train split: 1436982 examples [01:00, 14710.74 examples/s]Generating train split: 1448286 examples [01:00, 19738.03 examples/s]Generating train split: 1462684 examples [01:00, 27676.83 examples/s]Generating train split: 1474351 examples [01:00, 34943.25 examples/s]Generating train split: 1485774 examples [01:00, 42875.79 examples/s]Generating train split: 1497143 examples [01:00, 51438.01 examples/s]Generating train split: 1508684 examples [01:00, 60738.92 examples/s]Generating train split: 1520302 examples [01:01, 66101.80 examples/s]Generating train split: 1531926 examples [01:01, 74900.06 examples/s]Generating train split: 1543645 examples [01:01, 78901.89 examples/s]Generating train split: 1555113 examples [01:05, 7675.00 examples/s] Generating train split: 1566459 examples [01:06, 10508.24 examples/s]Generating train split: 1578093 examples [01:06, 14410.47 examples/s]Generating train split: 1589697 examples [01:06, 19394.73 examples/s]Generating train split: 1601162 examples [01:06, 25569.00 examples/s]Generating train split: 1612707 examples [01:06, 32601.70 examples/s]Generating train split: 1624206 examples [01:06, 40241.15 examples/s]Generating train split: 1635612 examples [01:06, 47451.68 examples/s]Generating train split: 1647185 examples [01:06, 56008.84 examples/s]Generating train split: 1658732 examples [01:07, 64227.60 examples/s]Generating train split: 1669957 examples [01:07, 69592.80 examples/s]Generating train split: 1681400 examples [01:07, 74759.51 examples/s]Generating train split: 1693141 examples [01:07, 77657.62 examples/s]Generating train split: 1707347 examples [01:12, 8279.86 examples/s] Generating train split: 1718960 examples [01:12, 11193.67 examples/s]Generating train split: 1733171 examples [01:12, 16003.50 examples/s]Generating train split: 1744807 examples [01:12, 20760.42 examples/s]Generating train split: 1759150 examples [01:12, 28484.50 examples/s]Generating train split: 1773715 examples [01:12, 37708.95 examples/s]Generating train split: 1785128 examples [01:12, 45129.28 examples/s]Generating train split: 1796841 examples [01:12, 52756.37 examples/s]Generating train split: 1808489 examples [01:13, 61918.94 examples/s]Generating train split: 1819795 examples [01:13, 70076.88 examples/s]Generating train split: 1833849 examples [01:13, 79534.28 examples/s]Generating train split: 1848132 examples [01:19, 7131.26 examples/s] Generating train split: 1862432 examples [01:19, 10145.28 examples/s]Generating train split: 1874028 examples [01:19, 13362.23 examples/s]Generating train split: 1885614 examples [01:19, 17463.20 examples/s]Generating train split: 1897225 examples [01:19, 22678.69 examples/s]Generating train split: 1908754 examples [01:19, 29096.48 examples/s]Generating train split: 1920048 examples [01:19, 35794.89 examples/s]Generating train split: 1931705 examples [01:19, 44558.30 examples/s]Generating train split: 1943245 examples [01:20, 53327.14 examples/s]Generating train split: 1948786 examples [01:25, 22865.27 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.
Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
Process #0 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00000_of_00016.arrow
Process #1 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00001_of_00016.arrow
Process #2 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00002_of_00016.arrow
Process #3 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00003_of_00016.arrow
Process #4 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00004_of_00016.arrow
Process #5 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00005_of_00016.arrow
Process #6 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00006_of_00016.arrow
Process #7 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00007_of_00016.arrow
Process #8 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00008_of_00016.arrow
Process #9 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00009_of_00016.arrow
Process #10 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00010_of_00016.arrow
Process #11 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00011_of_00016.arrow
Process #12 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00012_of_00016.arrow
Process #13 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00013_of_00016.arrow
Process #14 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00014_of_00016.arrow
Process #15 will write at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00015_of_00016.arrow
Spawning 16 processes
Converting format of dataset (num_proc=16):   0%|          | 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00000_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00002_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00004_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00005_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00001_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00003_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00007_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00006_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00008_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00009_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00010_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00011_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00013_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00014_of_00016.arrow
Converting format of dataset (num_proc=16):   2%|▏         | 1000/50000 [00:00<00:10, 4741.92 examples/s]Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00012_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-e190469ed47188cb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-aeca7c98a99d8816_00015_of_00016.arrow
Converting format of dataset (num_proc=16):  78%|███████▊  | 39000/50000 [00:00<00:00, 145965.69 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 50000/50000 [00:03<00:00, 12903.60 examples/s] 
Concatenating 16 shards
05/30/2024 10:45:34 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ru_2b.jsonl.
Using custom data configuration default-ee57b168c979bed9
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Downloading and preparing dataset json/default to /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 4775 examples [00:00, 33687.97 examples/s]Generating train split: 11310 examples [00:00, 41779.08 examples/s]Generating train split: 17859 examples [00:00, 42075.82 examples/s]Generating train split: 24395 examples [00:00, 46191.01 examples/s]Generating train split: 30613 examples [00:00, 46151.99 examples/s]Generating train split: 36899 examples [00:00, 48362.69 examples/s]Generating train split: 43249 examples [00:00, 47642.19 examples/s]Generating train split: 49750 examples [00:01, 48851.90 examples/s]Generating train split: 56300 examples [00:01, 50253.52 examples/s]Generating train split: 62656 examples [00:01, 49466.62 examples/s]Generating train split: 69160 examples [00:01, 47298.26 examples/s]Generating train split: 75785 examples [00:01, 43912.73 examples/s]Generating train split: 82554 examples [00:06, 4464.00 examples/s] Generating train split: 89033 examples [00:06, 6149.93 examples/s]Generating train split: 95560 examples [00:06, 8376.15 examples/s]Generating train split: 102388 examples [00:06, 11362.45 examples/s]Generating train split: 108924 examples [00:06, 14799.85 examples/s]Generating train split: 115289 examples [00:06, 18499.14 examples/s]Generating train split: 121763 examples [00:06, 22899.78 examples/s]Generating train split: 128065 examples [00:07, 27154.31 examples/s]Generating train split: 134583 examples [00:07, 31780.43 examples/s]Generating train split: 142719 examples [00:07, 34920.94 examples/s]Generating train split: 149418 examples [00:07, 37323.81 examples/s]Generating train split: 155868 examples [00:07, 40719.05 examples/s]Generating train split: 160880 examples [00:12, 4236.33 examples/s] Generating train split: 167446 examples [00:12, 5907.93 examples/s]Generating train split: 174184 examples [00:12, 8159.01 examples/s]Generating train split: 180746 examples [00:12, 10916.50 examples/s]Generating train split: 187311 examples [00:12, 14363.82 examples/s]Generating train split: 193913 examples [00:12, 18490.60 examples/s]Generating train split: 200312 examples [00:12, 23093.78 examples/s]Generating train split: 206867 examples [00:13, 27693.57 examples/s]Generating train split: 213599 examples [00:13, 33007.27 examples/s]Generating train split: 220211 examples [00:13, 38074.09 examples/s]Generating train split: 226885 examples [00:13, 43159.86 examples/s]Generating train split: 233300 examples [00:13, 46113.64 examples/s]Generating train split: 239805 examples [00:18, 4408.97 examples/s] Generating train split: 246296 examples [00:18, 6040.96 examples/s]Generating train split: 252854 examples [00:18, 8203.56 examples/s]Generating train split: 259423 examples [00:18, 10926.73 examples/s]Generating train split: 266203 examples [00:18, 14551.03 examples/s]Generating train split: 272871 examples [00:18, 18678.25 examples/s]Generating train split: 279627 examples [00:18, 23625.75 examples/s]Generating train split: 286160 examples [00:18, 28825.37 examples/s]Generating train split: 292555 examples [00:19, 32359.40 examples/s]Generating train split: 298782 examples [00:19, 35957.61 examples/s]Generating train split: 305622 examples [00:19, 40650.15 examples/s]Generating train split: 312038 examples [00:19, 43669.33 examples/s]Generating train split: 318329 examples [00:24, 4432.20 examples/s] Generating train split: 324696 examples [00:24, 6088.67 examples/s]Generating train split: 331279 examples [00:24, 8287.96 examples/s]Generating train split: 337645 examples [00:24, 11016.23 examples/s]Generating train split: 344394 examples [00:24, 14573.08 examples/s]Generating train split: 350746 examples [00:24, 18588.78 examples/s]Generating train split: 357253 examples [00:24, 23190.18 examples/s]Generating train split: 363746 examples [00:24, 27920.77 examples/s]Generating train split: 370515 examples [00:24, 33301.28 examples/s]Generating train split: 377257 examples [00:25, 37319.05 examples/s]Generating train split: 383846 examples [00:25, 41567.27 examples/s]Generating train split: 390515 examples [00:25, 45568.50 examples/s]Generating train split: 396870 examples [00:29, 4494.52 examples/s] Generating train split: 403358 examples [00:30, 6149.22 examples/s]Generating train split: 409915 examples [00:30, 8371.11 examples/s]Generating train split: 416532 examples [00:30, 11176.34 examples/s]Generating train split: 423299 examples [00:30, 14728.89 examples/s]Generating train split: 429746 examples [00:30, 18355.16 examples/s]Generating train split: 436555 examples [00:30, 22743.50 examples/s]Generating train split: 443044 examples [00:30, 27640.89 examples/s]Generating train split: 449648 examples [00:30, 32586.88 examples/s]Generating train split: 456405 examples [00:31, 37570.41 examples/s]Generating train split: 463063 examples [00:31, 40014.34 examples/s]Generating train split: 469530 examples [00:31, 42721.96 examples/s]Generating train split: 476026 examples [00:35, 4504.28 examples/s] Generating train split: 482844 examples [00:35, 6283.00 examples/s]Generating train split: 489528 examples [00:36, 8524.72 examples/s]Generating train split: 496289 examples [00:36, 11477.12 examples/s]Generating train split: 502791 examples [00:36, 14929.53 examples/s]Generating train split: 509215 examples [00:36, 18878.37 examples/s]Generating train split: 515463 examples [00:36, 23180.11 examples/s]Generating train split: 522049 examples [00:36, 27900.92 examples/s]Generating train split: 528544 examples [00:36, 32667.77 examples/s]Generating train split: 535146 examples [00:36, 37171.33 examples/s]Generating train split: 541441 examples [00:37, 39957.76 examples/s]Generating train split: 548054 examples [00:37, 43605.23 examples/s]Generating train split: 554545 examples [00:41, 4456.69 examples/s] Generating train split: 561118 examples [00:41, 6172.50 examples/s]Generating train split: 567473 examples [00:41, 8328.22 examples/s]Generating train split: 574088 examples [00:42, 11280.61 examples/s]Generating train split: 580524 examples [00:42, 14609.55 examples/s]Generating train split: 587040 examples [00:42, 18749.03 examples/s]Generating train split: 593707 examples [00:42, 23548.42 examples/s]Generating train split: 600377 examples [00:42, 27781.90 examples/s]Generating train split: 607093 examples [00:42, 33234.60 examples/s]Generating train split: 613428 examples [00:42, 37213.74 examples/s]Generating train split: 619844 examples [00:42, 39799.70 examples/s]Generating train split: 625904 examples [00:43, 42099.56 examples/s]Generating train split: 632492 examples [00:48, 3808.21 examples/s] Generating train split: 638965 examples [00:48, 5269.56 examples/s]Generating train split: 645594 examples [00:48, 7268.68 examples/s]Generating train split: 652160 examples [00:48, 9865.90 examples/s]Generating train split: 659026 examples [00:48, 13387.00 examples/s]Generating train split: 665253 examples [00:48, 17129.57 examples/s]Generating train split: 671886 examples [00:49, 21367.75 examples/s]Generating train split: 678338 examples [00:49, 25543.03 examples/s]Generating train split: 685055 examples [00:49, 30934.75 examples/s]Generating train split: 691583 examples [00:49, 35130.74 examples/s]Generating train split: 698248 examples [00:49, 38396.30 examples/s]Generating train split: 704905 examples [00:49, 41962.09 examples/s]Generating train split: 711208 examples [00:54, 4370.35 examples/s] Generating train split: 717797 examples [00:54, 6053.02 examples/s]Generating train split: 724338 examples [00:54, 8257.00 examples/s]Generating train split: 730729 examples [00:54, 10968.22 examples/s]Generating train split: 737094 examples [00:54, 14207.20 examples/s]Generating train split: 743691 examples [00:54, 18415.32 examples/s]Generating train split: 750409 examples [00:55, 23404.01 examples/s]Generating train split: 756448 examples [00:55, 27498.93 examples/s]Generating train split: 762778 examples [00:55, 31787.40 examples/s]Generating train split: 769280 examples [00:55, 36465.41 examples/s]Generating train split: 775540 examples [00:55, 39638.42 examples/s]Generating train split: 782070 examples [00:55, 43831.40 examples/s]Generating train split: 788375 examples [01:00, 4402.38 examples/s] Generating train split: 794796 examples [01:00, 6066.35 examples/s]Generating train split: 801469 examples [01:00, 8362.91 examples/s]Generating train split: 808117 examples [01:00, 11254.96 examples/s]Generating train split: 814481 examples [01:00, 14670.69 examples/s]Generating train split: 821124 examples [01:00, 19040.09 examples/s]Generating train split: 827727 examples [01:00, 23136.68 examples/s]Generating train split: 834272 examples [01:00, 27540.34 examples/s]Generating train split: 841166 examples [01:01, 31266.00 examples/s]Generating train split: 847977 examples [01:01, 35914.37 examples/s]Generating train split: 854796 examples [01:01, 40559.79 examples/s]Generating train split: 861282 examples [01:01, 43181.37 examples/s]Generating train split: 867798 examples [01:06, 4353.12 examples/s] Generating train split: 874354 examples [01:06, 5977.03 examples/s]Generating train split: 880734 examples [01:06, 8030.93 examples/s]Generating train split: 887668 examples [01:06, 10979.25 examples/s]Generating train split: 894118 examples [01:06, 14258.95 examples/s]Generating train split: 900780 examples [01:06, 18436.42 examples/s]Generating train split: 907208 examples [01:06, 22490.44 examples/s]Generating train split: 913685 examples [01:07, 27484.75 examples/s]Generating train split: 920014 examples [01:07, 31340.26 examples/s]Generating train split: 926563 examples [01:07, 37036.08 examples/s]Generating train split: 934705 examples [01:07, 43201.43 examples/s]Generating train split: 941206 examples [01:07, 47038.56 examples/s]Generating train split: 949316 examples [01:12, 4923.83 examples/s] Generating train split: 955890 examples [01:12, 6612.13 examples/s]Generating train split: 962094 examples [01:12, 8609.54 examples/s]Generating train split: 968567 examples [01:12, 11399.39 examples/s]Generating train split: 975107 examples [01:12, 14445.77 examples/s]Generating train split: 981640 examples [01:12, 18587.02 examples/s]Generating train split: 988115 examples [01:12, 23300.89 examples/s]Generating train split: 994662 examples [01:13, 27775.62 examples/s]Generating train split: 1000920 examples [01:13, 31611.53 examples/s]Generating train split: 1007609 examples [01:13, 35710.03 examples/s]Generating train split: 1014152 examples [01:13, 37714.60 examples/s]Generating train split: 1020748 examples [01:18, 4088.89 examples/s] Generating train split: 1027542 examples [01:18, 5690.11 examples/s]Generating train split: 1033838 examples [01:18, 7672.90 examples/s]Generating train split: 1040405 examples [01:18, 10384.41 examples/s]Generating train split: 1046870 examples [01:18, 13595.09 examples/s]Generating train split: 1053245 examples [01:19, 17458.00 examples/s]Generating train split: 1059800 examples [01:19, 22126.73 examples/s]Generating train split: 1066334 examples [01:19, 27100.72 examples/s]Generating train split: 1072967 examples [01:19, 32445.85 examples/s]Generating train split: 1079351 examples [01:19, 35033.65 examples/s]Generating train split: 1085723 examples [01:19, 37376.83 examples/s]Generating train split: 1091904 examples [01:19, 39694.99 examples/s]Generating train split: 1098547 examples [01:24, 4405.51 examples/s] Generating train split: 1104885 examples [01:24, 6046.48 examples/s]Generating train split: 1111317 examples [01:24, 8253.11 examples/s]Generating train split: 1117774 examples [01:24, 11079.19 examples/s]Generating train split: 1124438 examples [01:24, 14554.66 examples/s]Generating train split: 1130959 examples [01:24, 18470.85 examples/s]Generating train split: 1137638 examples [01:25, 22804.61 examples/s]Generating train split: 1144167 examples [01:25, 27855.93 examples/s]Generating train split: 1150832 examples [01:25, 32805.16 examples/s]Generating train split: 1157274 examples [01:25, 37262.59 examples/s]Generating train split: 1163789 examples [01:25, 41930.71 examples/s]Generating train split: 1170641 examples [01:25, 45548.14 examples/s]Generating train split: 1177113 examples [01:30, 4515.47 examples/s] Generating train split: 1183753 examples [01:30, 6228.24 examples/s]Generating train split: 1190144 examples [01:30, 8377.81 examples/s]Generating train split: 1196555 examples [01:30, 11199.15 examples/s]Generating train split: 1203103 examples [01:30, 14643.61 examples/s]Generating train split: 1209506 examples [01:30, 18563.75 examples/s]Generating train split: 1215681 examples [01:30, 21853.24 examples/s]Generating train split: 1222183 examples [01:31, 26758.66 examples/s]Generating train split: 1228925 examples [01:31, 30992.58 examples/s]Generating train split: 1235289 examples [01:31, 35315.11 examples/s]Generating train split: 1241793 examples [01:31, 38499.75 examples/s]Generating train split: 1248458 examples [01:31, 41643.98 examples/s]Generating train split: 1255261 examples [01:36, 4514.44 examples/s] Generating train split: 1262075 examples [01:36, 6239.39 examples/s]Generating train split: 1268589 examples [01:36, 8352.33 examples/s]Generating train split: 1275089 examples [01:36, 11125.91 examples/s]Generating train split: 1281760 examples [01:36, 14686.83 examples/s]Generating train split: 1288399 examples [01:36, 18822.98 examples/s]Generating train split: 1294675 examples [01:36, 22802.58 examples/s]Generating train split: 1301338 examples [01:37, 27080.63 examples/s]Generating train split: 1308184 examples [01:37, 31623.05 examples/s]Generating train split: 1314538 examples [01:37, 35426.80 examples/s]Generating train split: 1321336 examples [01:37, 39464.25 examples/s]Generating train split: 1327692 examples [01:37, 41239.27 examples/s]Generating train split: 1334385 examples [01:42, 4518.82 examples/s] Generating train split: 1340857 examples [01:42, 6168.17 examples/s]Generating train split: 1347435 examples [01:42, 8412.78 examples/s]Generating train split: 1353994 examples [01:42, 11264.70 examples/s]Generating train split: 1360684 examples [01:42, 14871.87 examples/s]Generating train split: 1367165 examples [01:42, 18911.41 examples/s]Generating train split: 1373774 examples [01:42, 23500.39 examples/s]Generating train split: 1380313 examples [01:42, 27912.25 examples/s]Generating train split: 1386693 examples [01:43, 32415.84 examples/s]Generating train split: 1393196 examples [01:43, 36069.37 examples/s]Generating train split: 1399811 examples [01:43, 41173.83 examples/s]Generating train split: 1406280 examples [01:43, 44384.47 examples/s]Generating train split: 1412956 examples [01:48, 4194.51 examples/s] Generating train split: 1419763 examples [01:48, 5863.25 examples/s]Generating train split: 1426254 examples [01:48, 7930.45 examples/s]Generating train split: 1432870 examples [01:48, 10704.33 examples/s]Generating train split: 1439311 examples [01:48, 13903.55 examples/s]Generating train split: 1445745 examples [01:48, 17917.68 examples/s]Generating train split: 1452419 examples [01:49, 22655.39 examples/s]Generating train split: 1458941 examples [01:49, 26805.01 examples/s]Generating train split: 1465546 examples [01:49, 31588.91 examples/s]Generating train split: 1471992 examples [01:49, 35309.10 examples/s]Generating train split: 1478105 examples [01:49, 38385.55 examples/s]Generating train split: 1485007 examples [01:49, 41897.55 examples/s]Generating train split: 1491592 examples [01:54, 4510.84 examples/s] Generating train split: 1498128 examples [01:54, 6176.87 examples/s]Generating train split: 1504670 examples [01:54, 8393.45 examples/s]Generating train split: 1511376 examples [01:54, 11179.60 examples/s]Generating train split: 1517889 examples [01:54, 14572.91 examples/s]Generating train split: 1524497 examples [01:54, 18538.72 examples/s]Generating train split: 1531178 examples [01:54, 23064.74 examples/s]Generating train split: 1537891 examples [01:55, 28093.22 examples/s]Generating train split: 1544488 examples [01:55, 32694.64 examples/s]Generating train split: 1550972 examples [01:55, 37018.85 examples/s]Generating train split: 1557556 examples [01:55, 40772.00 examples/s]Generating train split: 1564064 examples [01:55, 42001.84 examples/s]Generating train split: 1570697 examples [02:00, 4476.36 examples/s] Generating train split: 1577169 examples [02:00, 6085.24 examples/s]Generating train split: 1583946 examples [02:00, 8327.86 examples/s]Generating train split: 1590635 examples [02:00, 11077.91 examples/s]Generating train split: 1597228 examples [02:00, 14494.05 examples/s]Generating train split: 1603712 examples [02:00, 18233.63 examples/s]Generating train split: 1609960 examples [02:00, 22391.07 examples/s]Generating train split: 1609960 examples [02:03, 13065.07 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.
Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': 'Москва помогает Южной Осетии – Учительская газета\nМосква помогает Южной Осетии\nВласти Москвы направляют пострадавшим в вооруженном конфликте в Южной Осетии гуманитарные грузы: первая партия была весом более 100 тонн на общую сумму 2,5 миллиарда рублей, вторые две – стоимостью около 35 миллионов рублей.\nВ гуманитарные грузы вошли в основном продовольствие (мука и крупы), коммунальная и стройтехника, уборочные машины. Столица России заказала для Цхинвали десять пассажирских автобусов, 30 остановочных пунктов. Все это доставят в Южную Осетию по железной дороге.'}
Process #0 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00000_of_00016.arrow
Process #1 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00001_of_00016.arrow
Process #2 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00002_of_00016.arrow
Process #3 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00003_of_00016.arrow
Process #4 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00004_of_00016.arrow
Process #5 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00005_of_00016.arrow
Process #6 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00006_of_00016.arrow
Process #7 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00007_of_00016.arrow
Process #8 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00008_of_00016.arrow
Process #9 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00009_of_00016.arrow
Process #10 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00010_of_00016.arrow
Process #11 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00011_of_00016.arrow
Process #12 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00012_of_00016.arrow
Process #13 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00013_of_00016.arrow
Process #14 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00014_of_00016.arrow
Process #15 will write at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00015_of_00016.arrow
Spawning 16 processes
Converting format of dataset (num_proc=16):   0%|          | 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00002_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00005_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00001_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00007_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00014_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00000_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00003_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00004_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00006_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00008_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00010_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00012_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00013_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00009_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00011_of_00016.arrow
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-ee57b168c979bed9/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3d1e574f2d1b4753_00015_of_00016.arrow
Converting format of dataset (num_proc=16):   2%|▏         | 1000/50000 [00:00<00:13, 3748.73 examples/s]Converting format of dataset (num_proc=16):  60%|██████    | 30000/50000 [00:00<00:00, 101244.91 examples/s]Converting format of dataset (num_proc=16):  94%|█████████▎| 46750/50000 [00:03<00:00, 12465.60 examples/s] Converting format of dataset (num_proc=16): 100%|██████████| 50000/50000 [00:06<00:00, 7991.66 examples/s] 
Concatenating 16 shards
Process #0 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00000_of_00016.arrow
Process #1 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00001_of_00016.arrow
Process #2 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00002_of_00016.arrow
Process #3 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00003_of_00016.arrow
Process #4 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00004_of_00016.arrow
Process #5 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00005_of_00016.arrow
Process #6 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00006_of_00016.arrow
Process #7 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00007_of_00016.arrow
Process #8 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00008_of_00016.arrow
Process #9 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00009_of_00016.arrow
Process #10 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00010_of_00016.arrow
Process #11 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00011_of_00016.arrow
Process #12 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00012_of_00016.arrow
Process #13 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00013_of_00016.arrow
Process #14 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00014_of_00016.arrow
Process #15 will write at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00015_of_00016.arrow
Spawning 16 processes
Running tokenizer on dataset (num_proc=16):   0%|          | 0/200000 [00:00<?, ? examples/s]05/30/2024 10:49:24 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
05/30/2024 10:49:24 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
05/30/2024 10:49:24 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 50000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 50000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 50000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00000_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   0%|          | 1000/200000 [00:09<32:46, 101.22 examples/s]Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00001_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   1%|          | 2000/200000 [00:11<16:45, 197.00 examples/s]Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00002_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   2%|▏         | 3000/200000 [00:14<13:18, 246.65 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:49:37,611 >> Token indices sequence length is longer than the specified maximum sequence length for this model (34999 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):   2%|▏         | 4000/200000 [00:15<08:58, 363.65 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:49:39,120 >> Token indices sequence length is longer than the specified maximum sequence length for this model (35848 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):   2%|▎         | 5000/200000 [00:16<07:25, 437.27 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:49:40,677 >> Token indices sequence length is longer than the specified maximum sequence length for this model (130268 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00003_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   3%|▎         | 6000/200000 [00:18<06:44, 479.78 examples/s]Running tokenizer on dataset (num_proc=16):   4%|▎         | 7000/200000 [00:19<05:47, 555.61 examples/s]Running tokenizer on dataset (num_proc=16):   4%|▍         | 8000/200000 [00:20<04:49, 662.07 examples/s]05/30/2024 10:49:43 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
05/30/2024 10:49:44 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
Running tokenizer on dataset (num_proc=16):   4%|▍         | 9000/200000 [00:22<04:58, 640.05 examples/s]05/30/2024 10:49:44 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00004_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   5%|▌         | 10000/200000 [00:22<03:47, 834.40 examples/s]Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
Running tokenizer on dataset (num_proc=16):   6%|▌         | 11000/200000 [00:23<03:57, 794.86 examples/s]Running tokenizer on dataset (num_proc=16):   6%|▌         | 12000/200000 [00:24<02:59, 1046.64 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:49:47,395 >> Token indices sequence length is longer than the specified maximum sequence length for this model (33868 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00005_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   6%|▋         | 13000/200000 [00:24<02:48, 1108.00 examples/s]Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00006_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   8%|▊         | 15000/200000 [00:27<03:40, 839.37 examples/s] [WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:49:51,327 >> Token indices sequence length is longer than the specified maximum sequence length for this model (51131 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):   8%|▊         | 16000/200000 [00:28<03:25, 894.47 examples/s]Running tokenizer on dataset (num_proc=16):  10%|▉         | 19000/200000 [00:30<02:14, 1344.03 examples/s]Running tokenizer on dataset (num_proc=16):  10%|█         | 20000/200000 [00:30<01:52, 1603.46 examples/s]Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00007_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  10%|█         | 21000/200000 [00:30<01:50, 1620.45 examples/s]Running tokenizer on dataset (num_proc=16):  11%|█         | 22000/200000 [00:31<02:03, 1436.37 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:49:54,741 >> Token indices sequence length is longer than the specified maximum sequence length for this model (36986 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00008_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  12%|█▏        | 23000/200000 [00:32<01:59, 1487.38 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▏        | 24000/200000 [00:34<02:50, 1032.08 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:49:57,047 >> Token indices sequence length is longer than the specified maximum sequence length for this model (36276 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00009_of_00016.arrow
05/30/2024 10:49:57 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Running tokenizer on dataset (num_proc=16):  12%|█▎        | 25000/200000 [00:34<02:27, 1188.33 examples/s]Running tokenizer on dataset (num_proc=16):  14%|█▎        | 27000/200000 [00:34<01:26, 2008.39 examples/s]Running tokenizer on dataset (num_proc=16):  14%|█▍        | 28000/200000 [00:34<01:11, 2400.18 examples/s]Running tokenizer on dataset (num_proc=16):  14%|█▍        | 29000/200000 [00:35<01:04, 2651.00 examples/s]05/30/2024 10:49:58 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
05/30/2024 10:49:58 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Running tokenizer on dataset (num_proc=16):  15%|█▌        | 30000/200000 [00:36<01:33, 1825.57 examples/s]Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
[WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:50:00,193 >> Token indices sequence length is longer than the specified maximum sequence length for this model (35336 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00010_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  16%|█▌        | 31000/200000 [00:37<02:11, 1286.14 examples/s]Running tokenizer on dataset (num_proc=16):  16%|█▌        | 32000/200000 [00:37<01:43, 1618.43 examples/s]Running tokenizer on dataset (num_proc=16):  17%|█▋        | 34000/200000 [00:38<01:22, 2020.94 examples/s]Running tokenizer on dataset (num_proc=16):  18%|█▊        | 35000/200000 [00:39<01:39, 1662.11 examples/s]Running tokenizer on dataset (num_proc=16):  18%|█▊        | 36000/200000 [00:39<01:22, 1995.36 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:50:03,010 >> Token indices sequence length is longer than the specified maximum sequence length for this model (43731 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00011_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  18%|█▊        | 37000/200000 [00:40<01:35, 1708.53 examples/s]Running tokenizer on dataset (num_proc=16):  20%|█▉        | 39000/200000 [00:40<00:59, 2709.55 examples/s]Running tokenizer on dataset (num_proc=16):  20%|██        | 40000/200000 [00:40<00:59, 2680.43 examples/s]Running tokenizer on dataset (num_proc=16):  20%|██        | 41000/200000 [00:41<00:50, 3124.51 examples/s]Running tokenizer on dataset (num_proc=16):  21%|██        | 42000/200000 [00:41<00:53, 2938.61 examples/s]Running tokenizer on dataset (num_proc=16):  22%|██▏       | 43000/200000 [00:43<01:56, 1342.02 examples/s]Running tokenizer on dataset (num_proc=16):  22%|██▏       | 44000/200000 [00:43<01:47, 1454.11 examples/s]Running tokenizer on dataset (num_proc=16):  22%|██▎       | 45000/200000 [00:44<01:25, 1813.23 examples/s]Running tokenizer on dataset (num_proc=16):  23%|██▎       | 46000/200000 [00:44<01:11, 2165.51 examples/s]Running tokenizer on dataset (num_proc=16):  24%|██▎       | 47000/200000 [00:44<01:13, 2082.72 examples/s]Running tokenizer on dataset (num_proc=16):  24%|██▍       | 48000/200000 [00:45<01:02, 2433.42 examples/s]Running tokenizer on dataset (num_proc=16):  24%|██▍       | 49000/200000 [00:45<00:58, 2573.80 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 50000/200000 [00:46<01:35, 1566.62 examples/s]Running tokenizer on dataset (num_proc=16):  26%|██▌       | 51000/200000 [00:47<01:21, 1828.52 examples/s]Running tokenizer on dataset (num_proc=16):  26%|██▋       | 53000/200000 [00:47<01:01, 2372.96 examples/s]Running tokenizer on dataset (num_proc=16):  27%|██▋       | 54000/200000 [00:47<00:51, 2811.49 examples/s]Running tokenizer on dataset (num_proc=16):  28%|██▊       | 55000/200000 [00:49<02:00, 1204.53 examples/s]Running tokenizer on dataset (num_proc=16):  28%|██▊       | 57000/200000 [00:50<01:13, 1954.89 examples/s]Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00012_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  30%|██▉       | 59000/200000 [00:50<00:48, 2910.30 examples/s]Running tokenizer on dataset (num_proc=16):  30%|███       | 60000/200000 [00:50<00:49, 2842.79 examples/s]Running tokenizer on dataset (num_proc=16):  30%|███       | 61000/200000 [00:50<00:45, 3025.31 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███       | 62000/200000 [00:51<00:40, 3402.04 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 63000/200000 [00:52<01:25, 1609.08 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:50:15,565 >> Token indices sequence length is longer than the specified maximum sequence length for this model (55928 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00013_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  32%|███▏      | 64000/200000 [00:53<01:18, 1732.46 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▎      | 65000/200000 [00:53<01:04, 2080.93 examples/s]Running tokenizer on dataset (num_proc=16):  33%|███▎      | 66000/200000 [00:53<00:51, 2579.76 examples/s]Running tokenizer on dataset (num_proc=16):  34%|███▍      | 69000/200000 [00:55<01:03, 2066.30 examples/s]Running tokenizer on dataset (num_proc=16):  35%|███▌      | 70000/200000 [00:55<00:56, 2310.08 examples/s]Running tokenizer on dataset (num_proc=16):  36%|███▌      | 71000/200000 [00:55<01:00, 2131.74 examples/s]Running tokenizer on dataset (num_proc=16):  36%|███▌      | 72000/200000 [00:56<00:51, 2476.16 examples/s]Running tokenizer on dataset (num_proc=16):  36%|███▋      | 73000/200000 [00:56<00:51, 2469.13 examples/s]Running tokenizer on dataset (num_proc=16):  37%|███▋      | 74000/200000 [00:56<00:42, 2945.25 examples/s]05/30/2024 10:50:19 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ru_2b.jsonl.
05/30/2024 10:50:19 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ru_2b.jsonl.
05/30/2024 10:50:19 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ru_2b.jsonl.
[WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:50:19,660 >> Token indices sequence length is longer than the specified maximum sequence length for this model (39444 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00014_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  38%|███▊      | 75000/200000 [00:57<01:03, 1980.40 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 77000/200000 [00:57<00:41, 2980.14 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:50:20,748 >> Token indices sequence length is longer than the specified maximum sequence length for this model (38249 > 32768). Running this sequence through the model will result in indexing errors
Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': 'Москва помогает Южной Осетии – Учительская газета\nМосква помогает Южной Осетии\nВласти Москвы направляют пострадавшим в вооруженном конфликте в Южной Осетии гуманитарные грузы: первая партия была весом более 100 тонн на общую сумму 2,5 миллиарда рублей, вторые две – стоимостью около 35 миллионов рублей.\nВ гуманитарные грузы вошли в основном продовольствие (мука и крупы), коммунальная и стройтехника, уборочные машины. Столица России заказала для Цхинвали десять пассажирских автобусов, 30 остановочных пунктов. Все это доставят в Южную Осетию по железной дороге.'}
Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': 'Москва помогает Южной Осетии – Учительская газета\nМосква помогает Южной Осетии\nВласти Москвы направляют пострадавшим в вооруженном конфликте в Южной Осетии гуманитарные грузы: первая партия была весом более 100 тонн на общую сумму 2,5 миллиарда рублей, вторые две – стоимостью около 35 миллионов рублей.\nВ гуманитарные грузы вошли в основном продовольствие (мука и крупы), коммунальная и стройтехника, уборочные машины. Столица России заказала для Цхинвали десять пассажирских автобусов, 30 остановочных пунктов. Все это доставят в Южную Осетию по железной дороге.'}
Dataset({
    features: ['text'],
    num_rows: 50000
})
{'text': 'Москва помогает Южной Осетии – Учительская газета\nМосква помогает Южной Осетии\nВласти Москвы направляют пострадавшим в вооруженном конфликте в Южной Осетии гуманитарные грузы: первая партия была весом более 100 тонн на общую сумму 2,5 миллиарда рублей, вторые две – стоимостью около 35 миллионов рублей.\nВ гуманитарные грузы вошли в основном продовольствие (мука и крупы), коммунальная и стройтехника, уборочные машины. Столица России заказала для Цхинвали десять пассажирских автобусов, 30 остановочных пунктов. Все это доставят в Южную Осетию по железной дороге.'}
Running tokenizer on dataset (num_proc=16):  39%|███▉      | 78000/200000 [00:58<00:45, 2707.41 examples/s]Running tokenizer on dataset (num_proc=16):  40%|███▉      | 79000/200000 [00:58<00:40, 2973.36 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:50:22,009 >> Token indices sequence length is longer than the specified maximum sequence length for this model (49032 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  40%|████      | 80000/200000 [00:59<00:49, 2429.88 examples/s]Caching processed dataset at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6c199d620a7ff038_00015_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  40%|████      | 81000/200000 [00:59<00:49, 2425.85 examples/s]Running tokenizer on dataset (num_proc=16):  41%|████      | 82000/200000 [00:59<00:41, 2828.12 examples/s]Running tokenizer on dataset (num_proc=16):  42%|████▏     | 83000/200000 [01:01<01:13, 1598.46 examples/s]Running tokenizer on dataset (num_proc=16):  42%|████▏     | 84000/200000 [01:01<01:03, 1821.27 examples/s]Running tokenizer on dataset (num_proc=16):  42%|████▎     | 85000/200000 [01:01<00:57, 2004.91 examples/s]Running tokenizer on dataset (num_proc=16):  43%|████▎     | 86000/200000 [01:02<00:44, 2554.23 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 88000/200000 [01:02<00:34, 3253.34 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 89000/200000 [01:02<00:40, 2733.29 examples/s]Running tokenizer on dataset (num_proc=16):  45%|████▌     | 90000/200000 [01:03<00:38, 2879.75 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▌     | 91000/200000 [01:03<00:31, 3438.63 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▌     | 92000/200000 [01:03<00:33, 3210.44 examples/s]Running tokenizer on dataset (num_proc=16):  47%|████▋     | 94000/200000 [01:03<00:20, 5114.88 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:50:27,156 >> Token indices sequence length is longer than the specified maximum sequence length for this model (38175 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  48%|████▊     | 95000/200000 [01:04<00:37, 2794.49 examples/s]Running tokenizer on dataset (num_proc=16):  48%|████▊     | 96000/200000 [01:05<00:49, 2118.68 examples/s]Running tokenizer on dataset (num_proc=16):  48%|████▊     | 97000/200000 [01:06<01:01, 1670.37 examples/s]Running tokenizer on dataset (num_proc=16):  49%|████▉     | 98000/200000 [01:06<00:49, 2054.69 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 100000/200000 [01:07<00:45, 2177.19 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 101000/200000 [01:07<00:38, 2594.68 examples/s]Running tokenizer on dataset (num_proc=16):  51%|█████     | 102000/200000 [01:08<00:42, 2311.53 examples/s]Running tokenizer on dataset (num_proc=16):  52%|█████▏    | 103000/200000 [01:08<00:48, 2017.94 examples/s]Running tokenizer on dataset (num_proc=16):  52%|█████▏    | 104000/200000 [01:09<00:37, 2578.82 examples/s]Running tokenizer on dataset (num_proc=16):  53%|█████▎    | 106000/200000 [01:09<00:29, 3214.65 examples/s]Running tokenizer on dataset (num_proc=16):  53%|█████▎    | 106500/200000 [01:09<00:29, 3119.36 examples/s]Running tokenizer on dataset (num_proc=16):  54%|█████▍    | 107500/200000 [01:10<00:32, 2888.81 examples/s]Running tokenizer on dataset (num_proc=16):  54%|█████▍    | 108500/200000 [01:10<00:36, 2533.82 examples/s]Running tokenizer on dataset (num_proc=16):  55%|█████▍    | 109500/200000 [01:10<00:30, 3015.42 examples/s]Running tokenizer on dataset (num_proc=16):  55%|█████▌    | 110500/200000 [01:11<00:34, 2594.33 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:50:34,110 >> Token indices sequence length is longer than the specified maximum sequence length for this model (41044 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 112500/200000 [01:11<00:28, 3059.44 examples/s]Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 113500/200000 [01:12<00:37, 2314.63 examples/s]Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 114500/200000 [01:12<00:32, 2591.10 examples/s]Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 115500/200000 [01:13<00:40, 2110.85 examples/s]Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 116000/200000 [01:13<00:45, 1852.79 examples/s]Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 117000/200000 [01:14<00:38, 2183.76 examples/s]Running tokenizer on dataset (num_proc=16):  60%|█████▉    | 119000/200000 [01:14<00:25, 3116.84 examples/s]Running tokenizer on dataset (num_proc=16):  60%|██████    | 120000/200000 [01:14<00:25, 3087.39 examples/s]Running tokenizer on dataset (num_proc=16):  60%|██████    | 121000/200000 [01:15<00:20, 3778.89 examples/s]Running tokenizer on dataset (num_proc=16):  61%|██████    | 122000/200000 [01:15<00:27, 2887.42 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 123000/200000 [01:15<00:24, 3136.84 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 124000/200000 [01:16<00:37, 2020.85 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 125000/200000 [01:17<00:32, 2285.65 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 125500/200000 [01:17<00:32, 2284.70 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 126500/200000 [01:17<00:36, 2022.66 examples/s]Running tokenizer on dataset (num_proc=16):  64%|██████▍   | 127500/200000 [01:18<00:47, 1529.10 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-05-30 10:50:42,062 >> Token indices sequence length is longer than the specified maximum sequence length for this model (84369 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  64%|██████▍   | 128000/200000 [01:19<00:50, 1429.03 examples/s]Running tokenizer on dataset (num_proc=16):  64%|██████▍   | 129000/200000 [01:19<00:35, 2006.10 examples/s]Running tokenizer on dataset (num_proc=16):  65%|██████▌   | 130000/200000 [01:19<00:25, 2725.57 examples/s]Running tokenizer on dataset (num_proc=16):  66%|██████▌   | 131000/200000 [01:19<00:21, 3205.92 examples/s]Running tokenizer on dataset (num_proc=16):  66%|██████▌   | 132000/200000 [01:20<00:21, 3131.09 examples/s]Running tokenizer on dataset (num_proc=16):  67%|██████▋   | 134000/200000 [01:20<00:13, 5057.84 examples/s]Running tokenizer on dataset (num_proc=16):  68%|██████▊   | 135000/200000 [01:20<00:13, 4759.70 examples/s]Running tokenizer on dataset (num_proc=16):  68%|██████▊   | 136000/200000 [01:21<00:20, 3143.04 examples/s]Running tokenizer on dataset (num_proc=16):  68%|██████▊   | 137000/200000 [01:21<00:26, 2408.58 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 138000/200000 [01:22<00:29, 2120.47 examples/s]Running tokenizer on dataset (num_proc=16):  70%|██████▉   | 139000/200000 [01:23<00:37, 1611.46 examples/s]Running tokenizer on dataset (num_proc=16):  70%|███████   | 140000/200000 [01:24<00:38, 1544.60 examples/s]Running tokenizer on dataset (num_proc=16):  70%|███████   | 141000/200000 [01:25<00:45, 1282.81 examples/s]Running tokenizer on dataset (num_proc=16):  71%|███████   | 142000/200000 [01:25<00:34, 1680.96 examples/s]Running tokenizer on dataset (num_proc=16):  72%|███████▏  | 143000/200000 [01:25<00:27, 2087.14 examples/s]Running tokenizer on dataset (num_proc=16):  72%|███████▏  | 144000/200000 [01:25<00:24, 2263.78 examples/s]Running tokenizer on dataset (num_proc=16):  73%|███████▎  | 146000/200000 [01:26<00:26, 2045.38 examples/s]Running tokenizer on dataset (num_proc=16):  74%|███████▎  | 147000/200000 [01:27<00:27, 1960.58 examples/s]Running tokenizer on dataset (num_proc=16):  74%|███████▍  | 148000/200000 [01:27<00:21, 2472.51 examples/s]Running tokenizer on dataset (num_proc=16):  74%|███████▍  | 149000/200000 [01:28<00:25, 2010.00 examples/s]Running tokenizer on dataset (num_proc=16):  76%|███████▌  | 151500/200000 [01:28<00:15, 3125.57 examples/s]Running tokenizer on dataset (num_proc=16):  76%|███████▋  | 152500/200000 [01:29<00:23, 1989.71 examples/s]Running tokenizer on dataset (num_proc=16):  77%|███████▋  | 153500/200000 [01:30<00:20, 2272.58 examples/s]Running tokenizer on dataset (num_proc=16):  77%|███████▋  | 154500/200000 [01:30<00:16, 2825.06 examples/s]Running tokenizer on dataset (num_proc=16):  78%|███████▊  | 155500/200000 [01:32<00:34, 1280.75 examples/s]Running tokenizer on dataset (num_proc=16):  78%|███████▊  | 156500/200000 [01:32<00:31, 1392.81 examples/s]Running tokenizer on dataset (num_proc=16):  78%|███████▊  | 157000/200000 [01:34<00:46, 916.20 examples/s] Running tokenizer on dataset (num_proc=16):  80%|███████▉  | 159000/200000 [01:34<00:24, 1676.05 examples/s]Running tokenizer on dataset (num_proc=16):  80%|████████  | 160000/200000 [01:34<00:22, 1770.15 examples/s]Running tokenizer on dataset (num_proc=16):  80%|████████  | 161000/200000 [01:35<00:20, 1878.05 examples/s]Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 163500/200000 [01:36<00:20, 1756.27 examples/s]Running tokenizer on dataset (num_proc=16):  83%|████████▎ | 165500/200000 [01:36<00:13, 2519.98 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▎ | 167000/200000 [01:37<00:10, 3032.33 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▍ | 168500/200000 [01:37<00:11, 2778.48 examples/s]Running tokenizer on dataset (num_proc=16):  84%|████████▍ | 169000/200000 [01:39<00:19, 1571.21 examples/s]Running tokenizer on dataset (num_proc=16):  85%|████████▌ | 170000/200000 [01:39<00:18, 1653.00 examples/s]Running tokenizer on dataset (num_proc=16):  86%|████████▌ | 171000/200000 [01:42<00:33, 874.28 examples/s] Running tokenizer on dataset (num_proc=16):  86%|████████▋ | 172500/200000 [01:42<00:21, 1285.70 examples/s]Running tokenizer on dataset (num_proc=16):  87%|████████▋ | 174500/200000 [01:44<00:22, 1156.33 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 175500/200000 [01:44<00:17, 1426.96 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 176000/200000 [01:44<00:16, 1459.12 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 177000/200000 [01:49<00:41, 560.31 examples/s] Running tokenizer on dataset (num_proc=16):  89%|████████▉ | 178000/200000 [01:49<00:29, 754.00 examples/s]Running tokenizer on dataset (num_proc=16):  90%|████████▉ | 179000/200000 [01:51<00:30, 679.33 examples/s]Running tokenizer on dataset (num_proc=16):  90%|█████████ | 180000/200000 [01:51<00:22, 887.60 examples/s]Running tokenizer on dataset (num_proc=16):  90%|█████████ | 181000/200000 [01:56<00:41, 459.96 examples/s]Running tokenizer on dataset (num_proc=16):  91%|█████████ | 182000/200000 [01:56<00:29, 619.83 examples/s]Running tokenizer on dataset (num_proc=16):  92%|█████████▏| 183000/200000 [01:59<00:31, 538.86 examples/s]Running tokenizer on dataset (num_proc=16):  92%|█████████▎| 185000/200000 [02:03<00:29, 516.70 examples/s]Running tokenizer on dataset (num_proc=16):  93%|█████████▎| 186000/200000 [02:03<00:22, 613.60 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▎| 187000/200000 [02:06<00:23, 560.13 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 188000/200000 [02:07<00:19, 619.85 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 189000/200000 [02:10<00:21, 511.65 examples/s]Running tokenizer on dataset (num_proc=16):  95%|█████████▌| 190000/200000 [02:11<00:18, 551.95 examples/s]Running tokenizer on dataset (num_proc=16):  96%|█████████▌| 191000/200000 [02:12<00:14, 603.62 examples/s]Running tokenizer on dataset (num_proc=16):  96%|█████████▌| 192000/200000 [02:14<00:12, 635.92 examples/s]Running tokenizer on dataset (num_proc=16):  96%|█████████▋| 193000/200000 [02:16<00:13, 532.22 examples/s]Running tokenizer on dataset (num_proc=16):  97%|█████████▋| 194000/200000 [02:18<00:11, 519.07 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 195000/200000 [02:19<00:07, 646.47 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 195500/200000 [02:19<00:06, 712.47 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 196500/200000 [02:20<00:03, 883.79 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 197000/200000 [02:23<00:06, 466.56 examples/s]Running tokenizer on dataset (num_proc=16):  99%|█████████▉| 198000/200000 [02:25<00:04, 477.96 examples/s]Running tokenizer on dataset (num_proc=16): 100%|█████████▉| 199000/200000 [02:26<00:01, 591.90 examples/s]Running tokenizer on dataset (num_proc=16): 100%|█████████▉| 199500/200000 [02:29<00:01, 434.37 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 200000/200000 [02:30<00:00, 429.98 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 200000/200000 [02:31<00:00, 1320.65 examples/s]
Concatenating 16 shards
input_ids:
[12093, 70290, 6859, 6484, 43, 890, 27134, 2448, 7395, 30400, 55, 549, 26209, 198, 6622, 198, 16578, 70290, 6859, 6484, 43, 890, 27134, 2448, 7395, 30400, 55, 198, 6622, 198, 151643, 7039, 33976, 432, 646, 387, 264, 2699, 312, 2015, 1388, 311, 3270, 458, 4549, 911, 279, 10990, 7070, 14429, 3840, 438, 1052, 374, 825, 304, 1449, 1614, 14418, 504, 882, 311, 882, 11, 323, 279, 3482, 374, 49485, 448, 1105, 304, 1449, 4128, 369, 5019, 879, 30997, 311, 1349, 1105, 13, 2055, 358, 686, 36355, 304, 11629, 1246, 358, 5798, 323, 23983, 279, 1614, 624, 34762, 1635, 4134, 16145, 6635, 311, 1281, 1549, 862, 10990, 7070, 14429, 31496, 438, 807, 1030, 264, 501, 3093, 315, 50270, 82, 429, 1410, 15551, 279, 12188, 504, 2155, 11067, 323, 25941, 323, 8450, 1281, 64255, 323, 803, 35201, 9666, 13, 2379, 3381, 429, 3432, 419, 501, 5440, 432, 1035, 387, 2664, 311, 8193, 1549, 279, 330, 1040, 1211, 1, 1091, 894, 1008, 25546, 6174, 323, 432, 4977, 429, 807, 1033, 1290, 438, 1449, 1463, 7073, 10788, 825, 52163, 269, 803, 624, 9485, 501, 50270, 82, 1033, 537, 279, 1172, 501, 3166, 304, 1493, 4119, 11, 807, 1030, 264, 738, 315, 13918, 7746, 2669, 18663, 504, 279, 8151, 1137, 5527, 311, 41740, 11, 1045, 6548, 36780, 9317, 5479, 323, 501, 97685, 624, 2121, 5135, 438, 358, 8930, 279, 3745, 358, 1030, 264, 1602, 2797, 2168, 315, 279, 3093, 315, 1614, 358, 4829, 311, 1281, 11, 358, 1030, 3884, 10077, 315, 24248, 304, 279, 6467, 11, 1602, 76873, 12645, 98732, 448, 25386, 424, 429, 95758, 2310, 7218, 76024, 624, 2461, 419, 2390, 358, 6635, 311, 990, 279, 5235, 18457, 53514, 22293, 738, 369, 279, 10990, 7070, 14429, 429, 374, 2167, 14452, 323, 18304, 13942, 624, 2132, 4436, 944, 16965, 438, 279, 5479, 4946, 1602, 1632, 323, 279, 11221, 525, 1602, 2797, 11, 2337, 279, 1882, 582, 686, 1172, 614, 311, 1896, 2453, 979, 11589, 279, 2632, 5479, 311, 5648, 14719, 1105, 476, 11785, 11, 775, 14421, 1105, 13, 1634, 847, 4522, 504, 279, 7167, 572, 311, 4009, 264, 12896, 448, 279, 23603, 86968, 18824, 24569, 23704, 700, 11, 358, 23983, 279, 1614, 448, 5938, 12258, 25685, 12463, 438, 358, 5798, 432, 311, 614, 419, 12463, 2331, 369, 279, 2937, 330, 35012, 18824, 1, 14762, 358, 1035, 3796, 389, 279, 86968, 13, 2055, 358, 5798, 279, 44909, 323, 279, 64386, 9380, 15663, 279, 305, 9118, 11, 22696, 11, 39932, 13569, 11, 4992, 13, 14576, 311, 6707, 6177, 279, 65739, 448, 279, 15625, 476, 279, 68348, 13, 151643, 641, 3213, 1635, 358, 614, 65806, 3807, 52269, 14697, 304, 847, 2205, 3082, 13, 20035, 537, 3884, 894, 32319, 15570, 4730, 5926, 358, 572, 18442, 311, 1490, 458, 52269, 16262, 825, 315, 847, 22791, 14697, 264, 5625, 315, 2849, 4134, 13, 60033, 358, 9099, 847, 8849, 6249, 14046, 311, 1490, 421, 279, 52269, 374, 264, 5792, 20181, 13, 3197, 358, 10067, 279, 21852, 419, 6556, 358, 572, 33972, 311, 1490, 358, 614, 264, 6716, 315, 32319, 15570, 4730, 304, 21682, 13, 358, 2776, 10282, 4297, 27031, 1431, 304, 279, 3900, 429]
inputs:
@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject
@end
@implementation PodsDummy_XCDLumberjackNSLogger_OSX
@end
<|endoftext|>Nowadays it can be a bit reiterative to write an article about the Panzer III history as there is one in every model magazine from time to time, and the web is saturated with them in every language for everyone who desires to read them. So I will concentrate in telling how I built and painted the model.
Several years ago Dragon decided to make again their Panzer III kits as they had a new kind of moulds that could inject the plastic from different sides and angles and thus make thinner and more delicate pieces. They thought that having this new technology it would be better to produce again the "classics" than any other newer ones and it seems that they were right as every modeller bought one…..or more.
These new moulds were not the only new thing in these models, they had a set of tracks links already separated from the sprues ready to assemble, some photoetched metal parts and new decals.
As soon as I opened the box I had a very clear image of the kind of model I wanted to make, I had seen lots of photographs in the books, very dusty machines cramped with equipage that resembled old moving vans.
For this project I decided to use the Blackdog resin accessories set for the Panzer III that is really suitable and fits perfectly.
It isn't complicated as the parts fit very well and the instructions are very clear, during the process we will only have to take care when handling the little parts to avoid breaking them or worst, loosing them. As my idea from the beginning was to represent a tank with the desert camouflage painting badly worn out, I painted the model with German Dark Grey colour as I built it to have this colour base for the later "soap painting" technique I would apply on the camouflage. So I built the chassis and the turret leaving aside the hatches, wheels, antenna rail, etc. mainly to easily paint the scratches with the brush or the sponge.<|endoftext|>In recent years I have erected several owl boxes in my local area. Having not seen any barn owls recently I was pleased to see an owl entering one of my nest boxes a couple of days ago. Yesterday I placed my trail camera nearby to see if the owl is a regular visitor. When I checked the footage this morning I was delighted to see I have a pair of barn owls in residence. I'm keeping everything crossed now in the hope that
Caching indices mapping at /home/nfs04/wangzj/dataset/cache/json/default-5f559cb30c539bf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-221cac279bce8197.arrow
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 441676
})
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): Qwen2ForCausalLM(
      (model): Qwen2Model(
        (embed_tokens): Embedding(151936, 2048)
        (layers): ModuleList(
          (0): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (1): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (2): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (3): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (4): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (5): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (6): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (7): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (8): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (9): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (10): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (11): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (12): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (13): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (14): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (15): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (16): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (17): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (18): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (19): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (20): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (21): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (22): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (23): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
        )
        (norm): Qwen2RMSNorm()
      )
      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
    )
  )
)
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:586] 2024-05-30 10:51:54,678 >> Using auto half precision backend
[2024-05-30 10:51:54,934] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 441676
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 441676
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 441676
})
[2024-05-30 10:51:59,907] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-30 10:51:59,908] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-05-30 10:51:59,908] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-30 10:51:59,911] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-05-30 10:51:59,911] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-05-30 10:51:59,911] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-05-30 10:51:59,911] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-05-30 10:51:59,911] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-05-30 10:51:59,911] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: False
[2024-05-30 10:51:59,911] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-05-30 10:52:00,372] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-05-30 10:52:00,373] [INFO] [utils.py:803:see_memory_usage] MA 5.38 GB         Max_MA 5.38 GB         CA 5.66 GB         Max_CA 6 GB 
[2024-05-30 10:52:00,373] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.41 GB, percent = 12.0%
[2024-05-30 10:52:00,562] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-05-30 10:52:00,562] [INFO] [utils.py:803:see_memory_usage] MA 5.38 GB         Max_MA 5.38 GB         CA 5.66 GB         Max_CA 6 GB 
[2024-05-30 10:52:00,562] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.41 GB, percent = 12.0%
[2024-05-30 10:52:00,562] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-05-30 10:52:00,743] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-05-30 10:52:00,743] [INFO] [utils.py:803:see_memory_usage] MA 5.38 GB         Max_MA 5.38 GB         CA 5.66 GB         Max_CA 6 GB 
[2024-05-30 10:52:00,744] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 28.41 GB, percent = 12.0%
[2024-05-30 10:52:00,744] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-05-30 10:52:00,744] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-05-30 10:52:00,744] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-05-30 10:52:00,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2024-05-30 10:52:00,746] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-05-30 10:52:00,746] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-05-30 10:52:00,746] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-05-30 10:52:00,746] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-05-30 10:52:00,746] [INFO] [config.py:978:print]   amp_params ................... False
[2024-05-30 10:52:00,746] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-05-30 10:52:00,746] [INFO] [config.py:978:print]   bfloat16_enabled ............. False
[2024-05-30 10:52:00,746] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-05-30 10:52:00,746] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-05-30 10:52:00,746] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-05-30 10:52:00,746] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2a588927a0>
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   dump_state ................... False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   fp16_auto_cast ............... False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   fp16_enabled ................. True
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 8
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 65536
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   loss_scale ................... 0
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-05-30 10:52:00,747] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   pld_params ................... False
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   train_batch_size ............. 512
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  16
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   world_size ................... 4
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-05-30 10:52:00,748] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-05-30 10:52:00,748] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 512, 
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": false
    }
}
[INFO|trainer.py:1748] 2024-05-30 10:52:00,748 >> ***** Running training *****
[INFO|trainer.py:1749] 2024-05-30 10:52:00,748 >>   Num examples = 441,676
[INFO|trainer.py:1750] 2024-05-30 10:52:00,748 >>   Num Epochs = 1
[INFO|trainer.py:1751] 2024-05-30 10:52:00,748 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1754] 2024-05-30 10:52:00,748 >>   Total train batch size (w. parallel, distributed & accumulation) = 512
[INFO|trainer.py:1755] 2024-05-30 10:52:00,748 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1756] 2024-05-30 10:52:00,748 >>   Total optimization steps = 862
[INFO|trainer.py:1757] 2024-05-30 10:52:00,750 >>   Number of trainable parameters = 98,304
  0%|          | 0/862 [00:00<?, ?it/s]  0%|          | 1/862 [00:21<5:14:17, 21.90s/it]  0%|          | 2/862 [00:40<4:49:36, 20.21s/it]  0%|          | 3/862 [00:59<4:41:47, 19.68s/it]  0%|          | 4/862 [01:19<4:38:14, 19.46s/it]  1%|          | 5/862 [01:38<4:36:16, 19.34s/it]  1%|          | 6/862 [01:57<4:35:09, 19.29s/it]  1%|          | 7/862 [02:16<4:34:35, 19.27s/it]  1%|          | 8/862 [02:35<4:34:04, 19.26s/it]  1%|          | 9/862 [02:55<4:33:40, 19.25s/it]  1%|          | 10/862 [03:14<4:33:28, 19.26s/it]                                                  {'loss': 2.4374, 'learning_rate': 0.00019993359402677323, 'epoch': 0.01}
  1%|          | 10/862 [03:14<4:33:28, 19.26s/it]                                                  {'learning_rate': 0.00019993359402677323, 'aux_loss': 0.19836080074310303, 'epoch': 0.01}
  1%|          | 10/862 [03:15<4:33:28, 19.26s/it]  1%|▏         | 11/862 [03:33<4:33:10, 19.26s/it]  1%|▏         | 12/862 [03:52<4:32:50, 19.26s/it]  2%|▏         | 13/862 [04:12<4:32:39, 19.27s/it]  2%|▏         | 14/862 [04:31<4:32:30, 19.28s/it]  2%|▏         | 15/862 [04:50<4:32:15, 19.29s/it]  2%|▏         | 16/862 [05:10<4:32:50, 19.35s/it]  2%|▏         | 17/862 [05:29<4:32:22, 19.34s/it]  2%|▏         | 18/862 [05:48<4:31:52, 19.33s/it]  2%|▏         | 19/862 [06:08<4:31:26, 19.32s/it]  2%|▏         | 20/862 [06:27<4:30:56, 19.31s/it]                                                  {'loss': 2.4477, 'learning_rate': 0.0001997344643021585, 'epoch': 0.02}
  2%|▏         | 20/862 [06:27<4:30:56, 19.31s/it]                                                  {'learning_rate': 0.0001997344643021585, 'aux_loss': 0.2042127549648285, 'epoch': 0.02}
  2%|▏         | 20/862 [06:28<4:30:56, 19.31s/it]  2%|▏         | 21/862 [06:46<4:30:48, 19.32s/it]  3%|▎         | 22/862 [07:06<4:30:21, 19.31s/it]  3%|▎         | 23/862 [07:25<4:30:01, 19.31s/it]  3%|▎         | 24/862 [07:44<4:29:41, 19.31s/it]  3%|▎         | 25/862 [08:04<4:29:13, 19.30s/it]  3%|▎         | 26/862 [08:23<4:28:55, 19.30s/it]  3%|▎         | 27/862 [08:42<4:28:36, 19.30s/it]  3%|▎         | 28/862 [09:01<4:28:22, 19.31s/it]  3%|▎         | 29/862 [09:21<4:27:58, 19.30s/it]  3%|▎         | 30/862 [09:40<4:27:42, 19.31s/it]                                                  {'loss': 2.4504, 'learning_rate': 0.00019940287529421902, 'epoch': 0.03}
  3%|▎         | 30/862 [09:40<4:27:42, 19.31s/it]                                                  {'learning_rate': 0.00019940287529421902, 'aux_loss': 0.18534928560256958, 'epoch': 0.03}
  3%|▎         | 30/862 [09:41<4:27:42, 19.31s/it]  4%|▎         | 31/862 [09:59<4:27:42, 19.33s/it]  4%|▎         | 32/862 [10:19<4:28:15, 19.39s/it]  4%|▍         | 33/862 [10:38<4:28:02, 19.40s/it]  4%|▍         | 34/862 [10:58<4:27:21, 19.37s/it]  4%|▍         | 35/862 [11:17<4:26:43, 19.35s/it]  4%|▍         | 36/862 [11:36<4:26:11, 19.34s/it]  4%|▍         | 37/862 [11:56<4:25:43, 19.33s/it]  4%|▍         | 38/862 [12:15<4:25:25, 19.33s/it]  5%|▍         | 39/862 [12:34<4:24:52, 19.31s/it]  5%|▍         | 40/862 [12:54<4:24:27, 19.30s/it]                                                  {'loss': 2.4322, 'learning_rate': 0.0001989392673927705, 'epoch': 0.05}
  5%|▍         | 40/862 [12:54<4:24:27, 19.30s/it]                                                  {'learning_rate': 0.0001989392673927705, 'aux_loss': 0.20375576615333557, 'epoch': 0.05}
  5%|▍         | 40/862 [12:54<4:24:27, 19.30s/it]  5%|▍         | 41/862 [13:13<4:24:11, 19.31s/it]  5%|▍         | 42/862 [13:32<4:23:54, 19.31s/it]  5%|▍         | 43/862 [13:51<4:23:37, 19.31s/it]  5%|▌         | 44/862 [14:11<4:23:22, 19.32s/it]  5%|▌         | 45/862 [14:30<4:23:07, 19.32s/it]  5%|▌         | 46/862 [14:49<4:22:53, 19.33s/it]  5%|▌         | 47/862 [15:09<4:22:33, 19.33s/it]  6%|▌         | 48/862 [15:28<4:23:34, 19.43s/it]  6%|▌         | 49/862 [15:48<4:22:40, 19.39s/it]  6%|▌         | 50/862 [16:07<4:22:00, 19.36s/it]                                                  {'loss': 2.4247, 'learning_rate': 0.00019834425632449075, 'epoch': 0.06}
  6%|▌         | 50/862 [16:07<4:22:00, 19.36s/it]                                                  {'learning_rate': 0.00019834425632449075, 'aux_loss': 0.21976570785045624, 'epoch': 0.06}
  6%|▌         | 50/862 [16:08<4:22:00, 19.36s/it]  6%|▌         | 51/862 [16:26<4:21:37, 19.36s/it]  6%|▌         | 52/862 [16:46<4:21:10, 19.35s/it]  6%|▌         | 53/862 [17:05<4:20:39, 19.33s/it]  6%|▋         | 54/862 [17:24<4:20:09, 19.32s/it]  6%|▋         | 55/862 [17:44<4:19:47, 19.32s/it]  6%|▋         | 56/862 [18:03<4:19:28, 19.32s/it]  7%|▋         | 57/862 [18:22<4:19:04, 19.31s/it]  7%|▋         | 58/862 [18:42<4:18:40, 19.30s/it]  7%|▋         | 59/862 [19:01<4:18:25, 19.31s/it]  7%|▋         | 60/862 [19:20<4:18:03, 19.31s/it]                                                  {'loss': 2.4259, 'learning_rate': 0.00019761863233516117, 'epoch': 0.07}
  7%|▋         | 60/862 [19:20<4:18:03, 19.31s/it]                                                  {'learning_rate': 0.00019761863233516117, 'aux_loss': 0.17859481275081635, 'epoch': 0.07}
  7%|▋         | 60/862 [19:21<4:18:03, 19.31s/it]  7%|▋         | 61/862 [19:39<4:17:43, 19.31s/it]  7%|▋         | 62/862 [19:59<4:17:32, 19.32s/it]  7%|▋         | 63/862 [20:18<4:17:13, 19.32s/it]  7%|▋         | 64/862 [20:38<4:17:41, 19.37s/it]  8%|▊         | 65/862 [20:57<4:17:54, 19.42s/it]  8%|▊         | 66/862 [21:16<4:17:13, 19.39s/it]  8%|▊         | 67/862 [21:36<4:16:33, 19.36s/it]  8%|▊         | 68/862 [21:55<4:15:51, 19.33s/it]  8%|▊         | 69/862 [22:14<4:15:24, 19.32s/it]  8%|▊         | 70/862 [22:34<4:14:57, 19.32s/it]                                                  {'loss': 2.4227, 'learning_rate': 0.0001967633591401259, 'epoch': 0.08}
  8%|▊         | 70/862 [22:34<4:14:57, 19.32s/it]                                                  {'learning_rate': 0.0001967633591401259, 'aux_loss': 0.1949348896741867, 'epoch': 0.08}
  8%|▊         | 70/862 [22:34<4:14:57, 19.32s/it]  8%|▊         | 71/862 [22:53<4:14:33, 19.31s/it]  8%|▊         | 72/862 [23:12<4:14:04, 19.30s/it]  8%|▊         | 73/862 [23:31<4:13:46, 19.30s/it]  9%|▊         | 74/862 [23:51<4:13:24, 19.30s/it]  9%|▊         | 75/862 [24:10<4:13:08, 19.30s/it]  9%|▉         | 76/862 [24:29<4:12:49, 19.30s/it]  9%|▉         | 77/862 [24:49<4:12:32, 19.30s/it]  9%|▉         | 78/862 [25:08<4:12:11, 19.30s/it]  9%|▉         | 79/862 [25:27<4:11:55, 19.30s/it]  9%|▉         | 80/862 [25:47<4:11:34, 19.30s/it]                                                  {'loss': 2.4325, 'learning_rate': 0.0001957795726443628, 'epoch': 0.09}
  9%|▉         | 80/862 [25:47<4:11:34, 19.30s/it]                                                  {'learning_rate': 0.0001957795726443628, 'aux_loss': 0.2571057379245758, 'epoch': 0.09}
  9%|▉         | 80/862 [25:47<4:11:34, 19.30s/it]  9%|▉         | 81/862 [26:06<4:12:34, 19.40s/it] 10%|▉         | 82/862 [26:26<4:11:47, 19.37s/it] 10%|▉         | 83/862 [26:45<4:11:12, 19.35s/it] 10%|▉         | 84/862 [27:04<4:10:45, 19.34s/it] 10%|▉         | 85/862 [27:23<4:10:16, 19.33s/it] 10%|▉         | 86/862 [27:43<4:09:47, 19.31s/it] 10%|█         | 87/862 [28:02<4:09:25, 19.31s/it] 10%|█         | 88/862 [28:21<4:09:11, 19.32s/it] 10%|█         | 89/862 [28:41<4:08:50, 19.32s/it] 10%|█         | 90/862 [29:00<4:08:34, 19.32s/it]                                                  {'loss': 2.4178, 'learning_rate': 0.0001946685794338658, 'epoch': 0.1}
 10%|█         | 90/862 [29:00<4:08:34, 19.32s/it]                                                  {'learning_rate': 0.0001946685794338658, 'aux_loss': 0.21037137508392334, 'epoch': 0.1}
 10%|█         | 90/862 [29:01<4:08:34, 19.32s/it] 11%|█         | 91/862 [29:19<4:08:16, 19.32s/it] 11%|█         | 92/862 [29:39<4:07:52, 19.32s/it] 11%|█         | 93/862 [29:58<4:07:31, 19.31s/it] 11%|█         | 94/862 [30:17<4:07:15, 19.32s/it] 11%|█         | 95/862 [30:37<4:06:53, 19.31s/it] 11%|█         | 96/862 [30:56<4:07:31, 19.39s/it] 11%|█▏        | 97/862 [31:16<4:07:22, 19.40s/it] 11%|█▏        | 98/862 [31:35<4:06:38, 19.37s/it] 11%|█▏        | 99/862 [31:54<4:06:00, 19.34s/it] 12%|█▏        | 100/862 [32:13<4:05:25, 19.32s/it]                                                   {'loss': 2.4231, 'learning_rate': 0.00019343185504034277, 'epoch': 0.12}
 12%|█▏        | 100/862 [32:13<4:05:25, 19.32s/it]                                                   {'learning_rate': 0.00019343185504034277, 'aux_loss': 0.20267045497894287, 'epoch': 0.12}
 12%|█▏        | 100/862 [32:14<4:05:25, 19.32s/it] 12%|█▏        | 101/862 [32:33<4:05:17, 19.34s/it] 12%|█▏        | 102/862 [32:52<4:04:45, 19.32s/it] 12%|█▏        | 103/862 [33:11<4:04:17, 19.31s/it] 12%|█▏        | 104/862 [33:31<4:04:04, 19.32s/it] 12%|█▏        | 105/862 [33:50<4:03:34, 19.31s/it] 12%|█▏        | 106/862 [34:09<4:03:13, 19.30s/it] 12%|█▏        | 107/862 [34:29<4:02:53, 19.30s/it] 13%|█▎        | 108/862 [34:48<4:02:34, 19.30s/it] 13%|█▎        | 109/862 [35:07<4:02:12, 19.30s/it] 13%|█▎        | 110/862 [35:26<4:01:50, 19.30s/it]                                                   {'loss': 2.4339, 'learning_rate': 0.00019207104198153295, 'epoch': 0.13}
 13%|█▎        | 110/862 [35:26<4:01:50, 19.30s/it]                                                   {'learning_rate': 0.00019207104198153295, 'aux_loss': 0.18540644645690918, 'epoch': 0.13}
 13%|█▎        | 110/862 [35:27<4:01:50, 19.30s/it] 13%|█▎        | 111/862 [35:46<4:01:32, 19.30s/it] 13%|█▎        | 112/862 [36:05<4:01:16, 19.30s/it] 13%|█▎        | 113/862 [36:25<4:02:31, 19.43s/it] 13%|█▎        | 114/862 [36:44<4:01:48, 19.40s/it] 13%|█▎        | 115/862 [37:03<4:01:13, 19.38s/it] 13%|█▎        | 116/862 [37:23<4:00:36, 19.35s/it] 14%|█▎        | 117/862 [37:42<3:59:58, 19.33s/it] 14%|█▎        | 118/862 [38:01<3:59:34, 19.32s/it] 14%|█▍        | 119/862 [38:21<3:59:02, 19.30s/it] 14%|█▍        | 120/862 [38:40<3:58:42, 19.30s/it]                                                   {'loss': 2.4171, 'learning_rate': 0.0001905879475797474, 'epoch': 0.14}
 14%|█▍        | 120/862 [38:40<3:58:42, 19.30s/it]                                                   {'learning_rate': 0.0001905879475797474, 'aux_loss': 0.22883063554763794, 'epoch': 0.14}
 14%|█▍        | 120/862 [38:41<3:58:42, 19.30s/it] 14%|█▍        | 121/862 [38:59<3:58:26, 19.31s/it] 14%|█▍        | 122/862 [39:19<3:58:14, 19.32s/it] 14%|█▍        | 123/862 [39:38<3:57:55, 19.32s/it] 14%|█▍        | 124/862 [39:57<3:57:35, 19.32s/it] 15%|█▍        | 125/862 [40:16<3:57:15, 19.32s/it] 15%|█▍        | 126/862 [40:36<3:56:55, 19.31s/it] 15%|█▍        | 127/862 [40:55<3:56:33, 19.31s/it] 15%|█▍        | 128/862 [41:14<3:56:09, 19.30s/it] 15%|█▍        | 129/862 [41:34<3:57:22, 19.43s/it] 15%|█▌        | 130/862 [41:53<3:56:32, 19.39s/it]                                                   {'loss': 2.4114, 'learning_rate': 0.00018898454156152886, 'epoch': 0.15}
 15%|█▌        | 130/862 [41:53<3:56:32, 19.39s/it]                                                   {'learning_rate': 0.00018898454156152886, 'aux_loss': 0.2220323383808136, 'epoch': 0.15}
 15%|█▌        | 130/862 [41:54<3:56:32, 19.39s/it] 15%|█▌        | 131/862 [42:13<3:55:52, 19.36s/it] 15%|█▌        | 132/862 [42:32<3:55:25, 19.35s/it] 15%|█▌        | 133/862 [42:51<3:54:58, 19.34s/it] 16%|█▌        | 134/862 [43:11<3:54:30, 19.33s/it] 16%|█▌        | 135/862 [43:30<3:54:07, 19.32s/it] 16%|█▌        | 136/862 [43:49<3:53:51, 19.33s/it] 16%|█▌        | 137/862 [44:09<3:53:21, 19.31s/it] 16%|█▌        | 138/862 [44:28<3:52:56, 19.30s/it] 16%|█▌        | 139/862 [44:47<3:52:36, 19.30s/it] 16%|█▌        | 140/862 [45:06<3:52:18, 19.31s/it]                                                   {'loss': 2.4252, 'learning_rate': 0.0001872629534416197, 'epoch': 0.16}
 16%|█▌        | 140/862 [45:06<3:52:18, 19.31s/it]                                                   {'learning_rate': 0.0001872629534416197, 'aux_loss': 0.19232839345932007, 'epoch': 0.16}
 16%|█▌        | 140/862 [45:07<3:52:18, 19.31s/it] 16%|█▋        | 141/862 [45:26<3:52:02, 19.31s/it] 16%|█▋        | 142/862 [45:45<3:51:46, 19.31s/it] 17%|█▋        | 143/862 [46:04<3:51:22, 19.31s/it] 17%|█▋        | 144/862 [46:24<3:51:06, 19.31s/it] 17%|█▋        | 145/862 [46:43<3:52:00, 19.41s/it] 17%|█▋        | 146/862 [47:03<3:51:23, 19.39s/it] 17%|█▋        | 147/862 [47:22<3:50:42, 19.36s/it] 17%|█▋        | 148/862 [47:41<3:50:10, 19.34s/it] 17%|█▋        | 149/862 [48:01<3:49:44, 19.33s/it] 17%|█▋        | 150/862 [48:20<3:49:19, 19.32s/it]                                                   {'loss': 2.4081, 'learning_rate': 0.00018542546969471183, 'epoch': 0.17}
 17%|█▋        | 150/862 [48:20<3:49:19, 19.32s/it]                                                   {'learning_rate': 0.00018542546969471183, 'aux_loss': 0.16295693814754486, 'epoch': 0.17}
 17%|█▋        | 150/862 [48:21<3:49:19, 19.32s/it] 18%|█▊        | 151/862 [48:39<3:49:00, 19.33s/it] 18%|█▊        | 152/862 [48:59<3:48:33, 19.31s/it] 18%|█▊        | 153/862 [49:18<3:48:08, 19.31s/it] 18%|█▊        | 154/862 [49:37<3:47:50, 19.31s/it] 18%|█▊        | 155/862 [49:56<3:47:27, 19.30s/it] 18%|█▊        | 156/862 [50:16<3:47:12, 19.31s/it] 18%|█▊        | 157/862 [50:35<3:46:55, 19.31s/it] 18%|█▊        | 158/862 [50:54<3:46:32, 19.31s/it] 18%|█▊        | 159/862 [51:14<3:46:13, 19.31s/it] 19%|█▊        | 160/862 [51:33<3:46:01, 19.32s/it]                                                   {'loss': 2.4127, 'learning_rate': 0.00018347453071873536, 'epoch': 0.19}
 19%|█▊        | 160/862 [51:33<3:46:01, 19.32s/it]                                                   {'learning_rate': 0.00018347453071873536, 'aux_loss': 0.20434236526489258, 'epoch': 0.19}
 19%|█▊        | 160/862 [51:34<3:46:01, 19.32s/it] 19%|█▊        | 161/862 [51:53<3:47:04, 19.44s/it] 19%|█▉        | 162/862 [52:12<3:46:18, 19.40s/it] 19%|█▉        | 163/862 [52:31<3:45:50, 19.39s/it] 19%|█▉        | 164/862 [52:51<3:45:12, 19.36s/it] 19%|█▉        | 165/862 [53:10<3:44:32, 19.33s/it] 19%|█▉        | 166/862 [53:29<3:43:59, 19.31s/it] 19%|█▉        | 167/862 [53:49<3:43:43, 19.31s/it] 19%|█▉        | 168/862 [54:08<3:43:24, 19.32s/it] 20%|█▉        | 169/862 [54:27<3:42:54, 19.30s/it] 20%|█▉        | 170/862 [54:46<3:42:37, 19.30s/it]                                                   {'loss': 2.4101, 'learning_rate': 0.0001814127275937183, 'epoch': 0.2}
 20%|█▉        | 170/862 [54:46<3:42:37, 19.30s/it]                                                   {'learning_rate': 0.0001814127275937183, 'aux_loss': 0.16960637271404266, 'epoch': 0.2}
 20%|█▉        | 170/862 [54:47<3:42:37, 19.30s/it] 20%|█▉        | 171/862 [55:06<3:42:14, 19.30s/it] 20%|█▉        | 172/862 [55:25<3:41:51, 19.29s/it] 20%|██        | 173/862 [55:44<3:41:29, 19.29s/it] 20%|██        | 174/862 [56:04<3:41:16, 19.30s/it] 20%|██        | 175/862 [56:23<3:40:55, 19.29s/it] 20%|██        | 176/862 [56:42<3:40:29, 19.29s/it] 21%|██        | 177/862 [57:02<3:40:56, 19.35s/it] 21%|██        | 178/862 [57:21<3:41:03, 19.39s/it] 21%|██        | 179/862 [57:40<3:40:23, 19.36s/it] 21%|██        | 180/862 [58:00<3:39:48, 19.34s/it]                                                   {'loss': 2.4255, 'learning_rate': 0.00017924279864052313, 'epoch': 0.21}
 21%|██        | 180/862 [58:00<3:39:48, 19.34s/it]                                                   {'learning_rate': 0.00017924279864052313, 'aux_loss': 0.17715418338775635, 'epoch': 0.21}
 21%|██        | 180/862 [58:00<3:39:48, 19.34s/it] 21%|██        | 181/862 [58:19<3:39:27, 19.34s/it] 21%|██        | 182/862 [58:38<3:38:57, 19.32s/it] 21%|██        | 183/862 [58:58<3:38:32, 19.31s/it] 21%|██▏       | 184/862 [59:17<3:38:14, 19.31s/it] 21%|██▏       | 185/862 [59:36<3:37:53, 19.31s/it] 22%|██▏       | 186/862 [59:55<3:37:24, 19.30s/it] 22%|██▏       | 187/862 [1:00:15<3:37:05, 19.30s/it] 22%|██▏       | 188/862 [1:00:34<3:36:46, 19.30s/it] 22%|██▏       | 189/862 [1:00:53<3:36:25, 19.29s/it] 22%|██▏       | 190/862 [1:01:13<3:36:07, 19.30s/it]                                                     {'loss': 2.4201, 'learning_rate': 0.00017696762578402918, 'epoch': 0.22}
 22%|██▏       | 190/862 [1:01:13<3:36:07, 19.30s/it]                                                     {'learning_rate': 0.00017696762578402918, 'aux_loss': 0.18822380900382996, 'epoch': 0.22}
 22%|██▏       | 190/862 [1:01:13<3:36:07, 19.30s/it] 22%|██▏       | 191/862 [1:01:32<3:35:55, 19.31s/it] 22%|██▏       | 192/862 [1:01:51<3:35:32, 19.30s/it] 22%|██▏       | 193/862 [1:02:11<3:35:52, 19.36s/it] 23%|██▎       | 194/862 [1:02:30<3:35:50, 19.39s/it] 23%|██▎       | 195/862 [1:02:50<3:35:17, 19.37s/it] 23%|██▎       | 196/862 [1:03:09<3:34:42, 19.34s/it] 23%|██▎       | 197/862 [1:03:28<3:34:18, 19.34s/it] 23%|██▎       | 198/862 [1:03:47<3:33:51, 19.32s/it] 23%|██▎       | 199/862 [1:04:07<3:33:23, 19.31s/it] 23%|██▎       | 200/862 [1:04:26<3:33:02, 19.31s/it]                                                     {'loss': 2.4179, 'learning_rate': 0.0001745902307255924, 'epoch': 0.23}
 23%|██▎       | 200/862 [1:04:26<3:33:02, 19.31s/it]                                                     {'learning_rate': 0.0001745902307255924, 'aux_loss': 0.19930502772331238, 'epoch': 0.23}
 23%|██▎       | 200/862 [1:04:27<3:33:02, 19.31s/it] 23%|██▎       | 201/862 [1:04:45<3:32:47, 19.32s/it] 23%|██▎       | 202/862 [1:05:05<3:32:30, 19.32s/it] 24%|██▎       | 203/862 [1:05:24<3:32:08, 19.31s/it] 24%|██▎       | 204/862 [1:05:43<3:31:43, 19.31s/it] 24%|██▍       | 205/862 [1:06:03<3:31:27, 19.31s/it] 24%|██▍       | 206/862 [1:06:22<3:31:05, 19.31s/it] 24%|██▍       | 207/862 [1:06:41<3:30:46, 19.31s/it] 24%|██▍       | 208/862 [1:07:01<3:30:24, 19.30s/it] 24%|██▍       | 209/862 [1:07:20<3:30:48, 19.37s/it] 24%|██▍       | 210/862 [1:07:40<3:30:50, 19.40s/it]                                                     {'loss': 2.4154, 'learning_rate': 0.00017211377092986476, 'epoch': 0.24}
 24%|██▍       | 210/862 [1:07:40<3:30:50, 19.40s/it]                                                     {'learning_rate': 0.00017211377092986476, 'aux_loss': 0.18450191617012024, 'epoch': 0.24}
 24%|██▍       | 210/862 [1:07:40<3:30:50, 19.40s/it] 24%|██▍       | 211/862 [1:07:59<3:30:15, 19.38s/it] 25%|██▍       | 212/862 [1:08:18<3:29:34, 19.35s/it] 25%|██▍       | 213/862 [1:08:37<3:29:00, 19.32s/it] 25%|██▍       | 214/862 [1:08:57<3:28:30, 19.31s/it] 25%|██▍       | 215/862 [1:09:16<3:28:01, 19.29s/it] 25%|██▌       | 216/862 [1:09:35<3:27:42, 19.29s/it] 25%|██▌       | 217/862 [1:09:54<3:27:21, 19.29s/it] 25%|██▌       | 218/862 [1:10:14<3:27:02, 19.29s/it] 25%|██▌       | 219/862 [1:10:33<3:26:49, 19.30s/it] 26%|██▌       | 220/862 [1:10:52<3:26:35, 19.31s/it]                                                     {'loss': 2.4008, 'learning_rate': 0.00016954153543130405, 'epoch': 0.25}
 26%|██▌       | 220/862 [1:10:52<3:26:35, 19.31s/it]                                                     {'learning_rate': 0.00016954153543130405, 'aux_loss': 0.18301188945770264, 'epoch': 0.25}
 26%|██▌       | 220/862 [1:10:53<3:26:35, 19.31s/it] 26%|██▌       | 221/862 [1:11:12<3:26:14, 19.30s/it] 26%|██▌       | 222/862 [1:11:31<3:25:51, 19.30s/it] 26%|██▌       | 223/862 [1:11:50<3:25:34, 19.30s/it] 26%|██▌       | 224/862 [1:12:10<3:25:12, 19.30s/it] 26%|██▌       | 225/862 [1:12:29<3:24:52, 19.30s/it] 26%|██▌       | 226/862 [1:12:49<3:25:43, 19.41s/it] 26%|██▋       | 227/862 [1:13:08<3:25:04, 19.38s/it] 26%|██▋       | 228/862 [1:13:27<3:24:27, 19.35s/it] 27%|██▋       | 229/862 [1:13:47<3:24:06, 19.35s/it] 27%|██▋       | 230/862 [1:14:06<3:23:35, 19.33s/it]                                                     {'loss': 2.4045, 'learning_rate': 0.0001668769404659434, 'epoch': 0.27}
 27%|██▋       | 230/862 [1:14:06<3:23:35, 19.33s/it]                                                     {'learning_rate': 0.0001668769404659434, 'aux_loss': 0.172087162733078, 'epoch': 0.27}
 27%|██▋       | 230/862 [1:14:07<3:23:35, 19.33s/it] 27%|██▋       | 231/862 [1:14:25<3:23:07, 19.32s/it] 27%|██▋       | 232/862 [1:14:44<3:22:45, 19.31s/it] 27%|██▋       | 233/862 [1:15:04<3:22:24, 19.31s/it] 27%|██▋       | 234/862 [1:15:23<3:22:02, 19.30s/it] 27%|██▋       | 235/862 [1:15:42<3:21:37, 19.30s/it] 27%|██▋       | 236/862 [1:16:02<3:21:19, 19.30s/it] 27%|██▋       | 237/862 [1:16:21<3:21:03, 19.30s/it] 28%|██▊       | 238/862 [1:16:40<3:20:53, 19.32s/it] 28%|██▊       | 239/862 [1:17:00<3:20:35, 19.32s/it] 28%|██▊       | 240/862 [1:17:19<3:20:13, 19.32s/it]                                                     {'loss': 2.4112, 'learning_rate': 0.00016412352493422132, 'epoch': 0.28}
 28%|██▊       | 240/862 [1:17:19<3:20:13, 19.32s/it]                                                     {'learning_rate': 0.00016412352493422132, 'aux_loss': 0.19387398660182953, 'epoch': 0.28}
 28%|██▊       | 240/862 [1:17:20<3:20:13, 19.32s/it] 28%|██▊       | 241/862 [1:17:38<3:19:53, 19.31s/it] 28%|██▊       | 242/862 [1:17:58<3:20:42, 19.42s/it] 28%|██▊       | 243/862 [1:18:17<3:19:55, 19.38s/it] 28%|██▊       | 244/862 [1:18:36<3:19:24, 19.36s/it] 28%|██▊       | 245/862 [1:18:56<3:18:54, 19.34s/it] 29%|██▊       | 246/862 [1:19:15<3:18:20, 19.32s/it] 29%|██▊       | 247/862 [1:19:34<3:17:56, 19.31s/it] 29%|██▉       | 248/862 [1:19:54<3:17:33, 19.31s/it] 29%|██▉       | 249/862 [1:20:13<3:17:11, 19.30s/it] 29%|██▉       | 250/862 [1:20:32<3:16:49, 19.30s/it]                                                     {'loss': 2.3982, 'learning_rate': 0.00016128494570089944, 'epoch': 0.29}
 29%|██▉       | 250/862 [1:20:32<3:16:49, 19.30s/it]                                                     {'learning_rate': 0.00016128494570089944, 'aux_loss': 0.19550228118896484, 'epoch': 0.29}
 29%|██▉       | 250/862 [1:20:33<3:16:49, 19.30s/it] 29%|██▉       | 251/862 [1:20:52<3:17:18, 19.38s/it] 29%|██▉       | 252/862 [1:21:11<3:16:42, 19.35s/it] 29%|██▉       | 253/862 [1:21:30<3:16:08, 19.32s/it] 29%|██▉       | 254/862 [1:21:50<3:15:47, 19.32s/it] 30%|██▉       | 255/862 [1:22:09<3:15:20, 19.31s/it] 30%|██▉       | 256/862 [1:22:28<3:15:01, 19.31s/it] 30%|██▉       | 257/862 [1:22:47<3:14:39, 19.30s/it] 30%|██▉       | 258/862 [1:23:07<3:15:22, 19.41s/it] 30%|███       | 259/862 [1:23:26<3:14:37, 19.37s/it] 30%|███       | 260/862 [1:23:46<3:13:58, 19.33s/it]                                                     {'loss': 2.4102, 'learning_rate': 0.0001583649727383092, 'epoch': 0.3}
 30%|███       | 260/862 [1:23:46<3:13:58, 19.33s/it]                                                     {'learning_rate': 0.0001583649727383092, 'aux_loss': 0.16431771218776703, 'epoch': 0.3}
 30%|███       | 260/862 [1:23:46<3:13:58, 19.33s/it] 30%|███       | 261/862 [1:24:05<3:13:37, 19.33s/it] 30%|███       | 262/862 [1:24:24<3:13:10, 19.32s/it] 31%|███       | 263/862 [1:24:44<3:12:45, 19.31s/it] 31%|███       | 264/862 [1:25:03<3:12:23, 19.30s/it] 31%|███       | 265/862 [1:25:22<3:12:07, 19.31s/it] 31%|███       | 266/862 [1:25:41<3:11:48, 19.31s/it] 31%|███       | 267/862 [1:26:01<3:11:25, 19.30s/it] 31%|███       | 268/862 [1:26:20<3:11:05, 19.30s/it] 31%|███       | 269/862 [1:26:39<3:10:37, 19.29s/it] 31%|███▏      | 270/862 [1:26:59<3:10:16, 19.28s/it]                                                     {'loss': 2.4121, 'learning_rate': 0.00015536748411937814, 'epoch': 0.31}
 31%|███▏      | 270/862 [1:26:59<3:10:16, 19.28s/it]                                                     {'learning_rate': 0.00015536748411937814, 'aux_loss': 0.1538228690624237, 'epoch': 0.31}
 31%|███▏      | 270/862 [1:26:59<3:10:16, 19.28s/it] 31%|███▏      | 271/862 [1:27:18<3:09:59, 19.29s/it] 32%|███▏      | 272/862 [1:27:37<3:09:41, 19.29s/it] 32%|███▏      | 273/862 [1:27:56<3:09:21, 19.29s/it] 32%|███▏      | 274/862 [1:28:16<3:10:03, 19.39s/it] 32%|███▏      | 275/862 [1:28:35<3:09:29, 19.37s/it] 32%|███▏      | 276/862 [1:28:55<3:08:55, 19.34s/it] 32%|███▏      | 277/862 [1:29:14<3:08:23, 19.32s/it] 32%|███▏      | 278/862 [1:29:33<3:08:01, 19.32s/it] 32%|███▏      | 279/862 [1:29:53<3:07:37, 19.31s/it] 32%|███▏      | 280/862 [1:30:12<3:07:13, 19.30s/it]                                                     {'loss': 2.4071, 'learning_rate': 0.00015229646086708574, 'epoch': 0.32}
 32%|███▏      | 280/862 [1:30:12<3:07:13, 19.30s/it]                                                     {'learning_rate': 0.00015229646086708574, 'aux_loss': 0.17617766559123993, 'epoch': 0.32}
 32%|███▏      | 280/862 [1:30:13<3:07:13, 19.30s/it] 33%|███▎      | 281/862 [1:30:31<3:06:52, 19.30s/it] 33%|███▎      | 282/862 [1:30:50<3:06:40, 19.31s/it] 33%|███▎      | 283/862 [1:31:10<3:06:21, 19.31s/it] 33%|███▎      | 284/862 [1:31:29<3:06:02, 19.31s/it] 33%|███▎      | 285/862 [1:31:48<3:05:39, 19.31s/it] 33%|███▎      | 286/862 [1:32:08<3:05:18, 19.30s/it] 33%|███▎      | 287/862 [1:32:27<3:04:58, 19.30s/it] 33%|███▎      | 288/862 [1:32:46<3:04:36, 19.30s/it] 34%|███▎      | 289/862 [1:33:06<3:04:21, 19.30s/it] 34%|███▎      | 290/862 [1:33:25<3:04:55, 19.40s/it]                                                     {'loss': 2.4277, 'learning_rate': 0.00014915598166718945, 'epoch': 0.34}
 34%|███▎      | 290/862 [1:33:25<3:04:55, 19.40s/it]                                                     {'learning_rate': 0.00014915598166718945, 'aux_loss': 0.20202895998954773, 'epoch': 0.34}
 34%|███▎      | 290/862 [1:33:26<3:04:55, 19.40s/it] 34%|███▍      | 291/862 [1:33:45<3:04:22, 19.37s/it] 34%|███▍      | 292/862 [1:34:04<3:03:51, 19.35s/it] 34%|███▍      | 293/862 [1:34:23<3:03:13, 19.32s/it] 34%|███▍      | 294/862 [1:34:42<3:02:45, 19.31s/it] 34%|███▍      | 295/862 [1:35:02<3:02:20, 19.30s/it] 34%|███▍      | 296/862 [1:35:21<3:02:12, 19.32s/it] 34%|███▍      | 297/862 [1:35:40<3:01:42, 19.30s/it] 35%|███▍      | 298/862 [1:36:00<3:01:23, 19.30s/it] 35%|███▍      | 299/862 [1:36:19<3:01:07, 19.30s/it] 35%|███▍      | 300/862 [1:36:38<3:00:49, 19.30s/it]                                                     {'loss': 2.4034, 'learning_rate': 0.0001459502174512426, 'epoch': 0.35}
 35%|███▍      | 300/862 [1:36:38<3:00:49, 19.30s/it]                                                     {'learning_rate': 0.0001459502174512426, 'aux_loss': 0.15015697479248047, 'epoch': 0.35}
 35%|███▍      | 300/862 [1:36:39<3:00:49, 19.30s/it] 35%|███▍      | 301/862 [1:36:57<3:00:30, 19.30s/it] 35%|███▌      | 302/862 [1:37:17<3:00:07, 19.30s/it] 35%|███▌      | 303/862 [1:37:36<2:59:55, 19.31s/it] 35%|███▌      | 304/862 [1:37:55<2:59:31, 19.30s/it] 35%|███▌      | 305/862 [1:38:15<2:59:09, 19.30s/it] 35%|███▌      | 306/862 [1:38:34<2:59:27, 19.37s/it] 36%|███▌      | 307/862 [1:38:54<2:59:28, 19.40s/it] 36%|███▌      | 308/862 [1:39:13<2:58:48, 19.37s/it] 36%|███▌      | 309/862 [1:39:32<2:58:23, 19.36s/it] 36%|███▌      | 310/862 [1:39:52<2:57:57, 19.34s/it]                                                     {'loss': 2.4025, 'learning_rate': 0.00014268342585709913, 'epoch': 0.36}
 36%|███▌      | 310/862 [1:39:52<2:57:57, 19.34s/it]                                                     {'learning_rate': 0.00014268342585709913, 'aux_loss': 0.1631624549627304, 'epoch': 0.36}
 36%|███▌      | 310/862 [1:39:52<2:57:57, 19.34s/it] 36%|███▌      | 311/862 [1:40:11<2:57:29, 19.33s/it] 36%|███▌      | 312/862 [1:40:30<2:57:09, 19.33s/it] 36%|███▋      | 313/862 [1:40:50<2:56:47, 19.32s/it] 36%|███▋      | 314/862 [1:41:09<2:56:28, 19.32s/it] 37%|███▋      | 315/862 [1:41:28<2:56:07, 19.32s/it] 37%|███▋      | 316/862 [1:41:47<2:55:45, 19.31s/it] 37%|███▋      | 317/862 [1:42:07<2:55:32, 19.33s/it] 37%|███▋      | 318/862 [1:42:26<2:55:06, 19.31s/it] 37%|███▋      | 319/862 [1:42:45<2:54:41, 19.30s/it] 37%|███▋      | 320/862 [1:43:05<2:54:21, 19.30s/it]                                                     {'loss': 2.4176, 'learning_rate': 0.0001393599455742618, 'epoch': 0.37}
 37%|███▋      | 320/862 [1:43:05<2:54:21, 19.30s/it]                                                     {'learning_rate': 0.0001393599455742618, 'aux_loss': 0.16416257619857788, 'epoch': 0.37}
 37%|███▋      | 320/862 [1:43:05<2:54:21, 19.30s/it] 37%|███▋      | 321/862 [1:43:24<2:54:01, 19.30s/it] 37%|███▋      | 322/862 [1:43:43<2:54:14, 19.36s/it] 37%|███▋      | 323/862 [1:44:03<2:54:04, 19.38s/it] 38%|███▊      | 324/862 [1:44:22<2:53:34, 19.36s/it] 38%|███▊      | 325/862 [1:44:41<2:53:01, 19.33s/it] 38%|███▊      | 326/862 [1:45:01<2:52:38, 19.33s/it] 38%|███▊      | 327/862 [1:45:20<2:52:17, 19.32s/it] 38%|███▊      | 328/862 [1:45:39<2:51:57, 19.32s/it] 38%|███▊      | 329/862 [1:45:59<2:51:33, 19.31s/it] 38%|███▊      | 330/862 [1:46:18<2:51:09, 19.30s/it]                                                     {'loss': 2.4113, 'learning_rate': 0.0001359841905815842, 'epoch': 0.38}
 38%|███▊      | 330/862 [1:46:18<2:51:09, 19.30s/it]                                                     {'learning_rate': 0.0001359841905815842, 'aux_loss': 0.16763722896575928, 'epoch': 0.38}
 38%|███▊      | 330/862 [1:46:19<2:51:09, 19.30s/it] 38%|███▊      | 331/862 [1:46:37<2:50:52, 19.31s/it] 39%|███▊      | 332/862 [1:46:57<2:50:34, 19.31s/it] 39%|███▊      | 333/862 [1:47:16<2:50:11, 19.30s/it] 39%|███▊      | 334/862 [1:47:35<2:49:58, 19.32s/it] 39%|███▉      | 335/862 [1:47:55<2:49:35, 19.31s/it] 39%|███▉      | 336/862 [1:48:14<2:49:16, 19.31s/it] 39%|███▉      | 337/862 [1:48:33<2:48:59, 19.31s/it] 39%|███▉      | 338/862 [1:48:53<2:49:16, 19.38s/it] 39%|███▉      | 339/862 [1:49:12<2:48:59, 19.39s/it] 39%|███▉      | 340/862 [1:49:31<2:48:28, 19.36s/it]                                                     {'loss': 2.4253, 'learning_rate': 0.00013256064428497966, 'epoch': 0.39}
 39%|███▉      | 340/862 [1:49:31<2:48:28, 19.36s/it]                                                     {'learning_rate': 0.00013256064428497966, 'aux_loss': 0.16860973834991455, 'epoch': 0.39}
 39%|███▉      | 340/862 [1:49:32<2:48:28, 19.36s/it] 40%|███▉      | 341/862 [1:49:51<2:48:03, 19.35s/it] 40%|███▉      | 342/862 [1:50:10<2:47:35, 19.34s/it] 40%|███▉      | 343/862 [1:50:29<2:47:06, 19.32s/it] 40%|███▉      | 344/862 [1:50:49<2:46:45, 19.32s/it] 40%|████      | 345/862 [1:51:08<2:46:22, 19.31s/it] 40%|████      | 346/862 [1:51:27<2:46:02, 19.31s/it] 40%|████      | 347/862 [1:51:47<2:45:37, 19.30s/it] 40%|████      | 348/862 [1:52:06<2:45:24, 19.31s/it] 40%|████      | 349/862 [1:52:25<2:45:00, 19.30s/it] 41%|████      | 350/862 [1:52:44<2:44:39, 19.30s/it]                                                     {'loss': 2.4052, 'learning_rate': 0.0001290938535629224, 'epoch': 0.41}
 41%|████      | 350/862 [1:52:44<2:44:39, 19.30s/it]                                                     {'learning_rate': 0.0001290938535629224, 'aux_loss': 0.17253075540065765, 'epoch': 0.41}
 41%|████      | 350/862 [1:52:45<2:44:39, 19.30s/it] 41%|████      | 351/862 [1:53:04<2:44:21, 19.30s/it] 41%|████      | 352/862 [1:53:23<2:44:04, 19.30s/it] 41%|████      | 353/862 [1:53:42<2:43:40, 19.29s/it] 41%|████      | 354/862 [1:54:02<2:43:25, 19.30s/it] 41%|████      | 355/862 [1:54:21<2:44:11, 19.43s/it] 41%|████▏     | 356/862 [1:54:41<2:43:32, 19.39s/it] 41%|████▏     | 357/862 [1:55:00<2:43:00, 19.37s/it] 42%|████▏     | 358/862 [1:55:19<2:42:30, 19.35s/it] 42%|████▏     | 359/862 [1:55:39<2:42:03, 19.33s/it] 42%|████▏     | 360/862 [1:55:58<2:41:37, 19.32s/it]                                                     {'loss': 2.4099, 'learning_rate': 0.0001255884227276499, 'epoch': 0.42}
 42%|████▏     | 360/862 [1:55:58<2:41:37, 19.32s/it]                                                     {'learning_rate': 0.0001255884227276499, 'aux_loss': 0.13794519007205963, 'epoch': 0.42}
 42%|████▏     | 360/862 [1:55:59<2:41:37, 19.32s/it] 42%|████▏     | 361/862 [1:56:17<2:41:13, 19.31s/it] 42%|████▏     | 362/862 [1:56:36<2:40:58, 19.32s/it] 42%|████▏     | 363/862 [1:56:56<2:40:38, 19.32s/it] 42%|████▏     | 364/862 [1:57:15<2:40:15, 19.31s/it] 42%|████▏     | 365/862 [1:57:34<2:39:56, 19.31s/it] 42%|████▏     | 366/862 [1:57:54<2:39:40, 19.32s/it] 43%|████▎     | 367/862 [1:58:13<2:39:20, 19.31s/it] 43%|████▎     | 368/862 [1:58:32<2:38:59, 19.31s/it] 43%|████▎     | 369/862 [1:58:52<2:38:41, 19.31s/it] 43%|████▎     | 370/862 [1:59:11<2:38:20, 19.31s/it]                                                     {'loss': 2.4114, 'learning_rate': 0.000122049007410086, 'epoch': 0.43}
 43%|████▎     | 370/862 [1:59:11<2:38:20, 19.31s/it]                                                     {'learning_rate': 0.000122049007410086, 'aux_loss': 0.16882097721099854, 'epoch': 0.43}
 43%|████▎     | 370/862 [1:59:12<2:38:20, 19.31s/it] 43%|████▎     | 371/862 [1:59:31<2:39:06, 19.44s/it] 43%|████▎     | 372/862 [1:59:50<2:38:23, 19.40s/it] 43%|████▎     | 373/862 [2:00:09<2:37:56, 19.38s/it] 43%|████▎     | 374/862 [2:00:29<2:37:23, 19.35s/it] 44%|████▎     | 375/862 [2:00:48<2:36:54, 19.33s/it] 44%|████▎     | 376/862 [2:01:07<2:36:28, 19.32s/it] 44%|████▎     | 377/862 [2:01:26<2:36:01, 19.30s/it] 44%|████▍     | 378/862 [2:01:46<2:35:40, 19.30s/it] 44%|████▍     | 379/862 [2:02:05<2:35:21, 19.30s/it] 44%|████▍     | 380/862 [2:02:24<2:35:02, 19.30s/it]                                                     {'loss': 2.4162, 'learning_rate': 0.00011848030837660709, 'epoch': 0.44}
 44%|████▍     | 380/862 [2:02:24<2:35:02, 19.30s/it]                                                     {'learning_rate': 0.00011848030837660709, 'aux_loss': 0.17115625739097595, 'epoch': 0.44}
 44%|████▍     | 380/862 [2:02:25<2:35:02, 19.30s/it] 44%|████▍     | 381/862 [2:02:44<2:34:42, 19.30s/it] 44%|████▍     | 382/862 [2:03:03<2:34:24, 19.30s/it] 44%|████▍     | 383/862 [2:03:22<2:34:06, 19.30s/it] 45%|████▍     | 384/862 [2:03:42<2:33:41, 19.29s/it] 45%|████▍     | 385/862 [2:04:01<2:33:20, 19.29s/it] 45%|████▍     | 386/862 [2:04:20<2:33:03, 19.29s/it] 45%|████▍     | 387/862 [2:04:40<2:33:43, 19.42s/it] 45%|████▌     | 388/862 [2:04:59<2:33:05, 19.38s/it] 45%|████▌     | 389/862 [2:05:18<2:32:37, 19.36s/it] 45%|████▌     | 390/862 [2:05:38<2:32:07, 19.34s/it]                                                     {'loss': 2.4183, 'learning_rate': 0.00011488706528586261, 'epoch': 0.45}
 45%|████▌     | 390/862 [2:05:38<2:32:07, 19.34s/it]                                                     {'learning_rate': 0.00011488706528586261, 'aux_loss': 0.1675015389919281, 'epoch': 0.45}
 45%|████▌     | 390/862 [2:05:39<2:32:07, 19.34s/it] 45%|████▌     | 391/862 [2:05:57<2:31:46, 19.33s/it] 45%|████▌     | 392/862 [2:06:16<2:31:19, 19.32s/it] 46%|████▌     | 393/862 [2:06:36<2:30:57, 19.31s/it] 46%|████▌     | 394/862 [2:06:55<2:30:38, 19.31s/it] 46%|████▌     | 395/862 [2:07:14<2:30:14, 19.30s/it] 46%|████▌     | 396/862 [2:07:34<2:29:54, 19.30s/it] 46%|████▌     | 397/862 [2:07:53<2:29:35, 19.30s/it] 46%|████▌     | 398/862 [2:08:12<2:29:18, 19.31s/it] 46%|████▋     | 399/862 [2:08:31<2:28:54, 19.30s/it] 46%|████▋     | 400/862 [2:08:51<2:28:34, 19.30s/it]                                                     {'loss': 2.4189, 'learning_rate': 0.00011127405039394216, 'epoch': 0.46}
 46%|████▋     | 400/862 [2:08:51<2:28:34, 19.30s/it]                                                     {'learning_rate': 0.00011127405039394216, 'aux_loss': 0.12736570835113525, 'epoch': 0.46}
 46%|████▋     | 400/862 [2:08:51<2:28:34, 19.30s/it] 47%|████▋     | 401/862 [2:09:10<2:28:16, 19.30s/it] 47%|████▋     | 402/862 [2:09:29<2:27:52, 19.29s/it] 47%|████▋     | 403/862 [2:09:49<2:28:28, 19.41s/it] 47%|████▋     | 404/862 [2:10:08<2:27:57, 19.38s/it] 47%|████▋     | 405/862 [2:10:28<2:27:24, 19.35s/it] 47%|████▋     | 406/862 [2:10:47<2:26:53, 19.33s/it] 47%|████▋     | 407/862 [2:11:06<2:26:30, 19.32s/it] 47%|████▋     | 408/862 [2:11:25<2:26:05, 19.31s/it] 47%|████▋     | 409/862 [2:11:45<2:25:41, 19.30s/it] 48%|████▊     | 410/862 [2:12:04<2:25:21, 19.30s/it]                                                     {'loss': 2.3947, 'learning_rate': 0.00010764606221624933, 'epoch': 0.48}
 48%|████▊     | 410/862 [2:12:04<2:25:21, 19.30s/it]                                                     {'learning_rate': 0.00010764606221624933, 'aux_loss': 0.17547035217285156, 'epoch': 0.48}
 48%|████▊     | 410/862 [2:12:05<2:25:21, 19.30s/it] 48%|████▊     | 411/862 [2:12:23<2:25:00, 19.29s/it] 48%|████▊     | 412/862 [2:12:43<2:24:43, 19.30s/it] 48%|████▊     | 413/862 [2:13:02<2:24:23, 19.30s/it] 48%|████▊     | 414/862 [2:13:21<2:24:02, 19.29s/it] 48%|████▊     | 415/862 [2:13:40<2:23:45, 19.30s/it] 48%|████▊     | 416/862 [2:14:00<2:23:22, 19.29s/it] 48%|████▊     | 417/862 [2:14:19<2:23:02, 19.29s/it] 48%|████▊     | 418/862 [2:14:38<2:22:45, 19.29s/it] 49%|████▊     | 419/862 [2:14:58<2:22:55, 19.36s/it] 49%|████▊     | 420/862 [2:15:17<2:22:54, 19.40s/it]                                                     {'loss': 2.4061, 'learning_rate': 0.00010400791915450009, 'epoch': 0.49}
 49%|████▊     | 420/862 [2:15:17<2:22:54, 19.40s/it]                                                     {'learning_rate': 0.00010400791915450009, 'aux_loss': 0.1562061309814453, 'epoch': 0.49}
 49%|████▊     | 420/862 [2:15:18<2:22:54, 19.40s/it] 49%|████▉     | 421/862 [2:15:37<2:22:24, 19.37s/it] 49%|████▉     | 422/862 [2:15:56<2:21:54, 19.35s/it] 49%|████▉     | 423/862 [2:16:15<2:21:25, 19.33s/it] 49%|████▉     | 424/862 [2:16:35<2:21:03, 19.32s/it] 49%|████▉     | 425/862 [2:16:54<2:20:41, 19.32s/it] 49%|████▉     | 426/862 [2:17:13<2:20:18, 19.31s/it] 50%|████▉     | 427/862 [2:17:32<2:19:56, 19.30s/it] 50%|████▉     | 428/862 [2:17:52<2:19:42, 19.31s/it] 50%|████▉     | 429/862 [2:18:11<2:19:21, 19.31s/it] 50%|████▉     | 430/862 [2:18:30<2:19:01, 19.31s/it]                                                     {'loss': 2.407, 'learning_rate': 0.00010036445309730944, 'epoch': 0.5}
 50%|████▉     | 430/862 [2:18:30<2:19:01, 19.31s/it]                                                     {'learning_rate': 0.00010036445309730944, 'aux_loss': 0.18019092082977295, 'epoch': 0.5}
 50%|████▉     | 430/862 [2:18:31<2:19:01, 19.31s/it] 50%|█████     | 431/862 [2:18:50<2:18:40, 19.31s/it] 50%|█████     | 432/862 [2:19:09<2:18:23, 19.31s/it] 50%|█████     | 433/862 [2:19:28<2:18:00, 19.30s/it] 50%|█████     | 434/862 [2:19:48<2:17:42, 19.31s/it] 50%|█████     | 435/862 [2:20:07<2:18:15, 19.43s/it] 51%|█████     | 436/862 [2:20:27<2:17:38, 19.39s/it] 51%|█████     | 437/862 [2:20:46<2:17:07, 19.36s/it] 51%|█████     | 438/862 [2:21:05<2:16:34, 19.33s/it] 51%|█████     | 439/862 [2:21:24<2:16:14, 19.33s/it] 51%|█████     | 440/862 [2:21:44<2:15:49, 19.31s/it]                                                     {'loss': 2.4132, 'learning_rate': 9.672050300286636e-05, 'epoch': 0.51}
 51%|█████     | 440/862 [2:21:44<2:15:49, 19.31s/it]                                                     {'learning_rate': 9.672050300286636e-05, 'aux_loss': 0.13609281182289124, 'epoch': 0.51}
 51%|█████     | 440/862 [2:21:44<2:15:49, 19.31s/it] 51%|█████     | 441/862 [2:22:03<2:15:28, 19.31s/it] 51%|█████▏    | 442/862 [2:22:22<2:15:11, 19.31s/it] 51%|█████▏    | 443/862 [2:22:42<2:14:52, 19.31s/it] 52%|█████▏    | 444/862 [2:23:01<2:14:32, 19.31s/it] 52%|█████▏    | 445/862 [2:23:20<2:14:13, 19.31s/it] 52%|█████▏    | 446/862 [2:23:40<2:13:56, 19.32s/it] 52%|█████▏    | 447/862 [2:23:59<2:13:31, 19.31s/it] 52%|█████▏    | 448/862 [2:24:18<2:13:11, 19.30s/it] 52%|█████▏    | 449/862 [2:24:37<2:12:52, 19.30s/it] 52%|█████▏    | 450/862 [2:24:57<2:12:30, 19.30s/it]                                                     {'loss': 2.4141, 'learning_rate': 9.308090847221905e-05, 'epoch': 0.52}
 52%|█████▏    | 450/862 [2:24:57<2:12:30, 19.30s/it]                                                     {'learning_rate': 9.308090847221905e-05, 'aux_loss': 0.1654042899608612, 'epoch': 0.52}
 52%|█████▏    | 450/862 [2:24:58<2:12:30, 19.30s/it] 52%|█████▏    | 451/862 [2:25:16<2:12:36, 19.36s/it] 52%|█████▏    | 452/862 [2:25:36<2:12:33, 19.40s/it] 53%|█████▎    | 453/862 [2:25:55<2:12:02, 19.37s/it] 53%|█████▎    | 454/862 [2:26:14<2:11:31, 19.34s/it] 53%|█████▎    | 455/862 [2:26:34<2:11:07, 19.33s/it] 53%|█████▎    | 456/862 [2:26:53<2:10:49, 19.33s/it] 53%|█████▎    | 457/862 [2:27:12<2:10:25, 19.32s/it] 53%|█████▎    | 458/862 [2:27:32<2:10:02, 19.31s/it] 53%|█████▎    | 459/862 [2:27:51<2:09:46, 19.32s/it] 53%|█████▎    | 460/862 [2:28:10<2:09:28, 19.32s/it]                                                     {'loss': 2.4179, 'learning_rate': 8.945050332170672e-05, 'epoch': 0.53}
 53%|█████▎    | 460/862 [2:28:11<2:09:28, 19.32s/it]                                                     {'learning_rate': 8.945050332170672e-05, 'aux_loss': 0.14736628532409668, 'epoch': 0.53}
 53%|█████▎    | 460/862 [2:28:12<2:09:28, 19.32s/it] 53%|█████▎    | 461/862 [2:28:31<2:11:11, 19.63s/it] 54%|█████▎    | 462/862 [2:28:50<2:10:11, 19.53s/it] 54%|█████▎    | 463/862 [2:29:09<2:09:25, 19.46s/it] 54%|█████▍    | 464/862 [2:29:28<2:08:47, 19.42s/it] 54%|█████▍    | 465/862 [2:29:48<2:08:17, 19.39s/it] 54%|█████▍    | 466/862 [2:30:07<2:07:50, 19.37s/it] 54%|█████▍    | 467/862 [2:30:26<2:07:27, 19.36s/it] 54%|█████▍    | 468/862 [2:30:46<2:07:47, 19.46s/it] 54%|█████▍    | 469/862 [2:31:05<2:07:10, 19.42s/it] 55%|█████▍    | 470/862 [2:31:25<2:06:40, 19.39s/it]                                                     {'loss': 2.4251, 'learning_rate': 8.583410916307386e-05, 'epoch': 0.54}
 55%|█████▍    | 470/862 [2:31:26<2:06:40, 19.39s/it]                                                     {'learning_rate': 8.583410916307386e-05, 'aux_loss': 0.17397662997245789, 'epoch': 0.54}
 55%|█████▍    | 470/862 [2:31:27<2:06:40, 19.39s/it] 55%|█████▍    | 471/862 [2:31:46<2:08:59, 19.79s/it] 55%|█████▍    | 472/862 [2:32:05<2:07:44, 19.65s/it] 55%|█████▍    | 473/862 [2:32:24<2:06:42, 19.54s/it] 55%|█████▍    | 474/862 [2:32:43<2:05:57, 19.48s/it] 55%|█████▌    | 475/862 [2:33:03<2:05:14, 19.42s/it] 55%|█████▌    | 476/862 [2:33:22<2:04:41, 19.38s/it] 55%|█████▌    | 477/862 [2:33:41<2:04:17, 19.37s/it] 55%|█████▌    | 478/862 [2:34:01<2:03:48, 19.34s/it] 56%|█████▌    | 479/862 [2:34:20<2:03:23, 19.33s/it] 56%|█████▌    | 480/862 [2:34:39<2:03:01, 19.32s/it]                                                     {'loss': 2.3984, 'learning_rate': 8.223652899979402e-05, 'epoch': 0.56}
 56%|█████▌    | 480/862 [2:34:40<2:03:01, 19.32s/it]                                                     {'learning_rate': 8.223652899979402e-05, 'aux_loss': 0.18356788158416748, 'epoch': 0.56}
 56%|█████▌    | 480/862 [2:34:41<2:03:01, 19.32s/it] 56%|█████▌    | 481/862 [2:35:00<2:04:27, 19.60s/it] 56%|█████▌    | 482/862 [2:35:19<2:03:32, 19.51s/it] 56%|█████▌    | 483/862 [2:35:38<2:03:13, 19.51s/it] 56%|█████▌    | 484/862 [2:35:58<2:02:50, 19.50s/it] 56%|█████▋    | 485/862 [2:36:17<2:02:08, 19.44s/it] 56%|█████▋    | 486/862 [2:36:36<2:01:32, 19.39s/it] 56%|█████▋    | 487/862 [2:36:56<2:01:00, 19.36s/it] 57%|█████▋    | 488/862 [2:37:15<2:00:37, 19.35s/it] 57%|█████▋    | 489/862 [2:37:34<2:00:10, 19.33s/it] 57%|█████▋    | 490/862 [2:37:54<1:59:47, 19.32s/it]                                                     {'loss': 2.4234, 'learning_rate': 7.866254084810724e-05, 'epoch': 0.57}
 57%|█████▋    | 490/862 [2:37:55<1:59:47, 19.32s/it]                                                     {'learning_rate': 7.866254084810724e-05, 'aux_loss': 0.14415621757507324, 'epoch': 0.57}
 57%|█████▋    | 490/862 [2:37:57<1:59:47, 19.32s/it] 57%|█████▋    | 491/862 [2:38:15<2:03:40, 20.00s/it] 57%|█████▋    | 492/862 [2:38:34<2:02:03, 19.79s/it] 57%|█████▋    | 493/862 [2:38:54<2:00:53, 19.66s/it] 57%|█████▋    | 494/862 [2:39:13<1:59:54, 19.55s/it] 57%|█████▋    | 495/862 [2:39:32<1:59:05, 19.47s/it] 58%|█████▊    | 496/862 [2:39:52<1:58:27, 19.42s/it] 58%|█████▊    | 497/862 [2:40:11<1:57:52, 19.38s/it] 58%|█████▊    | 498/862 [2:40:30<1:57:27, 19.36s/it] 58%|█████▊    | 499/862 [2:40:50<1:57:02, 19.35s/it] 58%|█████▊    | 500/862 [2:41:09<1:57:14, 19.43s/it]                                                     {'loss': 2.4018, 'learning_rate': 7.511689139124382e-05, 'epoch': 0.58}
 58%|█████▊    | 500/862 [2:41:09<1:57:14, 19.43s/it]                                                     {'learning_rate': 7.511689139124382e-05, 'aux_loss': 0.15338990092277527, 'epoch': 0.58}
 58%|█████▊    | 500/862 [2:41:10<1:57:14, 19.43s/it] 58%|█████▊    | 501/862 [2:41:29<1:56:46, 19.41s/it] 58%|█████▊    | 502/862 [2:41:48<1:56:15, 19.38s/it] 58%|█████▊    | 503/862 [2:42:07<1:55:47, 19.35s/it] 58%|█████▊    | 504/862 [2:42:27<1:55:25, 19.34s/it] 59%|█████▊    | 505/862 [2:42:46<1:55:02, 19.34s/it] 59%|█████▊    | 506/862 [2:43:05<1:54:35, 19.31s/it] 59%|█████▉    | 507/862 [2:43:24<1:54:15, 19.31s/it] 59%|█████▉    | 508/862 [2:43:44<1:53:55, 19.31s/it] 59%|█████▉    | 509/862 [2:44:03<1:53:38, 19.32s/it] 59%|█████▉    | 510/862 [2:44:22<1:53:20, 19.32s/it]                                                     {'loss': 2.4088, 'learning_rate': 7.160428967526187e-05, 'epoch': 0.59}
 59%|█████▉    | 510/862 [2:44:22<1:53:20, 19.32s/it]                                                     {'learning_rate': 7.160428967526187e-05, 'aux_loss': 0.16311833262443542, 'epoch': 0.59}
 59%|█████▉    | 510/862 [2:44:23<1:53:20, 19.32s/it] 59%|█████▉    | 511/862 [2:44:42<1:52:59, 19.31s/it] 59%|█████▉    | 512/862 [2:45:01<1:52:40, 19.32s/it] 60%|█████▉    | 513/862 [2:45:20<1:52:18, 19.31s/it] 60%|█████▉    | 514/862 [2:45:40<1:51:59, 19.31s/it] 60%|█████▉    | 515/862 [2:45:59<1:51:50, 19.34s/it] 60%|█████▉    | 516/862 [2:46:19<1:52:01, 19.43s/it] 60%|█████▉    | 517/862 [2:46:38<1:51:32, 19.40s/it] 60%|██████    | 518/862 [2:46:57<1:51:02, 19.37s/it] 60%|██████    | 519/862 [2:47:17<1:50:37, 19.35s/it] 60%|██████    | 520/862 [2:47:36<1:50:13, 19.34s/it]                                                     {'loss': 2.4095, 'learning_rate': 6.81294008548715e-05, 'epoch': 0.6}
 60%|██████    | 520/862 [2:47:36<1:50:13, 19.34s/it]                                                     {'learning_rate': 6.81294008548715e-05, 'aux_loss': 0.15308065712451935, 'epoch': 0.6}
 60%|██████    | 520/862 [2:47:37<1:50:13, 19.34s/it] 60%|██████    | 521/862 [2:47:55<1:49:49, 19.32s/it] 61%|██████    | 522/862 [2:48:15<1:49:35, 19.34s/it] 61%|██████    | 523/862 [2:48:34<1:49:13, 19.33s/it] 61%|██████    | 524/862 [2:48:53<1:48:52, 19.33s/it] 61%|██████    | 525/862 [2:49:13<1:48:33, 19.33s/it] 61%|██████    | 526/862 [2:49:32<1:48:12, 19.32s/it] 61%|██████    | 527/862 [2:49:51<1:47:54, 19.33s/it] 61%|██████▏   | 528/862 [2:50:10<1:47:31, 19.32s/it] 61%|██████▏   | 529/862 [2:50:30<1:47:10, 19.31s/it] 61%|██████▏   | 530/862 [2:50:49<1:46:51, 19.31s/it]                                                     {'loss': 2.4172, 'learning_rate': 6.46968399975522e-05, 'epoch': 0.61}
 61%|██████▏   | 530/862 [2:50:49<1:46:51, 19.31s/it]                                                     {'learning_rate': 6.46968399975522e-05, 'aux_loss': 0.1473563015460968, 'epoch': 0.61}
 61%|██████▏   | 530/862 [2:50:50<1:46:51, 19.31s/it] 62%|██████▏   | 531/862 [2:51:08<1:46:31, 19.31s/it] 62%|██████▏   | 532/862 [2:51:28<1:46:48, 19.42s/it] 62%|██████▏   | 533/862 [2:51:47<1:46:22, 19.40s/it] 62%|██████▏   | 534/862 [2:52:07<1:45:56, 19.38s/it] 62%|██████▏   | 535/862 [2:52:26<1:45:29, 19.36s/it] 62%|██████▏   | 536/862 [2:52:45<1:45:06, 19.35s/it] 62%|██████▏   | 537/862 [2:53:05<1:44:43, 19.33s/it] 62%|██████▏   | 538/862 [2:53:24<1:44:21, 19.33s/it] 63%|██████▎   | 539/862 [2:53:43<1:44:00, 19.32s/it] 63%|██████▎   | 540/862 [2:54:03<1:43:38, 19.31s/it]                                                     {'loss': 2.3961, 'learning_rate': 6.131116595419178e-05, 'epoch': 0.63}
 63%|██████▎   | 540/862 [2:54:03<1:43:38, 19.31s/it]                                                     {'learning_rate': 6.131116595419178e-05, 'aux_loss': 0.16277313232421875, 'epoch': 0.63}
 63%|██████▎   | 540/862 [2:54:04<1:43:38, 19.31s/it] 63%|██████▎   | 541/862 [2:54:23<1:44:20, 19.50s/it] 63%|██████▎   | 542/862 [2:54:42<1:43:40, 19.44s/it] 63%|██████▎   | 543/862 [2:55:01<1:43:10, 19.41s/it] 63%|██████▎   | 544/862 [2:55:20<1:42:40, 19.37s/it] 63%|██████▎   | 545/862 [2:55:40<1:42:17, 19.36s/it] 63%|██████▎   | 546/862 [2:55:59<1:41:57, 19.36s/it] 63%|██████▎   | 547/862 [2:56:18<1:41:35, 19.35s/it] 64%|██████▎   | 548/862 [2:56:38<1:41:53, 19.47s/it] 64%|██████▎   | 549/862 [2:56:57<1:41:16, 19.41s/it] 64%|██████▍   | 550/862 [2:57:17<1:40:47, 19.38s/it]                                                     {'loss': 2.4031, 'learning_rate': 5.7976875304387756e-05, 'epoch': 0.64}
 64%|██████▍   | 550/862 [2:57:17<1:40:47, 19.38s/it]                                                     {'learning_rate': 5.7976875304387756e-05, 'aux_loss': 0.16358543932437897, 'epoch': 0.64}
 64%|██████▍   | 550/862 [2:57:18<1:40:47, 19.38s/it] 64%|██████▍   | 551/862 [2:57:36<1:40:21, 19.36s/it] 64%|██████▍   | 552/862 [2:57:55<1:39:59, 19.35s/it] 64%|██████▍   | 553/862 [2:58:15<1:39:38, 19.35s/it] 64%|██████▍   | 554/862 [2:58:34<1:39:17, 19.34s/it] 64%|██████▍   | 555/862 [2:58:53<1:38:55, 19.33s/it] 65%|██████▍   | 556/862 [2:59:13<1:38:32, 19.32s/it] 65%|██████▍   | 557/862 [2:59:32<1:38:14, 19.32s/it] 65%|██████▍   | 558/862 [2:59:51<1:37:50, 19.31s/it] 65%|██████▍   | 559/862 [3:00:11<1:37:31, 19.31s/it] 65%|██████▍   | 560/862 [3:00:30<1:37:17, 19.33s/it]                                                     {'loss': 2.4164, 'learning_rate': 5.46983963844526e-05, 'epoch': 0.65}
 65%|██████▍   | 560/862 [3:00:30<1:37:17, 19.33s/it]                                                     {'learning_rate': 5.46983963844526e-05, 'aux_loss': 0.1426703929901123, 'epoch': 0.65}
 65%|██████▍   | 560/862 [3:00:31<1:37:17, 19.33s/it] 65%|██████▌   | 561/862 [3:00:49<1:36:55, 19.32s/it] 65%|██████▌   | 562/862 [3:01:09<1:36:33, 19.31s/it] 65%|██████▌   | 563/862 [3:01:28<1:36:10, 19.30s/it] 65%|██████▌   | 564/862 [3:01:47<1:36:13, 19.37s/it] 66%|██████▌   | 565/862 [3:02:07<1:35:59, 19.39s/it] 66%|██████▌   | 566/862 [3:02:26<1:35:33, 19.37s/it] 66%|██████▌   | 567/862 [3:02:46<1:35:10, 19.36s/it] 66%|██████▌   | 568/862 [3:03:05<1:34:46, 19.34s/it] 66%|██████▌   | 569/862 [3:03:24<1:34:20, 19.32s/it] 66%|██████▌   | 570/862 [3:03:43<1:33:57, 19.31s/it]                                                     {'loss': 2.4199, 'learning_rate': 5.148008340605392e-05, 'epoch': 0.66}
 66%|██████▌   | 570/862 [3:03:43<1:33:57, 19.31s/it]                                                     {'learning_rate': 5.148008340605392e-05, 'aux_loss': 0.18133041262626648, 'epoch': 0.66}
 66%|██████▌   | 570/862 [3:03:44<1:33:57, 19.31s/it] 66%|██████▌   | 571/862 [3:04:03<1:33:36, 19.30s/it] 66%|██████▋   | 572/862 [3:04:22<1:33:15, 19.29s/it] 66%|██████▋   | 573/862 [3:04:41<1:32:54, 19.29s/it] 67%|██████▋   | 574/862 [3:05:01<1:32:37, 19.30s/it] 67%|██████▋   | 575/862 [3:05:20<1:32:17, 19.29s/it] 67%|██████▋   | 576/862 [3:05:39<1:31:55, 19.29s/it] 67%|██████▋   | 577/862 [3:05:58<1:31:37, 19.29s/it] 67%|██████▋   | 578/862 [3:06:18<1:31:18, 19.29s/it] 67%|██████▋   | 579/862 [3:06:37<1:30:56, 19.28s/it] 67%|██████▋   | 580/862 [3:06:56<1:30:54, 19.34s/it]                                                     {'loss': 2.4174, 'learning_rate': 4.8326210673301284e-05, 'epoch': 0.67}
 67%|██████▋   | 580/862 [3:06:56<1:30:54, 19.34s/it]                                                     {'learning_rate': 4.8326210673301284e-05, 'aux_loss': 0.1485668420791626, 'epoch': 0.67}
 67%|██████▋   | 580/862 [3:06:57<1:30:54, 19.34s/it] 67%|██████▋   | 581/862 [3:07:16<1:30:42, 19.37s/it] 68%|██████▊   | 582/862 [3:07:35<1:30:16, 19.35s/it] 68%|██████▊   | 583/862 [3:07:54<1:29:53, 19.33s/it] 68%|██████▊   | 584/862 [3:08:14<1:29:33, 19.33s/it] 68%|██████▊   | 585/862 [3:08:33<1:29:12, 19.32s/it] 68%|██████▊   | 586/862 [3:08:52<1:28:49, 19.31s/it] 68%|██████▊   | 587/862 [3:09:12<1:28:27, 19.30s/it] 68%|██████▊   | 588/862 [3:09:31<1:28:09, 19.31s/it] 68%|██████▊   | 589/862 [3:09:50<1:27:51, 19.31s/it] 68%|██████▊   | 590/862 [3:10:10<1:27:31, 19.31s/it]                                                     {'loss': 2.4121, 'learning_rate': 4.524096690595978e-05, 'epoch': 0.68}
 68%|██████▊   | 590/862 [3:10:11<1:27:31, 19.31s/it]                                                     {'learning_rate': 4.524096690595978e-05, 'aux_loss': 0.16701798141002655, 'epoch': 0.68}
 68%|██████▊   | 590/862 [3:10:12<1:27:31, 19.31s/it] 69%|██████▊   | 591/862 [3:10:31<1:29:45, 19.87s/it] 69%|██████▊   | 592/862 [3:10:50<1:28:39, 19.70s/it] 69%|██████▉   | 593/862 [3:11:09<1:27:48, 19.59s/it] 69%|██████▉   | 594/862 [3:11:29<1:27:05, 19.50s/it] 69%|██████▉   | 595/862 [3:11:48<1:26:31, 19.44s/it] 69%|██████▉   | 596/862 [3:12:08<1:26:24, 19.49s/it] 69%|██████▉   | 597/862 [3:12:27<1:26:05, 19.49s/it] 69%|██████▉   | 598/862 [3:12:46<1:25:29, 19.43s/it] 69%|██████▉   | 599/862 [3:13:06<1:25:02, 19.40s/it] 70%|██████▉   | 600/862 [3:13:25<1:24:36, 19.37s/it]                                                     {'loss': 2.401, 'learning_rate': 4.2228449676329616e-05, 'epoch': 0.7}
 70%|██████▉   | 600/862 [3:13:26<1:24:36, 19.37s/it]                                                     {'learning_rate': 4.2228449676329616e-05, 'aux_loss': 0.15139591693878174, 'epoch': 0.7}
 70%|██████▉   | 600/862 [3:13:27<1:24:36, 19.37s/it] 70%|██████▉   | 601/862 [3:13:46<1:26:09, 19.81s/it] 70%|██████▉   | 602/862 [3:14:05<1:25:11, 19.66s/it] 70%|██████▉   | 603/862 [3:14:24<1:24:25, 19.56s/it] 70%|███████   | 604/862 [3:14:44<1:23:47, 19.48s/it] 70%|███████   | 605/862 [3:15:03<1:23:13, 19.43s/it] 70%|███████   | 606/862 [3:15:22<1:22:46, 19.40s/it] 70%|███████   | 607/862 [3:15:42<1:22:18, 19.37s/it] 71%|███████   | 608/862 [3:16:01<1:21:52, 19.34s/it] 71%|███████   | 609/862 [3:16:20<1:21:32, 19.34s/it] 71%|███████   | 610/862 [3:16:40<1:21:11, 19.33s/it]                                                     {'loss': 2.4104, 'learning_rate': 3.929265996718072e-05, 'epoch': 0.71}
 71%|███████   | 610/862 [3:16:41<1:21:11, 19.33s/it]                                                     {'learning_rate': 3.929265996718072e-05, 'aux_loss': 0.15313169360160828, 'epoch': 0.71}
 71%|███████   | 610/862 [3:16:42<1:21:11, 19.33s/it] 71%|███████   | 611/862 [3:17:01<1:23:16, 19.91s/it] 71%|███████   | 612/862 [3:17:20<1:22:12, 19.73s/it] 71%|███████   | 613/862 [3:17:40<1:21:58, 19.75s/it] 71%|███████   | 614/862 [3:17:59<1:21:03, 19.61s/it] 71%|███████▏  | 615/862 [3:18:19<1:20:22, 19.52s/it] 71%|███████▏  | 616/862 [3:18:38<1:19:50, 19.47s/it] 72%|███████▏  | 617/862 [3:18:57<1:19:18, 19.42s/it] 72%|███████▏  | 618/862 [3:19:17<1:18:53, 19.40s/it] 72%|███████▏  | 619/862 [3:19:36<1:18:26, 19.37s/it] 72%|███████▏  | 620/862 [3:19:55<1:18:00, 19.34s/it]                                                     {'loss': 2.3993, 'learning_rate': 3.6437496857969566e-05, 'epoch': 0.72}
 72%|███████▏  | 620/862 [3:19:56<1:18:00, 19.34s/it]                                                     {'learning_rate': 3.6437496857969566e-05, 'aux_loss': 0.13817116618156433, 'epoch': 0.72}
 72%|███████▏  | 620/862 [3:19:58<1:18:00, 19.34s/it] 72%|███████▏  | 621/862 [3:20:16<1:19:28, 19.78s/it] 72%|███████▏  | 622/862 [3:20:35<1:18:32, 19.63s/it] 72%|███████▏  | 623/862 [3:20:55<1:18:07, 19.61s/it] 72%|███████▏  | 624/862 [3:21:14<1:17:24, 19.52s/it] 73%|███████▎  | 625/862 [3:21:33<1:16:49, 19.45s/it] 73%|███████▎  | 626/862 [3:21:53<1:16:22, 19.42s/it] 73%|███████▎  | 627/862 [3:22:12<1:15:53, 19.38s/it] 73%|███████▎  | 628/862 [3:22:31<1:15:29, 19.36s/it] 73%|███████▎  | 629/862 [3:22:51<1:15:34, 19.46s/it] 73%|███████▎  | 630/862 [3:23:10<1:15:02, 19.41s/it]                                                     {'loss': 2.406, 'learning_rate': 3.366675234639601e-05, 'epoch': 0.73}
 73%|███████▎  | 630/862 [3:23:11<1:15:02, 19.41s/it]                                                     {'learning_rate': 3.366675234639601e-05, 'aux_loss': 0.14962385594844818, 'epoch': 0.73}
 73%|███████▎  | 630/862 [3:23:12<1:15:02, 19.41s/it] 73%|███████▎  | 631/862 [3:23:31<1:15:37, 19.64s/it] 73%|███████▎  | 632/862 [3:23:50<1:14:55, 19.54s/it] 73%|███████▎  | 633/862 [3:24:09<1:14:21, 19.48s/it] 74%|███████▎  | 634/862 [3:24:29<1:14:01, 19.48s/it] 74%|███████▎  | 635/862 [3:24:48<1:13:30, 19.43s/it] 74%|███████▍  | 636/862 [3:25:07<1:13:03, 19.39s/it] 74%|███████▍  | 637/862 [3:25:27<1:12:40, 19.38s/it] 74%|███████▍  | 638/862 [3:25:46<1:12:16, 19.36s/it] 74%|███████▍  | 639/862 [3:26:05<1:11:52, 19.34s/it] 74%|███████▍  | 640/862 [3:26:25<1:11:32, 19.34s/it]                                                     {'loss': 2.4275, 'learning_rate': 3.0984106312177e-05, 'epoch': 0.74}
 74%|███████▍  | 640/862 [3:26:25<1:11:32, 19.34s/it]                                                     {'learning_rate': 3.0984106312177e-05, 'aux_loss': 0.1479611098766327, 'epoch': 0.74}
 74%|███████▍  | 640/862 [3:26:25<1:11:32, 19.34s/it] 74%|███████▍  | 641/862 [3:26:44<1:11:09, 19.32s/it] 74%|███████▍  | 642/862 [3:27:03<1:10:48, 19.31s/it] 75%|███████▍  | 643/862 [3:27:22<1:10:28, 19.31s/it] 75%|███████▍  | 644/862 [3:27:42<1:10:09, 19.31s/it] 75%|███████▍  | 645/862 [3:28:01<1:10:14, 19.42s/it] 75%|███████▍  | 646/862 [3:28:21<1:09:46, 19.38s/it] 75%|███████▌  | 647/862 [3:28:40<1:09:22, 19.36s/it] 75%|███████▌  | 648/862 [3:28:59<1:08:58, 19.34s/it] 75%|███████▌  | 649/862 [3:29:19<1:08:33, 19.31s/it] 75%|███████▌  | 650/862 [3:29:38<1:08:14, 19.31s/it]                                                     {'loss': 2.4119, 'learning_rate': 2.8393121629727138e-05, 'epoch': 0.75}
 75%|███████▌  | 650/862 [3:29:38<1:08:14, 19.31s/it]                                                     {'learning_rate': 2.8393121629727138e-05, 'aux_loss': 0.12994274497032166, 'epoch': 0.75}
 75%|███████▌  | 650/862 [3:29:39<1:08:14, 19.31s/it] 76%|███████▌  | 651/862 [3:29:57<1:07:53, 19.31s/it] 76%|███████▌  | 652/862 [3:30:16<1:07:34, 19.31s/it] 76%|███████▌  | 653/862 [3:30:36<1:07:14, 19.30s/it] 76%|███████▌  | 654/862 [3:30:55<1:06:55, 19.31s/it] 76%|███████▌  | 655/862 [3:31:14<1:06:37, 19.31s/it] 76%|███████▌  | 656/862 [3:31:34<1:06:16, 19.30s/it] 76%|███████▌  | 657/862 [3:31:53<1:05:57, 19.31s/it] 76%|███████▋  | 658/862 [3:32:12<1:05:37, 19.30s/it] 76%|███████▋  | 659/862 [3:32:32<1:05:17, 19.30s/it] 77%|███████▋  | 660/862 [3:32:51<1:04:57, 19.30s/it]                                                     {'loss': 2.419, 'learning_rate': 2.5897239436235466e-05, 'epoch': 0.76}
 77%|███████▋  | 660/862 [3:32:51<1:04:57, 19.30s/it]                                                     {'learning_rate': 2.5897239436235466e-05, 'aux_loss': 0.14796319603919983, 'epoch': 0.76}
 77%|███████▋  | 660/862 [3:32:52<1:04:57, 19.30s/it] 77%|███████▋  | 661/862 [3:33:10<1:04:59, 19.40s/it] 77%|███████▋  | 662/862 [3:33:30<1:04:32, 19.36s/it] 77%|███████▋  | 663/862 [3:33:49<1:04:06, 19.33s/it] 77%|███████▋  | 664/862 [3:34:08<1:03:45, 19.32s/it] 77%|███████▋  | 665/862 [3:34:28<1:03:25, 19.32s/it] 77%|███████▋  | 666/862 [3:34:47<1:03:07, 19.33s/it] 77%|███████▋  | 667/862 [3:35:06<1:02:48, 19.33s/it] 77%|███████▋  | 668/862 [3:35:26<1:02:31, 19.34s/it] 78%|███████▊  | 669/862 [3:35:45<1:02:10, 19.33s/it] 78%|███████▊  | 670/862 [3:36:04<1:01:47, 19.31s/it]                                                     {'loss': 2.415, 'learning_rate': 2.3499774561424327e-05, 'epoch': 0.78}
 78%|███████▊  | 670/862 [3:36:04<1:01:47, 19.31s/it]                                                     {'learning_rate': 2.3499774561424327e-05, 'aux_loss': 0.17432987689971924, 'epoch': 0.78}
 78%|███████▊  | 670/862 [3:36:05<1:01:47, 19.31s/it] 78%|███████▊  | 671/862 [3:36:24<1:01:29, 19.31s/it] 78%|███████▊  | 672/862 [3:36:43<1:01:08, 19.31s/it] 78%|███████▊  | 673/862 [3:37:02<1:00:47, 19.30s/it] 78%|███████▊  | 674/862 [3:37:21<1:00:28, 19.30s/it] 78%|███████▊  | 675/862 [3:37:41<1:00:09, 19.30s/it] 78%|███████▊  | 676/862 [3:38:00<59:48, 19.29s/it]   79%|███████▊  | 677/862 [3:38:20<59:48, 19.40s/it] 79%|███████▊  | 678/862 [3:38:39<59:22, 19.36s/it] 79%|███████▉  | 679/862 [3:38:58<58:57, 19.33s/it] 79%|███████▉  | 680/862 [3:39:17<58:34, 19.31s/it]                                                   {'loss': 2.4003, 'learning_rate': 2.120391112505955e-05, 'epoch': 0.79}
 79%|███████▉  | 680/862 [3:39:18<58:34, 19.31s/it]                                                   {'learning_rate': 2.120391112505955e-05, 'aux_loss': 0.12723016738891602, 'epoch': 0.79}
 79%|███████▉  | 680/862 [3:39:19<58:34, 19.31s/it] 79%|███████▉  | 681/862 [3:39:37<58:29, 19.39s/it] 79%|███████▉  | 682/862 [3:39:56<58:06, 19.37s/it] 79%|███████▉  | 683/862 [3:40:16<57:42, 19.34s/it] 79%|███████▉  | 684/862 [3:40:35<57:20, 19.33s/it] 79%|███████▉  | 685/862 [3:40:54<57:00, 19.33s/it] 80%|███████▉  | 686/862 [3:41:14<56:38, 19.31s/it] 80%|███████▉  | 687/862 [3:41:33<56:19, 19.31s/it] 80%|███████▉  | 688/862 [3:41:52<55:58, 19.30s/it] 80%|███████▉  | 689/862 [3:42:11<55:38, 19.30s/it] 80%|████████  | 690/862 [3:42:31<55:18, 19.30s/it]                                                   {'loss': 2.3996, 'learning_rate': 1.9012698308058852e-05, 'epoch': 0.8}
 80%|████████  | 690/862 [3:42:31<55:18, 19.30s/it]                                                   {'learning_rate': 1.9012698308058852e-05, 'aux_loss': 0.1397865116596222, 'epoch': 0.8}
 80%|████████  | 690/862 [3:42:32<55:18, 19.30s/it] 80%|████████  | 691/862 [3:42:50<54:59, 19.30s/it] 80%|████████  | 692/862 [3:43:09<54:40, 19.30s/it] 80%|████████  | 693/862 [3:43:29<54:32, 19.37s/it] 81%|████████  | 694/862 [3:43:48<54:14, 19.37s/it] 81%|████████  | 695/862 [3:44:07<53:49, 19.34s/it] 81%|████████  | 696/862 [3:44:27<53:29, 19.33s/it] 81%|████████  | 697/862 [3:44:46<53:07, 19.32s/it] 81%|████████  | 698/862 [3:45:05<52:46, 19.31s/it] 81%|████████  | 699/862 [3:45:25<52:27, 19.31s/it] 81%|████████  | 700/862 [3:45:44<52:06, 19.30s/it]                                                   {'loss': 2.4213, 'learning_rate': 1.6929046302815443e-05, 'epoch': 0.81}
 81%|████████  | 700/862 [3:45:44<52:06, 19.30s/it]                                                   {'learning_rate': 1.6929046302815443e-05, 'aux_loss': 0.14236749708652496, 'epoch': 0.81}
 81%|████████  | 700/862 [3:45:45<52:06, 19.30s/it] 81%|████████▏ | 701/862 [3:46:03<51:46, 19.29s/it] 81%|████████▏ | 702/862 [3:46:23<51:26, 19.29s/it] 82%|████████▏ | 703/862 [3:46:42<51:08, 19.30s/it] 82%|████████▏ | 704/862 [3:47:01<50:47, 19.29s/it] 82%|████████▏ | 705/862 [3:47:20<50:28, 19.29s/it] 82%|████████▏ | 706/862 [3:47:40<50:09, 19.29s/it] 82%|████████▏ | 707/862 [3:47:59<49:48, 19.28s/it] 82%|████████▏ | 708/862 [3:48:18<49:30, 19.29s/it] 82%|████████▏ | 709/862 [3:48:38<49:21, 19.36s/it] 82%|████████▏ | 710/862 [3:48:57<49:07, 19.39s/it]                                                   {'loss': 2.4141, 'learning_rate': 1.4955722448114807e-05, 'epoch': 0.82}
 82%|████████▏ | 710/862 [3:48:58<49:07, 19.39s/it]                                                   {'learning_rate': 1.4955722448114807e-05, 'aux_loss': 0.15202753245830536, 'epoch': 0.82}
 82%|████████▏ | 710/862 [3:48:59<49:07, 19.39s/it] 82%|████████▏ | 711/862 [3:49:17<49:07, 19.52s/it] 83%|████████▎ | 712/862 [3:49:36<48:37, 19.45s/it] 83%|████████▎ | 713/862 [3:49:56<48:11, 19.41s/it] 83%|████████▎ | 714/862 [3:50:15<47:47, 19.37s/it] 83%|████████▎ | 715/862 [3:50:34<47:23, 19.34s/it] 83%|████████▎ | 716/862 [3:50:53<47:00, 19.32s/it] 83%|████████▎ | 717/862 [3:51:13<46:41, 19.32s/it] 83%|████████▎ | 718/862 [3:51:32<46:21, 19.31s/it] 83%|████████▎ | 719/862 [3:51:51<46:01, 19.31s/it] 84%|████████▎ | 720/862 [3:52:11<45:42, 19.31s/it]                                                   {'loss': 2.392, 'learning_rate': 1.3095347553778193e-05, 'epoch': 0.83}
 84%|████████▎ | 720/862 [3:52:11<45:42, 19.31s/it]                                                   {'learning_rate': 1.3095347553778193e-05, 'aux_loss': 0.13987889885902405, 'epoch': 0.83}
 84%|████████▎ | 720/862 [3:52:12<45:42, 19.31s/it] 84%|████████▎ | 721/862 [3:52:30<45:21, 19.30s/it] 84%|████████▍ | 722/862 [3:52:49<45:01, 19.29s/it] 84%|████████▍ | 723/862 [3:53:09<44:42, 19.30s/it] 84%|████████▍ | 724/862 [3:53:28<44:22, 19.29s/it] 84%|████████▍ | 725/862 [3:53:47<44:10, 19.34s/it] 84%|████████▍ | 726/862 [3:54:07<43:54, 19.37s/it] 84%|████████▍ | 727/862 [3:54:26<43:31, 19.34s/it] 84%|████████▍ | 728/862 [3:54:45<43:09, 19.33s/it] 85%|████████▍ | 729/862 [3:55:05<42:49, 19.32s/it] 85%|████████▍ | 730/862 [3:55:24<42:28, 19.31s/it]                                                   {'loss': 2.4071, 'learning_rate': 1.135039241991408e-05, 'epoch': 0.85}
 85%|████████▍ | 730/862 [3:55:24<42:28, 19.31s/it]                                                   {'learning_rate': 1.135039241991408e-05, 'aux_loss': 0.18002758920192719, 'epoch': 0.85}
 85%|████████▍ | 730/862 [3:55:25<42:28, 19.31s/it] 85%|████████▍ | 731/862 [3:55:43<42:09, 19.31s/it] 85%|████████▍ | 732/862 [3:56:03<41:49, 19.30s/it] 85%|████████▌ | 733/862 [3:56:22<41:29, 19.30s/it] 85%|████████▌ | 734/862 [3:56:41<41:11, 19.31s/it] 85%|████████▌ | 735/862 [3:57:00<40:50, 19.30s/it] 85%|████████▌ | 736/862 [3:57:20<40:31, 19.30s/it] 85%|████████▌ | 737/862 [3:57:39<40:11, 19.29s/it] 86%|████████▌ | 738/862 [3:57:58<39:52, 19.29s/it] 86%|████████▌ | 739/862 [3:58:18<39:32, 19.29s/it] 86%|████████▌ | 740/862 [3:58:37<39:13, 19.29s/it]                                                   {'loss': 2.4068, 'learning_rate': 9.72317455540055e-06, 'epoch': 0.86}
 86%|████████▌ | 740/862 [3:58:37<39:13, 19.29s/it]                                                   {'learning_rate': 9.72317455540055e-06, 'aux_loss': 0.16150125861167908, 'epoch': 0.86}
 86%|████████▌ | 740/862 [3:58:38<39:13, 19.29s/it] 86%|████████▌ | 741/862 [3:58:56<38:56, 19.31s/it] 86%|████████▌ | 742/862 [3:59:16<38:47, 19.40s/it] 86%|████████▌ | 743/862 [3:59:35<38:23, 19.36s/it] 86%|████████▋ | 744/862 [3:59:54<38:01, 19.34s/it] 86%|████████▋ | 745/862 [4:00:14<37:41, 19.33s/it] 87%|████████▋ | 746/862 [4:00:33<37:20, 19.31s/it] 87%|████████▋ | 747/862 [4:00:52<37:01, 19.32s/it] 87%|████████▋ | 748/862 [4:01:12<36:42, 19.32s/it] 87%|████████▋ | 749/862 [4:01:31<36:22, 19.31s/it] 87%|████████▋ | 750/862 [4:01:50<36:01, 19.30s/it]                                                   {'loss': 2.4105, 'learning_rate': 8.215855099956472e-06, 'epoch': 0.87}
 87%|████████▋ | 750/862 [4:01:50<36:01, 19.30s/it]                                                   {'learning_rate': 8.215855099956472e-06, 'aux_loss': 0.14802998304367065, 'epoch': 0.87}
 87%|████████▋ | 750/862 [4:01:51<36:01, 19.30s/it] 87%|████████▋ | 751/862 [4:02:10<35:44, 19.32s/it] 87%|████████▋ | 752/862 [4:02:29<35:24, 19.31s/it] 87%|████████▋ | 753/862 [4:02:48<35:04, 19.31s/it] 87%|████████▋ | 754/862 [4:03:07<34:44, 19.30s/it] 88%|████████▊ | 755/862 [4:03:27<34:26, 19.32s/it] 88%|████████▊ | 756/862 [4:03:46<34:06, 19.30s/it] 88%|████████▊ | 757/862 [4:04:05<33:47, 19.31s/it] 88%|████████▊ | 758/862 [4:04:25<33:41, 19.44s/it] 88%|████████▊ | 759/862 [4:04:44<33:19, 19.41s/it] 88%|████████▊ | 760/862 [4:05:04<32:56, 19.38s/it]                                                   {'loss': 2.4041, 'learning_rate': 6.83043595388988e-06, 'epoch': 0.88}
 88%|████████▊ | 760/862 [4:05:04<32:56, 19.38s/it]                                                   {'learning_rate': 6.83043595388988e-06, 'aux_loss': 0.11727029085159302, 'epoch': 0.88}
 88%|████████▊ | 760/862 [4:05:05<32:56, 19.38s/it] 88%|████████▊ | 761/862 [4:05:23<32:34, 19.35s/it] 88%|████████▊ | 762/862 [4:05:42<32:14, 19.34s/it] 89%|████████▊ | 763/862 [4:06:02<31:52, 19.32s/it] 89%|████████▊ | 764/862 [4:06:21<31:32, 19.31s/it] 89%|████████▊ | 765/862 [4:06:40<31:13, 19.31s/it] 89%|████████▉ | 766/862 [4:07:00<30:54, 19.32s/it] 89%|████████▉ | 767/862 [4:07:19<30:33, 19.30s/it] 89%|████████▉ | 768/862 [4:07:38<30:14, 19.30s/it] 89%|████████▉ | 769/862 [4:07:57<29:55, 19.31s/it] 89%|████████▉ | 770/862 [4:08:17<29:36, 19.31s/it]                                                   {'loss': 2.4118, 'learning_rate': 5.568757119335244e-06, 'epoch': 0.89}
 89%|████████▉ | 770/862 [4:08:17<29:36, 19.31s/it]                                                   {'learning_rate': 5.568757119335244e-06, 'aux_loss': 0.1266161948442459, 'epoch': 0.89}
 89%|████████▉ | 770/862 [4:08:18<29:36, 19.31s/it] 89%|████████▉ | 771/862 [4:08:36<29:17, 19.31s/it] 90%|████████▉ | 772/862 [4:08:55<28:57, 19.31s/it] 90%|████████▉ | 773/862 [4:09:15<28:38, 19.30s/it] 90%|████████▉ | 774/862 [4:09:34<28:28, 19.42s/it] 90%|████████▉ | 775/862 [4:09:54<28:05, 19.37s/it] 90%|█████████ | 776/862 [4:10:13<27:44, 19.36s/it] 90%|█████████ | 777/862 [4:10:32<27:23, 19.33s/it] 90%|█████████ | 778/862 [4:10:51<27:02, 19.31s/it] 90%|█████████ | 779/862 [4:11:11<26:42, 19.31s/it] 90%|█████████ | 780/862 [4:11:30<26:22, 19.30s/it]                                                   {'loss': 2.4214, 'learning_rate': 4.432494256510711e-06, 'epoch': 0.9}
 90%|█████████ | 780/862 [4:11:31<26:22, 19.30s/it]                                                   {'learning_rate': 4.432494256510711e-06, 'aux_loss': 0.15067800879478455, 'epoch': 0.9}
 90%|█████████ | 780/862 [4:11:32<26:22, 19.30s/it] 91%|█████████ | 781/862 [4:11:51<26:32, 19.66s/it] 91%|█████████ | 782/862 [4:12:10<26:03, 19.55s/it] 91%|█████████ | 783/862 [4:12:29<25:38, 19.48s/it] 91%|█████████ | 784/862 [4:12:48<25:14, 19.42s/it] 91%|█████████ | 785/862 [4:13:08<24:51, 19.38s/it] 91%|█████████ | 786/862 [4:13:27<24:30, 19.35s/it] 91%|█████████▏| 787/862 [4:13:46<24:09, 19.33s/it] 91%|█████████▏| 788/862 [4:14:06<23:49, 19.32s/it] 92%|█████████▏| 789/862 [4:14:25<23:29, 19.30s/it] 92%|█████████▏| 790/862 [4:14:45<23:17, 19.41s/it]                                                   {'loss': 2.4098, 'learning_rate': 3.4231564582412167e-06, 'epoch': 0.92}
 92%|█████████▏| 790/862 [4:14:46<23:17, 19.41s/it]                                                   {'learning_rate': 3.4231564582412167e-06, 'aux_loss': 0.1797686070203781, 'epoch': 0.92}
 92%|█████████▏| 790/862 [4:14:47<23:17, 19.41s/it] 92%|█████████▏| 791/862 [4:15:05<23:26, 19.81s/it] 92%|█████████▏| 792/862 [4:15:25<22:55, 19.66s/it] 92%|█████████▏| 793/862 [4:15:44<22:28, 19.55s/it] 92%|█████████▏| 794/862 [4:16:03<22:03, 19.47s/it] 92%|█████████▏| 795/862 [4:16:22<21:41, 19.42s/it] 92%|█████████▏| 796/862 [4:16:42<21:19, 19.39s/it] 92%|█████████▏| 797/862 [4:17:01<20:58, 19.36s/it] 93%|█████████▎| 798/862 [4:17:20<20:37, 19.34s/it] 93%|█████████▎| 799/862 [4:17:40<20:17, 19.32s/it] 93%|█████████▎| 800/862 [4:17:59<19:58, 19.32s/it]                                                   {'loss': 2.4079, 'learning_rate': 2.542084245702947e-06, 'epoch': 0.93}
 93%|█████████▎| 800/862 [4:17:59<19:58, 19.32s/it]                                                   {'learning_rate': 2.542084245702947e-06, 'aux_loss': 0.15738575160503387, 'epoch': 0.93}
 93%|█████████▎| 800/862 [4:18:00<19:58, 19.32s/it] 93%|█████████▎| 801/862 [4:18:18<19:39, 19.34s/it] 93%|█████████▎| 802/862 [4:18:38<19:19, 19.33s/it] 93%|█████████▎| 803/862 [4:18:57<18:59, 19.31s/it] 93%|█████████▎| 804/862 [4:19:16<18:39, 19.30s/it] 93%|█████████▎| 805/862 [4:19:35<18:20, 19.30s/it] 94%|█████████▎| 806/862 [4:19:55<18:04, 19.36s/it] 94%|█████████▎| 807/862 [4:20:14<17:45, 19.38s/it] 94%|█████████▎| 808/862 [4:20:34<17:25, 19.36s/it] 94%|█████████▍| 809/862 [4:20:53<17:04, 19.33s/it] 94%|█████████▍| 810/862 [4:21:12<16:44, 19.32s/it]                                                   {'loss': 2.4095, 'learning_rate': 1.7904477880510307e-06, 'epoch': 0.94}
 94%|█████████▍| 810/862 [4:21:12<16:44, 19.32s/it]                                                   {'learning_rate': 1.7904477880510307e-06, 'aux_loss': 0.13898411393165588, 'epoch': 0.94}
 94%|█████████▍| 810/862 [4:21:13<16:44, 19.32s/it] 94%|█████████▍| 811/862 [4:21:32<16:25, 19.32s/it] 94%|█████████▍| 812/862 [4:21:51<16:04, 19.30s/it] 94%|█████████▍| 813/862 [4:22:10<15:45, 19.29s/it] 94%|█████████▍| 814/862 [4:22:29<15:26, 19.29s/it] 95%|█████████▍| 815/862 [4:22:49<15:06, 19.29s/it] 95%|█████████▍| 816/862 [4:23:08<14:47, 19.29s/it] 95%|█████████▍| 817/862 [4:23:27<14:27, 19.28s/it] 95%|█████████▍| 818/862 [4:23:47<14:08, 19.29s/it] 95%|█████████▌| 819/862 [4:24:06<13:49, 19.28s/it] 95%|█████████▌| 820/862 [4:24:25<13:29, 19.28s/it]                                                   {'loss': 2.3916, 'learning_rate': 1.1692453482951115e-06, 'epoch': 0.95}
 95%|█████████▌| 820/862 [4:24:25<13:29, 19.28s/it]                                                   {'learning_rate': 1.1692453482951115e-06, 'aux_loss': 0.1573818325996399, 'epoch': 0.95}
 95%|█████████▌| 820/862 [4:24:26<13:29, 19.28s/it] 95%|█████████▌| 821/862 [4:24:44<13:10, 19.29s/it] 95%|█████████▌| 822/862 [4:25:04<12:55, 19.38s/it] 95%|█████████▌| 823/862 [4:25:23<12:34, 19.35s/it] 96%|█████████▌| 824/862 [4:25:43<12:14, 19.33s/it] 96%|█████████▌| 825/862 [4:26:02<11:54, 19.31s/it] 96%|█████████▌| 826/862 [4:26:21<11:35, 19.31s/it] 96%|█████████▌| 827/862 [4:26:40<11:15, 19.31s/it] 96%|█████████▌| 828/862 [4:27:00<10:56, 19.32s/it] 96%|█████████▌| 829/862 [4:27:19<10:37, 19.32s/it] 96%|█████████▋| 830/862 [4:27:38<10:18, 19.32s/it]                                                   {'loss': 2.4054, 'learning_rate': 6.793019574868775e-07, 'epoch': 0.96}
 96%|█████████▋| 830/862 [4:27:38<10:18, 19.32s/it]                                                   {'learning_rate': 6.793019574868775e-07, 'aux_loss': 0.16911093890666962, 'epoch': 0.96}
 96%|█████████▋| 830/862 [4:27:39<10:18, 19.32s/it] 96%|█████████▋| 831/862 [4:27:58<09:58, 19.32s/it] 97%|█████████▋| 832/862 [4:28:17<09:40, 19.34s/it] 97%|█████████▋| 833/862 [4:28:36<09:20, 19.33s/it] 97%|█████████▋| 834/862 [4:28:56<09:01, 19.33s/it] 97%|█████████▋| 835/862 [4:29:15<08:41, 19.33s/it] 97%|█████████▋| 836/862 [4:29:34<08:22, 19.32s/it] 97%|█████████▋| 837/862 [4:29:54<08:02, 19.31s/it] 97%|█████████▋| 838/862 [4:30:13<07:45, 19.38s/it] 97%|█████████▋| 839/862 [4:30:33<07:26, 19.41s/it] 97%|█████████▋| 840/862 [4:30:52<07:06, 19.38s/it]                                                   {'loss': 2.4206, 'learning_rate': 3.212683189801724e-07, 'epoch': 0.97}
 97%|█████████▋| 840/862 [4:30:52<07:06, 19.38s/it]                                                   {'learning_rate': 3.212683189801724e-07, 'aux_loss': 0.12431545555591583, 'epoch': 0.97}
 97%|█████████▋| 840/862 [4:30:54<07:06, 19.38s/it] 98%|█████████▊| 841/862 [4:31:12<06:51, 19.59s/it] 98%|█████████▊| 842/862 [4:31:31<06:29, 19.50s/it] 98%|█████████▊| 843/862 [4:31:51<06:09, 19.43s/it] 98%|█████████▊| 844/862 [4:32:10<05:48, 19.39s/it] 98%|█████████▊| 845/862 [4:32:29<05:29, 19.37s/it] 98%|█████████▊| 846/862 [4:32:49<05:09, 19.34s/it] 98%|█████████▊| 847/862 [4:33:08<04:49, 19.32s/it] 98%|█████████▊| 848/862 [4:33:27<04:30, 19.32s/it] 98%|█████████▊| 849/862 [4:33:46<04:11, 19.32s/it] 99%|█████████▊| 850/862 [4:34:06<03:51, 19.31s/it]                                                   {'loss': 2.4057, 'learning_rate': 9.561994421924958e-08, 'epoch': 0.99}
 99%|█████████▊| 850/862 [4:34:06<03:51, 19.31s/it]                                                   {'learning_rate': 9.561994421924958e-08, 'aux_loss': 0.15747380256652832, 'epoch': 0.99}
 99%|█████████▊| 850/862 [4:34:07<03:51, 19.31s/it] 99%|█████████▊| 851/862 [4:34:25<03:33, 19.41s/it] 99%|█████████▉| 852/862 [4:34:45<03:13, 19.39s/it] 99%|█████████▉| 853/862 [4:35:04<02:54, 19.37s/it] 99%|█████████▉| 854/862 [4:35:23<02:34, 19.34s/it] 99%|█████████▉| 855/862 [4:35:43<02:16, 19.44s/it] 99%|█████████▉| 856/862 [4:36:02<01:56, 19.40s/it] 99%|█████████▉| 857/862 [4:36:22<01:36, 19.38s/it]100%|█████████▉| 858/862 [4:36:41<01:17, 19.35s/it]100%|█████████▉| 859/862 [4:37:00<00:58, 19.34s/it]100%|█████████▉| 860/862 [4:37:20<00:38, 19.33s/it]                                                   {'loss': 2.4007, 'learning_rate': 2.656521202770712e-09, 'epoch': 1.0}
100%|█████████▉| 860/862 [4:37:20<00:38, 19.33s/it]                                                   {'learning_rate': 2.656521202770712e-09, 'aux_loss': 0.13990779221057892, 'epoch': 1.0}
100%|█████████▉| 860/862 [4:37:21<00:38, 19.33s/it]100%|█████████▉| 861/862 [4:37:39<00:19, 19.46s/it]100%|██████████| 862/862 [4:37:59<00:00, 19.41s/it][INFO|trainer.py:1989] 2024-05-30 15:29:59,886 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   {'train_runtime': 16679.136, 'train_samples_per_second': 26.481, 'train_steps_per_second': 0.052, 'train_loss': 2.413431011053913, 'epoch': 1.0}
100%|██████████| 862/862 [4:37:59<00:00, 19.41s/it]100%|██████████| 862/862 [4:37:59<00:00, 19.35s/it]
[INFO|trainer.py:2981] 2024-05-30 15:30:04,405 >> Saving model checkpoint to /home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w
/home/nfs02/wangzj/public_code/hitsz/peft/src/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/nfs02/wangzj/models/Qwen1.5-1.8B - will assume that the vocabulary was not modified.
  warnings.warn(
[2024-05-30 15:30:09,504] [INFO] [launch.py:347:main] Process 3186 exits successfully.
[2024-05-30 15:30:10,506] [INFO] [launch.py:347:main] Process 3184 exits successfully.
[2024-05-30 15:30:10,506] [INFO] [launch.py:347:main] Process 3185 exits successfully.
[INFO|tokenization_utils_base.py:2435] 2024-05-30 15:30:41,314 >> tokenizer config file saved in /home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-05-30 15:30:41,475 >> Special tokens file saved in /home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w/special_tokens_map.json
[INFO|tokenization_utils_base.py:2495] 2024-05-30 15:30:41,582 >> added tokens file saved in /home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w/added_tokens.json
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     2.4134
  train_runtime            = 4:37:59.13
  train_samples_per_second =     26.481
  train_steps_per_second   =      0.052
Figure saved: /home/nfs04/wangzj/checkpoints/moe/1.8b-arderu6b-top2auxrouter-entropy0.001-enarderu5w/training_loss.png
05/30/2024 15:30:44 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-05-30 15:30:44,120 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-05-30 15:30:47,547] [INFO] [launch.py:347:main] Process 3183 exits successfully.
