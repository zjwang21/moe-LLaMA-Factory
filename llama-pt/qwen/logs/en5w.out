[2024-07-10 15:01:55,555] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 15:02:04,028] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-10 15:02:04,076] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/alimoe/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/nfs04/wangzj/models/Qwen1.5-1.8B --adapter_name_or_path /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01 --train_only_router --ce_loss_coef 0.05 --do_train --dataset slimpajam_1b --max_samples 50000 --preprocessing_num_workers 16 --cutoff_len 512 --finetuning_type moe --output_dir /home/nfs04/wangzj/checkpoints/moe/test --overwrite_output_dir --per_device_train_batch_size 16 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 100 --save_steps 10000 --save_only_model --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-07-10 15:02:05,623] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 15:02:07,805] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-10 15:02:07,805] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-10 15:02:07,805] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-10 15:02:07,805] [INFO] [launch.py:163:main] dist_world_size=8
[2024-07-10 15:02:07,805] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-10 15:02:24,116] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 15:02:24,118] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 15:02:24,119] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 15:02:24,119] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 15:02:24,119] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 15:02:24,119] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 15:02:24,120] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 15:02:24,121] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 15:02:36,656] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 15:02:36,656] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 15:02:36,656] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 15:02:36,657] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 15:02:36,657] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 15:02:36,657] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 15:02:36,657] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-10 15:02:36,657] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 15:02:36,662] [INFO] [comm.py:637:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul10_15-02-36_2080ti-2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|tokenization_utils_base.py:2027] 2024-07-10 15:02:37,770 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2027] 2024-07-10 15:02:37,770 >> loading file merges.txt
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Process rank: 6, device: cuda:6, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2027] 2024-07-10 15:02:37,770 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-07-10 15:02:37,770 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-07-10 15:02:37,770 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-07-10 15:02:37,770 >> loading file tokenizer.json
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=6,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul10_15-02-36_2080ti-2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul10_15-02-36_2080ti-2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Process rank: 7, device: cuda:7, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul10_15-02-36_2080ti-2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=7,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul10_15-02-36_2080ti-2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul10_15-02-36_2080ti-2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Process rank: 5, device: cuda:5, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=5,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul10_15-02-36_2080ti-2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Process rank: 4, device: cuda:4, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
07/10/2024 15:02:37 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=4,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jul10_15-02-36_2080ti-2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-07-10 15:02:38,119 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:727] 2024-07-10 15:02:38,120 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/config.json
[INFO|configuration_utils.py:792] 2024-07-10 15:02:38,122 >> Model config Qwen2Config {
  "_name_or_path": "/home/nfs04/wangzj/models/Qwen1.5-1.8B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3334] 2024-07-10 15:02:38,225 >> loading weights file /home/nfs04/wangzj/models/Qwen1.5-1.8B/model.safetensors
[INFO|modeling_utils.py:1459] 2024-07-10 15:02:38,240 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:827] 2024-07-10 15:02:38,242 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}

07/10/2024 15:02:42 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/10/2024 15:02:42 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/10/2024 15:02:42 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
07/10/2024 15:02:43 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/10/2024 15:02:43 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/10/2024 15:02:43 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
[INFO|modeling_utils.py:4070] 2024-07-10 15:02:46,297 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4078] 2024-07-10 15:02:46,297 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/nfs04/wangzj/models/Qwen1.5-1.8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
07/10/2024 15:02:46 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
07/10/2024 15:02:46 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
07/10/2024 15:02:46 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
[INFO|configuration_utils.py:780] 2024-07-10 15:02:46,340 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/generation_config.json
[INFO|configuration_utils.py:827] 2024-07-10 15:02:46,340 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "max_new_tokens": 2048
}

07/10/2024 15:02:46 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
07/10/2024 15:02:46 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
07/10/2024 15:02:46 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
07/10/2024 15:02:46 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01.
07/10/2024 15:03:39 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
07/10/2024 15:03:39 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/10/2024 15:03:39 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/10/2024 15:03:39 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/10/2024 15:03:40 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/10/2024 15:03:40 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/10/2024 15:03:40 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/10/2024 15:03:40 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/10/2024 15:03:40 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/10/2024 15:03:40 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=4, num_heads=4, topk=1, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
07/10/2024 15:03:40 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/aliyun/1.8b-elhutr8b-4expert-top2-lr5e-5-load_balance_0.01_polar0.01
07/10/2024 15:03:40 - INFO - llmtuner.model.loader - trainable params: 196608 || all params: 4271818752 || trainable%: 0.0046
07/10/2024 15:03:47 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Using custom data configuration default-74e4f6a24fa3d738
Loading Dataset Infos from /home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Downloading and preparing dataset json/default to /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 2360 examples [00:00, 9127.78 examples/s]Generating train split: 11597 examples [00:00, 33370.97 examples/s]Generating train split: 15798 examples [00:00, 32925.73 examples/s]Generating train split: 22470 examples [00:00, 38381.76 examples/s]Generating train split: 29541 examples [00:00, 43222.37 examples/s]Generating train split: 38236 examples [00:00, 45218.37 examples/s]Generating train split: 47390 examples [00:01, 46476.93 examples/s]Generating train split: 54006 examples [00:01, 40788.37 examples/s]Generating train split: 63183 examples [00:01, 44269.49 examples/s]Generating train split: 67766 examples [00:01, 43840.89 examples/s]Generating train split: 74619 examples [00:01, 45079.99 examples/s]Generating train split: 83930 examples [00:02, 46449.09 examples/s]Generating train split: 92877 examples [00:02, 45626.40 examples/s]Generating train split: 99643 examples [00:02, 44880.81 examples/s]Generating train split: 109274 examples [00:02, 47061.39 examples/s]Generating train split: 116498 examples [00:13, 2384.56 examples/s] Generating train split: 121255 examples [00:13, 2959.20 examples/s]Generating train split: 130120 examples [00:13, 4426.34 examples/s]Generating train split: 139333 examples [00:13, 6485.36 examples/s]Generating train split: 148487 examples [00:13, 9093.75 examples/s]Generating train split: 158099 examples [00:13, 12606.55 examples/s]Generating train split: 167231 examples [00:14, 16524.68 examples/s]Generating train split: 176904 examples [00:14, 21503.18 examples/s]Generating train split: 186378 examples [00:14, 26744.58 examples/s]Generating train split: 193184 examples [00:14, 29985.83 examples/s]Generating train split: 199969 examples [00:14, 33370.25 examples/s]Generating train split: 209120 examples [00:14, 36775.76 examples/s]Generating train split: 215976 examples [00:15, 39024.65 examples/s]Generating train split: 225535 examples [00:15, 42368.84 examples/s]Generating train split: 230248 examples [00:29, 42368.84 examples/s]Generating train split: 232119 examples [00:31, 1566.75 examples/s] Generating train split: 241247 examples [00:31, 2315.26 examples/s]Generating train split: 250163 examples [00:31, 3314.36 examples/s]Generating train split: 256750 examples [00:32, 4325.74 examples/s]Generating train split: 266174 examples [00:32, 6277.34 examples/s]Generating train split: 272721 examples [00:32, 7986.01 examples/s]Generating train split: 281976 examples [00:32, 11158.07 examples/s]Generating train split: 288977 examples [00:32, 14027.16 examples/s]Generating train split: 295818 examples [00:32, 17340.11 examples/s]Generating train split: 304569 examples [00:33, 21633.83 examples/s]Generating train split: 311391 examples [00:33, 25326.65 examples/s]Generating train split: 318280 examples [00:33, 27839.04 examples/s]Generating train split: 325214 examples [00:33, 31521.77 examples/s]Generating train split: 333994 examples [00:33, 33633.17 examples/s]Generating train split: 343112 examples [00:33, 37912.30 examples/s]Generating train split: 350046 examples [00:49, 1623.73 examples/s] Generating train split: 359220 examples [00:49, 2408.92 examples/s]Generating train split: 368512 examples [00:49, 3512.64 examples/s]Generating train split: 378015 examples [00:49, 5041.72 examples/s]Generating train split: 387194 examples [00:50, 7012.97 examples/s]Generating train split: 396140 examples [00:50, 9478.25 examples/s]Generating train split: 406340 examples [00:50, 13151.47 examples/s]Generating train split: 415408 examples [00:50, 16897.88 examples/s]Generating train split: 424728 examples [00:50, 21445.77 examples/s]Generating train split: 433805 examples [00:50, 26285.26 examples/s]Generating train split: 443210 examples [00:50, 31632.42 examples/s]Generating train split: 452476 examples [00:51, 37147.57 examples/s]Generating train split: 462060 examples [01:04, 2180.09 examples/s] Generating train split: 471442 examples [01:05, 3064.99 examples/s]Generating train split: 480430 examples [01:05, 4223.14 examples/s]Generating train split: 489997 examples [01:05, 5904.48 examples/s]Generating train split: 499591 examples [01:05, 8120.92 examples/s]Generating train split: 506548 examples [01:05, 10095.93 examples/s]Generating train split: 513388 examples [01:05, 12458.19 examples/s]Generating train split: 523137 examples [01:06, 16757.70 examples/s]Generating train split: 532880 examples [01:06, 21768.72 examples/s]Generating train split: 540193 examples [01:06, 25465.20 examples/s]Generating train split: 549135 examples [01:06, 29147.25 examples/s]Generating train split: 558576 examples [01:06, 34141.68 examples/s]Generating train split: 568426 examples [01:06, 38134.45 examples/s]Generating train split: 577876 examples [01:07, 40568.62 examples/s]Generating train split: 577876 examples [01:19, 40568.62 examples/s]Generating train split: 579849 examples [01:22, 1459.28 examples/s] Generating train split: 589804 examples [01:22, 2306.65 examples/s]Generating train split: 599092 examples [01:23, 3390.57 examples/s]Generating train split: 606081 examples [01:23, 4499.89 examples/s]Generating train split: 615399 examples [01:23, 6488.90 examples/s]Generating train split: 624528 examples [01:23, 9005.32 examples/s]Generating train split: 633962 examples [01:23, 12156.23 examples/s]Generating train split: 640944 examples [01:23, 14921.31 examples/s]Generating train split: 647661 examples [01:24, 18008.92 examples/s]Generating train split: 654424 examples [01:24, 21120.30 examples/s]Generating train split: 663716 examples [01:24, 26591.01 examples/s]Generating train split: 668531 examples [01:24, 28399.09 examples/s]Generating train split: 673161 examples [01:24, 30727.88 examples/s]Generating train split: 680364 examples [01:24, 35298.67 examples/s]Generating train split: 687454 examples [01:24, 38081.54 examples/s]Generating train split: 696687 examples [01:33, 2944.26 examples/s] Generating train split: 703782 examples [01:33, 4047.77 examples/s]Generating train split: 713659 examples [01:33, 6122.72 examples/s]Generating train split: 722847 examples [01:33, 8633.60 examples/s]Generating train split: 732059 examples [01:33, 11782.65 examples/s]Generating train split: 741164 examples [01:33, 15426.80 examples/s]Generating train split: 748330 examples [01:34, 18949.25 examples/s]Generating train split: 757231 examples [01:34, 23819.38 examples/s]Generating train split: 764104 examples [01:34, 27701.69 examples/s]Generating train split: 773470 examples [01:34, 33462.46 examples/s]Generating train split: 780090 examples [01:34, 36133.92 examples/s]Generating train split: 789194 examples [01:34, 40497.63 examples/s]Generating train split: 798410 examples [01:34, 43346.54 examples/s]Generating train split: 807768 examples [01:35, 46301.58 examples/s]Generating train split: 817029 examples [01:35, 46764.52 examples/s]Generating train split: 826305 examples [01:35, 48578.80 examples/s]Generating train split: 835345 examples [01:35, 49995.66 examples/s]Generating train split: 841848 examples [01:35, 49021.74 examples/s]Generating train split: 850994 examples [01:35, 51679.43 examples/s]Generating train split: 855529 examples [01:36, 8905.00 examples/s] 
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00000_of_00016.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00001_of_00016.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00002_of_00016.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00003_of_00016.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00004_of_00016.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00005_of_00016.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00006_of_00016.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00007_of_00016.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00008_of_00016.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00009_of_00016.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00010_of_00016.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00011_of_00016.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00012_of_00016.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00013_of_00016.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00014_of_00016.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00015_of_00016.arrow
Spawning 16 processes
Converting format of dataset (num_proc=16):   0%|          | 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00000_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00003_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00004_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00002_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00009_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00006_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00013_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00001_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00012_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00010_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00007_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00014_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00008_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00015_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00011_of_00016.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-6e672ed57372d230_00005_of_00016.arrow
Converting format of dataset (num_proc=16):   2%|▏         | 1000/50000 [00:00<00:13, 3645.28 examples/s]Converting format of dataset (num_proc=16):  64%|██████▍   | 32000/50000 [00:00<00:00, 108228.59 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 50000/50000 [00:04<00:00, 10938.49 examples/s] 
Concatenating 16 shards
Map:   0%|          | 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-ed6c79c94d5babf6.arrow
Map:   2%|▏         | 1000/50000 [00:00<00:05, 9060.22 examples/s]Map:   4%|▍         | 2124/50000 [00:00<00:04, 10282.78 examples/s]Map:   7%|▋         | 3261/50000 [00:00<00:04, 10762.91 examples/s]Map:   9%|▉         | 4411/50000 [00:00<00:04, 11044.76 examples/s]Map:  12%|█▏        | 6000/50000 [00:00<00:04, 10720.51 examples/s]Map:  14%|█▍        | 7147/50000 [00:00<00:03, 10941.01 examples/s]Map:  17%|█▋        | 8676/50000 [00:00<00:03, 10501.04 examples/s]Map:  20%|█▉        | 9832/50000 [00:00<00:03, 10783.31 examples/s]Map:  22%|██▏       | 10937/50000 [00:01<00:03, 10854.76 examples/s]Map:  25%|██▌       | 12644/50000 [00:01<00:03, 10668.26 examples/s]Map:  28%|██▊       | 13781/50000 [00:01<00:03, 10844.55 examples/s]Map:  30%|██▉       | 14922/50000 [00:01<00:03, 10992.83 examples/s]Map:  33%|███▎      | 16352/50000 [00:01<00:03, 10458.97 examples/s]Map:  35%|███▌      | 17609/50000 [00:01<00:03, 10638.85 examples/s]Map:  38%|███▊      | 19071/50000 [00:01<00:03, 10297.80 examples/s]Map:  40%|████      | 20210/50000 [00:01<00:02, 10566.63 examples/s]Map:  43%|████▎     | 21331/50000 [00:01<00:02, 10733.01 examples/s]Map:  46%|████▌     | 22971/50000 [00:02<00:02, 10802.01 examples/s]Map:  49%|████▉     | 24666/50000 [00:02<00:02, 10625.84 examples/s]Map:  52%|█████▏    | 25803/50000 [00:02<00:02, 10802.19 examples/s]Map:  54%|█████▍    | 26959/50000 [00:02<00:02, 10993.60 examples/s]Map:  57%|█████▋    | 28577/50000 [00:02<00:01, 10797.42 examples/s]Map:  59%|█████▉    | 29672/50000 [00:02<00:01, 10833.78 examples/s]Map:  62%|██████▏   | 31000/50000 [00:02<00:01, 9932.90 examples/s] Map:  65%|██████▌   | 32673/50000 [00:03<00:01, 10326.33 examples/s]Map:  68%|██████▊   | 33766/50000 [00:03<00:01, 10468.23 examples/s]Map:  70%|██████▉   | 34941/50000 [00:03<00:01, 10790.96 examples/s]Map:  73%|███████▎  | 36637/50000 [00:03<00:01, 10642.68 examples/s]Map:  76%|███████▌  | 38094/50000 [00:03<00:01, 10335.11 examples/s]Map:  78%|███████▊  | 39201/50000 [00:03<00:01, 10508.94 examples/s]Map:  81%|████████  | 40290/50000 [00:03<00:00, 10604.05 examples/s]Map:  83%|████████▎ | 41400/50000 [00:03<00:00, 10733.31 examples/s]Map:  85%|████████▌ | 42652/50000 [00:04<00:00, 10957.65 examples/s]Map:  88%|████████▊ | 43773/50000 [00:04<00:00, 11023.96 examples/s]Map:  90%|█████████ | 45236/50000 [00:04<00:00, 10546.42 examples/s]Map:  93%|█████████▎| 46402/50000 [00:04<00:00, 10836.80 examples/s]Map:  95%|█████████▌| 47641/50000 [00:04<00:00, 11030.74 examples/s]Map:  98%|█████████▊| 49131/50000 [00:04<00:00, 10581.88 examples/s]Map: 100%|██████████| 50000/50000 [00:08<00:00, 6127.75 examples/s] 
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00000_of_00016.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00001_of_00016.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00002_of_00016.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00003_of_00016.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00004_of_00016.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00005_of_00016.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00006_of_00016.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00007_of_00016.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00008_of_00016.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00009_of_00016.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00010_of_00016.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00011_of_00016.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00012_of_00016.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00013_of_00016.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00014_of_00016.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00015_of_00016.arrow
Spawning 16 processes
Running tokenizer on dataset (num_proc=16):   0%|          | 0/50000 [00:00<?, ? examples/s]07/10/2024 15:05:58 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
07/10/2024 15:05:58 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
07/10/2024 15:05:58 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
07/10/2024 15:05:59 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
07/10/2024 15:05:59 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
07/10/2024 15:05:59 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
07/10/2024 15:05:59 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00000_of_00016.arrow
Running tokenizer on dataset (num_proc=16):   2%|▏         | 1000/50000 [00:09<08:05, 100.93 examples/s]Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00001_of_00016.arrow
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Running tokenizer on dataset (num_proc=16):   4%|▍         | 2000/50000 [00:12<04:28, 178.98 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00002_of_00016.arrow
[WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:12,395 >> Token indices sequence length is longer than the specified maximum sequence length for this model (34999 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):   6%|▌         | 3000/50000 [00:15<03:31, 221.96 examples/s]Running tokenizer on dataset (num_proc=16):   8%|▊         | 4000/50000 [00:15<02:08, 357.42 examples/s]Running tokenizer on dataset (num_proc=16):  10%|█         | 5000/50000 [00:18<02:02, 366.99 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:15,852 >> Token indices sequence length is longer than the specified maximum sequence length for this model (39228 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00003_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  12%|█▏        | 6000/50000 [00:19<01:33, 472.33 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00004_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  14%|█▍        | 7000/50000 [00:21<01:30, 475.20 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:19,009 >> Token indices sequence length is longer than the specified maximum sequence length for this model (129191 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  16%|█▌        | 8000/50000 [00:22<01:08, 616.37 examples/s]Running tokenizer on dataset (num_proc=16):  18%|█▊        | 9000/50000 [00:22<00:51, 790.48 examples/s]Running tokenizer on dataset (num_proc=16):  20%|██        | 10125/50000 [00:23<00:49, 805.77 examples/s]Running tokenizer on dataset (num_proc=16):  22%|██▎       | 11250/50000 [00:26<00:58, 659.28 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:24,025 >> Token indices sequence length is longer than the specified maximum sequence length for this model (35848 > 32768). Running this sequence through the model will result in indexing errors
[WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:24,062 >> Token indices sequence length is longer than the specified maximum sequence length for this model (43744 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00005_of_00016.arrow
[WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:24,443 >> Token indices sequence length is longer than the specified maximum sequence length for this model (38252 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=16):  24%|██▍       | 12250/50000 [00:27<00:54, 697.12 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00006_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  26%|██▋       | 13250/50000 [00:27<00:40, 906.09 examples/s]Running tokenizer on dataset (num_proc=16):  28%|██▊       | 14250/50000 [00:27<00:29, 1211.76 examples/s]Running tokenizer on dataset (num_proc=16):  30%|███       | 15250/50000 [00:28<00:21, 1604.73 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:28,326 >> Token indices sequence length is longer than the specified maximum sequence length for this model (105369 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00007_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  33%|███▎      | 16375/50000 [00:31<00:50, 661.76 examples/s] Running tokenizer on dataset (num_proc=16):  35%|███▍      | 17375/50000 [00:32<00:45, 719.11 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00008_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  37%|███▋      | 18375/50000 [00:33<00:35, 879.26 examples/s]Running tokenizer on dataset (num_proc=16):  39%|███▉      | 19500/50000 [00:33<00:25, 1187.21 examples/s]Running tokenizer on dataset (num_proc=16):  43%|████▎     | 21500/50000 [00:33<00:14, 1964.96 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:32,930 >> Token indices sequence length is longer than the specified maximum sequence length for this model (51131 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00009_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  45%|████▌     | 22625/50000 [00:36<00:25, 1053.82 examples/s]Running tokenizer on dataset (num_proc=16):  47%|████▋     | 23625/50000 [00:37<00:29, 900.88 examples/s] Running tokenizer on dataset (num_proc=16):  49%|████▉     | 24625/50000 [00:39<00:30, 840.56 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:36,931 >> Token indices sequence length is longer than the specified maximum sequence length for this model (36232 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00010_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  51%|█████▏    | 25625/50000 [00:40<00:27, 900.92 examples/s]Running tokenizer on dataset (num_proc=16):  53%|█████▎    | 26625/50000 [00:40<00:19, 1169.39 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▌    | 27750/50000 [00:41<00:19, 1129.37 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:40,163 >> Token indices sequence length is longer than the specified maximum sequence length for this model (194308 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00011_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 28875/50000 [00:43<00:25, 834.27 examples/s] Running tokenizer on dataset (num_proc=16):  60%|██████    | 30000/50000 [00:44<00:20, 996.00 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 31000/50000 [00:44<00:15, 1255.70 examples/s]Running tokenizer on dataset (num_proc=16):  64%|██████▍   | 32125/50000 [00:45<00:14, 1269.60 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:43,515 >> Token indices sequence length is longer than the specified maximum sequence length for this model (130268 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00012_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  66%|██████▋   | 33125/50000 [00:47<00:17, 967.40 examples/s] Running tokenizer on dataset (num_proc=16):  68%|██████▊   | 34125/50000 [00:47<00:15, 1028.84 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:46,001 >> Token indices sequence length is longer than the specified maximum sequence length for this model (84321 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00013_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  70%|███████   | 35125/50000 [00:49<00:17, 866.22 examples/s] Running tokenizer on dataset (num_proc=16):  72%|███████▏  | 36125/50000 [00:50<00:16, 817.89 examples/s]Running tokenizer on dataset (num_proc=16):  74%|███████▍  | 37125/50000 [00:51<00:12, 1032.92 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:49,249 >> Token indices sequence length is longer than the specified maximum sequence length for this model (61819 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00014_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  76%|███████▋  | 38250/50000 [00:52<00:12, 914.97 examples/s] Running tokenizer on dataset (num_proc=16):  78%|███████▊  | 39250/50000 [00:53<00:09, 1172.00 examples/s]Running tokenizer on dataset (num_proc=16):  80%|████████  | 40250/50000 [00:53<00:06, 1404.03 examples/s]Running tokenizer on dataset (num_proc=16):  83%|████████▎ | 41375/50000 [00:56<00:10, 827.95 examples/s] [WARNING|tokenization_utils_base.py:3843] 2024-07-10 15:06:53,156 >> Token indices sequence length is longer than the specified maximum sequence length for this model (124423 > 32768). Running this sequence through the model will result in indexing errors
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-2e6beff5ebead4ea_00015_of_00016.arrow
Running tokenizer on dataset (num_proc=16):  85%|████████▍ | 42375/50000 [00:56<00:08, 912.51 examples/s]Running tokenizer on dataset (num_proc=16):  87%|████████▋ | 43375/50000 [00:57<00:06, 988.91 examples/s]Running tokenizer on dataset (num_proc=16):  89%|████████▉ | 44500/50000 [00:58<00:04, 1117.13 examples/s]Running tokenizer on dataset (num_proc=16):  91%|█████████ | 45500/50000 [00:58<00:03, 1220.23 examples/s]Running tokenizer on dataset (num_proc=16):  93%|█████████▎| 46625/50000 [01:02<00:04, 707.70 examples/s] Running tokenizer on dataset (num_proc=16):  95%|█████████▌| 47625/50000 [01:02<00:02, 841.80 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 48750/50000 [01:04<00:01, 776.35 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 48875/50000 [01:04<00:01, 699.91 examples/s]Running tokenizer on dataset (num_proc=16): 100%|█████████▉| 49875/50000 [01:07<00:00, 528.29 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 50000/50000 [01:08<00:00, 481.08 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 50000/50000 [01:08<00:00, 727.27 examples/s]
Concatenating 16 shards
input_ids:
[12093, 70290, 6859, 6484, 43, 890, 27134, 2448, 7395, 30400, 55, 549, 26209, 198, 6622, 198, 16578, 70290, 6859, 6484, 43, 890, 27134, 2448, 7395, 30400, 55, 198, 6622, 198, 151643, 7039, 33976, 432, 646, 387, 264, 2699, 312, 2015, 1388, 311, 3270, 458, 4549, 911, 279, 10990, 7070, 14429, 3840, 438, 1052, 374, 825, 304, 1449, 1614, 14418, 504, 882, 311, 882, 11, 323, 279, 3482, 374, 49485, 448, 1105, 304, 1449, 4128, 369, 5019, 879, 30997, 311, 1349, 1105, 13, 2055, 358, 686, 36355, 304, 11629, 1246, 358, 5798, 323, 23983, 279, 1614, 624, 34762, 1635, 4134, 16145, 6635, 311, 1281, 1549, 862, 10990, 7070, 14429, 31496, 438, 807, 1030, 264, 501, 3093, 315, 50270, 82, 429, 1410, 15551, 279, 12188, 504, 2155, 11067, 323, 25941, 323, 8450, 1281, 64255, 323, 803, 35201, 9666, 13, 2379, 3381, 429, 3432, 419, 501, 5440, 432, 1035, 387, 2664, 311, 8193, 1549, 279, 330, 1040, 1211, 1, 1091, 894, 1008, 25546, 6174, 323, 432, 4977, 429, 807, 1033, 1290, 438, 1449, 1463, 7073, 10788, 825, 52163, 269, 803, 624, 9485, 501, 50270, 82, 1033, 537, 279, 1172, 501, 3166, 304, 1493, 4119, 11, 807, 1030, 264, 738, 315, 13918, 7746, 2669, 18663, 504, 279, 8151, 1137, 5527, 311, 41740, 11, 1045, 6548, 36780, 9317, 5479, 323, 501, 97685, 624, 2121, 5135, 438, 358, 8930, 279, 3745, 358, 1030, 264, 1602, 2797, 2168, 315, 279, 3093, 315, 1614, 358, 4829, 311, 1281, 11, 358, 1030, 3884, 10077, 315, 24248, 304, 279, 6467, 11, 1602, 76873, 12645, 98732, 448, 25386, 424, 429, 95758, 2310, 7218, 76024, 624, 2461, 419, 2390, 358, 6635, 311, 990, 279, 5235, 18457, 53514, 22293, 738, 369, 279, 10990, 7070, 14429, 429, 374, 2167, 14452, 323, 18304, 13942, 624, 2132, 4436, 944, 16965, 438, 279, 5479, 4946, 1602, 1632, 323, 279, 11221, 525, 1602, 2797, 11, 2337, 279, 1882, 582, 686, 1172, 614, 311, 1896, 2453, 979, 11589, 279, 2632, 5479, 311, 5648, 14719, 1105, 476, 11785, 11, 775, 14421, 1105, 13, 1634, 847, 4522, 504, 279, 7167, 572, 311, 4009, 264, 12896, 448, 279, 23603, 86968, 18824, 24569, 23704, 700, 11, 358, 23983, 279, 1614, 448, 5938, 12258, 25685, 12463, 438, 358, 5798, 432, 311, 614, 419, 12463, 2331, 369, 279, 2937, 330, 35012, 18824, 1, 14762, 358, 1035, 3796, 389, 279, 86968, 13, 2055, 358, 5798, 279, 44909, 323, 279, 64386, 9380, 15663, 279, 305, 9118, 11, 22696, 11, 39932, 13569, 11, 4992, 13, 14576, 311, 6707, 6177, 279, 65739, 448, 279, 15625, 476, 279, 68348, 13, 151643, 641, 3213, 1635, 358, 614, 65806, 3807, 52269, 14697, 304, 847, 2205, 3082, 13, 20035, 537, 3884, 894, 32319, 15570, 4730, 5926, 358, 572, 18442, 311, 1490, 458, 52269, 16262, 825, 315, 847, 22791, 14697, 264, 5625, 315, 2849, 4134, 13, 60033, 358, 9099, 847, 8849, 6249, 14046, 311, 1490, 421, 279, 52269, 374, 264, 5792, 20181, 13, 3197, 358, 10067, 279, 21852, 419, 6556, 358, 572, 33972, 311, 1490, 358, 614, 264, 6716, 315, 32319, 15570, 4730, 304, 21682, 13, 358, 2776, 10282, 4297, 27031, 1431, 304, 279, 3900, 429]
inputs:
@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject
@end
@implementation PodsDummy_XCDLumberjackNSLogger_OSX
@end
<|endoftext|>Nowadays it can be a bit reiterative to write an article about the Panzer III history as there is one in every model magazine from time to time, and the web is saturated with them in every language for everyone who desires to read them. So I will concentrate in telling how I built and painted the model.
Several years ago Dragon decided to make again their Panzer III kits as they had a new kind of moulds that could inject the plastic from different sides and angles and thus make thinner and more delicate pieces. They thought that having this new technology it would be better to produce again the "classics" than any other newer ones and it seems that they were right as every modeller bought one…..or more.
These new moulds were not the only new thing in these models, they had a set of tracks links already separated from the sprues ready to assemble, some photoetched metal parts and new decals.
As soon as I opened the box I had a very clear image of the kind of model I wanted to make, I had seen lots of photographs in the books, very dusty machines cramped with equipage that resembled old moving vans.
For this project I decided to use the Blackdog resin accessories set for the Panzer III that is really suitable and fits perfectly.
It isn't complicated as the parts fit very well and the instructions are very clear, during the process we will only have to take care when handling the little parts to avoid breaking them or worst, loosing them. As my idea from the beginning was to represent a tank with the desert camouflage painting badly worn out, I painted the model with German Dark Grey colour as I built it to have this colour base for the later "soap painting" technique I would apply on the camouflage. So I built the chassis and the turret leaving aside the hatches, wheels, antenna rail, etc. mainly to easily paint the scratches with the brush or the sponge.<|endoftext|>In recent years I have erected several owl boxes in my local area. Having not seen any barn owls recently I was pleased to see an owl entering one of my nest boxes a couple of days ago. Yesterday I placed my trail camera nearby to see if the owl is a regular visitor. When I checked the footage this morning I was delighted to see I have a pair of barn owls in residence. I'm keeping everything crossed now in the hope that
Caching indices mapping at /home/wangzj/.cache/huggingface/datasets/json/default-74e4f6a24fa3d738/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-937de8d1c62b3659.arrow
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 101106
})
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): Qwen2ForCausalLM(
      (model): Qwen2Model(
        (embed_tokens): Embedding(151936, 2048)
        (layers): ModuleList(
          (0): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (1): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (2): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (3): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (4): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (5): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (6): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (7): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (8): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (9): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (10): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (11): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (12): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (13): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (14): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (15): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (16): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (17): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (18): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (19): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (20): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (21): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (22): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (23): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
        )
        (norm): Qwen2RMSNorm()
      )
      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
    )
  )
)
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
[INFO|trainer.py:586] 2024-07-10 15:07:06,182 >> Using auto half precision backend
[2024-07-10 15:07:06,484] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 101106
})
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 101106
})
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 101106
})
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 101106
})
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 101106
})
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 101106
})
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 101106
})
/home/nfs02/anaconda3/envs/alimoe/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
[2024-07-10 15:07:22,402] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-10 15:07:22,405] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-07-10 15:07:22,405] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-07-10 15:07:22,410] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-07-10 15:07:22,410] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-07-10 15:07:22,410] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-07-10 15:07:22,410] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-07-10 15:07:22,410] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-07-10 15:07:22,410] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: False
[2024-07-10 15:07:22,410] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-07-10 15:07:23,765] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-07-10 15:07:23,765] [INFO] [utils.py:803:see_memory_usage] MA 8.47 GB         Max_MA 8.47 GB         CA 8.76 GB         Max_CA 9 GB 
[2024-07-10 15:07:23,766] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 68.11 GB, percent = 18.1%
[2024-07-10 15:07:23,955] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-07-10 15:07:23,956] [INFO] [utils.py:803:see_memory_usage] MA 8.47 GB         Max_MA 8.47 GB         CA 8.76 GB         Max_CA 9 GB 
[2024-07-10 15:07:23,956] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 68.11 GB, percent = 18.1%
[2024-07-10 15:07:23,956] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-07-10 15:07:24,135] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-07-10 15:07:24,139] [INFO] [utils.py:803:see_memory_usage] MA 8.47 GB         Max_MA 8.47 GB         CA 8.76 GB         Max_CA 9 GB 
[2024-07-10 15:07:24,139] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 68.11 GB, percent = 18.1%
[2024-07-10 15:07:24,140] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-07-10 15:07:24,140] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-07-10 15:07:24,140] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-07-10 15:07:24,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2024-07-10 15:07:24,142] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   amp_params ................... False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   bfloat16_enabled ............. True
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4a5f48cb20>
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   dump_state ................... False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... None
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-07-10 15:07:24,143] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   fp16_auto_cast ............... None
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   fp16_enabled ................. False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 1
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 1
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   loss_scale ................... 1.0
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   pld_params ................... False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   train_batch_size ............. 128
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  16
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   world_size ................... 8
[2024-07-10 15:07:24,144] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-07-10 15:07:24,145] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-10 15:07:24,145] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-07-10 15:07:24,145] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-10 15:07:24,145] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-07-10 15:07:24,145] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1748] 2024-07-10 15:07:24,145 >> ***** Running training *****
[INFO|trainer.py:1749] 2024-07-10 15:07:24,145 >>   Num examples = 101,106
[INFO|trainer.py:1750] 2024-07-10 15:07:24,145 >>   Num Epochs = 1
[INFO|trainer.py:1751] 2024-07-10 15:07:24,145 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1754] 2024-07-10 15:07:24,145 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:1755] 2024-07-10 15:07:24,145 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1756] 2024-07-10 15:07:24,145 >>   Total optimization steps = 790
[INFO|trainer.py:1757] 2024-07-10 15:07:24,147 >>   Number of trainable parameters = 196,608
07/10/2024 15:07:24 - WARNING - llmtuner.extras.callbacks - Previous log file in this folder will be deleted.
  0%|          | 0/790 [00:00<?, ?it/s]  0%|          | 1/790 [00:01<25:12,  1.92s/it]  0%|          | 2/790 [00:03<18:59,  1.45s/it]  0%|          | 3/790 [00:04<16:56,  1.29s/it]  1%|          | 4/790 [00:05<15:58,  1.22s/it]  1%|          | 5/790 [00:06<15:27,  1.18s/it]  1%|          | 6/790 [00:07<15:07,  1.16s/it]  1%|          | 7/790 [00:08<14:59,  1.15s/it]  1%|          | 8/790 [00:09<14:49,  1.14s/it]  1%|          | 9/790 [00:10<14:43,  1.13s/it]  1%|▏         | 10/790 [00:11<14:39,  1.13s/it]                                                {'loss': 3.3789, 'learning_rate': 4.9980234930682546e-05, 'epoch': 0.01}
  1%|▏         | 10/790 [00:11<14:39,  1.13s/it]                                                {'router_ce_loss': 1.1064585447311401, 'old_lang_expert0_score': '0.24 0.04 0.06 0.21 0.76 0.78 0.25 0.84 0.43 0.67 0.21 0.84 0.23 0.77 0.89 0.54 0.79 0.69 0.82 0.79 0.75 0.82 0.77 0.9', 'epoch': 0.01}
  1%|▏         | 10/790 [00:12<14:39,  1.13s/it]  1%|▏         | 11/790 [00:13<14:34,  1.12s/it]  2%|▏         | 12/790 [00:14<14:31,  1.12s/it]  2%|▏         | 13/790 [00:15<14:28,  1.12s/it]  2%|▏         | 14/790 [00:16<14:28,  1.12s/it]  2%|▏         | 15/790 [00:17<14:26,  1.12s/it]  2%|▏         | 16/790 [00:18<14:25,  1.12s/it]  2%|▏         | 17/790 [00:19<14:25,  1.12s/it]  2%|▏         | 18/790 [00:20<14:23,  1.12s/it]  2%|▏         | 19/790 [00:22<14:23,  1.12s/it]  3%|▎         | 20/790 [00:23<14:22,  1.12s/it]                                                {'loss': 3.3187, 'learning_rate': 4.99209709753674e-05, 'epoch': 0.03}
  3%|▎         | 20/790 [00:23<14:22,  1.12s/it]                                                {'router_ce_loss': 1.0852060317993164, 'old_lang_expert0_score': '0.23 0.04 0.06 0.22 0.79 0.8 0.28 0.86 0.46 0.74 0.23 0.86 0.2 0.81 0.91 0.62 0.81 0.76 0.83 0.84 0.76 0.83 0.82 0.92', 'epoch': 0.03}
  3%|▎         | 20/790 [00:23<14:22,  1.12s/it]  3%|▎         | 21/790 [00:24<14:21,  1.12s/it]  3%|▎         | 22/790 [00:25<14:20,  1.12s/it]  3%|▎         | 23/790 [00:26<14:19,  1.12s/it]  3%|▎         | 24/790 [00:27<14:18,  1.12s/it]  3%|▎         | 25/790 [00:28<14:17,  1.12s/it]  3%|▎         | 26/790 [00:29<14:16,  1.12s/it]  3%|▎         | 27/790 [00:30<14:15,  1.12s/it]  4%|▎         | 28/790 [00:32<14:15,  1.12s/it]  4%|▎         | 29/790 [00:33<14:14,  1.12s/it]  4%|▍         | 30/790 [00:34<14:13,  1.12s/it]                                                {'loss': 3.2646, 'learning_rate': 4.982230184254933e-05, 'epoch': 0.04}
  4%|▍         | 30/790 [00:34<14:13,  1.12s/it]                                                {'router_ce_loss': 1.080919861793518, 'old_lang_expert0_score': '0.24 0.03 0.06 0.2 0.78 0.79 0.24 0.87 0.44 0.76 0.2 0.87 0.2 0.83 0.92 0.64 0.83 0.78 0.87 0.85 0.8 0.85 0.87 0.94', 'epoch': 0.04}
  4%|▍         | 30/790 [00:34<14:13,  1.12s/it]  4%|▍         | 31/790 [00:35<14:12,  1.12s/it]  4%|▍         | 32/790 [00:36<14:11,  1.12s/it]  4%|▍         | 33/790 [00:37<14:11,  1.12s/it]  4%|▍         | 34/790 [00:38<14:09,  1.12s/it]  4%|▍         | 35/790 [00:39<14:09,  1.12s/it]  5%|▍         | 36/790 [00:41<14:10,  1.13s/it]  5%|▍         | 37/790 [00:42<14:09,  1.13s/it]  5%|▍         | 38/790 [00:43<14:08,  1.13s/it]  5%|▍         | 39/790 [00:44<14:07,  1.13s/it]  5%|▌         | 40/790 [00:45<14:06,  1.13s/it]                                                {'loss': 3.2477, 'learning_rate': 4.968438354840834e-05, 'epoch': 0.05}
  5%|▌         | 40/790 [00:45<14:06,  1.13s/it]                                                {'router_ce_loss': 1.0762550830841064, 'old_lang_expert0_score': '0.24 0.04 0.07 0.22 0.81 0.81 0.25 0.88 0.41 0.76 0.17 0.87 0.18 0.84 0.94 0.68 0.87 0.8 0.87 0.87 0.82 0.85 0.86 0.94', 'epoch': 0.05}
  5%|▌         | 40/790 [00:45<14:06,  1.13s/it]  5%|▌         | 41/790 [00:46<14:05,  1.13s/it]  5%|▌         | 42/790 [00:47<14:04,  1.13s/it]  5%|▌         | 43/790 [00:49<14:02,  1.13s/it]  6%|▌         | 44/790 [00:50<14:01,  1.13s/it]  6%|▌         | 45/790 [00:51<14:05,  1.13s/it]  6%|▌         | 46/790 [00:52<14:04,  1.13s/it]  6%|▌         | 47/790 [00:53<14:04,  1.14s/it]  6%|▌         | 48/790 [00:54<14:02,  1.13s/it]  6%|▌         | 49/790 [00:55<13:59,  1.13s/it]  6%|▋         | 50/790 [00:56<13:57,  1.13s/it]                                                {'loss': 3.2049, 'learning_rate': 4.950743417011591e-05, 'epoch': 0.06}
  6%|▋         | 50/790 [00:56<13:57,  1.13s/it]                                                {'router_ce_loss': 1.0587581396102905, 'old_lang_expert0_score': '0.24 0.04 0.07 0.21 0.79 0.83 0.21 0.88 0.47 0.81 0.19 0.9 0.18 0.89 0.96 0.75 0.9 0.86 0.9 0.89 0.85 0.89 0.87 0.95', 'epoch': 0.06}
  6%|▋         | 50/790 [00:57<13:57,  1.13s/it]  6%|▋         | 51/790 [00:58<13:57,  1.13s/it]  7%|▋         | 52/790 [00:59<13:56,  1.13s/it]  7%|▋         | 53/790 [01:00<13:55,  1.13s/it]  7%|▋         | 54/790 [01:01<13:54,  1.13s/it]  7%|▋         | 55/790 [01:02<13:53,  1.13s/it]  7%|▋         | 56/790 [01:03<13:51,  1.13s/it]  7%|▋         | 57/790 [01:04<13:50,  1.13s/it]  7%|▋         | 58/790 [01:06<13:50,  1.13s/it]  7%|▋         | 59/790 [01:07<13:49,  1.14s/it]  8%|▊         | 60/790 [01:08<13:48,  1.14s/it]                                                {'loss': 3.1713, 'learning_rate': 4.929173350101025e-05, 'epoch': 0.08}
  8%|▊         | 60/790 [01:08<13:48,  1.14s/it]                                                {'router_ce_loss': 1.0768392086029053, 'old_lang_expert0_score': '0.24 0.03 0.06 0.19 0.78 0.79 0.2 0.87 0.43 0.76 0.15 0.88 0.16 0.85 0.93 0.72 0.85 0.81 0.89 0.87 0.85 0.88 0.9 0.95', 'epoch': 0.08}
  8%|▊         | 60/790 [01:08<13:48,  1.14s/it]  8%|▊         | 61/790 [01:09<13:47,  1.13s/it]  8%|▊         | 62/790 [01:10<13:46,  1.13s/it]  8%|▊         | 63/790 [01:11<13:45,  1.14s/it]  8%|▊         | 64/790 [01:12<13:44,  1.14s/it]  8%|▊         | 65/790 [01:13<13:43,  1.14s/it]  8%|▊         | 66/790 [01:15<13:42,  1.14s/it]  8%|▊         | 67/790 [01:16<13:42,  1.14s/it]  9%|▊         | 68/790 [01:17<13:41,  1.14s/it]  9%|▊         | 69/790 [01:18<13:41,  1.14s/it]  9%|▉         | 70/790 [01:19<13:40,  1.14s/it]                                                {'loss': 3.1837, 'learning_rate': 4.903762260818551e-05, 'epoch': 0.09}
  9%|▉         | 70/790 [01:19<13:40,  1.14s/it]                                                {'router_ce_loss': 1.0523241758346558, 'old_lang_expert0_score': '0.23 0.03 0.06 0.21 0.81 0.84 0.21 0.89 0.49 0.81 0.16 0.91 0.18 0.9 0.95 0.79 0.89 0.86 0.91 0.91 0.88 0.9 0.92 0.96', 'epoch': 0.09}
  9%|▉         | 70/790 [01:20<13:40,  1.14s/it]  9%|▉         | 71/790 [01:21<14:40,  1.22s/it]  9%|▉         | 72/790 [01:22<14:20,  1.20s/it]  9%|▉         | 73/790 [01:23<14:07,  1.18s/it]  9%|▉         | 74/790 [01:24<13:57,  1.17s/it]  9%|▉         | 75/790 [01:25<13:50,  1.16s/it] 10%|▉         | 76/790 [01:26<13:44,  1.15s/it] 10%|▉         | 77/790 [01:27<13:41,  1.15s/it] 10%|▉         | 78/790 [01:29<13:38,  1.15s/it] 10%|█         | 79/790 [01:30<13:36,  1.15s/it] 10%|█         | 80/790 [01:31<13:35,  1.15s/it]                                                {'loss': 3.138, 'learning_rate': 4.874550329319457e-05, 'epoch': 0.1}
 10%|█         | 80/790 [01:31<13:35,  1.15s/it]                                                {'router_ce_loss': 1.050802230834961, 'old_lang_expert0_score': '0.23 0.04 0.06 0.19 0.81 0.82 0.22 0.9 0.5 0.81 0.18 0.92 0.2 0.89 0.95 0.8 0.88 0.87 0.91 0.9 0.88 0.9 0.93 0.96', 'epoch': 0.1}
 10%|█         | 80/790 [01:31<13:35,  1.15s/it] 10%|█         | 81/790 [01:32<13:33,  1.15s/it] 10%|█         | 82/790 [01:33<13:31,  1.15s/it] 11%|█         | 83/790 [01:34<13:29,  1.15s/it] 11%|█         | 84/790 [01:35<13:28,  1.15s/it] 11%|█         | 85/790 [01:37<13:28,  1.15s/it] 11%|█         | 86/790 [01:38<13:27,  1.15s/it] 11%|█         | 87/790 [01:39<13:26,  1.15s/it] 11%|█         | 88/790 [01:40<13:24,  1.15s/it] 11%|█▏        | 89/790 [01:41<13:24,  1.15s/it] 11%|█▏        | 90/790 [01:42<13:23,  1.15s/it]                                                {'loss': 3.1097, 'learning_rate': 4.84158374567182e-05, 'epoch': 0.11}
 11%|█▏        | 90/790 [01:42<13:23,  1.15s/it]                                                {'router_ce_loss': 1.0479172468185425, 'old_lang_expert0_score': '0.22 0.02 0.05 0.18 0.79 0.84 0.21 0.9 0.58 0.84 0.16 0.92 0.25 0.89 0.95 0.79 0.9 0.86 0.92 0.92 0.89 0.88 0.94 0.96', 'epoch': 0.11}
 11%|█▏        | 90/790 [01:43<13:23,  1.15s/it] 12%|█▏        | 91/790 [01:44<13:26,  1.15s/it] 12%|█▏        | 92/790 [01:45<13:27,  1.16s/it] 12%|█▏        | 93/790 [01:46<13:28,  1.16s/it] 12%|█▏        | 94/790 [01:47<13:28,  1.16s/it] 12%|█▏        | 95/790 [01:48<13:27,  1.16s/it] 12%|█▏        | 96/790 [01:49<13:27,  1.16s/it] 12%|█▏        | 97/790 [01:51<13:28,  1.17s/it] 12%|█▏        | 98/790 [01:52<13:28,  1.17s/it] 13%|█▎        | 99/790 [01:53<13:27,  1.17s/it] 13%|█▎        | 100/790 [01:54<13:26,  1.17s/it]                                                 {'loss': 3.0774, 'learning_rate': 4.804914636820517e-05, 'epoch': 0.13}
 13%|█▎        | 100/790 [01:54<13:26,  1.17s/it]                                                 {'router_ce_loss': 1.0487148761749268, 'old_lang_expert0_score': '0.23 0.03 0.05 0.18 0.82 0.84 0.19 0.89 0.53 0.82 0.13 0.92 0.16 0.91 0.96 0.83 0.91 0.88 0.92 0.93 0.91 0.91 0.94 0.97', 'epoch': 0.13}
 13%|█▎        | 100/790 [01:54<13:26,  1.17s/it] 13%|█▎        | 101/790 [01:55<13:26,  1.17s/it] 13%|█▎        | 102/790 [01:56<13:21,  1.17s/it] 13%|█▎        | 103/790 [01:58<13:18,  1.16s/it] 13%|█▎        | 104/790 [01:59<13:15,  1.16s/it] 13%|█▎        | 105/790 [02:00<13:12,  1.16s/it] 13%|█▎        | 106/790 [02:01<13:10,  1.16s/it] 14%|█▎        | 107/790 [02:02<13:09,  1.16s/it] 14%|█▎        | 108/790 [02:03<13:08,  1.16s/it] 14%|█▍        | 109/790 [02:04<13:08,  1.16s/it] 14%|█▍        | 110/790 [02:06<13:07,  1.16s/it]                                                 {'loss': 3.0949, 'learning_rate': 4.764600984163808e-05, 'epoch': 0.14}
 14%|█▍        | 110/790 [02:06<13:07,  1.16s/it]                                                 {'router_ce_loss': 1.0366865396499634, 'old_lang_expert0_score': '0.23 0.02 0.06 0.19 0.84 0.84 0.22 0.91 0.61 0.85 0.18 0.92 0.23 0.91 0.96 0.81 0.9 0.89 0.94 0.92 0.91 0.91 0.96 0.97', 'epoch': 0.14}
 14%|█▍        | 110/790 [02:06<13:07,  1.16s/it] 14%|█▍        | 111/790 [02:07<13:06,  1.16s/it] 14%|█▍        | 112/790 [02:08<13:05,  1.16s/it] 14%|█▍        | 113/790 [02:09<13:04,  1.16s/it] 14%|█▍        | 114/790 [02:10<13:03,  1.16s/it] 15%|█▍        | 115/790 [02:11<13:02,  1.16s/it] 15%|█▍        | 116/790 [02:13<13:00,  1.16s/it] 15%|█▍        | 117/790 [02:14<13:00,  1.16s/it] 15%|█▍        | 118/790 [02:15<13:00,  1.16s/it] 15%|█▌        | 119/790 [02:16<12:59,  1.16s/it] 15%|█▌        | 120/790 [02:17<12:57,  1.16s/it]                                                 {'loss': 3.082, 'learning_rate': 4.72070653187283e-05, 'epoch': 0.15}
 15%|█▌        | 120/790 [02:17<12:57,  1.16s/it]                                                 {'router_ce_loss': 1.0488442182540894, 'old_lang_expert0_score': '0.23 0.04 0.07 0.2 0.81 0.82 0.2 0.89 0.58 0.84 0.15 0.92 0.19 0.91 0.96 0.82 0.9 0.88 0.92 0.91 0.87 0.89 0.92 0.95', 'epoch': 0.15}
 15%|█▌        | 120/790 [02:18<12:57,  1.16s/it] 15%|█▌        | 121/790 [02:18<12:57,  1.16s/it] 15%|█▌        | 122/790 [02:20<12:57,  1.16s/it] 16%|█▌        | 123/790 [02:21<12:56,  1.16s/it] 16%|█▌        | 124/790 [02:22<12:55,  1.16s/it] 16%|█▌        | 125/790 [02:23<12:55,  1.17s/it] 16%|█▌        | 126/790 [02:24<12:59,  1.17s/it] 16%|█▌        | 127/790 [02:25<12:58,  1.17s/it] 16%|█▌        | 128/790 [02:27<12:57,  1.17s/it] 16%|█▋        | 129/790 [02:28<12:55,  1.17s/it] 16%|█▋        | 130/790 [02:29<12:55,  1.17s/it]                                                 {'loss': 3.1178, 'learning_rate': 4.673300686098957e-05, 'epoch': 0.16}
 16%|█▋        | 130/790 [02:29<12:55,  1.17s/it]                                                 {'router_ce_loss': 1.0292037725448608, 'old_lang_expert0_score': '0.23 0.03 0.06 0.19 0.82 0.86 0.22 0.92 0.65 0.88 0.14 0.94 0.2 0.95 0.97 0.87 0.93 0.91 0.94 0.94 0.92 0.92 0.94 0.97', 'epoch': 0.16}
 16%|█▋        | 130/790 [02:29<12:55,  1.17s/it] 17%|█▋        | 131/790 [02:30<12:58,  1.18s/it] 17%|█▋        | 132/790 [02:31<12:59,  1.18s/it] 17%|█▋        | 133/790 [02:33<13:01,  1.19s/it] 17%|█▋        | 134/790 [02:34<12:59,  1.19s/it] 17%|█▋        | 135/790 [02:35<12:58,  1.19s/it] 17%|█▋        | 136/790 [02:36<12:56,  1.19s/it] 17%|█▋        | 137/790 [02:37<13:02,  1.20s/it] 17%|█▋        | 138/790 [02:39<13:03,  1.20s/it] 18%|█▊        | 139/790 [02:40<13:51,  1.28s/it] 18%|█▊        | 140/790 [02:41<13:34,  1.25s/it]                                                 {'loss': 3.0952, 'learning_rate': 4.6224584052284106e-05, 'epoch': 0.18}
 18%|█▊        | 140/790 [02:41<13:34,  1.25s/it]                                                 {'router_ce_loss': 1.035297155380249, 'old_lang_expert0_score': '0.23 0.02 0.06 0.2 0.84 0.86 0.2 0.91 0.65 0.85 0.13 0.93 0.18 0.93 0.96 0.87 0.93 0.9 0.94 0.93 0.91 0.93 0.95 0.97', 'epoch': 0.18}
 18%|█▊        | 140/790 [02:42<13:34,  1.25s/it] 18%|█▊        | 141/790 [02:43<14:10,  1.31s/it] 18%|█▊        | 142/790 [02:44<13:56,  1.29s/it] 18%|█▊        | 143/790 [02:45<13:40,  1.27s/it] 18%|█▊        | 144/790 [02:46<13:34,  1.26s/it] 18%|█▊        | 145/790 [02:48<13:32,  1.26s/it] 18%|█▊        | 146/790 [02:49<13:46,  1.28s/it] 19%|█▊        | 147/790 [02:50<13:56,  1.30s/it] 19%|█▊        | 148/790 [02:51<13:36,  1.27s/it] 19%|█▉        | 149/790 [02:53<13:30,  1.26s/it] 19%|█▉        | 150/790 [02:54<13:24,  1.26s/it]                                                 {'loss': 3.0857, 'learning_rate': 4.5682600813576435e-05, 'epoch': 0.19}
 19%|█▉        | 150/790 [02:54<13:24,  1.26s/it]                                                 {'router_ce_loss': 1.0256794691085815, 'old_lang_expert0_score': '0.23 0.02 0.05 0.19 0.84 0.87 0.23 0.92 0.64 0.88 0.14 0.94 0.17 0.94 0.97 0.88 0.94 0.92 0.96 0.95 0.94 0.95 0.97 0.98', 'epoch': 0.19}
 19%|█▉        | 150/790 [02:54<13:24,  1.26s/it] 19%|█▉        | 151/790 [02:55<13:27,  1.26s/it] 19%|█▉        | 152/790 [02:57<13:56,  1.31s/it] 19%|█▉        | 153/790 [02:58<14:29,  1.37s/it] 19%|█▉        | 154/790 [02:59<14:24,  1.36s/it] 20%|█▉        | 155/790 [03:01<14:10,  1.34s/it] 20%|█▉        | 156/790 [03:02<14:13,  1.35s/it] 20%|█▉        | 157/790 [03:03<14:03,  1.33s/it] 20%|██        | 158/790 [03:05<14:05,  1.34s/it] 20%|██        | 159/790 [03:06<14:28,  1.38s/it] 20%|██        | 160/790 [03:08<14:45,  1.41s/it]                                                 {'loss': 3.0798, 'learning_rate': 4.510791413176912e-05, 'epoch': 0.2}
 20%|██        | 160/790 [03:08<14:45,  1.41s/it]                                                 {'router_ce_loss': 1.0324565172195435, 'old_lang_expert0_score': '0.23 0.03 0.06 0.22 0.85 0.87 0.22 0.91 0.64 0.85 0.14 0.92 0.18 0.91 0.96 0.84 0.92 0.89 0.93 0.94 0.92 0.93 0.97 0.98', 'epoch': 0.2}
 20%|██        | 160/790 [03:08<14:45,  1.41s/it] 20%|██        | 161/790 [03:09<14:42,  1.40s/it] 21%|██        | 162/790 [03:10<14:32,  1.39s/it] 21%|██        | 163/790 [03:12<14:49,  1.42s/it] 21%|██        | 164/790 [03:13<14:50,  1.42s/it] 21%|██        | 165/790 [03:15<14:57,  1.44s/it] 21%|██        | 166/790 [03:16<15:10,  1.46s/it] 21%|██        | 167/790 [03:18<15:22,  1.48s/it] 21%|██▏       | 168/790 [03:19<15:21,  1.48s/it] 21%|██▏       | 169/790 [03:21<15:03,  1.46s/it] 22%|██▏       | 170/790 [03:22<14:48,  1.43s/it]                                                 {'loss': 3.0675, 'learning_rate': 4.4501432704630305e-05, 'epoch': 0.22}
 22%|██▏       | 170/790 [03:22<14:48,  1.43s/it]                                                 {'router_ce_loss': 1.0272256135940552, 'old_lang_expert0_score': '0.22 0.03 0.06 0.2 0.83 0.86 0.2 0.92 0.69 0.87 0.15 0.94 0.24 0.93 0.96 0.86 0.92 0.9 0.94 0.94 0.92 0.94 0.97 0.98', 'epoch': 0.22}
 22%|██▏       | 170/790 [03:23<14:48,  1.43s/it] 22%|██▏       | 171/790 [03:24<15:00,  1.46s/it] 22%|██▏       | 172/790 [03:25<14:55,  1.45s/it] 22%|██▏       | 173/790 [03:27<15:16,  1.49s/it] 22%|██▏       | 174/790 [03:28<15:48,  1.54s/it] 22%|██▏       | 175/790 [03:30<15:39,  1.53s/it] 22%|██▏       | 176/790 [03:31<15:13,  1.49s/it] 22%|██▏       | 177/790 [03:33<14:58,  1.47s/it] 23%|██▎       | 178/790 [03:34<14:40,  1.44s/it] 23%|██▎       | 179/790 [03:36<15:15,  1.50s/it] 23%|██▎       | 180/790 [03:37<15:22,  1.51s/it]                                                 {'loss': 3.1032, 'learning_rate': 4.386411550395576e-05, 'epoch': 0.23}
 23%|██▎       | 180/790 [03:37<15:22,  1.51s/it]                                                 {'router_ce_loss': 1.0267200469970703, 'old_lang_expert0_score': '0.23 0.03 0.08 0.2 0.83 0.86 0.21 0.92 0.7 0.87 0.12 0.94 0.19 0.94 0.97 0.89 0.94 0.93 0.95 0.95 0.9 0.95 0.96 0.98', 'epoch': 0.23}
 23%|██▎       | 180/790 [03:38<15:22,  1.51s/it] 23%|██▎       | 181/790 [03:39<15:07,  1.49s/it] 23%|██▎       | 182/790 [03:40<15:00,  1.48s/it] 23%|██▎       | 183/790 [03:42<15:57,  1.58s/it] 23%|██▎       | 184/790 [03:44<16:00,  1.58s/it] 23%|██▎       | 185/790 [03:45<16:12,  1.61s/it] 24%|██▎       | 186/790 [03:47<16:16,  1.62s/it] 24%|██▎       | 187/790 [03:48<16:14,  1.62s/it] 24%|██▍       | 188/790 [03:50<15:44,  1.57s/it] 24%|██▍       | 189/790 [03:51<15:47,  1.58s/it] 24%|██▍       | 190/790 [03:53<16:18,  1.63s/it]                                                 {'loss': 3.0599, 'learning_rate': 4.319697025923736e-05, 'epoch': 0.24}
 24%|██▍       | 190/790 [03:53<16:18,  1.63s/it]                                                 {'router_ce_loss': 1.0191034078598022, 'old_lang_expert0_score': '0.24 0.03 0.05 0.2 0.86 0.89 0.22 0.93 0.71 0.89 0.12 0.95 0.16 0.96 0.98 0.92 0.94 0.92 0.95 0.94 0.95 0.96 0.97 0.98', 'epoch': 0.24}
 24%|██▍       | 190/790 [03:54<16:18,  1.63s/it] 24%|██▍       | 191/790 [03:55<16:26,  1.65s/it] 24%|██▍       | 192/790 [03:57<16:16,  1.63s/it] 24%|██▍       | 193/790 [03:58<16:44,  1.68s/it] 25%|██▍       | 194/790 [04:00<16:41,  1.68s/it] 25%|██▍       | 195/790 [04:02<16:29,  1.66s/it] 25%|██▍       | 196/790 [04:03<16:53,  1.71s/it] 25%|██▍       | 197/790 [04:05<16:09,  1.63s/it] 25%|██▌       | 198/790 [04:06<15:40,  1.59s/it] 25%|██▌       | 199/790 [04:08<15:22,  1.56s/it] 25%|██▌       | 200/790 [04:09<15:28,  1.57s/it]                                                 {'loss': 3.0549, 'learning_rate': 4.2501051864235636e-05, 'epoch': 0.25}
 25%|██▌       | 200/790 [04:09<15:28,  1.57s/it]                                                 {'router_ce_loss': 1.019829273223877, 'old_lang_expert0_score': '0.23 0.02 0.06 0.21 0.85 0.88 0.21 0.93 0.74 0.88 0.12 0.94 0.22 0.94 0.97 0.89 0.94 0.93 0.95 0.95 0.95 0.94 0.97 0.98', 'epoch': 0.25}
 25%|██▌       | 200/790 [04:10<15:28,  1.57s/it] 25%|██▌       | 201/790 [04:11<15:37,  1.59s/it] 26%|██▌       | 202/790 [04:13<15:28,  1.58s/it] 26%|██▌       | 203/790 [04:14<16:05,  1.64s/it] 26%|██▌       | 204/790 [04:16<16:28,  1.69s/it] 26%|██▌       | 205/790 [04:18<16:24,  1.68s/it] 26%|██▌       | 206/790 [04:19<15:51,  1.63s/it] 26%|██▌       | 207/790 [04:21<15:22,  1.58s/it] 26%|██▋       | 208/790 [04:23<16:16,  1.68s/it] 26%|██▋       | 209/790 [04:24<16:07,  1.66s/it] 27%|██▋       | 210/790 [04:26<16:48,  1.74s/it]                                                 {'loss': 3.0187, 'learning_rate': 4.177746070897592e-05, 'epoch': 0.27}
 27%|██▋       | 210/790 [04:26<16:48,  1.74s/it]                                                 {'router_ce_loss': 1.0258525609970093, 'old_lang_expert0_score': '0.24 0.02 0.05 0.17 0.84 0.86 0.2 0.92 0.72 0.89 0.14 0.94 0.19 0.93 0.97 0.87 0.93 0.91 0.95 0.94 0.94 0.94 0.98 0.98', 'epoch': 0.27}
 27%|██▋       | 210/790 [04:27<16:48,  1.74s/it] 27%|██▋       | 211/790 [04:28<16:49,  1.74s/it] 27%|██▋       | 212/790 [04:30<16:15,  1.69s/it] 27%|██▋       | 213/790 [04:31<16:05,  1.67s/it] 27%|██▋       | 214/790 [04:33<16:00,  1.67s/it] 27%|██▋       | 215/790 [04:34<15:38,  1.63s/it] 27%|██▋       | 216/790 [04:36<15:23,  1.61s/it] 27%|██▋       | 217/790 [04:38<16:15,  1.70s/it] 28%|██▊       | 218/790 [04:40<16:08,  1.69s/it] 28%|██▊       | 219/790 [04:41<15:53,  1.67s/it] 28%|██▊       | 220/790 [04:43<15:45,  1.66s/it]                                                 {'loss': 3.0391, 'learning_rate': 4.10273409398055e-05, 'epoch': 0.28}
 28%|██▊       | 220/790 [04:43<15:45,  1.66s/it]                                                 {'router_ce_loss': 1.0114305019378662, 'old_lang_expert0_score': '0.23 0.03 0.05 0.2 0.87 0.91 0.2 0.94 0.77 0.9 0.14 0.96 0.21 0.95 0.98 0.94 0.94 0.94 0.96 0.95 0.95 0.96 0.98 0.98', 'epoch': 0.28}
 28%|██▊       | 220/790 [04:43<15:45,  1.66s/it] 28%|██▊       | 221/790 [04:45<15:51,  1.67s/it] 28%|██▊       | 222/790 [04:46<16:09,  1.71s/it] 28%|██▊       | 223/790 [04:48<15:52,  1.68s/it] 28%|██▊       | 224/790 [04:50<15:41,  1.66s/it] 28%|██▊       | 225/790 [04:51<15:48,  1.68s/it] 29%|██▊       | 226/790 [04:53<15:43,  1.67s/it] 29%|██▊       | 227/790 [04:55<15:36,  1.66s/it] 29%|██▉       | 228/790 [04:56<15:32,  1.66s/it] 29%|██▉       | 229/790 [04:58<15:28,  1.66s/it] 29%|██▉       | 230/790 [05:00<15:28,  1.66s/it]                                                 {'loss': 3.0067, 'learning_rate': 4.025187865026311e-05, 'epoch': 0.29}
 29%|██▉       | 230/790 [05:00<15:28,  1.66s/it]                                                 {'router_ce_loss': 1.0158103704452515, 'old_lang_expert0_score': '0.24 0.02 0.05 0.18 0.87 0.89 0.2 0.93 0.78 0.89 0.11 0.95 0.19 0.95 0.97 0.92 0.95 0.94 0.96 0.96 0.96 0.96 0.98 0.98', 'epoch': 0.29}
 29%|██▉       | 230/790 [05:00<15:28,  1.66s/it] 29%|██▉       | 231/790 [05:01<15:45,  1.69s/it] 29%|██▉       | 232/790 [05:03<15:26,  1.66s/it] 29%|██▉       | 233/790 [05:05<15:16,  1.65s/it] 30%|██▉       | 234/790 [05:06<15:27,  1.67s/it] 30%|██▉       | 235/790 [05:09<18:40,  2.02s/it] 30%|██▉       | 236/790 [05:11<17:05,  1.85s/it] 30%|███       | 237/790 [05:12<15:59,  1.73s/it] 30%|███       | 238/790 [05:14<15:13,  1.65s/it] 30%|███       | 239/790 [05:18<23:14,  2.53s/it] 30%|███       | 240/790 [05:23<29:08,  3.18s/it]                                                 {'loss': 3.0421, 'learning_rate': 3.945230000562121e-05, 'epoch': 0.3}
 30%|███       | 240/790 [05:23<29:08,  3.18s/it]                                                 {'router_ce_loss': 1.0176087617874146, 'old_lang_expert0_score': '0.25 0.02 0.05 0.16 0.86 0.88 0.19 0.91 0.78 0.89 0.12 0.95 0.21 0.94 0.97 0.92 0.94 0.93 0.97 0.96 0.95 0.95 0.98 0.98', 'epoch': 0.3}
 30%|███       | 240/790 [05:23<29:08,  3.18s/it] 31%|███       | 241/790 [05:25<26:56,  2.94s/it] 31%|███       | 242/790 [05:26<22:07,  2.42s/it] 31%|███       | 243/790 [05:28<19:12,  2.11s/it] 31%|███       | 244/790 [05:29<17:03,  1.87s/it] 31%|███       | 245/790 [05:31<17:04,  1.88s/it] 31%|███       | 246/790 [05:35<22:47,  2.51s/it] 31%|███▏      | 247/790 [05:39<25:41,  2.84s/it] 31%|███▏      | 248/790 [05:42<26:56,  2.98s/it] 32%|███▏      | 249/790 [05:45<26:58,  2.99s/it] 32%|███▏      | 250/790 [05:46<22:02,  2.45s/it]                                                 {'loss': 3.066, 'learning_rate': 3.862986930406669e-05, 'epoch': 0.32}
 32%|███▏      | 250/790 [05:46<22:02,  2.45s/it]                                                 {'router_ce_loss': 1.0158073902130127, 'old_lang_expert0_score': '0.23 0.02 0.06 0.19 0.83 0.88 0.2 0.93 0.78 0.89 0.11 0.95 0.21 0.95 0.98 0.91 0.95 0.94 0.97 0.96 0.96 0.94 0.98 0.98', 'epoch': 0.32}
 32%|███▏      | 250/790 [05:46<22:02,  2.45s/it] 32%|███▏      | 251/790 [05:47<18:47,  2.09s/it] 32%|███▏      | 252/790 [05:49<16:35,  1.85s/it] 32%|███▏      | 253/790 [05:50<15:00,  1.68s/it] 32%|███▏      | 254/790 [05:51<14:19,  1.60s/it] 32%|███▏      | 255/790 [05:53<13:54,  1.56s/it] 32%|███▏      | 256/790 [05:54<13:29,  1.52s/it] 33%|███▎      | 257/790 [05:56<14:28,  1.63s/it] 33%|███▎      | 258/790 [05:57<13:35,  1.53s/it] 33%|███▎      | 259/790 [05:59<12:52,  1.45s/it] 33%|███▎      | 260/790 [06:00<12:57,  1.47s/it]                                                 {'loss': 3.0506, 'learning_rate': 3.778588697758556e-05, 'epoch': 0.33}
 33%|███▎      | 260/790 [06:00<12:57,  1.47s/it]                                                 {'router_ce_loss': 1.0177611112594604, 'old_lang_expert0_score': '0.23 0.03 0.06 0.21 0.85 0.88 0.22 0.93 0.77 0.89 0.15 0.95 0.22 0.94 0.96 0.89 0.93 0.92 0.95 0.95 0.95 0.94 0.97 0.97', 'epoch': 0.33}
 33%|███▎      | 260/790 [06:01<12:57,  1.47s/it] 33%|███▎      | 261/790 [06:02<12:39,  1.44s/it] 33%|███▎      | 262/790 [06:03<12:38,  1.44s/it] 33%|███▎      | 263/790 [06:04<12:31,  1.43s/it] 33%|███▎      | 264/790 [06:06<12:58,  1.48s/it] 34%|███▎      | 265/790 [06:08<13:19,  1.52s/it] 34%|███▎      | 266/790 [06:09<13:51,  1.59s/it] 34%|███▍      | 267/790 [06:11<13:59,  1.61s/it] 34%|███▍      | 268/790 [06:13<14:07,  1.62s/it] 34%|███▍      | 269/790 [06:14<14:19,  1.65s/it] 34%|███▍      | 270/790 [06:16<14:02,  1.62s/it]                                                 {'loss': 3.0286, 'learning_rate': 3.6921687535712656e-05, 'epoch': 0.34}
 34%|███▍      | 270/790 [06:16<14:02,  1.62s/it]                                                 {'router_ce_loss': 1.0120112895965576, 'old_lang_expert0_score': '0.23 0.02 0.06 0.2 0.87 0.89 0.2 0.94 0.8 0.9 0.11 0.95 0.2 0.95 0.98 0.93 0.95 0.95 0.97 0.96 0.96 0.97 0.98 0.99', 'epoch': 0.34}
 34%|███▍      | 270/790 [06:16<14:02,  1.62s/it] 34%|███▍      | 271/790 [06:18<13:58,  1.61s/it] 34%|███▍      | 272/790 [06:19<13:30,  1.56s/it] 35%|███▍      | 273/790 [06:21<14:13,  1.65s/it] 35%|███▍      | 274/790 [06:22<14:10,  1.65s/it] 35%|███▍      | 275/790 [06:24<13:53,  1.62s/it] 35%|███▍      | 276/790 [06:26<13:58,  1.63s/it] 35%|███▌      | 277/790 [06:27<13:55,  1.63s/it] 35%|███▌      | 278/790 [06:29<14:11,  1.66s/it] 35%|███▌      | 279/790 [06:31<14:10,  1.66s/it] 35%|███▌      | 280/790 [06:32<14:04,  1.66s/it]                                                 {'loss': 3.0276, 'learning_rate': 3.60386374553978e-05, 'epoch': 0.35}
 35%|███▌      | 280/790 [06:32<14:04,  1.66s/it]                                                 {'router_ce_loss': 1.0169435739517212, 'old_lang_expert0_score': '0.23 0.02 0.05 0.19 0.86 0.88 0.21 0.92 0.8 0.89 0.12 0.95 0.21 0.94 0.97 0.9 0.94 0.93 0.96 0.96 0.96 0.95 0.98 0.99', 'epoch': 0.35}
 35%|███▌      | 280/790 [06:33<14:04,  1.66s/it] 36%|███▌      | 281/790 [06:34<14:16,  1.68s/it] 36%|███▌      | 282/790 [06:36<14:27,  1.71s/it] 36%|███▌      | 283/790 [06:37<13:46,  1.63s/it] 36%|███▌      | 284/790 [06:39<13:51,  1.64s/it] 36%|███▌      | 285/790 [06:41<14:49,  1.76s/it] 36%|███▌      | 286/790 [06:43<14:46,  1.76s/it] 36%|███▋      | 287/790 [06:44<14:19,  1.71s/it] 36%|███▋      | 288/790 [06:46<14:35,  1.74s/it] 37%|███▋      | 289/790 [06:48<14:33,  1.74s/it] 37%|███▋      | 290/790 [06:50<14:19,  1.72s/it]                                                 {'loss': 3.0461, 'learning_rate': 3.5138133020324845e-05, 'epoch': 0.37}
 37%|███▋      | 290/790 [06:50<14:19,  1.72s/it]                                                 {'router_ce_loss': 1.0105948448181152, 'old_lang_expert0_score': '0.23 0.03 0.06 0.2 0.86 0.9 0.23 0.93 0.8 0.9 0.12 0.96 0.21 0.96 0.98 0.92 0.95 0.94 0.95 0.96 0.96 0.96 0.98 0.99', 'epoch': 0.37}
 37%|███▋      | 290/790 [06:50<14:19,  1.72s/it] 37%|███▋      | 291/790 [06:51<14:11,  1.71s/it] 37%|███▋      | 292/790 [06:53<14:09,  1.70s/it] 37%|███▋      | 293/790 [06:55<14:00,  1.69s/it] 37%|███▋      | 294/790 [06:56<13:56,  1.69s/it] 37%|███▋      | 295/790 [06:58<13:53,  1.68s/it] 37%|███▋      | 296/790 [07:00<13:53,  1.69s/it] 38%|███▊      | 297/790 [07:01<13:48,  1.68s/it] 38%|███▊      | 298/790 [07:03<14:02,  1.71s/it] 38%|███▊      | 299/790 [07:05<14:43,  1.80s/it] 38%|███▊      | 300/790 [07:07<15:00,  1.84s/it]                                                 {'loss': 3.0278, 'learning_rate': 3.4221598113100195e-05, 'epoch': 0.38}
 38%|███▊      | 300/790 [07:07<15:00,  1.84s/it]                                                 {'router_ce_loss': 1.0137100219726562, 'old_lang_expert0_score': '0.24 0.02 0.05 0.19 0.87 0.88 0.22 0.93 0.79 0.9 0.13 0.95 0.22 0.94 0.97 0.91 0.93 0.93 0.96 0.96 0.95 0.96 0.98 0.99', 'epoch': 0.38}
 38%|███▊      | 300/790 [07:08<15:00,  1.84s/it] 38%|███▊      | 301/790 [07:09<15:08,  1.86s/it] 38%|███▊      | 302/790 [07:11<15:00,  1.85s/it] 38%|███▊      | 303/790 [07:13<14:44,  1.82s/it] 38%|███▊      | 304/790 [07:14<14:23,  1.78s/it] 39%|███▊      | 305/790 [07:16<14:11,  1.75s/it] 39%|███▊      | 306/790 [07:18<13:51,  1.72s/it] 39%|███▉      | 307/790 [07:19<13:38,  1.70s/it] 39%|███▉      | 308/790 [07:21<13:47,  1.72s/it] 39%|███▉      | 309/790 [07:23<14:03,  1.75s/it] 39%|███▉      | 310/790 [07:25<14:17,  1.79s/it]                                                 {'loss': 3.0118, 'learning_rate': 3.32904819638017e-05, 'epoch': 0.39}
 39%|███▉      | 310/790 [07:25<14:17,  1.79s/it]                                                 {'router_ce_loss': 1.0132249593734741, 'old_lang_expert0_score': '0.24 0.02 0.05 0.2 0.87 0.89 0.23 0.92 0.8 0.91 0.11 0.94 0.19 0.96 0.98 0.93 0.94 0.94 0.97 0.96 0.96 0.96 0.98 0.98', 'epoch': 0.39}
 39%|███▉      | 310/790 [07:25<14:17,  1.79s/it] 39%|███▉      | 311/790 [07:26<14:21,  1.80s/it] 39%|███▉      | 312/790 [07:28<14:26,  1.81s/it] 40%|███▉      | 313/790 [07:30<14:20,  1.80s/it] 40%|███▉      | 314/790 [07:32<14:00,  1.76s/it] 40%|███▉      | 315/790 [07:33<13:37,  1.72s/it] 40%|████      | 316/790 [07:35<13:51,  1.75s/it] 40%|████      | 317/790 [07:37<14:14,  1.81s/it] 40%|████      | 318/790 [07:39<14:40,  1.86s/it] 40%|████      | 319/790 [07:41<14:35,  1.86s/it] 41%|████      | 320/790 [07:43<14:21,  1.83s/it]                                                 {'loss': 3.0267, 'learning_rate': 3.234625685844803e-05, 'epoch': 0.41}
 41%|████      | 320/790 [07:43<14:21,  1.83s/it]                                                 {'router_ce_loss': 1.0123721361160278, 'old_lang_expert0_score': '0.23 0.02 0.05 0.19 0.86 0.89 0.22 0.93 0.8 0.91 0.12 0.95 0.22 0.95 0.97 0.92 0.94 0.93 0.96 0.96 0.97 0.96 0.98 0.99', 'epoch': 0.41}
 41%|████      | 320/790 [07:43<14:21,  1.83s/it] 41%|████      | 321/790 [07:44<13:48,  1.77s/it] 41%|████      | 322/790 [07:46<13:25,  1.72s/it] 41%|████      | 323/790 [07:48<13:37,  1.75s/it] 41%|████      | 324/790 [07:49<13:14,  1.71s/it] 41%|████      | 325/790 [07:51<13:01,  1.68s/it] 41%|████▏     | 326/790 [07:53<13:08,  1.70s/it] 41%|████▏     | 327/790 [07:54<12:56,  1.68s/it] 42%|████▏     | 328/790 [07:56<12:58,  1.69s/it] 42%|████▏     | 329/790 [07:58<12:51,  1.67s/it] 42%|████▏     | 330/790 [07:59<12:49,  1.67s/it]                                                 {'loss': 3.0022, 'learning_rate': 3.139041581101187e-05, 'epoch': 0.42}
 42%|████▏     | 330/790 [07:59<12:49,  1.67s/it]                                                 {'router_ce_loss': 1.0097633600234985, 'old_lang_expert0_score': '0.23 0.02 0.05 0.21 0.87 0.9 0.25 0.94 0.8 0.9 0.11 0.96 0.21 0.96 0.98 0.92 0.95 0.95 0.97 0.97 0.96 0.97 0.98 0.99', 'epoch': 0.42}
 42%|████▏     | 330/790 [08:00<12:49,  1.67s/it] 42%|████▏     | 331/790 [08:01<13:14,  1.73s/it] 42%|████▏     | 332/790 [08:03<13:57,  1.83s/it] 42%|████▏     | 333/790 [08:05<14:03,  1.85s/it] 42%|████▏     | 334/790 [08:07<14:08,  1.86s/it] 42%|████▏     | 335/790 [08:09<14:06,  1.86s/it] 43%|████▎     | 336/790 [08:13<18:38,  2.46s/it] 43%|████▎     | 337/790 [08:17<21:43,  2.88s/it] 43%|████▎     | 338/790 [08:20<23:12,  3.08s/it] 43%|████▎     | 339/790 [08:24<23:36,  3.14s/it] 43%|████▎     | 340/790 [08:27<23:26,  3.13s/it]                                                 {'loss': 2.9696, 'learning_rate': 3.042447020265795e-05, 'epoch': 0.43}
 43%|████▎     | 340/790 [08:27<23:26,  3.13s/it]                                                 {'router_ce_loss': 1.006339430809021, 'old_lang_expert0_score': '0.23 0.02 0.06 0.21 0.87 0.9 0.24 0.94 0.83 0.91 0.13 0.95 0.21 0.95 0.98 0.93 0.96 0.95 0.97 0.97 0.96 0.97 0.98 0.99', 'epoch': 0.43}
 43%|████▎     | 340/790 [08:27<23:26,  3.13s/it] 43%|████▎     | 341/790 [08:30<22:52,  3.06s/it] 43%|████▎     | 342/790 [08:32<22:04,  2.96s/it] 43%|████▎     | 343/790 [08:35<21:12,  2.85s/it] 44%|████▎     | 344/790 [08:36<18:26,  2.48s/it] 44%|████▎     | 345/790 [08:38<15:34,  2.10s/it] 44%|████▍     | 346/790 [08:39<13:56,  1.88s/it] 44%|████▍     | 347/790 [08:41<12:53,  1.75s/it] 44%|████▍     | 348/790 [08:42<12:15,  1.66s/it] 44%|████▍     | 349/790 [08:43<11:34,  1.57s/it] 44%|████▍     | 350/790 [08:45<10:51,  1.48s/it]                                                 {'loss': 2.9844, 'learning_rate': 2.9449947391938766e-05, 'epoch': 0.44}
 44%|████▍     | 350/790 [08:45<10:51,  1.48s/it]                                                 {'router_ce_loss': 1.00637686252594, 'old_lang_expert0_score': '0.23 0.02 0.07 0.21 0.86 0.9 0.26 0.93 0.8 0.91 0.13 0.96 0.22 0.95 0.98 0.92 0.95 0.95 0.96 0.96 0.96 0.97 0.98 0.99', 'epoch': 0.44}
 44%|████▍     | 350/790 [08:45<10:51,  1.48s/it] 44%|████▍     | 351/790 [08:46<10:45,  1.47s/it] 45%|████▍     | 352/790 [08:48<10:57,  1.50s/it] 45%|████▍     | 353/790 [08:49<10:45,  1.48s/it] 45%|████▍     | 354/790 [08:51<11:07,  1.53s/it] 45%|████▍     | 355/790 [08:52<11:22,  1.57s/it] 45%|████▌     | 356/790 [08:54<11:23,  1.58s/it] 45%|████▌     | 357/790 [08:56<11:25,  1.58s/it] 45%|████▌     | 358/790 [08:57<11:18,  1.57s/it] 45%|████▌     | 359/790 [08:59<11:19,  1.58s/it] 46%|████▌     | 360/790 [09:00<11:19,  1.58s/it]                                                 {'loss': 3.0245, 'learning_rate': 2.8468388299726712e-05, 'epoch': 0.46}
 46%|████▌     | 360/790 [09:00<11:19,  1.58s/it]                                                 {'router_ce_loss': 1.0005687475204468, 'old_lang_expert0_score': '0.24 0.02 0.06 0.2 0.88 0.91 0.3 0.95 0.84 0.93 0.16 0.96 0.25 0.96 0.98 0.92 0.95 0.94 0.96 0.96 0.95 0.96 0.98 0.98', 'epoch': 0.46}
 46%|████▌     | 360/790 [09:01<11:19,  1.58s/it] 46%|████▌     | 361/790 [09:02<11:37,  1.63s/it] 46%|████▌     | 362/790 [09:04<11:48,  1.65s/it] 46%|████▌     | 363/790 [09:05<11:49,  1.66s/it] 46%|████▌     | 364/790 [09:07<12:11,  1.72s/it] 46%|████▌     | 365/790 [09:09<12:05,  1.71s/it] 46%|████▋     | 366/790 [09:11<11:47,  1.67s/it] 46%|████▋     | 367/790 [09:12<11:26,  1.62s/it] 47%|████▋     | 368/790 [09:14<11:38,  1.66s/it] 47%|████▋     | 369/790 [09:16<13:24,  1.91s/it] 47%|████▋     | 370/790 [09:18<13:02,  1.86s/it]                                                 {'loss': 3.0029, 'learning_rate': 2.7481344972701545e-05, 'epoch': 0.47}
 47%|████▋     | 370/790 [09:18<13:02,  1.86s/it]                                                 {'router_ce_loss': 1.0031591653823853, 'old_lang_expert0_score': '0.23 0.02 0.06 0.21 0.88 0.9 0.27 0.93 0.83 0.92 0.11 0.96 0.23 0.95 0.98 0.93 0.96 0.96 0.97 0.97 0.97 0.96 0.98 0.99', 'epoch': 0.47}
 47%|████▋     | 370/790 [09:19<13:02,  1.86s/it] 47%|████▋     | 371/790 [09:20<12:36,  1.80s/it] 47%|████▋     | 372/790 [09:21<12:33,  1.80s/it] 47%|████▋     | 373/790 [09:23<12:06,  1.74s/it] 47%|████▋     | 374/790 [09:25<12:29,  1.80s/it] 47%|████▋     | 375/790 [09:27<12:23,  1.79s/it] 48%|████▊     | 376/790 [09:29<12:10,  1.76s/it] 48%|████▊     | 377/790 [09:30<12:00,  1.75s/it] 48%|████▊     | 378/790 [09:32<11:48,  1.72s/it] 48%|████▊     | 379/790 [09:34<11:52,  1.73s/it] 48%|████▊     | 380/790 [09:35<12:04,  1.77s/it]                                                 {'loss': 3.0381, 'learning_rate': 2.6490378129245498e-05, 'epoch': 0.48}
 48%|████▊     | 380/790 [09:35<12:04,  1.77s/it]                                                 {'router_ce_loss': 1.005243182182312, 'old_lang_expert0_score': '0.24 0.02 0.04 0.19 0.89 0.91 0.22 0.94 0.85 0.92 0.1 0.96 0.21 0.96 0.98 0.94 0.96 0.96 0.98 0.97 0.97 0.96 0.98 0.99', 'epoch': 0.48}
 48%|████▊     | 380/790 [09:36<12:04,  1.77s/it] 48%|████▊     | 381/790 [09:37<12:00,  1.76s/it] 48%|████▊     | 382/790 [09:39<11:57,  1.76s/it] 48%|████▊     | 383/790 [09:41<11:45,  1.73s/it] 49%|████▊     | 384/790 [09:42<11:47,  1.74s/it] 49%|████▊     | 385/790 [09:44<11:30,  1.70s/it] 49%|████▉     | 386/790 [09:46<11:06,  1.65s/it] 49%|████▉     | 387/790 [09:47<11:03,  1.65s/it] 49%|████▉     | 388/790 [09:49<11:13,  1.68s/it] 49%|████▉     | 389/790 [09:51<11:18,  1.69s/it] 49%|████▉     | 390/790 [09:52<11:25,  1.71s/it]                                                 {'loss': 3.0198, 'learning_rate': 2.5497054691626753e-05, 'epoch': 0.49}
 49%|████▉     | 390/790 [09:52<11:25,  1.71s/it]                                                 {'router_ce_loss': 1.022080898284912, 'old_lang_expert0_score': '0.23 0.03 0.06 0.19 0.86 0.87 0.25 0.91 0.82 0.9 0.13 0.93 0.25 0.93 0.95 0.89 0.92 0.92 0.94 0.94 0.95 0.93 0.95 0.96', 'epoch': 0.49}
 49%|████▉     | 390/790 [09:53<11:25,  1.71s/it] 49%|████▉     | 391/790 [09:57<16:36,  2.50s/it] 50%|████▉     | 392/790 [10:01<19:40,  2.97s/it] 50%|████▉     | 393/790 [10:05<21:04,  3.18s/it] 50%|████▉     | 394/790 [10:08<21:25,  3.25s/it] 50%|█████     | 395/790 [10:11<21:09,  3.21s/it] 50%|█████     | 396/790 [10:14<20:29,  3.12s/it] 50%|█████     | 397/790 [10:17<19:40,  3.00s/it] 50%|█████     | 398/790 [10:19<18:52,  2.89s/it] 51%|█████     | 399/790 [10:22<17:57,  2.76s/it] 51%|█████     | 400/790 [10:23<15:40,  2.41s/it]                                                 {'loss': 2.9951, 'learning_rate': 2.4502945308373246e-05, 'epoch': 0.51}
 51%|█████     | 400/790 [10:23<15:40,  2.41s/it]                                                 {'router_ce_loss': 1.0019689798355103, 'old_lang_expert0_score': '0.23 0.02 0.06 0.19 0.87 0.9 0.27 0.94 0.85 0.91 0.13 0.96 0.28 0.96 0.98 0.93 0.94 0.95 0.98 0.97 0.97 0.97 0.98 0.99', 'epoch': 0.51}
 51%|█████     | 400/790 [10:24<15:40,  2.41s/it] 51%|█████     | 401/790 [10:25<13:16,  2.05s/it] 51%|█████     | 402/790 [10:26<11:49,  1.83s/it] 51%|█████     | 403/790 [10:27<11:05,  1.72s/it] 51%|█████     | 404/790 [10:29<10:31,  1.64s/it] 51%|█████▏    | 405/790 [10:30<10:10,  1.59s/it] 51%|█████▏    | 406/790 [10:32<09:46,  1.53s/it] 52%|█████▏    | 407/790 [10:33<09:30,  1.49s/it] 52%|█████▏    | 408/790 [10:35<09:30,  1.49s/it] 52%|█████▏    | 409/790 [10:36<09:33,  1.50s/it] 52%|█████▏    | 410/790 [10:38<09:38,  1.52s/it]                                                 {'loss': 2.9908, 'learning_rate': 2.3509621870754505e-05, 'epoch': 0.52}
 52%|█████▏    | 410/790 [10:38<09:38,  1.52s/it]                                                 {'router_ce_loss': 1.0031105279922485, 'old_lang_expert0_score': '0.24 0.02 0.06 0.19 0.88 0.9 0.25 0.95 0.85 0.92 0.13 0.96 0.24 0.96 0.98 0.93 0.95 0.95 0.97 0.96 0.96 0.96 0.98 0.98', 'epoch': 0.52}
 52%|█████▏    | 410/790 [10:38<09:38,  1.52s/it] 52%|█████▏    | 411/790 [10:39<09:42,  1.54s/it] 52%|█████▏    | 412/790 [10:41<09:34,  1.52s/it] 52%|█████▏    | 413/790 [10:42<09:23,  1.49s/it] 52%|█████▏    | 414/790 [10:44<09:33,  1.53s/it] 53%|█████▎    | 415/790 [10:45<09:50,  1.58s/it] 53%|█████▎    | 416/790 [10:47<09:43,  1.56s/it] 53%|█████▎    | 417/790 [10:48<09:28,  1.52s/it] 53%|█████▎    | 418/790 [10:50<09:43,  1.57s/it] 53%|█████▎    | 419/790 [10:52<10:14,  1.66s/it] 53%|█████▎    | 420/790 [10:54<10:27,  1.70s/it]                                                 {'loss': 3.0281, 'learning_rate': 2.2518655027298464e-05, 'epoch': 0.53}
 53%|█████▎    | 420/790 [10:54<10:27,  1.70s/it]                                                 {'router_ce_loss': 0.9950951933860779, 'old_lang_expert0_score': '0.24 0.02 0.07 0.23 0.89 0.93 0.29 0.96 0.86 0.93 0.12 0.97 0.19 0.97 0.98 0.95 0.97 0.97 0.98 0.98 0.97 0.98 0.99 0.99', 'epoch': 0.53}
 53%|█████▎    | 420/790 [10:54<10:27,  1.70s/it] 53%|█████▎    | 421/790 [10:55<10:23,  1.69s/it] 53%|█████▎    | 422/790 [10:57<10:16,  1.67s/it] 54%|█████▎    | 423/790 [10:59<10:01,  1.64s/it] 54%|█████▎    | 424/790 [11:00<09:55,  1.63s/it] 54%|█████▍    | 425/790 [11:02<09:44,  1.60s/it] 54%|█████▍    | 426/790 [11:03<09:46,  1.61s/it] 54%|█████▍    | 427/790 [11:06<11:54,  1.97s/it] 54%|█████▍    | 428/790 [11:08<11:34,  1.92s/it] 54%|█████▍    | 429/790 [11:10<11:12,  1.86s/it] 54%|█████▍    | 430/790 [11:11<10:39,  1.78s/it]                                                 {'loss': 3.0317, 'learning_rate': 2.1531611700273297e-05, 'epoch': 0.54}
 54%|█████▍    | 430/790 [11:11<10:39,  1.78s/it]                                                 {'router_ce_loss': 0.9983534216880798, 'old_lang_expert0_score': '0.23 0.02 0.06 0.21 0.87 0.91 0.29 0.94 0.83 0.93 0.14 0.96 0.25 0.96 0.98 0.94 0.96 0.96 0.98 0.97 0.97 0.98 0.98 0.99', 'epoch': 0.54}
 54%|█████▍    | 430/790 [11:12<10:39,  1.78s/it] 55%|█████▍    | 431/790 [11:13<10:15,  1.72s/it] 55%|█████▍    | 432/790 [11:15<10:12,  1.71s/it] 55%|█████▍    | 433/790 [11:16<10:20,  1.74s/it] 55%|█████▍    | 434/790 [11:18<10:21,  1.75s/it] 55%|█████▌    | 435/790 [11:20<10:20,  1.75s/it] 55%|█████▌    | 436/790 [11:22<10:20,  1.75s/it] 55%|█████▌    | 437/790 [11:23<10:14,  1.74s/it] 55%|█████▌    | 438/790 [11:25<10:17,  1.75s/it] 56%|█████▌    | 439/790 [11:27<10:09,  1.74s/it] 56%|█████▌    | 440/790 [11:28<10:03,  1.72s/it]                                                 {'loss': 2.9738, 'learning_rate': 2.055005260806125e-05, 'epoch': 0.56}
 56%|█████▌    | 440/790 [11:28<10:03,  1.72s/it]                                                 {'router_ce_loss': 0.9954733848571777, 'old_lang_expert0_score': '0.23 0.02 0.05 0.24 0.89 0.92 0.32 0.95 0.85 0.92 0.13 0.96 0.22 0.96 0.98 0.95 0.96 0.96 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.56}
 56%|█████▌    | 440/790 [11:29<10:03,  1.72s/it] 56%|█████▌    | 441/790 [11:30<09:56,  1.71s/it] 56%|█████▌    | 442/790 [11:32<10:03,  1.74s/it] 56%|█████▌    | 443/790 [11:34<10:00,  1.73s/it] 56%|█████▌    | 444/790 [11:35<09:49,  1.70s/it] 56%|█████▋    | 445/790 [11:37<09:54,  1.72s/it] 56%|█████▋    | 446/790 [11:39<09:51,  1.72s/it] 57%|█████▋    | 447/790 [11:41<09:58,  1.75s/it] 57%|█████▋    | 448/790 [11:43<10:20,  1.82s/it] 57%|█████▋    | 449/790 [11:45<10:28,  1.84s/it] 57%|█████▋    | 450/790 [11:46<10:21,  1.83s/it]                                                 {'loss': 3.0105, 'learning_rate': 1.957552979734205e-05, 'epoch': 0.57}
 57%|█████▋    | 450/790 [11:46<10:21,  1.83s/it]                                                 {'router_ce_loss': 1.004952311515808, 'old_lang_expert0_score': '0.25 0.02 0.05 0.19 0.88 0.9 0.26 0.93 0.83 0.92 0.11 0.96 0.22 0.95 0.98 0.93 0.96 0.95 0.98 0.97 0.97 0.97 0.98 0.99', 'epoch': 0.57}
 57%|█████▋    | 450/790 [11:47<10:21,  1.83s/it] 57%|█████▋    | 451/790 [11:48<10:37,  1.88s/it] 57%|█████▋    | 452/790 [11:50<10:33,  1.87s/it] 57%|█████▋    | 453/790 [11:52<10:03,  1.79s/it] 57%|█████▋    | 454/790 [11:54<09:57,  1.78s/it] 58%|█████▊    | 455/790 [11:55<10:04,  1.80s/it] 58%|█████▊    | 456/790 [11:57<09:53,  1.78s/it] 58%|█████▊    | 457/790 [11:59<09:41,  1.75s/it] 58%|█████▊    | 458/790 [12:01<09:53,  1.79s/it] 58%|█████▊    | 459/790 [12:02<09:53,  1.79s/it] 58%|█████▊    | 460/790 [12:04<09:59,  1.82s/it]                                                 {'loss': 3.0309, 'learning_rate': 1.8609584188988136e-05, 'epoch': 0.58}
 58%|█████▊    | 460/790 [12:04<09:59,  1.82s/it]                                                 {'router_ce_loss': 1.0047413110733032, 'old_lang_expert0_score': '0.23 0.02 0.05 0.19 0.86 0.89 0.27 0.94 0.84 0.92 0.11 0.97 0.22 0.97 0.98 0.93 0.96 0.96 0.97 0.97 0.97 0.98 0.99 0.99', 'epoch': 0.58}
 58%|█████▊    | 460/790 [12:05<09:59,  1.82s/it] 58%|█████▊    | 461/790 [12:06<09:45,  1.78s/it] 58%|█████▊    | 462/790 [12:08<09:48,  1.80s/it] 59%|█████▊    | 463/790 [12:10<09:44,  1.79s/it] 59%|█████▊    | 464/790 [12:12<09:56,  1.83s/it] 59%|█████▉    | 465/790 [12:13<09:45,  1.80s/it] 59%|█████▉    | 466/790 [12:15<09:24,  1.74s/it] 59%|█████▉    | 467/790 [12:16<09:01,  1.68s/it] 59%|█████▉    | 468/790 [12:18<09:03,  1.69s/it] 59%|█████▉    | 469/790 [12:20<09:24,  1.76s/it] 59%|█████▉    | 470/790 [12:22<09:27,  1.77s/it]                                                 {'loss': 2.9997, 'learning_rate': 1.7653743141551983e-05, 'epoch': 0.59}
 59%|█████▉    | 470/790 [12:22<09:27,  1.77s/it]                                                 {'router_ce_loss': 1.0007432699203491, 'old_lang_expert0_score': '0.24 0.02 0.06 0.19 0.88 0.9 0.26 0.95 0.86 0.93 0.11 0.96 0.23 0.96 0.98 0.94 0.96 0.97 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.59}
 59%|█████▉    | 470/790 [12:22<09:27,  1.77s/it] 60%|█████▉    | 471/790 [12:24<09:22,  1.76s/it] 60%|█████▉    | 472/790 [12:25<08:52,  1.68s/it] 60%|█████▉    | 473/790 [12:27<09:15,  1.75s/it] 60%|██████    | 474/790 [12:29<09:27,  1.80s/it] 60%|██████    | 475/790 [12:31<09:30,  1.81s/it] 60%|██████    | 476/790 [12:33<09:24,  1.80s/it] 60%|██████    | 477/790 [12:34<09:00,  1.73s/it] 61%|██████    | 478/790 [12:36<09:10,  1.76s/it] 61%|██████    | 479/790 [12:38<09:18,  1.80s/it] 61%|██████    | 480/790 [12:40<09:17,  1.80s/it]                                                 {'loss': 2.9711, 'learning_rate': 1.6709518036198308e-05, 'epoch': 0.61}
 61%|██████    | 480/790 [12:40<09:17,  1.80s/it]                                                 {'router_ce_loss': 1.001428484916687, 'old_lang_expert0_score': '0.23 0.02 0.05 0.18 0.88 0.9 0.27 0.94 0.86 0.93 0.13 0.96 0.28 0.95 0.98 0.94 0.96 0.95 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.61}
 61%|██████    | 480/790 [12:40<09:17,  1.80s/it] 61%|██████    | 481/790 [12:42<09:42,  1.89s/it] 61%|██████    | 482/790 [12:43<09:27,  1.84s/it] 61%|██████    | 483/790 [12:45<09:03,  1.77s/it] 61%|██████▏   | 484/790 [12:47<08:47,  1.73s/it] 61%|██████▏   | 485/790 [12:48<08:43,  1.72s/it] 62%|██████▏   | 486/790 [12:50<08:34,  1.69s/it] 62%|██████▏   | 487/790 [12:52<08:43,  1.73s/it] 62%|██████▏   | 488/790 [12:54<09:01,  1.79s/it] 62%|██████▏   | 489/790 [12:56<09:09,  1.82s/it] 62%|██████▏   | 490/790 [12:57<09:06,  1.82s/it]                                                 {'loss': 2.9699, 'learning_rate': 1.5778401886899807e-05, 'epoch': 0.62}
 62%|██████▏   | 490/790 [12:57<09:06,  1.82s/it]                                                 {'router_ce_loss': 0.9932554364204407, 'old_lang_expert0_score': '0.23 0.02 0.06 0.22 0.9 0.91 0.31 0.96 0.88 0.94 0.12 0.97 0.23 0.96 0.99 0.95 0.97 0.97 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.62}
 62%|██████▏   | 490/790 [12:58<09:06,  1.82s/it] 62%|██████▏   | 491/790 [12:59<09:11,  1.85s/it] 62%|██████▏   | 492/790 [13:01<08:46,  1.77s/it] 62%|██████▏   | 493/790 [13:03<08:49,  1.78s/it] 63%|██████▎   | 494/790 [13:05<08:55,  1.81s/it] 63%|██████▎   | 495/790 [13:06<08:50,  1.80s/it] 63%|██████▎   | 496/790 [13:08<08:39,  1.77s/it] 63%|██████▎   | 497/790 [13:10<08:55,  1.83s/it] 63%|██████▎   | 498/790 [13:12<08:53,  1.83s/it] 63%|██████▎   | 499/790 [13:13<08:32,  1.76s/it] 63%|██████▎   | 500/790 [13:15<08:18,  1.72s/it]                                                 {'loss': 2.9727, 'learning_rate': 1.4861866979675154e-05, 'epoch': 0.63}
 63%|██████▎   | 500/790 [13:15<08:18,  1.72s/it]                                                 {'router_ce_loss': 0.9955241084098816, 'old_lang_expert0_score': '0.23 0.02 0.06 0.2 0.89 0.92 0.29 0.95 0.87 0.93 0.13 0.97 0.24 0.97 0.98 0.95 0.96 0.96 0.98 0.97 0.98 0.98 0.99 0.99', 'epoch': 0.63}
 63%|██████▎   | 500/790 [13:16<08:18,  1.72s/it] 63%|██████▎   | 501/790 [13:17<08:26,  1.75s/it] 64%|██████▎   | 502/790 [13:19<08:12,  1.71s/it] 64%|██████▎   | 503/790 [13:20<08:05,  1.69s/it] 64%|██████▍   | 504/790 [13:22<08:19,  1.75s/it] 64%|██████▍   | 505/790 [13:24<08:44,  1.84s/it] 64%|██████▍   | 506/790 [13:26<08:26,  1.78s/it] 64%|██████▍   | 507/790 [13:28<08:28,  1.80s/it] 64%|██████▍   | 508/790 [13:29<08:29,  1.81s/it] 64%|██████▍   | 509/790 [13:31<08:41,  1.86s/it] 65%|██████▍   | 510/790 [13:33<08:34,  1.84s/it]                                                 {'loss': 2.9816, 'learning_rate': 1.3961362544602213e-05, 'epoch': 0.65}
 65%|██████▍   | 510/790 [13:33<08:34,  1.84s/it]                                                 {'router_ce_loss': 1.0024598836898804, 'old_lang_expert0_score': '0.23 0.02 0.05 0.18 0.88 0.91 0.28 0.93 0.83 0.91 0.12 0.96 0.27 0.96 0.98 0.94 0.96 0.96 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.65}
 65%|██████▍   | 510/790 [13:34<08:34,  1.84s/it] 65%|██████▍   | 511/790 [13:35<08:33,  1.84s/it] 65%|██████▍   | 512/790 [13:37<08:27,  1.83s/it] 65%|██████▍   | 513/790 [13:39<08:21,  1.81s/it] 65%|██████▌   | 514/790 [13:40<08:24,  1.83s/it] 65%|██████▌   | 515/790 [13:42<08:07,  1.77s/it] 65%|██████▌   | 516/790 [13:44<08:13,  1.80s/it] 65%|██████▌   | 517/790 [13:46<07:54,  1.74s/it] 66%|██████▌   | 518/790 [13:47<07:43,  1.70s/it] 66%|██████▌   | 519/790 [13:49<07:53,  1.75s/it] 66%|██████▌   | 520/790 [13:51<08:12,  1.82s/it]                                                 {'loss': 2.9947, 'learning_rate': 1.3078312464287353e-05, 'epoch': 0.66}
 66%|██████▌   | 520/790 [13:51<08:12,  1.82s/it]                                                 {'router_ce_loss': 0.9937295913696289, 'old_lang_expert0_score': '0.25 0.02 0.05 0.19 0.88 0.93 0.31 0.95 0.88 0.94 0.13 0.97 0.22 0.97 0.99 0.95 0.97 0.97 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.66}
 66%|██████▌   | 520/790 [13:52<08:12,  1.82s/it] 66%|██████▌   | 521/790 [13:53<08:17,  1.85s/it] 66%|██████▌   | 522/790 [13:55<08:11,  1.83s/it] 66%|██████▌   | 523/790 [13:57<08:05,  1.82s/it] 66%|██████▋   | 524/790 [13:58<08:05,  1.83s/it] 66%|██████▋   | 525/790 [14:00<08:11,  1.85s/it] 67%|██████▋   | 526/790 [14:02<08:03,  1.83s/it] 67%|██████▋   | 527/790 [14:04<07:59,  1.82s/it] 67%|██████▋   | 528/790 [14:06<08:09,  1.87s/it] 67%|██████▋   | 529/790 [14:08<08:02,  1.85s/it] 67%|██████▋   | 530/790 [14:09<07:52,  1.82s/it]                                                 {'loss': 3.0293, 'learning_rate': 1.2214113022414448e-05, 'epoch': 0.67}
 67%|██████▋   | 530/790 [14:09<07:52,  1.82s/it]                                                 {'router_ce_loss': 1.001632571220398, 'old_lang_expert0_score': '0.24 0.02 0.05 0.19 0.87 0.92 0.27 0.94 0.87 0.93 0.1 0.96 0.24 0.95 0.98 0.94 0.96 0.96 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.67}
 67%|██████▋   | 530/790 [14:10<07:52,  1.82s/it] 67%|██████▋   | 531/790 [14:11<07:55,  1.84s/it] 67%|██████▋   | 532/790 [14:13<07:42,  1.79s/it] 67%|██████▋   | 533/790 [14:15<07:36,  1.78s/it] 68%|██████▊   | 534/790 [14:17<07:40,  1.80s/it] 68%|██████▊   | 535/790 [14:19<07:51,  1.85s/it] 68%|██████▊   | 536/790 [14:20<07:49,  1.85s/it] 68%|██████▊   | 537/790 [14:22<07:41,  1.83s/it] 68%|██████▊   | 538/790 [14:24<07:32,  1.79s/it] 68%|██████▊   | 539/790 [14:26<07:29,  1.79s/it] 68%|██████▊   | 540/790 [14:27<07:13,  1.73s/it]                                                 {'loss': 2.9954, 'learning_rate': 1.1370130695933318e-05, 'epoch': 0.68}
 68%|██████▊   | 540/790 [14:27<07:13,  1.73s/it]                                                 {'router_ce_loss': 0.9956873059272766, 'old_lang_expert0_score': '0.23 0.02 0.07 0.21 0.88 0.91 0.33 0.94 0.84 0.91 0.15 0.96 0.26 0.96 0.98 0.94 0.96 0.96 0.97 0.97 0.97 0.98 0.99 0.99', 'epoch': 0.68}
 68%|██████▊   | 540/790 [14:28<07:13,  1.73s/it] 68%|██████▊   | 541/790 [14:29<07:01,  1.69s/it] 69%|██████▊   | 542/790 [14:31<07:08,  1.73s/it] 69%|██████▊   | 543/790 [14:32<07:00,  1.70s/it] 69%|██████▉   | 544/790 [14:34<07:09,  1.75s/it] 69%|██████▉   | 545/790 [14:36<07:14,  1.77s/it] 69%|██████▉   | 546/790 [14:38<07:04,  1.74s/it] 69%|██████▉   | 547/790 [14:40<07:09,  1.77s/it] 69%|██████▉   | 548/790 [14:41<07:13,  1.79s/it] 69%|██████▉   | 549/790 [14:43<07:14,  1.80s/it] 70%|██████▉   | 550/790 [14:45<07:15,  1.82s/it]                                                 {'loss': 2.996, 'learning_rate': 1.0547699994378787e-05, 'epoch': 0.7}
 70%|██████▉   | 550/790 [14:45<07:15,  1.82s/it]                                                 {'router_ce_loss': 1.0029778480529785, 'old_lang_expert0_score': '0.24 0.02 0.06 0.21 0.87 0.9 0.29 0.93 0.84 0.92 0.11 0.96 0.2 0.96 0.98 0.94 0.96 0.96 0.98 0.97 0.97 0.97 0.98 0.99', 'epoch': 0.7}
 70%|██████▉   | 550/790 [14:46<07:15,  1.82s/it] 70%|██████▉   | 551/790 [14:47<07:21,  1.85s/it] 70%|██████▉   | 552/790 [14:49<07:17,  1.84s/it] 70%|███████   | 553/790 [14:50<07:07,  1.80s/it] 70%|███████   | 554/790 [14:52<06:50,  1.74s/it] 70%|███████   | 555/790 [14:54<06:56,  1.77s/it] 70%|███████   | 556/790 [14:55<06:40,  1.71s/it] 71%|███████   | 557/790 [14:58<07:24,  1.91s/it] 71%|███████   | 558/790 [14:59<07:01,  1.82s/it] 71%|███████   | 559/790 [15:01<07:08,  1.85s/it] 71%|███████   | 560/790 [15:03<07:05,  1.85s/it]                                                 {'loss': 2.9853, 'learning_rate': 9.748121349736892e-06, 'epoch': 0.71}
 71%|███████   | 560/790 [15:03<07:05,  1.85s/it]                                                 {'router_ce_loss': 0.9918358325958252, 'old_lang_expert0_score': '0.24 0.02 0.06 0.2 0.89 0.91 0.34 0.95 0.85 0.93 0.16 0.96 0.31 0.96 0.98 0.94 0.96 0.96 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.71}
 71%|███████   | 560/790 [15:04<07:05,  1.85s/it] 71%|███████   | 561/790 [15:05<07:01,  1.84s/it] 71%|███████   | 562/790 [15:07<07:00,  1.84s/it] 71%|███████▏  | 563/790 [15:09<07:02,  1.86s/it] 71%|███████▏  | 564/790 [15:11<06:59,  1.86s/it] 72%|███████▏  | 565/790 [15:13<06:58,  1.86s/it] 72%|███████▏  | 566/790 [15:14<06:41,  1.79s/it] 72%|███████▏  | 567/790 [15:16<06:38,  1.79s/it] 72%|███████▏  | 568/790 [15:18<06:57,  1.88s/it] 72%|███████▏  | 569/790 [15:20<06:52,  1.87s/it] 72%|███████▏  | 570/790 [15:22<06:43,  1.84s/it]                                                 {'loss': 2.9968, 'learning_rate': 8.972659060194506e-06, 'epoch': 0.72}
 72%|███████▏  | 570/790 [15:22<06:43,  1.84s/it]                                                 {'router_ce_loss': 0.998020350933075, 'old_lang_expert0_score': '0.23 0.02 0.05 0.21 0.88 0.9 0.31 0.94 0.85 0.93 0.13 0.96 0.24 0.96 0.98 0.94 0.96 0.96 0.98 0.97 0.98 0.98 0.98 0.99', 'epoch': 0.72}
 72%|███████▏  | 570/790 [15:22<06:43,  1.84s/it] 72%|███████▏  | 571/790 [15:23<06:36,  1.81s/it] 72%|███████▏  | 572/790 [15:25<06:35,  1.82s/it] 73%|███████▎  | 573/790 [15:27<06:24,  1.77s/it] 73%|███████▎  | 574/790 [15:29<06:17,  1.75s/it] 73%|███████▎  | 575/790 [15:30<06:22,  1.78s/it] 73%|███████▎  | 576/790 [15:32<06:25,  1.80s/it] 73%|███████▎  | 577/790 [15:34<06:13,  1.75s/it] 73%|███████▎  | 578/790 [15:36<06:24,  1.81s/it] 73%|███████▎  | 579/790 [15:38<06:26,  1.83s/it] 73%|███████▎  | 580/790 [15:40<06:28,  1.85s/it]                                                 {'loss': 2.9537, 'learning_rate': 8.222539291024078e-06, 'epoch': 0.73}
 73%|███████▎  | 580/790 [15:40<06:28,  1.85s/it]                                                 {'router_ce_loss': 0.9896116256713867, 'old_lang_expert0_score': '0.24 0.02 0.05 0.21 0.9 0.94 0.34 0.96 0.9 0.93 0.11 0.97 0.23 0.97 0.99 0.96 0.97 0.97 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.73}
 73%|███████▎  | 580/790 [15:40<06:28,  1.85s/it] 74%|███████▎  | 581/790 [15:41<06:19,  1.81s/it] 74%|███████▎  | 582/790 [15:43<06:14,  1.80s/it] 74%|███████▍  | 583/790 [15:45<06:15,  1.82s/it] 74%|███████▍  | 584/790 [15:47<06:11,  1.81s/it] 74%|███████▍  | 585/790 [15:49<06:08,  1.80s/it] 74%|███████▍  | 586/790 [15:50<06:06,  1.80s/it] 74%|███████▍  | 587/790 [15:52<06:12,  1.83s/it] 74%|███████▍  | 588/790 [15:54<06:05,  1.81s/it] 75%|███████▍  | 589/790 [15:56<06:00,  1.80s/it] 75%|███████▍  | 590/790 [15:58<06:03,  1.82s/it]                                                 {'loss': 2.9818, 'learning_rate': 7.4989481357643694e-06, 'epoch': 0.75}
 75%|███████▍  | 590/790 [15:58<06:03,  1.82s/it]                                                 {'router_ce_loss': 0.9946491122245789, 'old_lang_expert0_score': '0.23 0.02 0.07 0.21 0.89 0.91 0.35 0.95 0.86 0.92 0.14 0.96 0.27 0.95 0.98 0.93 0.95 0.95 0.97 0.96 0.97 0.97 0.99 0.99', 'epoch': 0.75}
 75%|███████▍  | 590/790 [15:58<06:03,  1.82s/it] 75%|███████▍  | 591/790 [15:59<05:59,  1.81s/it] 75%|███████▍  | 592/790 [16:01<05:47,  1.75s/it] 75%|███████▌  | 593/790 [16:03<05:49,  1.77s/it] 75%|███████▌  | 594/790 [16:05<05:49,  1.78s/it] 75%|███████▌  | 595/790 [16:07<05:49,  1.79s/it] 75%|███████▌  | 596/790 [16:08<05:52,  1.82s/it] 76%|███████▌  | 597/790 [16:10<05:39,  1.76s/it] 76%|███████▌  | 598/790 [16:12<05:39,  1.77s/it] 76%|███████▌  | 599/790 [16:14<05:44,  1.80s/it] 76%|███████▌  | 600/790 [16:16<05:47,  1.83s/it]                                                 {'loss': 2.9694, 'learning_rate': 6.803029740762648e-06, 'epoch': 0.76}
 76%|███████▌  | 600/790 [16:16<05:47,  1.83s/it]                                                 {'router_ce_loss': 0.9910942912101746, 'old_lang_expert0_score': '0.23 0.02 0.07 0.22 0.89 0.92 0.35 0.95 0.85 0.93 0.14 0.97 0.25 0.97 0.98 0.95 0.97 0.97 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.76}
 76%|███████▌  | 600/790 [16:16<05:47,  1.83s/it] 76%|███████▌  | 601/790 [16:18<06:12,  1.97s/it] 76%|███████▌  | 602/790 [16:20<05:54,  1.89s/it] 76%|███████▋  | 603/790 [16:21<05:38,  1.81s/it] 76%|███████▋  | 604/790 [16:23<05:34,  1.80s/it] 77%|███████▋  | 605/790 [16:25<05:25,  1.76s/it] 77%|███████▋  | 606/790 [16:26<05:29,  1.79s/it] 77%|███████▋  | 607/790 [16:28<05:23,  1.77s/it] 77%|███████▋  | 608/790 [16:30<05:26,  1.79s/it] 77%|███████▋  | 609/790 [16:32<05:37,  1.87s/it] 77%|███████▋  | 610/790 [16:34<05:42,  1.90s/it]                                                 {'loss': 2.9697, 'learning_rate': 6.135884496044244e-06, 'epoch': 0.77}
 77%|███████▋  | 610/790 [16:34<05:42,  1.90s/it]                                                 {'router_ce_loss': 0.9954684376716614, 'old_lang_expert0_score': '0.23 0.02 0.06 0.2 0.84 0.88 0.32 0.94 0.86 0.93 0.16 0.97 0.3 0.97 0.99 0.93 0.96 0.96 0.97 0.97 0.98 0.97 0.99 0.99', 'epoch': 0.77}
 77%|███████▋  | 610/790 [16:35<05:42,  1.90s/it] 77%|███████▋  | 611/790 [16:36<05:35,  1.88s/it] 77%|███████▋  | 612/790 [16:38<05:27,  1.84s/it] 78%|███████▊  | 613/790 [16:40<05:33,  1.89s/it] 78%|███████▊  | 614/790 [16:41<05:28,  1.87s/it] 78%|███████▊  | 615/790 [16:43<05:21,  1.83s/it] 78%|███████▊  | 616/790 [16:45<05:31,  1.90s/it] 78%|███████▊  | 617/790 [16:47<05:11,  1.80s/it] 78%|███████▊  | 618/790 [16:49<05:09,  1.80s/it] 78%|███████▊  | 619/790 [16:50<05:03,  1.77s/it] 78%|███████▊  | 620/790 [16:52<05:02,  1.78s/it]                                                 {'loss': 3.0127, 'learning_rate': 5.4985672953697e-06, 'epoch': 0.78}
 78%|███████▊  | 620/790 [16:52<05:02,  1.78s/it]                                                 {'router_ce_loss': 0.994551420211792, 'old_lang_expert0_score': '0.24 0.02 0.06 0.2 0.87 0.92 0.33 0.94 0.84 0.93 0.14 0.96 0.25 0.97 0.98 0.95 0.96 0.96 0.98 0.97 0.97 0.98 0.99 0.99', 'epoch': 0.78}
 78%|███████▊  | 620/790 [16:53<05:02,  1.78s/it] 79%|███████▊  | 621/790 [16:54<05:05,  1.81s/it] 79%|███████▊  | 622/790 [16:56<05:06,  1.82s/it] 79%|███████▉  | 623/790 [16:58<04:59,  1.80s/it] 79%|███████▉  | 624/790 [16:59<04:43,  1.71s/it] 79%|███████▉  | 625/790 [17:01<04:52,  1.77s/it] 79%|███████▉  | 626/790 [17:03<04:58,  1.82s/it] 79%|███████▉  | 627/790 [17:05<04:55,  1.81s/it] 79%|███████▉  | 628/790 [17:07<04:53,  1.81s/it] 80%|███████▉  | 629/790 [17:08<04:36,  1.72s/it] 80%|███████▉  | 630/790 [17:10<04:49,  1.81s/it]                                                 {'loss': 2.9705, 'learning_rate': 4.892085868230881e-06, 'epoch': 0.8}
 80%|███████▉  | 630/790 [17:10<04:49,  1.81s/it]                                                 {'router_ce_loss': 0.9934137463569641, 'old_lang_expert0_score': '0.24 0.02 0.05 0.22 0.9 0.92 0.35 0.94 0.86 0.93 0.12 0.97 0.25 0.97 0.98 0.95 0.96 0.96 0.98 0.97 0.98 0.97 0.99 0.99', 'epoch': 0.8}
 80%|███████▉  | 630/790 [17:11<04:49,  1.81s/it] 80%|███████▉  | 631/790 [17:12<04:45,  1.79s/it] 80%|████████  | 632/790 [17:14<04:44,  1.80s/it] 80%|████████  | 633/790 [17:16<04:50,  1.85s/it] 80%|████████  | 634/790 [17:18<04:50,  1.86s/it] 80%|████████  | 635/790 [17:19<04:43,  1.83s/it] 81%|████████  | 636/790 [17:21<04:36,  1.79s/it] 81%|████████  | 637/790 [17:23<04:39,  1.82s/it] 81%|████████  | 638/790 [17:25<04:33,  1.80s/it] 81%|████████  | 639/790 [17:26<04:16,  1.70s/it] 81%|████████  | 640/790 [17:28<04:05,  1.63s/it]                                                 {'loss': 2.9627, 'learning_rate': 4.317399186423574e-06, 'epoch': 0.81}
 81%|████████  | 640/790 [17:28<04:05,  1.63s/it]                                                 {'router_ce_loss': 1.010902762413025, 'old_lang_expert0_score': '0.24 0.02 0.05 0.18 0.87 0.89 0.32 0.92 0.82 0.89 0.14 0.95 0.29 0.94 0.96 0.9 0.93 0.93 0.95 0.94 0.95 0.96 0.97 0.97', 'epoch': 0.81}
 81%|████████  | 640/790 [17:28<04:05,  1.63s/it] 81%|████████  | 641/790 [17:30<04:19,  1.74s/it] 81%|████████▏ | 642/790 [17:31<04:20,  1.76s/it] 81%|████████▏ | 643/790 [17:33<04:28,  1.82s/it] 82%|████████▏ | 644/790 [17:35<04:29,  1.85s/it] 82%|████████▏ | 645/790 [17:40<06:25,  2.66s/it] 82%|████████▏ | 646/790 [17:44<07:48,  3.26s/it] 82%|████████▏ | 647/790 [17:47<07:05,  2.97s/it] 82%|████████▏ | 648/790 [17:48<05:49,  2.46s/it] 82%|████████▏ | 649/790 [17:50<05:14,  2.23s/it] 82%|████████▏ | 650/790 [17:51<04:37,  1.98s/it]                                                 {'loss': 2.9714, 'learning_rate': 3.775415947715899e-06, 'epoch': 0.82}
 82%|████████▏ | 650/790 [17:51<04:37,  1.98s/it]                                                 {'router_ce_loss': 0.9940733313560486, 'old_lang_expert0_score': '0.24 0.02 0.05 0.18 0.89 0.92 0.33 0.95 0.86 0.92 0.15 0.97 0.29 0.96 0.98 0.94 0.96 0.96 0.97 0.97 0.97 0.97 0.99 0.99', 'epoch': 0.82}
 82%|████████▏ | 650/790 [17:52<04:37,  1.98s/it] 82%|████████▏ | 651/790 [17:53<04:15,  1.84s/it] 83%|████████▎ | 652/790 [17:54<04:06,  1.79s/it] 83%|████████▎ | 653/790 [17:56<04:03,  1.78s/it] 83%|████████▎ | 654/790 [17:58<04:13,  1.86s/it] 83%|████████▎ | 655/790 [18:00<04:05,  1.82s/it] 83%|████████▎ | 656/790 [18:01<03:57,  1.77s/it] 83%|████████▎ | 657/790 [18:03<03:43,  1.68s/it] 83%|████████▎ | 658/790 [18:04<03:36,  1.64s/it] 83%|████████▎ | 659/790 [18:06<03:39,  1.68s/it] 84%|████████▎ | 660/790 [18:08<03:44,  1.72s/it]                                                 {'loss': 2.9884, 'learning_rate': 3.266993139010438e-06, 'epoch': 0.84}
 84%|████████▎ | 660/790 [18:08<03:44,  1.72s/it]                                                 {'router_ce_loss': 0.9996004700660706, 'old_lang_expert0_score': '0.24 0.02 0.06 0.19 0.87 0.9 0.32 0.92 0.82 0.92 0.15 0.96 0.3 0.95 0.97 0.94 0.95 0.95 0.97 0.96 0.97 0.97 0.98 0.99', 'epoch': 0.84}
 84%|████████▎ | 660/790 [18:09<03:44,  1.72s/it] 84%|████████▎ | 661/790 [18:10<03:47,  1.77s/it] 84%|████████▍ | 662/790 [18:12<03:44,  1.76s/it] 84%|████████▍ | 663/790 [18:14<03:52,  1.83s/it] 84%|████████▍ | 664/790 [18:15<03:48,  1.82s/it] 84%|████████▍ | 665/790 [18:17<03:42,  1.78s/it] 84%|████████▍ | 666/790 [18:19<03:36,  1.74s/it] 84%|████████▍ | 667/790 [18:21<03:34,  1.74s/it] 85%|████████▍ | 668/790 [18:22<03:34,  1.76s/it] 85%|████████▍ | 669/790 [18:24<03:31,  1.75s/it] 85%|████████▍ | 670/790 [18:26<03:26,  1.72s/it]                                                 {'loss': 2.9922, 'learning_rate': 2.792934681271708e-06, 'epoch': 0.85}
 85%|████████▍ | 670/790 [18:26<03:26,  1.72s/it]                                                 {'router_ce_loss': 0.995635986328125, 'old_lang_expert0_score': '0.23 0.02 0.05 0.21 0.88 0.9 0.33 0.94 0.86 0.93 0.13 0.96 0.26 0.96 0.98 0.95 0.96 0.96 0.98 0.97 0.98 0.97 0.98 0.99', 'epoch': 0.85}
 85%|████████▍ | 670/790 [18:26<03:26,  1.72s/it] 85%|████████▍ | 671/790 [18:28<03:26,  1.73s/it] 85%|████████▌ | 672/790 [18:29<03:29,  1.78s/it] 85%|████████▌ | 673/790 [18:31<03:27,  1.77s/it] 85%|████████▌ | 674/790 [18:33<03:26,  1.78s/it] 85%|████████▌ | 675/790 [18:35<03:24,  1.78s/it] 86%|████████▌ | 676/790 [18:36<03:20,  1.76s/it] 86%|████████▌ | 677/790 [18:38<03:17,  1.74s/it] 86%|████████▌ | 678/790 [18:40<03:13,  1.73s/it] 86%|████████▌ | 679/790 [18:42<03:13,  1.74s/it] 86%|████████▌ | 680/790 [18:46<04:43,  2.58s/it]                                                 {'loss': 2.9648, 'learning_rate': 2.3539901583619185e-06, 'epoch': 0.86}
 86%|████████▌ | 680/790 [18:46<04:43,  2.58s/it]                                                 {'router_ce_loss': 0.9944825172424316, 'old_lang_expert0_score': '0.24 0.02 0.06 0.22 0.89 0.92 0.33 0.94 0.85 0.92 0.12 0.96 0.25 0.96 0.98 0.95 0.96 0.96 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.86}
 86%|████████▌ | 680/790 [18:47<04:43,  2.58s/it] 86%|████████▌ | 681/790 [18:51<05:47,  3.19s/it] 86%|████████▋ | 682/790 [18:55<06:16,  3.49s/it] 86%|████████▋ | 683/790 [18:57<05:23,  3.02s/it] 87%|████████▋ | 684/790 [18:58<04:24,  2.50s/it] 87%|████████▋ | 685/790 [19:00<03:50,  2.20s/it] 87%|████████▋ | 686/790 [19:01<03:24,  1.97s/it] 87%|████████▋ | 687/790 [19:03<03:06,  1.81s/it] 87%|████████▋ | 688/790 [19:04<02:53,  1.70s/it] 87%|████████▋ | 689/790 [19:06<02:50,  1.69s/it] 87%|████████▋ | 690/790 [19:07<02:41,  1.61s/it]                                                 {'loss': 2.9705, 'learning_rate': 1.9508536317948357e-06, 'epoch': 0.87}
 87%|████████▋ | 690/790 [19:07<02:41,  1.61s/it]                                                 {'router_ce_loss': 0.9920141100883484, 'old_lang_expert0_score': '0.24 0.03 0.07 0.24 0.88 0.91 0.35 0.94 0.87 0.93 0.14 0.97 0.24 0.97 0.98 0.96 0.97 0.97 0.98 0.98 0.97 0.98 0.98 0.99', 'epoch': 0.87}
 87%|████████▋ | 690/790 [19:08<02:41,  1.61s/it] 87%|████████▋ | 691/790 [19:09<02:40,  1.62s/it] 88%|████████▊ | 692/790 [19:10<02:34,  1.57s/it] 88%|████████▊ | 693/790 [19:12<02:27,  1.52s/it] 88%|████████▊ | 694/790 [19:14<02:38,  1.66s/it] 88%|████████▊ | 695/790 [19:15<02:43,  1.72s/it] 88%|████████▊ | 696/790 [19:17<02:42,  1.73s/it] 88%|████████▊ | 697/790 [19:19<02:41,  1.74s/it] 88%|████████▊ | 698/790 [19:21<02:36,  1.70s/it] 88%|████████▊ | 699/790 [19:22<02:30,  1.65s/it] 89%|████████▊ | 700/790 [19:24<02:29,  1.66s/it]                                                 {'loss': 2.9863, 'learning_rate': 1.5841625432818057e-06, 'epoch': 0.89}
 89%|████████▊ | 700/790 [19:24<02:29,  1.66s/it]                                                 {'router_ce_loss': 0.9899231791496277, 'old_lang_expert0_score': '0.23 0.02 0.06 0.21 0.89 0.93 0.35 0.95 0.87 0.93 0.14 0.97 0.28 0.97 0.99 0.94 0.96 0.96 0.98 0.97 0.97 0.98 0.99 0.99', 'epoch': 0.89}
 89%|████████▊ | 700/790 [19:24<02:29,  1.66s/it] 89%|████████▊ | 701/790 [19:25<02:27,  1.65s/it] 89%|████████▉ | 702/790 [19:27<02:30,  1.71s/it] 89%|████████▉ | 703/790 [19:29<02:23,  1.65s/it] 89%|████████▉ | 704/790 [19:31<02:25,  1.70s/it] 89%|████████▉ | 705/790 [19:33<02:32,  1.79s/it] 89%|████████▉ | 706/790 [19:34<02:30,  1.79s/it] 89%|████████▉ | 707/790 [19:36<02:28,  1.79s/it] 90%|████████▉ | 708/790 [19:38<02:22,  1.74s/it] 90%|████████▉ | 709/790 [19:39<02:20,  1.73s/it] 90%|████████▉ | 710/790 [19:41<02:23,  1.79s/it]                                                 {'loss': 3.0281, 'learning_rate': 1.2544967068054332e-06, 'epoch': 0.9}
 90%|████████▉ | 710/790 [19:41<02:23,  1.79s/it]                                                 {'router_ce_loss': 0.9938369393348694, 'old_lang_expert0_score': '0.23 0.02 0.06 0.21 0.89 0.91 0.32 0.94 0.87 0.94 0.13 0.97 0.28 0.97 0.99 0.94 0.96 0.96 0.97 0.97 0.97 0.98 0.99 0.99', 'epoch': 0.9}
 90%|████████▉ | 710/790 [19:42<02:23,  1.79s/it] 90%|█████████ | 711/790 [19:43<02:21,  1.80s/it] 90%|█████████ | 712/790 [19:45<02:15,  1.74s/it] 90%|█████████ | 713/790 [19:47<02:19,  1.81s/it] 90%|█████████ | 714/790 [19:49<02:16,  1.79s/it] 91%|█████████ | 715/790 [19:50<02:11,  1.76s/it] 91%|█████████ | 716/790 [19:52<02:09,  1.75s/it] 91%|█████████ | 717/790 [19:54<02:08,  1.76s/it] 91%|█████████ | 718/790 [19:55<02:05,  1.75s/it] 91%|█████████ | 719/790 [19:57<02:06,  1.78s/it] 91%|█████████ | 720/790 [19:59<02:05,  1.80s/it]                                                 {'loss': 2.9788, 'learning_rate': 9.623773918144897e-07, 'epoch': 0.91}
 91%|█████████ | 720/790 [19:59<02:05,  1.80s/it]                                                 {'router_ce_loss': 0.9946430325508118, 'old_lang_expert0_score': '0.23 0.02 0.05 0.21 0.89 0.92 0.32 0.95 0.87 0.92 0.12 0.96 0.26 0.96 0.98 0.94 0.96 0.96 0.98 0.97 0.98 0.98 0.99 0.99', 'epoch': 0.91}
 91%|█████████ | 720/790 [20:00<02:05,  1.80s/it] 91%|█████████▏| 721/790 [20:01<02:07,  1.85s/it] 91%|█████████▏| 722/790 [20:03<02:06,  1.86s/it] 92%|█████████▏| 723/790 [20:05<02:01,  1.82s/it] 92%|█████████▏| 724/790 [20:06<01:58,  1.80s/it] 92%|█████████▏| 725/790 [20:08<01:57,  1.80s/it] 92%|█████████▏| 726/790 [20:10<01:54,  1.78s/it] 92%|█████████▏| 727/790 [20:12<01:54,  1.81s/it] 92%|█████████▏| 728/790 [20:14<01:51,  1.80s/it] 92%|█████████▏| 729/790 [20:15<01:45,  1.72s/it] 92%|█████████▏| 730/790 [20:17<01:42,  1.70s/it]                                                 {'loss': 2.9722, 'learning_rate': 7.082664989897487e-07, 'epoch': 0.92}
 92%|█████████▏| 730/790 [20:17<01:42,  1.70s/it]                                                 {'router_ce_loss': 0.995455265045166, 'old_lang_expert0_score': '0.24 0.02 0.06 0.2 0.88 0.9 0.31 0.95 0.87 0.93 0.13 0.97 0.27 0.96 0.99 0.95 0.96 0.96 0.98 0.97 0.98 0.98 0.99 0.99', 'epoch': 0.92}
 92%|█████████▏| 730/790 [20:17<01:42,  1.70s/it] 93%|█████████▎| 731/790 [20:18<01:38,  1.67s/it] 93%|█████████▎| 732/790 [20:20<01:38,  1.69s/it] 93%|█████████▎| 733/790 [20:22<01:40,  1.76s/it] 93%|█████████▎| 734/790 [20:24<01:38,  1.76s/it] 93%|█████████▎| 735/790 [20:26<01:37,  1.77s/it] 93%|█████████▎| 736/790 [20:28<01:37,  1.81s/it] 93%|█████████▎| 737/790 [20:29<01:35,  1.81s/it] 93%|█████████▎| 738/790 [20:31<01:37,  1.87s/it] 94%|█████████▎| 739/790 [20:33<01:36,  1.89s/it] 94%|█████████▎| 740/790 [20:35<01:33,  1.87s/it]                                                 {'loss': 2.9797, 'learning_rate': 4.925658298840979e-07, 'epoch': 0.94}
 94%|█████████▎| 740/790 [20:35<01:33,  1.87s/it]                                                 {'router_ce_loss': 0.9966500401496887, 'old_lang_expert0_score': '0.25 0.02 0.05 0.18 0.89 0.92 0.32 0.95 0.85 0.92 0.12 0.97 0.24 0.96 0.98 0.94 0.96 0.96 0.98 0.97 0.98 0.98 0.99 0.99', 'epoch': 0.94}
 94%|█████████▎| 740/790 [20:36<01:33,  1.87s/it] 94%|█████████▍| 741/790 [20:37<01:31,  1.86s/it] 94%|█████████▍| 742/790 [20:39<01:28,  1.83s/it] 94%|█████████▍| 743/790 [20:41<01:26,  1.83s/it] 94%|█████████▍| 744/790 [20:43<01:25,  1.85s/it] 94%|█████████▍| 745/790 [20:44<01:24,  1.89s/it] 94%|█████████▍| 746/790 [20:46<01:21,  1.86s/it] 95%|█████████▍| 747/790 [20:48<01:19,  1.84s/it] 95%|█████████▍| 748/790 [20:50<01:16,  1.83s/it] 95%|█████████▍| 749/790 [20:52<01:14,  1.82s/it] 95%|█████████▍| 750/790 [20:53<01:12,  1.82s/it]                                                 {'loss': 3.0296, 'learning_rate': 3.1561645159166597e-07, 'epoch': 0.95}
 95%|█████████▍| 750/790 [20:53<01:12,  1.82s/it]                                                 {'router_ce_loss': 0.9920802712440491, 'old_lang_expert0_score': '0.23 0.02 0.06 0.2 0.88 0.92 0.35 0.94 0.86 0.93 0.15 0.97 0.26 0.96 0.98 0.94 0.96 0.97 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.95}
 95%|█████████▍| 750/790 [20:54<01:12,  1.82s/it] 95%|█████████▌| 751/790 [20:55<01:10,  1.81s/it] 95%|█████████▌| 752/790 [20:57<01:09,  1.83s/it] 95%|█████████▌| 753/790 [20:59<01:07,  1.82s/it] 95%|█████████▌| 754/790 [21:01<01:06,  1.86s/it] 96%|█████████▌| 755/790 [21:03<01:04,  1.84s/it] 96%|█████████▌| 756/790 [21:04<01:01,  1.82s/it] 96%|█████████▌| 757/790 [21:06<00:59,  1.82s/it] 96%|█████████▌| 758/790 [21:08<00:57,  1.81s/it] 96%|█████████▌| 759/790 [21:10<00:56,  1.81s/it] 96%|█████████▌| 760/790 [21:12<00:56,  1.88s/it]                                                 {'loss': 2.9693, 'learning_rate': 1.7769815745066475e-07, 'epoch': 0.96}
 96%|█████████▌| 760/790 [21:12<00:56,  1.88s/it]                                                 {'router_ce_loss': 0.9922131896018982, 'old_lang_expert0_score': '0.24 0.03 0.07 0.21 0.89 0.92 0.33 0.94 0.87 0.93 0.14 0.97 0.24 0.97 0.99 0.95 0.97 0.97 0.98 0.98 0.98 0.98 0.99 0.99', 'epoch': 0.96}
 96%|█████████▌| 760/790 [21:12<00:56,  1.88s/it] 96%|█████████▋| 761/790 [21:14<00:53,  1.86s/it] 96%|█████████▋| 762/790 [21:16<00:51,  1.83s/it] 97%|█████████▋| 763/790 [21:17<00:49,  1.82s/it] 97%|█████████▋| 764/790 [21:19<00:47,  1.81s/it] 97%|█████████▋| 765/790 [21:21<00:45,  1.81s/it] 97%|█████████▋| 766/790 [21:23<00:43,  1.81s/it] 97%|█████████▋| 767/790 [21:25<00:42,  1.83s/it] 97%|█████████▋| 768/790 [21:26<00:39,  1.81s/it] 97%|█████████▋| 769/790 [21:28<00:37,  1.80s/it] 97%|█████████▋| 770/790 [21:30<00:36,  1.81s/it]                                                 {'loss': 2.9966, 'learning_rate': 7.90290246326042e-08, 'epoch': 0.97}
 97%|█████████▋| 770/790 [21:30<00:36,  1.81s/it]                                                 {'router_ce_loss': 0.9942560791969299, 'old_lang_expert0_score': '0.25 0.02 0.06 0.2 0.88 0.91 0.33 0.95 0.85 0.93 0.15 0.96 0.25 0.96 0.98 0.94 0.96 0.96 0.98 0.97 0.98 0.98 0.99 0.99', 'epoch': 0.97}
 97%|█████████▋| 770/790 [21:31<00:36,  1.81s/it] 98%|█████████▊| 771/790 [21:32<00:35,  1.87s/it] 98%|█████████▊| 772/790 [21:34<00:34,  1.94s/it] 98%|█████████▊| 773/790 [21:36<00:32,  1.94s/it] 98%|█████████▊| 774/790 [21:38<00:30,  1.92s/it] 98%|█████████▊| 775/790 [21:40<00:28,  1.88s/it] 98%|█████████▊| 776/790 [21:41<00:25,  1.85s/it] 98%|█████████▊| 777/790 [21:43<00:23,  1.83s/it] 98%|█████████▊| 778/790 [21:45<00:22,  1.87s/it] 99%|█████████▊| 779/790 [21:47<00:20,  1.84s/it] 99%|█████████▊| 780/790 [21:49<00:18,  1.82s/it]                                                 {'loss': 2.9964, 'learning_rate': 1.976506931745392e-08, 'epoch': 0.99}
 99%|█████████▊| 780/790 [21:49<00:18,  1.82s/it]                                                 {'router_ce_loss': 0.9897974133491516, 'old_lang_expert0_score': '0.23 0.02 0.06 0.23 0.89 0.92 0.36 0.95 0.88 0.94 0.13 0.96 0.26 0.97 0.98 0.94 0.96 0.96 0.98 0.98 0.98 0.97 0.99 0.99', 'epoch': 0.99}
 99%|█████████▊| 780/790 [21:49<00:18,  1.82s/it] 99%|█████████▉| 781/790 [21:51<00:16,  1.86s/it] 99%|█████████▉| 782/790 [21:53<00:14,  1.87s/it] 99%|█████████▉| 783/790 [21:54<00:13,  1.87s/it] 99%|█████████▉| 784/790 [21:56<00:11,  1.85s/it] 99%|█████████▉| 785/790 [21:58<00:09,  1.84s/it] 99%|█████████▉| 786/790 [22:00<00:07,  1.82s/it]100%|█████████▉| 787/790 [22:02<00:05,  1.80s/it]100%|█████████▉| 788/790 [22:03<00:03,  1.78s/it]100%|█████████▉| 789/790 [22:05<00:01,  1.78s/it]100%|██████████| 790/790 [22:07<00:00,  1.78s/it]                                                 {'loss': 2.9692, 'learning_rate': 0.0, 'epoch': 1.0}
100%|██████████| 790/790 [22:07<00:00,  1.78s/it][INFO|trainer.py:1989] 2024-07-10 15:29:31,552 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 1327.405, 'train_samples_per_second': 76.168, 'train_steps_per_second': 0.595, 'train_loss': 3.0371684110617334, 'epoch': 1.0}
100%|██████████| 790/790 [22:07<00:00,  1.78s/it]100%|██████████| 790/790 [22:07<00:00,  1.68s/it]
[INFO|trainer.py:2981] 2024-07-10 15:29:37,674 >> Saving model checkpoint to /home/nfs04/wangzj/checkpoints/moe/test
/home/nfs02/wangzj/aliyun/temp_data/peft/src/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/cpfs/zhouh/models/Qwen/Qwen1.5-1.8B - will assume that the vocabulary was not modified.
  warnings.warn(
[2024-07-10 15:29:40,642] [INFO] [launch.py:347:main] Process 2740888 exits successfully.
[2024-07-10 15:29:40,642] [INFO] [launch.py:347:main] Process 2740884 exits successfully.
[2024-07-10 15:29:40,642] [INFO] [launch.py:347:main] Process 2740889 exits successfully.
[2024-07-10 15:29:40,643] [INFO] [launch.py:347:main] Process 2740890 exits successfully.
[2024-07-10 15:29:40,643] [INFO] [launch.py:347:main] Process 2740887 exits successfully.
[2024-07-10 15:29:41,644] [INFO] [launch.py:347:main] Process 2740885 exits successfully.
[2024-07-10 15:29:41,644] [INFO] [launch.py:347:main] Process 2740886 exits successfully.
[INFO|tokenization_utils_base.py:2435] 2024-07-10 15:30:27,697 >> tokenizer config file saved in /home/nfs04/wangzj/checkpoints/moe/test/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-07-10 15:30:27,698 >> Special tokens file saved in /home/nfs04/wangzj/checkpoints/moe/test/special_tokens_map.json
[INFO|tokenization_utils_base.py:2495] 2024-07-10 15:30:27,699 >> added tokens file saved in /home/nfs04/wangzj/checkpoints/moe/test/added_tokens.json
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     3.0372
  train_runtime            = 0:22:07.40
  train_samples_per_second =     76.168
  train_steps_per_second   =      0.595
Figure saved: /home/nfs04/wangzj/checkpoints/moe/test/training_loss.png
07/10/2024 15:30:29 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-07-10 15:30:29,171 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-07-10 15:30:31,701] [INFO] [launch.py:347:main] Process 2740883 exits successfully.
