[2024-05-18 19:27:45,178] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-18 19:27:47,115] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-18 19:27:47,161] [INFO] [runner.py:570:main] cmd = /mnt/vol1/huangxin/anaconda3/envs/wzjsz/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed ./llama-pt/config/ds_config.json --stage pt --model_name_or_path /mnt/vol1/huangxin/nanda/model/Qwen1.5-1.8B --do_train --flash_attn --disable_gradient_checkpoint --dataset ar_2b,de_2b,is_2b,hi_2b --mix_strategy concat --preprocessing_num_workers 64 --cutoff_len 1024 --finetuning_type moe --moe_every_k_layers 1 --moe_router_type mh_moe --moe_num_experts 4 --moe_num_heads 4 --topk 2 --moe_with_aux --output_dir /mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe --overwrite_output_dir --per_device_train_batch_size 8 --gradient_accumulation_steps 16 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 1 --save_only_model --save_steps 10000 --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-05-18 19:27:48,540] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-18 19:27:50,134] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-05-18 19:27:50,134] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-05-18 19:27:50,134] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-05-18 19:27:50,134] [INFO] [launch.py:163:main] dist_world_size=8
[2024-05-18 19:27:50,134] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-05-18 19:27:54,235] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-18 19:27:54,248] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-18 19:27:54,321] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-18 19:27:54,452] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-18 19:27:54,511] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-18 19:27:54,557] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-18 19:27:54,570] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-18 19:27:54,579] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-18 19:27:59,774] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-18 19:27:59,775] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-18 19:27:59,872] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-18 19:27:59,901] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-18 19:28:00,052] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-18 19:28:00,053] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-18 19:28:00,073] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-18 19:28:00,148] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-18 19:28:00,360] [INFO] [comm.py:637:init_distributed] cdb=None
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Process rank: 7, device: cuda:7, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=7,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe/runs/May18_19-27-59_i-fvgceljl,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Process rank: 6, device: cuda:6, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=6,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe/runs/May18_19-28-00_i-fvgceljl,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe/runs/May18_19-27-59_i-fvgceljl,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe/runs/May18_19-28-00_i-fvgceljl,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe/runs/May18_19-28-00_i-fvgceljl,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Process rank: 4, device: cuda:4, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe/runs/May18_19-27-59_i-fvgceljl,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=4,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe/runs/May18_19-28-00_i-fvgceljl,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-05-18 19:28:00,390 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2027] 2024-05-18 19:28:00,390 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2027] 2024-05-18 19:28:00,391 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-05-18 19:28:00,391 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-05-18 19:28:00,391 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-05-18 19:28:00,391 >> loading file tokenizer.json
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Process rank: 5, device: cuda:5, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
05/18/2024 19:28:00 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=5,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe/runs/May18_19-28-00_i-fvgceljl,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/mnt/vol1/huangxin/nanda/checkpoints/llama-moe/qwen/1.8b-ardeishi-mhmoe,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
05/18/2024 19:28:00 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
05/18/2024 19:28:00 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
05/18/2024 19:28:00 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
05/18/2024 19:28:00 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
05/18/2024 19:28:00 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
05/18/2024 19:28:00 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
05/18/2024 19:28:00 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[WARNING|logging.py:314] 2024-05-18 19:28:00,608 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:727] 2024-05-18 19:28:00,608 >> loading configuration file /mnt/vol1/huangxin/nanda/model/Qwen1.5-1.8B/config.json
[INFO|configuration_utils.py:792] 2024-05-18 19:28:00,609 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/vol1/huangxin/nanda/model/Qwen1.5-1.8B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

05/18/2024 19:28:00 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
[WARNING|modeling_utils.py:2946] 2024-05-18 19:28:00,610 >> The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[INFO|modeling_utils.py:3334] 2024-05-18 19:28:00,611 >> loading weights file /mnt/vol1/huangxin/nanda/model/Qwen1.5-1.8B/model.safetensors
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|modeling_utils.py:1459] 2024-05-18 19:28:00,634 >> Instantiating Qwen2ForCausalMoeLM model under default dtype torch.bfloat16.
[WARNING|logging.py:329] 2024-05-18 19:28:00,635 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[WARNING|logging.py:329] 2024-05-18 19:28:00,637 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:827] 2024-05-18 19:28:00,639 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}

05/18/2024 19:28:01 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/18/2024 19:28:03 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/18/2024 19:28:06 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/18/2024 19:28:06 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/18/2024 19:28:06 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
[INFO|modeling_utils.py:4070] 2024-05-18 19:28:06,931 >> All model checkpoint weights were used when initializing Qwen2ForCausalMoeLM.

[INFO|modeling_utils.py:4078] 2024-05-18 19:28:06,931 >> All the weights of Qwen2ForCausalMoeLM were initialized from the model checkpoint at /mnt/vol1/huangxin/nanda/model/Qwen1.5-1.8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalMoeLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-05-18 19:28:06,933 >> loading configuration file /mnt/vol1/huangxin/nanda/model/Qwen1.5-1.8B/generation_config.json
[INFO|configuration_utils.py:827] 2024-05-18 19:28:06,933 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "max_new_tokens": 2048
}

05/18/2024 19:28:06 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/18/2024 19:28:07 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/18/2024 19:28:07 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
05/18/2024 19:28:14 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/18/2024 19:28:15 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/18/2024 19:28:16 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/18/2024 19:28:16 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/18/2024 19:28:16 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/18/2024 19:28:16 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/18/2024 19:28:16 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/18/2024 19:28:16 - INFO - llmtuner.model.loader - trainable params: 1013071872 || all params: 2849900544 || trainable%: 35.5476
05/18/2024 19:28:28 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
Using custom data configuration default-633ec56fb30fb8ad
Loading Dataset Infos from /mnt/vol1/huangxin/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Dataset({
    features: ['text'],
    num_rows: 1700419
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
Process #0 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00000_of_00064.arrow
Process #1 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00001_of_00064.arrow
Process #2 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00002_of_00064.arrow
Process #3 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00003_of_00064.arrow
Process #4 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00004_of_00064.arrow
Process #5 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00005_of_00064.arrow
Process #6 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00006_of_00064.arrow
Process #7 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00007_of_00064.arrow
Process #8 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00008_of_00064.arrow
Process #9 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00009_of_00064.arrow
Process #10 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00010_of_00064.arrow
Process #11 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00011_of_00064.arrow
Process #12 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00012_of_00064.arrow
Process #13 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00013_of_00064.arrow
Process #14 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00014_of_00064.arrow
Process #15 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00015_of_00064.arrow
Process #16 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00016_of_00064.arrow
Process #17 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00017_of_00064.arrow
Process #18 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00018_of_00064.arrow
Process #19 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00019_of_00064.arrow
Process #20 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00020_of_00064.arrow
Process #21 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00021_of_00064.arrow
Process #22 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00022_of_00064.arrow
Process #23 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00023_of_00064.arrow
Process #24 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00024_of_00064.arrow
Process #25 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00025_of_00064.arrow
Process #26 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00026_of_00064.arrow
Process #27 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00027_of_00064.arrow
Process #28 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00028_of_00064.arrow
Process #29 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00029_of_00064.arrow
Process #30 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00030_of_00064.arrow
Process #31 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00031_of_00064.arrow
Process #32 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00032_of_00064.arrow
Process #33 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00033_of_00064.arrow
Process #34 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00034_of_00064.arrow
Process #35 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00035_of_00064.arrow
Process #36 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00036_of_00064.arrow
Process #37 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00037_of_00064.arrow
Process #38 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00038_of_00064.arrow
Process #39 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00039_of_00064.arrow
Process #40 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00040_of_00064.arrow
Process #41 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00041_of_00064.arrow
Process #42 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00042_of_00064.arrow
Process #43 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00043_of_00064.arrow
Process #44 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00044_of_00064.arrow
Process #45 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00045_of_00064.arrow
Process #46 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00046_of_00064.arrow
Process #47 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00047_of_00064.arrow
Process #48 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00048_of_00064.arrow
Process #49 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00049_of_00064.arrow
Process #50 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00050_of_00064.arrow
Process #51 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00051_of_00064.arrow
Process #52 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00052_of_00064.arrow
Process #53 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00053_of_00064.arrow
Process #54 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00054_of_00064.arrow
Process #55 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00055_of_00064.arrow
Process #56 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00056_of_00064.arrow
Process #57 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00057_of_00064.arrow
Process #58 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00058_of_00064.arrow
Process #59 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00059_of_00064.arrow
Process #60 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00060_of_00064.arrow
Process #61 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00061_of_00064.arrow
Process #62 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00062_of_00064.arrow
Process #63 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_00063_of_00064.arrow
Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-467c2a451ac18b74_*_of_00064.arrow
Concatenating 64 shards
05/18/2024 19:28:39 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Using custom data configuration default-e3f92b4448719631
Loading Dataset Infos from /mnt/vol1/huangxin/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Dataset({
    features: ['text'],
    num_rows: 1948786
})
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
Process #0 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00000_of_00064.arrow
Process #1 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00001_of_00064.arrow
Process #2 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00002_of_00064.arrow
Process #3 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00003_of_00064.arrow
Process #4 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00004_of_00064.arrow
Process #5 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00005_of_00064.arrow
Process #6 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00006_of_00064.arrow
Process #7 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00007_of_00064.arrow
Process #8 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00008_of_00064.arrow
Process #9 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00009_of_00064.arrow
Process #10 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00010_of_00064.arrow
Process #11 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00011_of_00064.arrow
Process #12 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00012_of_00064.arrow
Process #13 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00013_of_00064.arrow
Process #14 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00014_of_00064.arrow
Process #15 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00015_of_00064.arrow
Process #16 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00016_of_00064.arrow
Process #17 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00017_of_00064.arrow
Process #18 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00018_of_00064.arrow
Process #19 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00019_of_00064.arrow
Process #20 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00020_of_00064.arrow
Process #21 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00021_of_00064.arrow
Process #22 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00022_of_00064.arrow
Process #23 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00023_of_00064.arrow
Process #24 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00024_of_00064.arrow
Process #25 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00025_of_00064.arrow
Process #26 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00026_of_00064.arrow
Process #27 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00027_of_00064.arrow
Process #28 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00028_of_00064.arrow
Process #29 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00029_of_00064.arrow
Process #30 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00030_of_00064.arrow
Process #31 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00031_of_00064.arrow
Process #32 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00032_of_00064.arrow
Process #33 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00033_of_00064.arrow
Process #34 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00034_of_00064.arrow
Process #35 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00035_of_00064.arrow
Process #36 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00036_of_00064.arrow
Process #37 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00037_of_00064.arrow
Process #38 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00038_of_00064.arrow
Process #39 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00039_of_00064.arrow
Process #40 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00040_of_00064.arrow
Process #41 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00041_of_00064.arrow
Process #42 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00042_of_00064.arrow
Process #43 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00043_of_00064.arrow
Process #44 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00044_of_00064.arrow
Process #45 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00045_of_00064.arrow
Process #46 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00046_of_00064.arrow
Process #47 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00047_of_00064.arrow
Process #48 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00048_of_00064.arrow
Process #49 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00049_of_00064.arrow
Process #50 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00050_of_00064.arrow
Process #51 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00051_of_00064.arrow
Process #52 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00052_of_00064.arrow
Process #53 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00053_of_00064.arrow
Process #54 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00054_of_00064.arrow
Process #55 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00055_of_00064.arrow
Process #56 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00056_of_00064.arrow
Process #57 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00057_of_00064.arrow
Process #58 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00058_of_00064.arrow
Process #59 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00059_of_00064.arrow
Process #60 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00060_of_00064.arrow
Process #61 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00061_of_00064.arrow
Process #62 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00062_of_00064.arrow
Process #63 will write at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_00063_of_00064.arrow
Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e3f92b4448719631/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9e214b72df4bef4c_*_of_00064.arrow
Concatenating 64 shards
05/18/2024 19:28:46 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_2b.jsonl.
Using custom data configuration default-091780489f458bf3
Loading Dataset Infos from /mnt/vol1/huangxin/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Dataset({
    features: ['text'],
    num_rows: 1305414
})
{'text': 'Snuist í hringi – Um tilgangsleysi allra hluta\nSnuist í hringi\nPosted byStefán\t 28. júní, 2005\nFyrst þegar við Steinunn fórum að ræða um að breyta eldhúsinu á Mánagötunni, lögðum við upp með eina grunnforsendu: við ætluðum að færa vaskinn frá veggnum og koma honum fyrir undir glugganum.\nEftir heilabrot og vangaveltur í\xad marga daga, þar sem fjöldi góðra manna og kvenna hefur komið að verki og gaukað að okkur hugmyndum, virðist niðurstaðan ætla að vera á þann veg að hverjum skáp og eldhústæki hafi verið skákað fram og til baka – en vaskurinn verði á nokkurn veginn upprunalegum stað.\nOkkur sýnist að eftir langt og gott samtal við hönnuð frá húsgagnafyrirtækinu sé loksins komin teikning sem við sættum okkur við.\nHún er ekki fullkomin. Hið fullkomna eldhús kemst tæplega fyrir í\xad eldhúsi sem er 2,4 sinnum 2,6 metrar. Fyrir tugþúsundir í\xad viðbót hefðum við getað stækkað gólfflötinn um kvartfermetra, með því\xad að saga burt strompinn. Jafnvel þótt við hefðum átt þá summu á lausu, er lí\xadtil hætta á að við hefðum látið til leiðast. Hvorugt okkar kærir sig um að hafa stórfé bundið í\xad mublum eða innréttingum.\nEftir stendur að mig sárvantar iðnaðarmenn. Þegar eldhúsinnréttingin verður tilbúin eftir dúk og disk hef ég aðgang að smiðum. Hluta af múrverkinu ættum við sömuleiðis að geta reddað eftir öðrum leiðum. Pí\xadparavinnan er lí\xadtil og dúkaraverkið sömuleiðis. – En stóra málið er rafvirkjavinnan!\nMánagatan 24 er gamalt hús, frá 1939 ef ég man rétt. Tenglar og rofar voru af skornum skammti og seinni tí\xadma viðbætur hafa allar verið gerðar af vanefnum. Á eldhúsinu er ástandið sérstaklega slæmt. Litlar eða engar raflagnir eru í\xad stærstum hluta þess, þar á meðal á stöðum sem við verðum að fá tengla á. Þetta verður helv… hausverkur.\nÞrælarnir á Sky Sport kynntu fyrstu áætlun um beinar útsendingar fram yfir miðjan nóvember. Enginn leikur með Luton á dagskránni. Ég er foxillur.\nBreiðbandið er ekki komið í\xad Mánagötuna og einhverra hluta vegna treystir Sí\xadminn sér ekki til að áætla hvenær úr því\xad verður bætt.\nOkkur labbakútunum á Breiðbandslausu svæðunum býðst þess í\xad stað að kaupa nettengingu af Sí\xadmanum og fá sjónvarpsþjónustuna í\xad gegnum þá tengingu.\nEn hvernig er það með viðskiptavini annarra sí\xadmafyrirtækja? Eiga þeir einhvern möguleika á að koma inn í\xad þetta kerfi? Eru tæknilegar hindranir í\xad vegi fyrir því\xad eða viðskiptalegar einvörðungu? Spyr sá sem ekki veit.'}
Process #0 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00000_of_00064.arrow
Process #1 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00001_of_00064.arrow
Process #2 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00002_of_00064.arrow
Process #3 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00003_of_00064.arrow
Process #4 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00004_of_00064.arrow
Process #5 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00005_of_00064.arrow
Process #6 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00006_of_00064.arrow
Process #7 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00007_of_00064.arrow
Process #8 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00008_of_00064.arrow
Process #9 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00009_of_00064.arrow
Process #10 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00010_of_00064.arrow
Process #11 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00011_of_00064.arrow
Process #12 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00012_of_00064.arrow
Process #13 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00013_of_00064.arrow
Process #14 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00014_of_00064.arrow
Process #15 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00015_of_00064.arrow
Process #16 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00016_of_00064.arrow
Process #17 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00017_of_00064.arrow
Process #18 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00018_of_00064.arrow
Process #19 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00019_of_00064.arrow
Process #20 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00020_of_00064.arrow
Process #21 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00021_of_00064.arrow
Process #22 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00022_of_00064.arrow
Process #23 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00023_of_00064.arrow
Process #24 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00024_of_00064.arrow
Process #25 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00025_of_00064.arrow
Process #26 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00026_of_00064.arrow
Process #27 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00027_of_00064.arrow
Process #28 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00028_of_00064.arrow
Process #29 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00029_of_00064.arrow
Process #30 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00030_of_00064.arrow
Process #31 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00031_of_00064.arrow
Process #32 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00032_of_00064.arrow
Process #33 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00033_of_00064.arrow
Process #34 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00034_of_00064.arrow
Process #35 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00035_of_00064.arrow
Process #36 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00036_of_00064.arrow
Process #37 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00037_of_00064.arrow
Process #38 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00038_of_00064.arrow
Process #39 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00039_of_00064.arrow
Process #40 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00040_of_00064.arrow
Process #41 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00041_of_00064.arrow
Process #42 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00042_of_00064.arrow
Process #43 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00043_of_00064.arrow
Process #44 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00044_of_00064.arrow
Process #45 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00045_of_00064.arrow
Process #46 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00046_of_00064.arrow
Process #47 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00047_of_00064.arrow
Process #48 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00048_of_00064.arrow
Process #49 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00049_of_00064.arrow
Process #50 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00050_of_00064.arrow
Process #51 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00051_of_00064.arrow
Process #52 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00052_of_00064.arrow
Process #53 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00053_of_00064.arrow
Process #54 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00054_of_00064.arrow
Process #55 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00055_of_00064.arrow
Process #56 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00056_of_00064.arrow
Process #57 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00057_of_00064.arrow
Process #58 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00058_of_00064.arrow
Process #59 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00059_of_00064.arrow
Process #60 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00060_of_00064.arrow
Process #61 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00061_of_00064.arrow
Process #62 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00062_of_00064.arrow
Process #63 will write at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_00063_of_00064.arrow
Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-091780489f458bf3/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37f982cd81c5aecb_*_of_00064.arrow
Concatenating 64 shards
05/18/2024 19:28:55 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/hi_2b.jsonl.
Using custom data configuration default-313adb5c043403fb
Loading Dataset Infos from /mnt/vol1/huangxin/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Dataset({
    features: ['text'],
    num_rows: 756931
})
{'text': 'दरिंदे पति को मिले कड़ी से कड़ी सजाOn 7/18/2012 7:05:43 PMChange font size:A | APrintE-mailCommentsRatingBookmark\nइंदौर। पत्नी के साथ दरिंदगी करने वाले पति को गिरफ्तार करने के बाद उसे जेल भेज दिया गया है। दूसरी ओर शहर भर में इस दरिंदगी को लेकर जबरदस्त आक्रोश है। महिला वकीलों का भी कहना है कि इस कृत्य ने अमानवीयता की सारी हदें पार कर दी हैं, इसे तो इतनी कठोर सजा मिलना चाहिए कि दोबारा कोई दरिंदा ऐसा करने की हिम्मत नहीं करे। पुलिस को इस प्रकरण में इतना मजबूत केस बनाना चाहिए कि अमानवीय व्यवहार करने वाले पति को कड़ी से कड़ी सजा मिले।\nउल्लेखनीय है कि इदरिस नगर में रहने वाले सोहन पिता नत्थूसिंह चौहान को जब अपनी पत्नी के चरित्र पर शक हुआ तो उसने पत्नी के जिस्म को गोदकर गुप्तांग पर ताला लगा दिया। करीब साढ़े तीन साल तक महिला इस पीड़ा को झेलती रही। पीड़ित महिला के मुताबिक उसका पति जब उसकी बेटी पर नीयत खराब करने लगा तो वह सहन नहीं कर सकी। उसके बाद उसने जहर खा लिया। अस्पताल पहुंचने के बाद सच्चई सामने आई तो पुलिस के साथ ही डाक्टरों के भी होश उड़ गए। पुलिस ने तत्काल पति को गिरफ्तार कर लिया। तब पति ने कहा कि मेरे परिवार की 9 महिलाएं घर से भाग चुकी हैं। इस कारण मुझे ये डर लगा रहता था कि कहीं मेरी पत्नी भी किसी के साथ भाग नहीं जाए। ताला लगाने के बाद पति चाबी भी अपनी पेंट के पायचे में छिपाकर रखता था।\nमहिला वकीलों में आक्रोश\nवैसे तो शहर भर में ही इस कृत्य के सामने आने के बाद आम लोगों में पति के प्रति जबरदस्त आक्रोश दिखाई दिया। महिला वकीलों का भी कहना था कि इस तरह की हरकत करने वाले को कड़ी से कड़ी सजा मिलना चाहिए।\nउसके साथ भी वही हो\nहाईकोर्ट वकील ज्योति माहेश्वरी कहती हैं कि आरोपी पति ने तो अमानवीयता की सारी हदें पार कर दी हैं। इसके साथ भी वही सलूक होना चाहिए जो इसने पत्नी के साथ किया। इस दरिंदे ने जो किया है उसके लिए तो हर सजा छोटी है। समाज को भी ऐसे लोगों का बहिष्कार कर दिया जाना चाहिए। समाज एक ओर तो कहता है कि महिला-पुरुष एक समान है वहीं इस तरह महिलाओं पर अत्याचार के मामले सामने आते हैं।\nमजबूत केस बनाए\nहाईकोर्ट एडवोकेट अर्चला जोशी के मुताबिक पुलिस मजबूत सबूत और तथ्य जुटाना चाहिए। पुलिस मेडिकल के आधार पर ही धाराएं लगाएगी। इस तरह के कृत्य करने वाले को किसी भी सूरत में माफ नहीं किया जा सकता। अन्य स्थानों पर भी यदि महिला का पति या कोई अन्य पुरुष जुल्म करता है तो इसे किसी भी कीमत पर बर्दाश्त नहीं किया जाना चाहिए। संयोगितागंज पुलिस ने आरोपी सोहन के खिलाफ धारा 498 ए और 326 के तहत प्रकरण दर्ज किया है। महिला की मेडिकल रिपोर्ट अभी नहीं मिली है। इस मामले में हाईकोर्ट एडवोकेट प्रवीण रावल कहते हैं कि पति ने जो किया है वह बेहद शर्मनाक है। पुलिस को इस मामले में मेडिकल इनवेस्टीगेशन पर गंभीरता से ध्यान देना चाहिए।\nMore News शराबी पति से परेशान महिला ने...घूमने गए बिल्डर के घर चोरों ...महिला ने फांसी लगाकर जान दी...वोल्टेज के उतार चढ़ाव से रहव...दहेज लोभी दूल्हा पिता व भाई ...दो घंटे ज्यादा दौड़ेगी आई बस...विधायक और सभापति के प्रचार ब...दुष्कर्म के बाद महिला की गला...विवि परिसर में युवक ने जहर ख...बिचौलिए कमा रहे हैं मुनाफा...बहू की हत्या में जेठ को उम्र...बाथरुम में खुद पर घासलेट डाल...अय्याशी पर रोज उड़ा रहे थे 1...दिखावा बनकर रह गई मुहिम...महिला चोर गैंग को घेराबंदी क...बुजुर्ग ने की बच्ची से अश्ली...दुल्हन के गहने और कपड़े खाक...अब दोपहर में भी चलेगी आई-बस...नर्सिग होम संचालकों को दस दि...5000 पेज का चालान, 90 हजार प...हरिहरन की जादुई आवाज में खो ...'}
Process #0 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00000_of_00064.arrow
Process #1 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00001_of_00064.arrow
Process #2 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00002_of_00064.arrow
Process #3 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00003_of_00064.arrow
Process #4 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00004_of_00064.arrow
Process #5 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00005_of_00064.arrow
Process #6 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00006_of_00064.arrow
Process #7 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00007_of_00064.arrow
Process #8 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00008_of_00064.arrow
Process #9 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00009_of_00064.arrow
Process #10 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00010_of_00064.arrow
Process #11 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00011_of_00064.arrow
Process #12 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00012_of_00064.arrow
Process #13 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00013_of_00064.arrow
Process #14 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00014_of_00064.arrow
Process #15 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00015_of_00064.arrow
Process #16 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00016_of_00064.arrow
Process #17 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00017_of_00064.arrow
Process #18 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00018_of_00064.arrow
Process #19 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00019_of_00064.arrow
Process #20 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00020_of_00064.arrow
Process #21 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00021_of_00064.arrow
Process #22 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00022_of_00064.arrow
Process #23 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00023_of_00064.arrow
Process #24 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00024_of_00064.arrow
Process #25 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00025_of_00064.arrow
Process #26 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00026_of_00064.arrow
Process #27 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00027_of_00064.arrow
Process #28 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00028_of_00064.arrow
Process #29 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00029_of_00064.arrow
Process #30 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00030_of_00064.arrow
Process #31 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00031_of_00064.arrow
Process #32 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00032_of_00064.arrow
Process #33 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00033_of_00064.arrow
Process #34 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00034_of_00064.arrow
Process #35 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00035_of_00064.arrow
Process #36 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00036_of_00064.arrow
Process #37 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00037_of_00064.arrow
Process #38 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00038_of_00064.arrow
Process #39 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00039_of_00064.arrow
Process #40 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00040_of_00064.arrow
Process #41 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00041_of_00064.arrow
Process #42 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00042_of_00064.arrow
Process #43 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00043_of_00064.arrow
Process #44 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00044_of_00064.arrow
Process #45 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00045_of_00064.arrow
Process #46 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00046_of_00064.arrow
Process #47 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00047_of_00064.arrow
Process #48 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00048_of_00064.arrow
Process #49 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00049_of_00064.arrow
Process #50 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00050_of_00064.arrow
Process #51 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00051_of_00064.arrow
Process #52 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00052_of_00064.arrow
Process #53 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00053_of_00064.arrow
Process #54 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00054_of_00064.arrow
Process #55 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00055_of_00064.arrow
Process #56 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00056_of_00064.arrow
Process #57 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00057_of_00064.arrow
Process #58 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00058_of_00064.arrow
Process #59 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00059_of_00064.arrow
Process #60 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00060_of_00064.arrow
Process #61 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00061_of_00064.arrow
Process #62 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00062_of_00064.arrow
Process #63 will write at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_00063_of_00064.arrow
Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-313adb5c043403fb/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-d2573b4a513b2883_*_of_00064.arrow
Concatenating 64 shards
Process #0 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00000_of_00064.arrow
Process #1 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00001_of_00064.arrow
Process #2 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00002_of_00064.arrow
Process #3 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00003_of_00064.arrow
Process #4 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00004_of_00064.arrow
Process #5 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00005_of_00064.arrow
Process #6 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00006_of_00064.arrow
Process #7 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00007_of_00064.arrow
Process #8 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00008_of_00064.arrow
Process #9 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00009_of_00064.arrow
Process #10 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00010_of_00064.arrow
Process #11 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00011_of_00064.arrow
Process #12 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00012_of_00064.arrow
Process #13 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00013_of_00064.arrow
Process #14 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00014_of_00064.arrow
Process #15 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00015_of_00064.arrow
Process #16 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00016_of_00064.arrow
Process #17 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00017_of_00064.arrow
Process #18 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00018_of_00064.arrow
Process #19 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00019_of_00064.arrow
Process #20 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00020_of_00064.arrow
Process #21 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00021_of_00064.arrow
Process #22 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00022_of_00064.arrow
Process #23 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00023_of_00064.arrow
Process #24 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00024_of_00064.arrow
Process #25 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00025_of_00064.arrow
Process #26 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00026_of_00064.arrow
Process #27 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00027_of_00064.arrow
Process #28 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00028_of_00064.arrow
Process #29 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00029_of_00064.arrow
Process #30 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00030_of_00064.arrow
Process #31 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00031_of_00064.arrow
Process #32 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00032_of_00064.arrow
Process #33 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00033_of_00064.arrow
Process #34 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00034_of_00064.arrow
Process #35 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00035_of_00064.arrow
Process #36 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00036_of_00064.arrow
Process #37 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00037_of_00064.arrow
Process #38 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00038_of_00064.arrow
Process #39 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00039_of_00064.arrow
Process #40 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00040_of_00064.arrow
Process #41 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00041_of_00064.arrow
Process #42 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00042_of_00064.arrow
Process #43 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00043_of_00064.arrow
Process #44 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00044_of_00064.arrow
Process #45 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00045_of_00064.arrow
Process #46 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00046_of_00064.arrow
Process #47 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00047_of_00064.arrow
Process #48 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00048_of_00064.arrow
Process #49 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00049_of_00064.arrow
Process #50 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00050_of_00064.arrow
Process #51 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00051_of_00064.arrow
Process #52 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00052_of_00064.arrow
Process #53 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00053_of_00064.arrow
Process #54 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00054_of_00064.arrow
Process #55 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00055_of_00064.arrow
Process #56 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00056_of_00064.arrow
Process #57 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00057_of_00064.arrow
Process #58 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00058_of_00064.arrow
Process #59 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00059_of_00064.arrow
Process #60 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00060_of_00064.arrow
Process #61 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00061_of_00064.arrow
Process #62 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00062_of_00064.arrow
Process #63 will write at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_00063_of_00064.arrow
Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a3039af699542fe3_*_of_00064.arrow
Concatenating 64 shards
input_ids:
[81778, 125768, 17166, 125089, 135564, 124072, 21360, 124523, 27490, 73441, 442, 2100, 52275, 10531, 2034, 1959, 82168, 128583, 123877, 124388, 49388, 198, 81778, 125768, 17166, 125089, 135564, 124072, 21360, 124523, 27490, 73441, 442, 2100, 52275, 10531, 2034, 198, 31459, 220, 15, 23, 11, 220, 17, 15, 16, 19, 304, 29342, 21033, 11, 15762, 198, 134204, 128473, 92381, 53710, 124035, 124387, 63237, 59842, 126628, 16157, 128248, 92072, 128541, 141949, 124669, 127630, 55334, 35038, 13, 128702, 63237, 63415, 123860, 63415, 14293, 14293, 128332, 128259, 134842, 141949, 124669, 128342, 55334, 35038, 77273, 123877, 140806, 124114, 27490, 124872, 128261, 39434, 130510, 74315, 35038, 33090, 123961, 123829, 43635, 123894, 124641, 8532, 624, 91962, 14293, 130104, 56794, 135746, 124387, 68785, 128660, 77273, 131032, 126212, 124080, 123963, 132875, 128473, 124072, 128473, 92381, 25871, 124209, 126842, 68785, 125007, 141949, 124669, 127630, 55334, 35038, 128420, 92072, 128541, 63415, 125184, 129308, 129458, 128842, 510, 1, 135746, 124387, 4894, 124075, 53710, 124035, 124387, 4894, 27490, 125657, 0, 77703, 125657, 0, 77703, 41593, 6913, 125267, 125007, 131723, 127761, 125007, 39434, 31073, 124014, 126418, 53710, 124035, 124387, 86941, 126880, 124006, 128287, 59842, 65398, 124265, 126406, 128325, 125007, 37524, 130510, 128342, 20931, 132568, 126815, 133781, 68785, 63237, 59842, 124363, 16157, 128416, 73274, 133326, 125007, 125100, 39697, 73771, 25871, 128261, 74315, 124284, 124006, 37524, 124224, 14293, 12, 126805, 136178, 123894, 137577, 12, 53710, 70604, 23224, 123913, 124009, 124691, 123913, 124876, 624, 124703, 53710, 124035, 124387, 37524, 29825, 123980, 63415, 125184, 124766, 123938, 16157, 68785, 73274, 132137, 126196, 128920, 77273, 27846, 126229, 52157, 124454, 94957, 123920, 125704, 73274, 130510, 74315, 35038, 33090, 123961, 123829, 43635, 123894, 124641, 8532, 13, 128671, 55891, 11798, 31382, 31073, 52157, 124328, 25871, 125555, 126229, 123890, 39434, 130510, 138788, 124376, 130063, 50243, 95975, 55334, 25871, 44330, 125633, 53710, 124035, 124387, 68785, 128416, 125555, 129327, 124006, 63415, 21360, 124642, 128587, 63415, 137867, 53710, 124035, 124387, 129445, 85153, 46586, 124983, 129472, 125490, 77273, 128305, 141189, 68785, 128587, 73274, 130510, 77273, 128773, 133674, 124269, 126504, 53710, 123938, 125072, 124376, 624, 31073, 124519, 17166, 123832, 124210, 68785, 77273, 128280, 128438, 68785, 56794, 43635, 126756, 126436, 124267, 13, 129920, 69423, 128412, 39434, 130510, 128248, 125637, 124678, 133394, 135110, 55057, 123913, 124226, 68785, 128259, 133114, 126383, 128248, 123894, 126995, 128464, 73274, 27490, 130151, 129046, 17166, 124187, 11798, 624, 134204, 128473, 92381, 53710, 124035, 124387, 63237, 59842, 126628, 16157, 68785, 128641, 128305, 53479, 124653, 126214, 128280, 123913, 126628, 135306, 133534, 125633, 44330, 125633, 16157, 128248, 53710, 125592, 16157, 13, 39434, 140518, 63415, 125481, 98719, 82168, 124131, 16157, 125475, 143516, 130543, 128248, 126198, 73274, 126727, 68785, 63237, 124080, 92381, 124653, 133462, 128248, 123877, 124388, 13, 126208, 39434, 21360, 124015, 125490, 59842, 124181, 128264, 125447, 133398, 68785, 128464, 125555, 125962, 63415, 130181, 23224, 16157, 128288, 124006, 140674, 25871, 68785, 128523, 128261, 128248, 63415, 124042, 49388, 16157, 123961, 133176, 13, 126214, 124147, 124144, 143122, 123941, 77273, 128858, 143400, 82168, 124131, 16157, 68785, 73274, 124938, 12653, 129163, 12653, 124080, 124014, 25871, 63237, 77703, 124325, 53710, 125592, 16157, 128252, 134259, 63415, 124522, 95975, 63415, 130181, 23224, 77703, 124677, 126236, 13, 56794, 130059, 25871, 125007, 68238, 5703, 72804, 94957, 124925, 73771, 31073, 68785, 125555, 31382, 124147, 124144, 133394, 27490, 124009, 32790, 124147, 20931, 124394, 128307, 73274, 130234, 63237, 128248, 135726, 124663, 5703, 125138, 68785, 124838, 23224, 84532, 44330, 10176, 126789, 16157, 56794, 141620, 16157, 141949, 124226, 128342, 55334, 35038, 63237, 131876, 129197, 68785, 128671, 131876, 5703, 124376, 126198, 86941, 41593, 23224, 124061, 124326, 16157, 124011, 98719, 128261, 82168, 123987, 125011, 73274, 124787, 131402, 125007, 138829, 124647, 129581, 128248, 126198, 73274, 126727, 624, 69682, 39697, 124210, 123877, 130392, 124420, 63237, 135726, 16157, 124766, 125573, 125449, 125603, 16157, 73274, 126610, 5703, 128637, 137855, 39434, 33090, 124642, 125555, 123920, 124075, 138821, 124265, 125179, 68785, 43982, 55334, 123940, 124376, 68785, 126198, 39434, 133985, 63237, 138821, 124265, 125179, 68785, 130444, 73274, 129538, 128478, 17166, 92381, 126880, 13, 125857, 72804, 138821, 128252, 53710, 10176, 65398, 37524, 130431, 68238, 123829, 43635, 126761, 128321, 140426, 128307, 128259, 73274, 130234, 37524, 124181, 20931, 128248, 77703, 124677, 126236, 13, 68238, 5703, 72804, 125007, 73274, 129801, 63237, 68238, 11798, 124328, 125011, 92072, 128541, 5703, 124376, 73274, 11798, 128842, 81768, 63415, 125184, 124766, 21360, 124642, 68785, 37524, 23224, 124523, 124009, 125857, 124335, 14293, 52157, 128583, 124642, 68785, 68238, 20064, 73771, 53710, 124035, 124387, 129346, 27846, 55334, 139663, 128617, 92072, 127328, 25871, 74315, 20931, 126756, 129458, 135500, 63237, 68238, 11798, 124328, 125011, 68785, 131933, 91335, 5703, 56794, 124210, 92381, 127046, 126214, 123913, 31073, 128541, 73274, 124429, 13325, 27846, 84532, 124388, 52157, 124454, 135726, 59842, 124363, 16157, 624, 138607, 135726, 125007, 27490, 124420, 59842, 133660, 129517, 125011, 128307, 27846, 47632, 129968, 73274, 125203, 20064, 73771, 13325, 132093, 129375, 77703, 124677, 126236, 68785, 131933, 91335, 5703, 74315, 125346, 14293, 43982, 123860, 16157, 126208, 124290, 125447, 133398, 23364, 35244, 73771, 123978, 25871, 27846, 35244, 47632, 10176, 50243, 29825, 125309, 23364, 43635, 123897, 124476, 127038, 68785, 63237, 135500, 25871, 63237, 126212, 125007, 27490, 124420, 149, 235, 23364, 125657, 124387, 25871, 135726, 125007, 27490, 124420, 13, 68238, 20064, 73771, 53710, 124035, 124387, 126941, 35244, 125320, 77273, 68238, 11798, 124328, 125011, 68785, 63415, 137867, 14293, 129072, 124006, 56794, 123904, 133212, 92072, 124144, 16157, 126196, 85153, 125664, 80970, 23224, 17166, 125126, 13, 125857, 72804, 14293, 128305, 124666, 35244, 125320, 128252, 130630, 124226, 52157, 126842, 39434, 31073, 65398, 125007, 39434, 55334, 124533, 82168, 8532, 91335, 37524, 124706, 135500, 63237, 128858, 143400, 82168, 124131, 16157, 53479, 81778, 43635, 55057, 124476, 124677, 124072, 81778, 124422, 124072, 140148, 53479, 131609, 124061, 624, 131752, 124880, 14293, 53710, 124035, 124387, 37524, 129424, 55057, 23364, 124125, 124210, 23364, 128785, 55057, 128248, 132093, 68785, 125040, 35038, 27490, 77273, 27846, 27490, 124511, 63237, 132033, 68785, 37524, 133955, 124006, 63415, 139479, 125007, 63415, 125184, 124766, 21360, 124642, 128416, 45577, 35038, 27490, 124642, 126530, 11071, 123997, 80970, 128252, 131865, 129197, 68785, 131865, 128259, 134842, 128985, 141949, 124669, 128342, 55334, 35038, 68785, 128464, 68238, 123829, 43635, 43982, 124641, 8532, 13, 37524, 133660, 130063, 124080, 124184, 128885, 17166, 124006, 124623, 136423, 124478, 136423, 73274, 81778, 125903, 128920, 123913, 133660, 125088, 138472, 68785, 127726, 126406, 624, 126929, 53710, 124035, 124387, 53710, 137610, 124072, 91335, 77273, 131865, 126198, 129375, 123877, 130392, 124420, 68785, 133593, 124009, 63415, 31073, 124014, 44330, 126456, 124072, 79820, 16157, 68785, 63415, 137867, 73274, 29825, 124261, 17166, 125173, 124511, 53479, 133077]
inputs:
غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام
غصن الزيتون والبندقية // ANONYMOUS
August 08, 2014 in INTERVIEW, TEXT
إستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.
إتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:
"رؤوف!
يا رؤوف!
قصف! قصف! قص-
قبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.
كان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.
كانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.
إستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.
أزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.
عبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.
إلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.
وجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابل
05/18/2024 19:29:15 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
05/18/2024 19:29:15 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
05/18/2024 19:29:15 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
05/18/2024 19:29:15 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
05/18/2024 19:29:15 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
05/18/2024 19:29:15 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 1700419
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
Dataset({
    features: ['text'],
    num_rows: 1700419
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
Dataset({
    features: ['text'],
    num_rows: 1700419
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
05/18/2024 19:29:16 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 1700419
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
Dataset({
    features: ['text'],
    num_rows: 1700419
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
Dataset({
    features: ['text'],
    num_rows: 1700419
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
Dataset({
    features: ['text'],
    num_rows: 1700419
})
{'text': 'غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام\nغصن الزيتون والبندقية // ANONYMOUS\nAugust 08, 2014 in INTERVIEW, TEXT\nإستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.\nإتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:\n"رؤوف!\nيا رؤوف!\nقصف! قصف! قص-\nقبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.\nكان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.\nكانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.\nإستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر كالقماش الخفيف الذي يزال من على فوق الطاولة، وبعث دماغه لجسمه صفارة إنذار من نوع آخر، كانت نوعاً ما كصعقة الكهرباء التي جعلته يتيقن أن بدنه ليس على ما يرام.\nأزاح الأنقاض من فوقه وأدار بنظره يمينا ثم شمال تجاه زوايا البيت الصغير، عذراً، ما تبقى من البيت الصغير، ولم يرى غير الظلام. تحول البيت إلى رماد وظل حائط واحد هو الوحيد الذي لا يزال واقف على قدميه. حاول أن يطلق من حنجرته صوتاً ينادي به أمه وأباه، وعندما تحركت شفتاه، حسّ رؤوف فقط بذبذبة صامتة خفيفة تنبعث من حنجرته، وعندها لاحظ كيف كان السكوت يخلد بثقل شديد فوق سمعه.\nعبر فوق أنقاض سقف بيته الذي بات الآن يتوسّد الأرض تحت قدميه، وعندها خطفت عينه لمحة ذراع مخّتمة بخاتم نحاسي مطلي بالذهب، منبعثة من بين أنقاضٍ مصفوفة فوق أنقاض. حسّ رؤوف بوخزة في حنجرته، أخذت طريقها لمنتصف صدره مع إبتلاع الريق. تحولت هذه الوخزة إلى حرارة شديدة تكاد أن تذيب جلده وتنبعث من جميع أنحاء جسمه المغطى بالدم والغبار والملابس الممزقة.\nإلتفت رؤوف ورأى مسباح ملقى على الأرض، غارق في بقعة من الدم، وحينها أدرك أن أمه وأباه قد فارقاه وإرتحلا إلى مكان آخر، مكان لا يوجد فيه صفارات إنذار، ولا حائط عازل. وقف أمام النجمتين الهاويتان اللتان يغطيهما السقف المنقض، بصمت.\nوجد رؤوف رفش والده في مكان ما تحت الأنقاض، وبعدما أكمل دفن والديه، أخذ يحفر البقعة المقابلة لشجرة الزيتون، التي لم يتبقى منها إلا غصن واحد. عندما سمع صوت الرفش يصطدم بشيء ما تحت التراب، بعث يده في داخل الحفرة وإجتث البندقية. أمسك بغصن الزيتون ورماه في النيران التي إشتعلت نتيجة الضوء الذي هوى فوق سقف دياره وأسرى بأمه وأبه إلى المكان الذي يخلو من صفارات الإنذار.'}
05/18/2024 19:29:28 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
05/18/2024 19:29:29 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
05/18/2024 19:29:29 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 1948786
})
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
05/18/2024 19:29:29 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
05/18/2024 19:29:29 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
05/18/2024 19:29:29 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 1948786
})
05/18/2024 19:29:29 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
Dataset({
    features: ['text'],
    num_rows: 1948786
})
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
Dataset({
    features: ['text'],
    num_rows: 1948786
})
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
Dataset({
    features: ['text'],
    num_rows: 1948786
})
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
Dataset({
    features: ['text'],
    num_rows: 1948786
})
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
Dataset({
    features: ['text'],
    num_rows: 1948786
})
{'text': '[Buchvorstellung] Die Frau im hellblauen Kleid - BEATE MAXIAN - MonerlS-bunte-Welt\n[Buchvorstellung] Die Frau im hellblauen Kleid – BEATE MAXIAN\n8. Dezember 2017 Monerl\n#166 Rezension\nWien. Marianne Altmann, einst ein gefeierter Filmstar, ist schockiert, als sie von Plänen ihrer Tochter Vera erfährt. Diese möchte einen Film über ihre Familie drehen. Marianne fürchtet, dass nun auch die Abgründe der Familie ans Tageslicht kommen könnten, und mit ihnen ein lange\nzurückliegendes Vergehen. Es reicht zurück ins Jahr 1927, als ihre Mutter Käthe in einem geliehenen Kleid am Theater vorsprach. Der Beginn einer beispiellosen Karriere – und einer verhängnisvollen Bekanntschaft mit Hans Bleck, der zum mächtigen Produzenten der Ufa aufsteigen sollte …\nEine Familiengeschichte über vier Generationen, von Ur-Großmutter zur Enkelin, verwoben mit tiefen Geheimnissen, beginnend in Wien, als 1927 das Mädchen Käthe den Mut aufbrachte, sich ihren Traum zu erfüllen.\nEs versprach eine spannende Familiengeschichte zu werden. Vier Frauen, die sich vom Schicksal, das der 2. Weltkrieg über sie gebracht hat, nicht von ihrem Weg haben abbringen lassen und letztendlich eine Schauspieler-Dynastie begründeten, wie es in Österreich vorher keine gegeben hat. Dabei kommt einiges ans Licht, das lange Zeit verborgen geblieben war.\nDer Rahmen des Romans ist sehr gelungen. Leider schafft es Beate Maxian aber nicht, ihre Geschichte sprachlich so umzusetzten, das man von ihr durchweg gefesselt ist. Der Erzähl- und Schreibstil ist sehr unrund. Oftmals wunderte ich mich über holprige und flache Dialoge. Inhaltlich enthüllten sie ein großes Geheimnis, doch Beate Maxians Charaktere nahmen dies einfach an ohne eine gewichtige Emotionalität, dich ich von den Figuren zur jeweiligen Enthüllung erwartet hätte. Somit wuchs zunehmend meine Distanz zu den Protagonistinnen. Ich konnte nicht mit ihnen fühlen, "sah" ihnen lediglich kopfschüttelnd zu.\nDas Beziehungs-Hin-und-Her von Sophie, der letzten in der Reihe der "Altmann-Frauen", das große Herzschmerzdrama und eine andauernde Sturheit ohne Einsicht, zerrte mehr und mehr an meinen Nerven. Die ständigen Wiederholungen zu Marianne, wie "Diva" und "betagte Dame", über die ich vermehrt stolperte, störten meinen Lesefluss. Ich hangelte mich von Kapitel zu Kapitel und freute mich immer auf ein Kapitel zur Vergangenheit, das über Käthe, die Ur-Großmutter, berichtete. Diese Rückblicke in die Vergangenheit waren es, die die Spannung aufrecht hielten und mich bis zum Ende haben durchhalten lassen.\nNach Beendigung der Lektüre denke ich, hätte sich die Autorin ausschließlich auf die historische Geschichte eingelassen, wäre ihr wahrscheinlich ein sehr guter Roman gelungen. Denn dieser Erzählstrang war gut recherchiert, mit einer wundervollen und für sich einnehmenden Protagonistin ausgearbeitet und einer Liebesgeschichte, die viel Potential gehabt hätte. So wäre auch der Konflikt mit und von Jakob, der Jude war, noch besser zur Geltung gekommen und es wären die Emotionen und Erwartungen des Lesers, die das wunderschöne Cover geweckt hatte, erfüllt worden.\nEin thematisch sehr interessantes Buch, mit einem tollen Handlungsumfeld, spannenden Geheimnissen und vielversprechenden Charakteren, das leider an der Umsetzung scheiterte.\nIch danke dem Heyne Verlag, der mir freundlicherweise dieses Buch als Rezensionsexemplar zur Verfügung gestellt hat.\nBeate Maxian Familiensaga Flop Heyne Verlag Österreich Rezension Rezensionsexemplar Wien\nPrevious Post Buchvorstellung – Das Fundament der Ewigkeit – KEN FOLLETT\nNext Post [Buchvorstellung] Das Geheimnis des Kalligraphen – RAFIK SCHAMI\nIch schau ja bei solchen Büchern nach den Kritiken, um zu erfahren, ob es was für meine Mutter wäre. Die liest nämlich eher diese Richtung, aber das hier kann ich wohl streichen 😉\n16. Dezember 2017 um 13:33 Uhr\nHey,Wenn du dich so bei den Rezis zu dem Buch umhörst, wirst du sehen, dass meine eher schlechte Meinung nicht so durchschnittlich ist. Die meisten waren von dem Buch sehr begeistert. Ich kenne deine Mutte jetzt nicht, ich hoffe, damit entgeht ihr nicht eine Geschichte, die ihr gefallen hätte. 😉 :-PGlG, monerl\n21. Dezember 2017 um 9:57 Uhr\nAlso wenn ich deine Kritikpunkte so sehe, dann bleib ich bei meiner Entscheidung 😛 In der Hinsicht weiß ich dann doch, was sie mag bzw wir sind uns da recht ähnlich.\n8. Dezember 2017 um 21:41 Uhr\nHallo liebe Andrea,ja, ich hatte mir viel von diesem Buch versprochen! Wenn deine Mama solche Geschichten liebt, dann empfehle ich dir die von TERESA SIMON oder auch BEATE RÖSLER z.B., siehe meine Rezis zu den Bücher der Autorinnen! Die sind klasse!GlG, monerl\nHallo liebes Monerl,der Klappentext hört sich ja sehr interessant an und lässt viel erhoffen, aber da Du nicht wirklich von dem Buch überzeugt bist, werde ich das als Geschenk ausschließen. Meine Mutter mag gerne solche Bücher und wäre natürlich für Weihnachten perfekt gewesen. Dann werde ich mich noch ein wenig umsehen. Herzlichen Dank für die Rezi. Liebe GrüßeAndrae\nLiebe monerl,"Hitorische Geschichte" – kommt es mir nur so vor oder gibt es derzeit vermehrt derartige Geschichten, die in dieser Zeit spielen? Manchmal habe ich das Gefühl, es ist besser, eine Biografie aus dieser Zeit zu lesen. Danke für die ehrliche Einschätzung.Liebe Grüße, Anne\nLiebe Anne,ja, da hast du gar nicht Unrecht. Manch Biografie kann einem historisch die Zeit viel näher bringen. Es gibt sehr gute Romane aus der Zeit. Ich kann jetzt gar nicht sagen, dass mir aufgefallen wäre, dass es zurzeit vermehrt Geschichten aus dieser Zeit gibt. Ich lese ja breit gefächert und bin nicht immer im Bilde. Aber zu viele gleiche Geschichten stehen sich gegenseitig im Weg.GlG vom monerl'}
05/18/2024 19:29:37 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 1305414
})
{'text': 'Snuist í hringi – Um tilgangsleysi allra hluta\nSnuist í hringi\nPosted byStefán\t 28. júní, 2005\nFyrst þegar við Steinunn fórum að ræða um að breyta eldhúsinu á Mánagötunni, lögðum við upp með eina grunnforsendu: við ætluðum að færa vaskinn frá veggnum og koma honum fyrir undir glugganum.\nEftir heilabrot og vangaveltur í\xad marga daga, þar sem fjöldi góðra manna og kvenna hefur komið að verki og gaukað að okkur hugmyndum, virðist niðurstaðan ætla að vera á þann veg að hverjum skáp og eldhústæki hafi verið skákað fram og til baka – en vaskurinn verði á nokkurn veginn upprunalegum stað.\nOkkur sýnist að eftir langt og gott samtal við hönnuð frá húsgagnafyrirtækinu sé loksins komin teikning sem við sættum okkur við.\nHún er ekki fullkomin. Hið fullkomna eldhús kemst tæplega fyrir í\xad eldhúsi sem er 2,4 sinnum 2,6 metrar. Fyrir tugþúsundir í\xad viðbót hefðum við getað stækkað gólfflötinn um kvartfermetra, með því\xad að saga burt strompinn. Jafnvel þótt við hefðum átt þá summu á lausu, er lí\xadtil hætta á að við hefðum látið til leiðast. Hvorugt okkar kærir sig um að hafa stórfé bundið í\xad mublum eða innréttingum.\nEftir stendur að mig sárvantar iðnaðarmenn. Þegar eldhúsinnréttingin verður tilbúin eftir dúk og disk hef ég aðgang að smiðum. Hluta af múrverkinu ættum við sömuleiðis að geta reddað eftir öðrum leiðum. Pí\xadparavinnan er lí\xadtil og dúkaraverkið sömuleiðis. – En stóra málið er rafvirkjavinnan!\nMánagatan 24 er gamalt hús, frá 1939 ef ég man rétt. Tenglar og rofar voru af skornum skammti og seinni tí\xadma viðbætur hafa allar verið gerðar af vanefnum. Á eldhúsinu er ástandið sérstaklega slæmt. Litlar eða engar raflagnir eru í\xad stærstum hluta þess, þar á meðal á stöðum sem við verðum að fá tengla á. Þetta verður helv… hausverkur.\nÞrælarnir á Sky Sport kynntu fyrstu áætlun um beinar útsendingar fram yfir miðjan nóvember. Enginn leikur með Luton á dagskránni. Ég er foxillur.\nBreiðbandið er ekki komið í\xad Mánagötuna og einhverra hluta vegna treystir Sí\xadminn sér ekki til að áætla hvenær úr því\xad verður bætt.\nOkkur labbakútunum á Breiðbandslausu svæðunum býðst þess í\xad stað að kaupa nettengingu af Sí\xadmanum og fá sjónvarpsþjónustuna í\xad gegnum þá tengingu.\nEn hvernig er það með viðskiptavini annarra sí\xadmafyrirtækja? Eiga þeir einhvern möguleika á að koma inn í\xad þetta kerfi? Eru tæknilegar hindranir í\xad vegi fyrir því\xad eða viðskiptalegar einvörðungu? Spyr sá sem ekki veit.'}
05/18/2024 19:29:38 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_2b.jsonl.
05/18/2024 19:29:39 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 1305414
})
{'text': 'Snuist í hringi – Um tilgangsleysi allra hluta\nSnuist í hringi\nPosted byStefán\t 28. júní, 2005\nFyrst þegar við Steinunn fórum að ræða um að breyta eldhúsinu á Mánagötunni, lögðum við upp með eina grunnforsendu: við ætluðum að færa vaskinn frá veggnum og koma honum fyrir undir glugganum.\nEftir heilabrot og vangaveltur í\xad marga daga, þar sem fjöldi góðra manna og kvenna hefur komið að verki og gaukað að okkur hugmyndum, virðist niðurstaðan ætla að vera á þann veg að hverjum skáp og eldhústæki hafi verið skákað fram og til baka – en vaskurinn verði á nokkurn veginn upprunalegum stað.\nOkkur sýnist að eftir langt og gott samtal við hönnuð frá húsgagnafyrirtækinu sé loksins komin teikning sem við sættum okkur við.\nHún er ekki fullkomin. Hið fullkomna eldhús kemst tæplega fyrir í\xad eldhúsi sem er 2,4 sinnum 2,6 metrar. Fyrir tugþúsundir í\xad viðbót hefðum við getað stækkað gólfflötinn um kvartfermetra, með því\xad að saga burt strompinn. Jafnvel þótt við hefðum átt þá summu á lausu, er lí\xadtil hætta á að við hefðum látið til leiðast. Hvorugt okkar kærir sig um að hafa stórfé bundið í\xad mublum eða innréttingum.\nEftir stendur að mig sárvantar iðnaðarmenn. Þegar eldhúsinnréttingin verður tilbúin eftir dúk og disk hef ég aðgang að smiðum. Hluta af múrverkinu ættum við sömuleiðis að geta reddað eftir öðrum leiðum. Pí\xadparavinnan er lí\xadtil og dúkaraverkið sömuleiðis. – En stóra málið er rafvirkjavinnan!\nMánagatan 24 er gamalt hús, frá 1939 ef ég man rétt. Tenglar og rofar voru af skornum skammti og seinni tí\xadma viðbætur hafa allar verið gerðar af vanefnum. Á eldhúsinu er ástandið sérstaklega slæmt. Litlar eða engar raflagnir eru í\xad stærstum hluta þess, þar á meðal á stöðum sem við verðum að fá tengla á. Þetta verður helv… hausverkur.\nÞrælarnir á Sky Sport kynntu fyrstu áætlun um beinar útsendingar fram yfir miðjan nóvember. Enginn leikur með Luton á dagskránni. Ég er foxillur.\nBreiðbandið er ekki komið í\xad Mánagötuna og einhverra hluta vegna treystir Sí\xadminn sér ekki til að áætla hvenær úr því\xad verður bætt.\nOkkur labbakútunum á Breiðbandslausu svæðunum býðst þess í\xad stað að kaupa nettengingu af Sí\xadmanum og fá sjónvarpsþjónustuna í\xad gegnum þá tengingu.\nEn hvernig er það með viðskiptavini annarra sí\xadmafyrirtækja? Eiga þeir einhvern möguleika á að koma inn í\xad þetta kerfi? Eru tæknilegar hindranir í\xad vegi fyrir því\xad eða viðskiptalegar einvörðungu? Spyr sá sem ekki veit.'}
05/18/2024 19:29:39 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_2b.jsonl.
05/18/2024 19:29:39 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_2b.jsonl.
05/18/2024 19:29:39 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_2b.jsonl.
05/18/2024 19:29:39 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 1305414
})
{'text': 'Snuist í hringi – Um tilgangsleysi allra hluta\nSnuist í hringi\nPosted byStefán\t 28. júní, 2005\nFyrst þegar við Steinunn fórum að ræða um að breyta eldhúsinu á Mánagötunni, lögðum við upp með eina grunnforsendu: við ætluðum að færa vaskinn frá veggnum og koma honum fyrir undir glugganum.\nEftir heilabrot og vangaveltur í\xad marga daga, þar sem fjöldi góðra manna og kvenna hefur komið að verki og gaukað að okkur hugmyndum, virðist niðurstaðan ætla að vera á þann veg að hverjum skáp og eldhústæki hafi verið skákað fram og til baka – en vaskurinn verði á nokkurn veginn upprunalegum stað.\nOkkur sýnist að eftir langt og gott samtal við hönnuð frá húsgagnafyrirtækinu sé loksins komin teikning sem við sættum okkur við.\nHún er ekki fullkomin. Hið fullkomna eldhús kemst tæplega fyrir í\xad eldhúsi sem er 2,4 sinnum 2,6 metrar. Fyrir tugþúsundir í\xad viðbót hefðum við getað stækkað gólfflötinn um kvartfermetra, með því\xad að saga burt strompinn. Jafnvel þótt við hefðum átt þá summu á lausu, er lí\xadtil hætta á að við hefðum látið til leiðast. Hvorugt okkar kærir sig um að hafa stórfé bundið í\xad mublum eða innréttingum.\nEftir stendur að mig sárvantar iðnaðarmenn. Þegar eldhúsinnréttingin verður tilbúin eftir dúk og disk hef ég aðgang að smiðum. Hluta af múrverkinu ættum við sömuleiðis að geta reddað eftir öðrum leiðum. Pí\xadparavinnan er lí\xadtil og dúkaraverkið sömuleiðis. – En stóra málið er rafvirkjavinnan!\nMánagatan 24 er gamalt hús, frá 1939 ef ég man rétt. Tenglar og rofar voru af skornum skammti og seinni tí\xadma viðbætur hafa allar verið gerðar af vanefnum. Á eldhúsinu er ástandið sérstaklega slæmt. Litlar eða engar raflagnir eru í\xad stærstum hluta þess, þar á meðal á stöðum sem við verðum að fá tengla á. Þetta verður helv… hausverkur.\nÞrælarnir á Sky Sport kynntu fyrstu áætlun um beinar útsendingar fram yfir miðjan nóvember. Enginn leikur með Luton á dagskránni. Ég er foxillur.\nBreiðbandið er ekki komið í\xad Mánagötuna og einhverra hluta vegna treystir Sí\xadminn sér ekki til að áætla hvenær úr því\xad verður bætt.\nOkkur labbakútunum á Breiðbandslausu svæðunum býðst þess í\xad stað að kaupa nettengingu af Sí\xadmanum og fá sjónvarpsþjónustuna í\xad gegnum þá tengingu.\nEn hvernig er það með viðskiptavini annarra sí\xadmafyrirtækja? Eiga þeir einhvern möguleika á að koma inn í\xad þetta kerfi? Eru tæknilegar hindranir í\xad vegi fyrir því\xad eða viðskiptalegar einvörðungu? Spyr sá sem ekki veit.'}
Dataset({
    features: ['text'],
    num_rows: 1305414
})
{'text': 'Snuist í hringi – Um tilgangsleysi allra hluta\nSnuist í hringi\nPosted byStefán\t 28. júní, 2005\nFyrst þegar við Steinunn fórum að ræða um að breyta eldhúsinu á Mánagötunni, lögðum við upp með eina grunnforsendu: við ætluðum að færa vaskinn frá veggnum og koma honum fyrir undir glugganum.\nEftir heilabrot og vangaveltur í\xad marga daga, þar sem fjöldi góðra manna og kvenna hefur komið að verki og gaukað að okkur hugmyndum, virðist niðurstaðan ætla að vera á þann veg að hverjum skáp og eldhústæki hafi verið skákað fram og til baka – en vaskurinn verði á nokkurn veginn upprunalegum stað.\nOkkur sýnist að eftir langt og gott samtal við hönnuð frá húsgagnafyrirtækinu sé loksins komin teikning sem við sættum okkur við.\nHún er ekki fullkomin. Hið fullkomna eldhús kemst tæplega fyrir í\xad eldhúsi sem er 2,4 sinnum 2,6 metrar. Fyrir tugþúsundir í\xad viðbót hefðum við getað stækkað gólfflötinn um kvartfermetra, með því\xad að saga burt strompinn. Jafnvel þótt við hefðum átt þá summu á lausu, er lí\xadtil hætta á að við hefðum látið til leiðast. Hvorugt okkar kærir sig um að hafa stórfé bundið í\xad mublum eða innréttingum.\nEftir stendur að mig sárvantar iðnaðarmenn. Þegar eldhúsinnréttingin verður tilbúin eftir dúk og disk hef ég aðgang að smiðum. Hluta af múrverkinu ættum við sömuleiðis að geta reddað eftir öðrum leiðum. Pí\xadparavinnan er lí\xadtil og dúkaraverkið sömuleiðis. – En stóra málið er rafvirkjavinnan!\nMánagatan 24 er gamalt hús, frá 1939 ef ég man rétt. Tenglar og rofar voru af skornum skammti og seinni tí\xadma viðbætur hafa allar verið gerðar af vanefnum. Á eldhúsinu er ástandið sérstaklega slæmt. Litlar eða engar raflagnir eru í\xad stærstum hluta þess, þar á meðal á stöðum sem við verðum að fá tengla á. Þetta verður helv… hausverkur.\nÞrælarnir á Sky Sport kynntu fyrstu áætlun um beinar útsendingar fram yfir miðjan nóvember. Enginn leikur með Luton á dagskránni. Ég er foxillur.\nBreiðbandið er ekki komið í\xad Mánagötuna og einhverra hluta vegna treystir Sí\xadminn sér ekki til að áætla hvenær úr því\xad verður bætt.\nOkkur labbakútunum á Breiðbandslausu svæðunum býðst þess í\xad stað að kaupa nettengingu af Sí\xadmanum og fá sjónvarpsþjónustuna í\xad gegnum þá tengingu.\nEn hvernig er það með viðskiptavini annarra sí\xadmafyrirtækja? Eiga þeir einhvern möguleika á að koma inn í\xad þetta kerfi? Eru tæknilegar hindranir í\xad vegi fyrir því\xad eða viðskiptalegar einvörðungu? Spyr sá sem ekki veit.'}
Dataset({
    features: ['text'],
    num_rows: 1305414
})
{'text': 'Snuist í hringi – Um tilgangsleysi allra hluta\nSnuist í hringi\nPosted byStefán\t 28. júní, 2005\nFyrst þegar við Steinunn fórum að ræða um að breyta eldhúsinu á Mánagötunni, lögðum við upp með eina grunnforsendu: við ætluðum að færa vaskinn frá veggnum og koma honum fyrir undir glugganum.\nEftir heilabrot og vangaveltur í\xad marga daga, þar sem fjöldi góðra manna og kvenna hefur komið að verki og gaukað að okkur hugmyndum, virðist niðurstaðan ætla að vera á þann veg að hverjum skáp og eldhústæki hafi verið skákað fram og til baka – en vaskurinn verði á nokkurn veginn upprunalegum stað.\nOkkur sýnist að eftir langt og gott samtal við hönnuð frá húsgagnafyrirtækinu sé loksins komin teikning sem við sættum okkur við.\nHún er ekki fullkomin. Hið fullkomna eldhús kemst tæplega fyrir í\xad eldhúsi sem er 2,4 sinnum 2,6 metrar. Fyrir tugþúsundir í\xad viðbót hefðum við getað stækkað gólfflötinn um kvartfermetra, með því\xad að saga burt strompinn. Jafnvel þótt við hefðum átt þá summu á lausu, er lí\xadtil hætta á að við hefðum látið til leiðast. Hvorugt okkar kærir sig um að hafa stórfé bundið í\xad mublum eða innréttingum.\nEftir stendur að mig sárvantar iðnaðarmenn. Þegar eldhúsinnréttingin verður tilbúin eftir dúk og disk hef ég aðgang að smiðum. Hluta af múrverkinu ættum við sömuleiðis að geta reddað eftir öðrum leiðum. Pí\xadparavinnan er lí\xadtil og dúkaraverkið sömuleiðis. – En stóra málið er rafvirkjavinnan!\nMánagatan 24 er gamalt hús, frá 1939 ef ég man rétt. Tenglar og rofar voru af skornum skammti og seinni tí\xadma viðbætur hafa allar verið gerðar af vanefnum. Á eldhúsinu er ástandið sérstaklega slæmt. Litlar eða engar raflagnir eru í\xad stærstum hluta þess, þar á meðal á stöðum sem við verðum að fá tengla á. Þetta verður helv… hausverkur.\nÞrælarnir á Sky Sport kynntu fyrstu áætlun um beinar útsendingar fram yfir miðjan nóvember. Enginn leikur með Luton á dagskránni. Ég er foxillur.\nBreiðbandið er ekki komið í\xad Mánagötuna og einhverra hluta vegna treystir Sí\xadminn sér ekki til að áætla hvenær úr því\xad verður bætt.\nOkkur labbakútunum á Breiðbandslausu svæðunum býðst þess í\xad stað að kaupa nettengingu af Sí\xadmanum og fá sjónvarpsþjónustuna í\xad gegnum þá tengingu.\nEn hvernig er það með viðskiptavini annarra sí\xadmafyrirtækja? Eiga þeir einhvern möguleika á að koma inn í\xad þetta kerfi? Eru tæknilegar hindranir í\xad vegi fyrir því\xad eða viðskiptalegar einvörðungu? Spyr sá sem ekki veit.'}
Dataset({
    features: ['text'],
    num_rows: 1305414
})
{'text': 'Snuist í hringi – Um tilgangsleysi allra hluta\nSnuist í hringi\nPosted byStefán\t 28. júní, 2005\nFyrst þegar við Steinunn fórum að ræða um að breyta eldhúsinu á Mánagötunni, lögðum við upp með eina grunnforsendu: við ætluðum að færa vaskinn frá veggnum og koma honum fyrir undir glugganum.\nEftir heilabrot og vangaveltur í\xad marga daga, þar sem fjöldi góðra manna og kvenna hefur komið að verki og gaukað að okkur hugmyndum, virðist niðurstaðan ætla að vera á þann veg að hverjum skáp og eldhústæki hafi verið skákað fram og til baka – en vaskurinn verði á nokkurn veginn upprunalegum stað.\nOkkur sýnist að eftir langt og gott samtal við hönnuð frá húsgagnafyrirtækinu sé loksins komin teikning sem við sættum okkur við.\nHún er ekki fullkomin. Hið fullkomna eldhús kemst tæplega fyrir í\xad eldhúsi sem er 2,4 sinnum 2,6 metrar. Fyrir tugþúsundir í\xad viðbót hefðum við getað stækkað gólfflötinn um kvartfermetra, með því\xad að saga burt strompinn. Jafnvel þótt við hefðum átt þá summu á lausu, er lí\xadtil hætta á að við hefðum látið til leiðast. Hvorugt okkar kærir sig um að hafa stórfé bundið í\xad mublum eða innréttingum.\nEftir stendur að mig sárvantar iðnaðarmenn. Þegar eldhúsinnréttingin verður tilbúin eftir dúk og disk hef ég aðgang að smiðum. Hluta af múrverkinu ættum við sömuleiðis að geta reddað eftir öðrum leiðum. Pí\xadparavinnan er lí\xadtil og dúkaraverkið sömuleiðis. – En stóra málið er rafvirkjavinnan!\nMánagatan 24 er gamalt hús, frá 1939 ef ég man rétt. Tenglar og rofar voru af skornum skammti og seinni tí\xadma viðbætur hafa allar verið gerðar af vanefnum. Á eldhúsinu er ástandið sérstaklega slæmt. Litlar eða engar raflagnir eru í\xad stærstum hluta þess, þar á meðal á stöðum sem við verðum að fá tengla á. Þetta verður helv… hausverkur.\nÞrælarnir á Sky Sport kynntu fyrstu áætlun um beinar útsendingar fram yfir miðjan nóvember. Enginn leikur með Luton á dagskránni. Ég er foxillur.\nBreiðbandið er ekki komið í\xad Mánagötuna og einhverra hluta vegna treystir Sí\xadminn sér ekki til að áætla hvenær úr því\xad verður bætt.\nOkkur labbakútunum á Breiðbandslausu svæðunum býðst þess í\xad stað að kaupa nettengingu af Sí\xadmanum og fá sjónvarpsþjónustuna í\xad gegnum þá tengingu.\nEn hvernig er það með viðskiptavini annarra sí\xadmafyrirtækja? Eiga þeir einhvern möguleika á að koma inn í\xad þetta kerfi? Eru tæknilegar hindranir í\xad vegi fyrir því\xad eða viðskiptalegar einvörðungu? Spyr sá sem ekki veit.'}
Dataset({
    features: ['text'],
    num_rows: 1305414
})
{'text': 'Snuist í hringi – Um tilgangsleysi allra hluta\nSnuist í hringi\nPosted byStefán\t 28. júní, 2005\nFyrst þegar við Steinunn fórum að ræða um að breyta eldhúsinu á Mánagötunni, lögðum við upp með eina grunnforsendu: við ætluðum að færa vaskinn frá veggnum og koma honum fyrir undir glugganum.\nEftir heilabrot og vangaveltur í\xad marga daga, þar sem fjöldi góðra manna og kvenna hefur komið að verki og gaukað að okkur hugmyndum, virðist niðurstaðan ætla að vera á þann veg að hverjum skáp og eldhústæki hafi verið skákað fram og til baka – en vaskurinn verði á nokkurn veginn upprunalegum stað.\nOkkur sýnist að eftir langt og gott samtal við hönnuð frá húsgagnafyrirtækinu sé loksins komin teikning sem við sættum okkur við.\nHún er ekki fullkomin. Hið fullkomna eldhús kemst tæplega fyrir í\xad eldhúsi sem er 2,4 sinnum 2,6 metrar. Fyrir tugþúsundir í\xad viðbót hefðum við getað stækkað gólfflötinn um kvartfermetra, með því\xad að saga burt strompinn. Jafnvel þótt við hefðum átt þá summu á lausu, er lí\xadtil hætta á að við hefðum látið til leiðast. Hvorugt okkar kærir sig um að hafa stórfé bundið í\xad mublum eða innréttingum.\nEftir stendur að mig sárvantar iðnaðarmenn. Þegar eldhúsinnréttingin verður tilbúin eftir dúk og disk hef ég aðgang að smiðum. Hluta af múrverkinu ættum við sömuleiðis að geta reddað eftir öðrum leiðum. Pí\xadparavinnan er lí\xadtil og dúkaraverkið sömuleiðis. – En stóra málið er rafvirkjavinnan!\nMánagatan 24 er gamalt hús, frá 1939 ef ég man rétt. Tenglar og rofar voru af skornum skammti og seinni tí\xadma viðbætur hafa allar verið gerðar af vanefnum. Á eldhúsinu er ástandið sérstaklega slæmt. Litlar eða engar raflagnir eru í\xad stærstum hluta þess, þar á meðal á stöðum sem við verðum að fá tengla á. Þetta verður helv… hausverkur.\nÞrælarnir á Sky Sport kynntu fyrstu áætlun um beinar útsendingar fram yfir miðjan nóvember. Enginn leikur með Luton á dagskránni. Ég er foxillur.\nBreiðbandið er ekki komið í\xad Mánagötuna og einhverra hluta vegna treystir Sí\xadminn sér ekki til að áætla hvenær úr því\xad verður bætt.\nOkkur labbakútunum á Breiðbandslausu svæðunum býðst þess í\xad stað að kaupa nettengingu af Sí\xadmanum og fá sjónvarpsþjónustuna í\xad gegnum þá tengingu.\nEn hvernig er það með viðskiptavini annarra sí\xadmafyrirtækja? Eiga þeir einhvern möguleika á að koma inn í\xad þetta kerfi? Eru tæknilegar hindranir í\xad vegi fyrir því\xad eða viðskiptalegar einvörðungu? Spyr sá sem ekki veit.'}
05/18/2024 19:29:46 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/hi_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 756931
})
{'text': 'दरिंदे पति को मिले कड़ी से कड़ी सजाOn 7/18/2012 7:05:43 PMChange font size:A | APrintE-mailCommentsRatingBookmark\nइंदौर। पत्नी के साथ दरिंदगी करने वाले पति को गिरफ्तार करने के बाद उसे जेल भेज दिया गया है। दूसरी ओर शहर भर में इस दरिंदगी को लेकर जबरदस्त आक्रोश है। महिला वकीलों का भी कहना है कि इस कृत्य ने अमानवीयता की सारी हदें पार कर दी हैं, इसे तो इतनी कठोर सजा मिलना चाहिए कि दोबारा कोई दरिंदा ऐसा करने की हिम्मत नहीं करे। पुलिस को इस प्रकरण में इतना मजबूत केस बनाना चाहिए कि अमानवीय व्यवहार करने वाले पति को कड़ी से कड़ी सजा मिले।\nउल्लेखनीय है कि इदरिस नगर में रहने वाले सोहन पिता नत्थूसिंह चौहान को जब अपनी पत्नी के चरित्र पर शक हुआ तो उसने पत्नी के जिस्म को गोदकर गुप्तांग पर ताला लगा दिया। करीब साढ़े तीन साल तक महिला इस पीड़ा को झेलती रही। पीड़ित महिला के मुताबिक उसका पति जब उसकी बेटी पर नीयत खराब करने लगा तो वह सहन नहीं कर सकी। उसके बाद उसने जहर खा लिया। अस्पताल पहुंचने के बाद सच्चई सामने आई तो पुलिस के साथ ही डाक्टरों के भी होश उड़ गए। पुलिस ने तत्काल पति को गिरफ्तार कर लिया। तब पति ने कहा कि मेरे परिवार की 9 महिलाएं घर से भाग चुकी हैं। इस कारण मुझे ये डर लगा रहता था कि कहीं मेरी पत्नी भी किसी के साथ भाग नहीं जाए। ताला लगाने के बाद पति चाबी भी अपनी पेंट के पायचे में छिपाकर रखता था।\nमहिला वकीलों में आक्रोश\nवैसे तो शहर भर में ही इस कृत्य के सामने आने के बाद आम लोगों में पति के प्रति जबरदस्त आक्रोश दिखाई दिया। महिला वकीलों का भी कहना था कि इस तरह की हरकत करने वाले को कड़ी से कड़ी सजा मिलना चाहिए।\nउसके साथ भी वही हो\nहाईकोर्ट वकील ज्योति माहेश्वरी कहती हैं कि आरोपी पति ने तो अमानवीयता की सारी हदें पार कर दी हैं। इसके साथ भी वही सलूक होना चाहिए जो इसने पत्नी के साथ किया। इस दरिंदे ने जो किया है उसके लिए तो हर सजा छोटी है। समाज को भी ऐसे लोगों का बहिष्कार कर दिया जाना चाहिए। समाज एक ओर तो कहता है कि महिला-पुरुष एक समान है वहीं इस तरह महिलाओं पर अत्याचार के मामले सामने आते हैं।\nमजबूत केस बनाए\nहाईकोर्ट एडवोकेट अर्चला जोशी के मुताबिक पुलिस मजबूत सबूत और तथ्य जुटाना चाहिए। पुलिस मेडिकल के आधार पर ही धाराएं लगाएगी। इस तरह के कृत्य करने वाले को किसी भी सूरत में माफ नहीं किया जा सकता। अन्य स्थानों पर भी यदि महिला का पति या कोई अन्य पुरुष जुल्म करता है तो इसे किसी भी कीमत पर बर्दाश्त नहीं किया जाना चाहिए। संयोगितागंज पुलिस ने आरोपी सोहन के खिलाफ धारा 498 ए और 326 के तहत प्रकरण दर्ज किया है। महिला की मेडिकल रिपोर्ट अभी नहीं मिली है। इस मामले में हाईकोर्ट एडवोकेट प्रवीण रावल कहते हैं कि पति ने जो किया है वह बेहद शर्मनाक है। पुलिस को इस मामले में मेडिकल इनवेस्टीगेशन पर गंभीरता से ध्यान देना चाहिए।\nMore News शराबी पति से परेशान महिला ने...घूमने गए बिल्डर के घर चोरों ...महिला ने फांसी लगाकर जान दी...वोल्टेज के उतार चढ़ाव से रहव...दहेज लोभी दूल्हा पिता व भाई ...दो घंटे ज्यादा दौड़ेगी आई बस...विधायक और सभापति के प्रचार ब...दुष्कर्म के बाद महिला की गला...विवि परिसर में युवक ने जहर ख...बिचौलिए कमा रहे हैं मुनाफा...बहू की हत्या में जेठ को उम्र...बाथरुम में खुद पर घासलेट डाल...अय्याशी पर रोज उड़ा रहे थे 1...दिखावा बनकर रह गई मुहिम...महिला चोर गैंग को घेराबंदी क...बुजुर्ग ने की बच्ची से अश्ली...दुल्हन के गहने और कपड़े खाक...अब दोपहर में भी चलेगी आई-बस...नर्सिग होम संचालकों को दस दि...5000 पेज का चालान, 90 हजार प...हरिहरन की जादुई आवाज में खो ...'}
05/18/2024 19:29:48 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/hi_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 756931
})
{'text': 'दरिंदे पति को मिले कड़ी से कड़ी सजाOn 7/18/2012 7:05:43 PMChange font size:A | APrintE-mailCommentsRatingBookmark\nइंदौर। पत्नी के साथ दरिंदगी करने वाले पति को गिरफ्तार करने के बाद उसे जेल भेज दिया गया है। दूसरी ओर शहर भर में इस दरिंदगी को लेकर जबरदस्त आक्रोश है। महिला वकीलों का भी कहना है कि इस कृत्य ने अमानवीयता की सारी हदें पार कर दी हैं, इसे तो इतनी कठोर सजा मिलना चाहिए कि दोबारा कोई दरिंदा ऐसा करने की हिम्मत नहीं करे। पुलिस को इस प्रकरण में इतना मजबूत केस बनाना चाहिए कि अमानवीय व्यवहार करने वाले पति को कड़ी से कड़ी सजा मिले।\nउल्लेखनीय है कि इदरिस नगर में रहने वाले सोहन पिता नत्थूसिंह चौहान को जब अपनी पत्नी के चरित्र पर शक हुआ तो उसने पत्नी के जिस्म को गोदकर गुप्तांग पर ताला लगा दिया। करीब साढ़े तीन साल तक महिला इस पीड़ा को झेलती रही। पीड़ित महिला के मुताबिक उसका पति जब उसकी बेटी पर नीयत खराब करने लगा तो वह सहन नहीं कर सकी। उसके बाद उसने जहर खा लिया। अस्पताल पहुंचने के बाद सच्चई सामने आई तो पुलिस के साथ ही डाक्टरों के भी होश उड़ गए। पुलिस ने तत्काल पति को गिरफ्तार कर लिया। तब पति ने कहा कि मेरे परिवार की 9 महिलाएं घर से भाग चुकी हैं। इस कारण मुझे ये डर लगा रहता था कि कहीं मेरी पत्नी भी किसी के साथ भाग नहीं जाए। ताला लगाने के बाद पति चाबी भी अपनी पेंट के पायचे में छिपाकर रखता था।\nमहिला वकीलों में आक्रोश\nवैसे तो शहर भर में ही इस कृत्य के सामने आने के बाद आम लोगों में पति के प्रति जबरदस्त आक्रोश दिखाई दिया। महिला वकीलों का भी कहना था कि इस तरह की हरकत करने वाले को कड़ी से कड़ी सजा मिलना चाहिए।\nउसके साथ भी वही हो\nहाईकोर्ट वकील ज्योति माहेश्वरी कहती हैं कि आरोपी पति ने तो अमानवीयता की सारी हदें पार कर दी हैं। इसके साथ भी वही सलूक होना चाहिए जो इसने पत्नी के साथ किया। इस दरिंदे ने जो किया है उसके लिए तो हर सजा छोटी है। समाज को भी ऐसे लोगों का बहिष्कार कर दिया जाना चाहिए। समाज एक ओर तो कहता है कि महिला-पुरुष एक समान है वहीं इस तरह महिलाओं पर अत्याचार के मामले सामने आते हैं।\nमजबूत केस बनाए\nहाईकोर्ट एडवोकेट अर्चला जोशी के मुताबिक पुलिस मजबूत सबूत और तथ्य जुटाना चाहिए। पुलिस मेडिकल के आधार पर ही धाराएं लगाएगी। इस तरह के कृत्य करने वाले को किसी भी सूरत में माफ नहीं किया जा सकता। अन्य स्थानों पर भी यदि महिला का पति या कोई अन्य पुरुष जुल्म करता है तो इसे किसी भी कीमत पर बर्दाश्त नहीं किया जाना चाहिए। संयोगितागंज पुलिस ने आरोपी सोहन के खिलाफ धारा 498 ए और 326 के तहत प्रकरण दर्ज किया है। महिला की मेडिकल रिपोर्ट अभी नहीं मिली है। इस मामले में हाईकोर्ट एडवोकेट प्रवीण रावल कहते हैं कि पति ने जो किया है वह बेहद शर्मनाक है। पुलिस को इस मामले में मेडिकल इनवेस्टीगेशन पर गंभीरता से ध्यान देना चाहिए।\nMore News शराबी पति से परेशान महिला ने...घूमने गए बिल्डर के घर चोरों ...महिला ने फांसी लगाकर जान दी...वोल्टेज के उतार चढ़ाव से रहव...दहेज लोभी दूल्हा पिता व भाई ...दो घंटे ज्यादा दौड़ेगी आई बस...विधायक और सभापति के प्रचार ब...दुष्कर्म के बाद महिला की गला...विवि परिसर में युवक ने जहर ख...बिचौलिए कमा रहे हैं मुनाफा...बहू की हत्या में जेठ को उम्र...बाथरुम में खुद पर घासलेट डाल...अय्याशी पर रोज उड़ा रहे थे 1...दिखावा बनकर रह गई मुहिम...महिला चोर गैंग को घेराबंदी क...बुजुर्ग ने की बच्ची से अश्ली...दुल्हन के गहने और कपड़े खाक...अब दोपहर में भी चलेगी आई-बस...नर्सिग होम संचालकों को दस दि...5000 पेज का चालान, 90 हजार प...हरिहरन की जादुई आवाज में खो ...'}
05/18/2024 19:29:49 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/hi_2b.jsonl.
05/18/2024 19:29:49 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/hi_2b.jsonl.
05/18/2024 19:29:50 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/hi_2b.jsonl.
05/18/2024 19:29:50 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/hi_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 756931
})
{'text': 'दरिंदे पति को मिले कड़ी से कड़ी सजाOn 7/18/2012 7:05:43 PMChange font size:A | APrintE-mailCommentsRatingBookmark\nइंदौर। पत्नी के साथ दरिंदगी करने वाले पति को गिरफ्तार करने के बाद उसे जेल भेज दिया गया है। दूसरी ओर शहर भर में इस दरिंदगी को लेकर जबरदस्त आक्रोश है। महिला वकीलों का भी कहना है कि इस कृत्य ने अमानवीयता की सारी हदें पार कर दी हैं, इसे तो इतनी कठोर सजा मिलना चाहिए कि दोबारा कोई दरिंदा ऐसा करने की हिम्मत नहीं करे। पुलिस को इस प्रकरण में इतना मजबूत केस बनाना चाहिए कि अमानवीय व्यवहार करने वाले पति को कड़ी से कड़ी सजा मिले।\nउल्लेखनीय है कि इदरिस नगर में रहने वाले सोहन पिता नत्थूसिंह चौहान को जब अपनी पत्नी के चरित्र पर शक हुआ तो उसने पत्नी के जिस्म को गोदकर गुप्तांग पर ताला लगा दिया। करीब साढ़े तीन साल तक महिला इस पीड़ा को झेलती रही। पीड़ित महिला के मुताबिक उसका पति जब उसकी बेटी पर नीयत खराब करने लगा तो वह सहन नहीं कर सकी। उसके बाद उसने जहर खा लिया। अस्पताल पहुंचने के बाद सच्चई सामने आई तो पुलिस के साथ ही डाक्टरों के भी होश उड़ गए। पुलिस ने तत्काल पति को गिरफ्तार कर लिया। तब पति ने कहा कि मेरे परिवार की 9 महिलाएं घर से भाग चुकी हैं। इस कारण मुझे ये डर लगा रहता था कि कहीं मेरी पत्नी भी किसी के साथ भाग नहीं जाए। ताला लगाने के बाद पति चाबी भी अपनी पेंट के पायचे में छिपाकर रखता था।\nमहिला वकीलों में आक्रोश\nवैसे तो शहर भर में ही इस कृत्य के सामने आने के बाद आम लोगों में पति के प्रति जबरदस्त आक्रोश दिखाई दिया। महिला वकीलों का भी कहना था कि इस तरह की हरकत करने वाले को कड़ी से कड़ी सजा मिलना चाहिए।\nउसके साथ भी वही हो\nहाईकोर्ट वकील ज्योति माहेश्वरी कहती हैं कि आरोपी पति ने तो अमानवीयता की सारी हदें पार कर दी हैं। इसके साथ भी वही सलूक होना चाहिए जो इसने पत्नी के साथ किया। इस दरिंदे ने जो किया है उसके लिए तो हर सजा छोटी है। समाज को भी ऐसे लोगों का बहिष्कार कर दिया जाना चाहिए। समाज एक ओर तो कहता है कि महिला-पुरुष एक समान है वहीं इस तरह महिलाओं पर अत्याचार के मामले सामने आते हैं।\nमजबूत केस बनाए\nहाईकोर्ट एडवोकेट अर्चला जोशी के मुताबिक पुलिस मजबूत सबूत और तथ्य जुटाना चाहिए। पुलिस मेडिकल के आधार पर ही धाराएं लगाएगी। इस तरह के कृत्य करने वाले को किसी भी सूरत में माफ नहीं किया जा सकता। अन्य स्थानों पर भी यदि महिला का पति या कोई अन्य पुरुष जुल्म करता है तो इसे किसी भी कीमत पर बर्दाश्त नहीं किया जाना चाहिए। संयोगितागंज पुलिस ने आरोपी सोहन के खिलाफ धारा 498 ए और 326 के तहत प्रकरण दर्ज किया है। महिला की मेडिकल रिपोर्ट अभी नहीं मिली है। इस मामले में हाईकोर्ट एडवोकेट प्रवीण रावल कहते हैं कि पति ने जो किया है वह बेहद शर्मनाक है। पुलिस को इस मामले में मेडिकल इनवेस्टीगेशन पर गंभीरता से ध्यान देना चाहिए।\nMore News शराबी पति से परेशान महिला ने...घूमने गए बिल्डर के घर चोरों ...महिला ने फांसी लगाकर जान दी...वोल्टेज के उतार चढ़ाव से रहव...दहेज लोभी दूल्हा पिता व भाई ...दो घंटे ज्यादा दौड़ेगी आई बस...विधायक और सभापति के प्रचार ब...दुष्कर्म के बाद महिला की गला...विवि परिसर में युवक ने जहर ख...बिचौलिए कमा रहे हैं मुनाफा...बहू की हत्या में जेठ को उम्र...बाथरुम में खुद पर घासलेट डाल...अय्याशी पर रोज उड़ा रहे थे 1...दिखावा बनकर रह गई मुहिम...महिला चोर गैंग को घेराबंदी क...बुजुर्ग ने की बच्ची से अश्ली...दुल्हन के गहने और कपड़े खाक...अब दोपहर में भी चलेगी आई-बस...नर्सिग होम संचालकों को दस दि...5000 पेज का चालान, 90 हजार प...हरिहरन की जादुई आवाज में खो ...'}
Dataset({
    features: ['text'],
    num_rows: 756931
})
{'text': 'दरिंदे पति को मिले कड़ी से कड़ी सजाOn 7/18/2012 7:05:43 PMChange font size:A | APrintE-mailCommentsRatingBookmark\nइंदौर। पत्नी के साथ दरिंदगी करने वाले पति को गिरफ्तार करने के बाद उसे जेल भेज दिया गया है। दूसरी ओर शहर भर में इस दरिंदगी को लेकर जबरदस्त आक्रोश है। महिला वकीलों का भी कहना है कि इस कृत्य ने अमानवीयता की सारी हदें पार कर दी हैं, इसे तो इतनी कठोर सजा मिलना चाहिए कि दोबारा कोई दरिंदा ऐसा करने की हिम्मत नहीं करे। पुलिस को इस प्रकरण में इतना मजबूत केस बनाना चाहिए कि अमानवीय व्यवहार करने वाले पति को कड़ी से कड़ी सजा मिले।\nउल्लेखनीय है कि इदरिस नगर में रहने वाले सोहन पिता नत्थूसिंह चौहान को जब अपनी पत्नी के चरित्र पर शक हुआ तो उसने पत्नी के जिस्म को गोदकर गुप्तांग पर ताला लगा दिया। करीब साढ़े तीन साल तक महिला इस पीड़ा को झेलती रही। पीड़ित महिला के मुताबिक उसका पति जब उसकी बेटी पर नीयत खराब करने लगा तो वह सहन नहीं कर सकी। उसके बाद उसने जहर खा लिया। अस्पताल पहुंचने के बाद सच्चई सामने आई तो पुलिस के साथ ही डाक्टरों के भी होश उड़ गए। पुलिस ने तत्काल पति को गिरफ्तार कर लिया। तब पति ने कहा कि मेरे परिवार की 9 महिलाएं घर से भाग चुकी हैं। इस कारण मुझे ये डर लगा रहता था कि कहीं मेरी पत्नी भी किसी के साथ भाग नहीं जाए। ताला लगाने के बाद पति चाबी भी अपनी पेंट के पायचे में छिपाकर रखता था।\nमहिला वकीलों में आक्रोश\nवैसे तो शहर भर में ही इस कृत्य के सामने आने के बाद आम लोगों में पति के प्रति जबरदस्त आक्रोश दिखाई दिया। महिला वकीलों का भी कहना था कि इस तरह की हरकत करने वाले को कड़ी से कड़ी सजा मिलना चाहिए।\nउसके साथ भी वही हो\nहाईकोर्ट वकील ज्योति माहेश्वरी कहती हैं कि आरोपी पति ने तो अमानवीयता की सारी हदें पार कर दी हैं। इसके साथ भी वही सलूक होना चाहिए जो इसने पत्नी के साथ किया। इस दरिंदे ने जो किया है उसके लिए तो हर सजा छोटी है। समाज को भी ऐसे लोगों का बहिष्कार कर दिया जाना चाहिए। समाज एक ओर तो कहता है कि महिला-पुरुष एक समान है वहीं इस तरह महिलाओं पर अत्याचार के मामले सामने आते हैं।\nमजबूत केस बनाए\nहाईकोर्ट एडवोकेट अर्चला जोशी के मुताबिक पुलिस मजबूत सबूत और तथ्य जुटाना चाहिए। पुलिस मेडिकल के आधार पर ही धाराएं लगाएगी। इस तरह के कृत्य करने वाले को किसी भी सूरत में माफ नहीं किया जा सकता। अन्य स्थानों पर भी यदि महिला का पति या कोई अन्य पुरुष जुल्म करता है तो इसे किसी भी कीमत पर बर्दाश्त नहीं किया जाना चाहिए। संयोगितागंज पुलिस ने आरोपी सोहन के खिलाफ धारा 498 ए और 326 के तहत प्रकरण दर्ज किया है। महिला की मेडिकल रिपोर्ट अभी नहीं मिली है। इस मामले में हाईकोर्ट एडवोकेट प्रवीण रावल कहते हैं कि पति ने जो किया है वह बेहद शर्मनाक है। पुलिस को इस मामले में मेडिकल इनवेस्टीगेशन पर गंभीरता से ध्यान देना चाहिए।\nMore News शराबी पति से परेशान महिला ने...घूमने गए बिल्डर के घर चोरों ...महिला ने फांसी लगाकर जान दी...वोल्टेज के उतार चढ़ाव से रहव...दहेज लोभी दूल्हा पिता व भाई ...दो घंटे ज्यादा दौड़ेगी आई बस...विधायक और सभापति के प्रचार ब...दुष्कर्म के बाद महिला की गला...विवि परिसर में युवक ने जहर ख...बिचौलिए कमा रहे हैं मुनाफा...बहू की हत्या में जेठ को उम्र...बाथरुम में खुद पर घासलेट डाल...अय्याशी पर रोज उड़ा रहे थे 1...दिखावा बनकर रह गई मुहिम...महिला चोर गैंग को घेराबंदी क...बुजुर्ग ने की बच्ची से अश्ली...दुल्हन के गहने और कपड़े खाक...अब दोपहर में भी चलेगी आई-बस...नर्सिग होम संचालकों को दस दि...5000 पेज का चालान, 90 हजार प...हरिहरन की जादुई आवाज में खो ...'}
Dataset({
    features: ['text'],
    num_rows: 756931
})
{'text': 'दरिंदे पति को मिले कड़ी से कड़ी सजाOn 7/18/2012 7:05:43 PMChange font size:A | APrintE-mailCommentsRatingBookmark\nइंदौर। पत्नी के साथ दरिंदगी करने वाले पति को गिरफ्तार करने के बाद उसे जेल भेज दिया गया है। दूसरी ओर शहर भर में इस दरिंदगी को लेकर जबरदस्त आक्रोश है। महिला वकीलों का भी कहना है कि इस कृत्य ने अमानवीयता की सारी हदें पार कर दी हैं, इसे तो इतनी कठोर सजा मिलना चाहिए कि दोबारा कोई दरिंदा ऐसा करने की हिम्मत नहीं करे। पुलिस को इस प्रकरण में इतना मजबूत केस बनाना चाहिए कि अमानवीय व्यवहार करने वाले पति को कड़ी से कड़ी सजा मिले।\nउल्लेखनीय है कि इदरिस नगर में रहने वाले सोहन पिता नत्थूसिंह चौहान को जब अपनी पत्नी के चरित्र पर शक हुआ तो उसने पत्नी के जिस्म को गोदकर गुप्तांग पर ताला लगा दिया। करीब साढ़े तीन साल तक महिला इस पीड़ा को झेलती रही। पीड़ित महिला के मुताबिक उसका पति जब उसकी बेटी पर नीयत खराब करने लगा तो वह सहन नहीं कर सकी। उसके बाद उसने जहर खा लिया। अस्पताल पहुंचने के बाद सच्चई सामने आई तो पुलिस के साथ ही डाक्टरों के भी होश उड़ गए। पुलिस ने तत्काल पति को गिरफ्तार कर लिया। तब पति ने कहा कि मेरे परिवार की 9 महिलाएं घर से भाग चुकी हैं। इस कारण मुझे ये डर लगा रहता था कि कहीं मेरी पत्नी भी किसी के साथ भाग नहीं जाए। ताला लगाने के बाद पति चाबी भी अपनी पेंट के पायचे में छिपाकर रखता था।\nमहिला वकीलों में आक्रोश\nवैसे तो शहर भर में ही इस कृत्य के सामने आने के बाद आम लोगों में पति के प्रति जबरदस्त आक्रोश दिखाई दिया। महिला वकीलों का भी कहना था कि इस तरह की हरकत करने वाले को कड़ी से कड़ी सजा मिलना चाहिए।\nउसके साथ भी वही हो\nहाईकोर्ट वकील ज्योति माहेश्वरी कहती हैं कि आरोपी पति ने तो अमानवीयता की सारी हदें पार कर दी हैं। इसके साथ भी वही सलूक होना चाहिए जो इसने पत्नी के साथ किया। इस दरिंदे ने जो किया है उसके लिए तो हर सजा छोटी है। समाज को भी ऐसे लोगों का बहिष्कार कर दिया जाना चाहिए। समाज एक ओर तो कहता है कि महिला-पुरुष एक समान है वहीं इस तरह महिलाओं पर अत्याचार के मामले सामने आते हैं।\nमजबूत केस बनाए\nहाईकोर्ट एडवोकेट अर्चला जोशी के मुताबिक पुलिस मजबूत सबूत और तथ्य जुटाना चाहिए। पुलिस मेडिकल के आधार पर ही धाराएं लगाएगी। इस तरह के कृत्य करने वाले को किसी भी सूरत में माफ नहीं किया जा सकता। अन्य स्थानों पर भी यदि महिला का पति या कोई अन्य पुरुष जुल्म करता है तो इसे किसी भी कीमत पर बर्दाश्त नहीं किया जाना चाहिए। संयोगितागंज पुलिस ने आरोपी सोहन के खिलाफ धारा 498 ए और 326 के तहत प्रकरण दर्ज किया है। महिला की मेडिकल रिपोर्ट अभी नहीं मिली है। इस मामले में हाईकोर्ट एडवोकेट प्रवीण रावल कहते हैं कि पति ने जो किया है वह बेहद शर्मनाक है। पुलिस को इस मामले में मेडिकल इनवेस्टीगेशन पर गंभीरता से ध्यान देना चाहिए।\nMore News शराबी पति से परेशान महिला ने...घूमने गए बिल्डर के घर चोरों ...महिला ने फांसी लगाकर जान दी...वोल्टेज के उतार चढ़ाव से रहव...दहेज लोभी दूल्हा पिता व भाई ...दो घंटे ज्यादा दौड़ेगी आई बस...विधायक और सभापति के प्रचार ब...दुष्कर्म के बाद महिला की गला...विवि परिसर में युवक ने जहर ख...बिचौलिए कमा रहे हैं मुनाफा...बहू की हत्या में जेठ को उम्र...बाथरुम में खुद पर घासलेट डाल...अय्याशी पर रोज उड़ा रहे थे 1...दिखावा बनकर रह गई मुहिम...महिला चोर गैंग को घेराबंदी क...बुजुर्ग ने की बच्ची से अश्ली...दुल्हन के गहने और कपड़े खाक...अब दोपहर में भी चलेगी आई-बस...नर्सिग होम संचालकों को दस दि...5000 पेज का चालान, 90 हजार प...हरिहरन की जादुई आवाज में खो ...'}
Dataset({
    features: ['text'],
    num_rows: 756931
})
{'text': 'दरिंदे पति को मिले कड़ी से कड़ी सजाOn 7/18/2012 7:05:43 PMChange font size:A | APrintE-mailCommentsRatingBookmark\nइंदौर। पत्नी के साथ दरिंदगी करने वाले पति को गिरफ्तार करने के बाद उसे जेल भेज दिया गया है। दूसरी ओर शहर भर में इस दरिंदगी को लेकर जबरदस्त आक्रोश है। महिला वकीलों का भी कहना है कि इस कृत्य ने अमानवीयता की सारी हदें पार कर दी हैं, इसे तो इतनी कठोर सजा मिलना चाहिए कि दोबारा कोई दरिंदा ऐसा करने की हिम्मत नहीं करे। पुलिस को इस प्रकरण में इतना मजबूत केस बनाना चाहिए कि अमानवीय व्यवहार करने वाले पति को कड़ी से कड़ी सजा मिले।\nउल्लेखनीय है कि इदरिस नगर में रहने वाले सोहन पिता नत्थूसिंह चौहान को जब अपनी पत्नी के चरित्र पर शक हुआ तो उसने पत्नी के जिस्म को गोदकर गुप्तांग पर ताला लगा दिया। करीब साढ़े तीन साल तक महिला इस पीड़ा को झेलती रही। पीड़ित महिला के मुताबिक उसका पति जब उसकी बेटी पर नीयत खराब करने लगा तो वह सहन नहीं कर सकी। उसके बाद उसने जहर खा लिया। अस्पताल पहुंचने के बाद सच्चई सामने आई तो पुलिस के साथ ही डाक्टरों के भी होश उड़ गए। पुलिस ने तत्काल पति को गिरफ्तार कर लिया। तब पति ने कहा कि मेरे परिवार की 9 महिलाएं घर से भाग चुकी हैं। इस कारण मुझे ये डर लगा रहता था कि कहीं मेरी पत्नी भी किसी के साथ भाग नहीं जाए। ताला लगाने के बाद पति चाबी भी अपनी पेंट के पायचे में छिपाकर रखता था।\nमहिला वकीलों में आक्रोश\nवैसे तो शहर भर में ही इस कृत्य के सामने आने के बाद आम लोगों में पति के प्रति जबरदस्त आक्रोश दिखाई दिया। महिला वकीलों का भी कहना था कि इस तरह की हरकत करने वाले को कड़ी से कड़ी सजा मिलना चाहिए।\nउसके साथ भी वही हो\nहाईकोर्ट वकील ज्योति माहेश्वरी कहती हैं कि आरोपी पति ने तो अमानवीयता की सारी हदें पार कर दी हैं। इसके साथ भी वही सलूक होना चाहिए जो इसने पत्नी के साथ किया। इस दरिंदे ने जो किया है उसके लिए तो हर सजा छोटी है। समाज को भी ऐसे लोगों का बहिष्कार कर दिया जाना चाहिए। समाज एक ओर तो कहता है कि महिला-पुरुष एक समान है वहीं इस तरह महिलाओं पर अत्याचार के मामले सामने आते हैं।\nमजबूत केस बनाए\nहाईकोर्ट एडवोकेट अर्चला जोशी के मुताबिक पुलिस मजबूत सबूत और तथ्य जुटाना चाहिए। पुलिस मेडिकल के आधार पर ही धाराएं लगाएगी। इस तरह के कृत्य करने वाले को किसी भी सूरत में माफ नहीं किया जा सकता। अन्य स्थानों पर भी यदि महिला का पति या कोई अन्य पुरुष जुल्म करता है तो इसे किसी भी कीमत पर बर्दाश्त नहीं किया जाना चाहिए। संयोगितागंज पुलिस ने आरोपी सोहन के खिलाफ धारा 498 ए और 326 के तहत प्रकरण दर्ज किया है। महिला की मेडिकल रिपोर्ट अभी नहीं मिली है। इस मामले में हाईकोर्ट एडवोकेट प्रवीण रावल कहते हैं कि पति ने जो किया है वह बेहद शर्मनाक है। पुलिस को इस मामले में मेडिकल इनवेस्टीगेशन पर गंभीरता से ध्यान देना चाहिए।\nMore News शराबी पति से परेशान महिला ने...घूमने गए बिल्डर के घर चोरों ...महिला ने फांसी लगाकर जान दी...वोल्टेज के उतार चढ़ाव से रहव...दहेज लोभी दूल्हा पिता व भाई ...दो घंटे ज्यादा दौड़ेगी आई बस...विधायक और सभापति के प्रचार ब...दुष्कर्म के बाद महिला की गला...विवि परिसर में युवक ने जहर ख...बिचौलिए कमा रहे हैं मुनाफा...बहू की हत्या में जेठ को उम्र...बाथरुम में खुद पर घासलेट डाल...अय्याशी पर रोज उड़ा रहे थे 1...दिखावा बनकर रह गई मुहिम...महिला चोर गैंग को घेराबंदी क...बुजुर्ग ने की बच्ची से अश्ली...दुल्हन के गहने और कपड़े खाक...अब दोपहर में भी चलेगी आई-बस...नर्सिग होम संचालकों को दस दि...5000 पेज का चालान, 90 हजार प...हरिहरन की जादुई आवाज में खो ...'}
05/18/2024 19:29:51 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/hi_2b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 756931
})
{'text': 'दरिंदे पति को मिले कड़ी से कड़ी सजाOn 7/18/2012 7:05:43 PMChange font size:A | APrintE-mailCommentsRatingBookmark\nइंदौर। पत्नी के साथ दरिंदगी करने वाले पति को गिरफ्तार करने के बाद उसे जेल भेज दिया गया है। दूसरी ओर शहर भर में इस दरिंदगी को लेकर जबरदस्त आक्रोश है। महिला वकीलों का भी कहना है कि इस कृत्य ने अमानवीयता की सारी हदें पार कर दी हैं, इसे तो इतनी कठोर सजा मिलना चाहिए कि दोबारा कोई दरिंदा ऐसा करने की हिम्मत नहीं करे। पुलिस को इस प्रकरण में इतना मजबूत केस बनाना चाहिए कि अमानवीय व्यवहार करने वाले पति को कड़ी से कड़ी सजा मिले।\nउल्लेखनीय है कि इदरिस नगर में रहने वाले सोहन पिता नत्थूसिंह चौहान को जब अपनी पत्नी के चरित्र पर शक हुआ तो उसने पत्नी के जिस्म को गोदकर गुप्तांग पर ताला लगा दिया। करीब साढ़े तीन साल तक महिला इस पीड़ा को झेलती रही। पीड़ित महिला के मुताबिक उसका पति जब उसकी बेटी पर नीयत खराब करने लगा तो वह सहन नहीं कर सकी। उसके बाद उसने जहर खा लिया। अस्पताल पहुंचने के बाद सच्चई सामने आई तो पुलिस के साथ ही डाक्टरों के भी होश उड़ गए। पुलिस ने तत्काल पति को गिरफ्तार कर लिया। तब पति ने कहा कि मेरे परिवार की 9 महिलाएं घर से भाग चुकी हैं। इस कारण मुझे ये डर लगा रहता था कि कहीं मेरी पत्नी भी किसी के साथ भाग नहीं जाए। ताला लगाने के बाद पति चाबी भी अपनी पेंट के पायचे में छिपाकर रखता था।\nमहिला वकीलों में आक्रोश\nवैसे तो शहर भर में ही इस कृत्य के सामने आने के बाद आम लोगों में पति के प्रति जबरदस्त आक्रोश दिखाई दिया। महिला वकीलों का भी कहना था कि इस तरह की हरकत करने वाले को कड़ी से कड़ी सजा मिलना चाहिए।\nउसके साथ भी वही हो\nहाईकोर्ट वकील ज्योति माहेश्वरी कहती हैं कि आरोपी पति ने तो अमानवीयता की सारी हदें पार कर दी हैं। इसके साथ भी वही सलूक होना चाहिए जो इसने पत्नी के साथ किया। इस दरिंदे ने जो किया है उसके लिए तो हर सजा छोटी है। समाज को भी ऐसे लोगों का बहिष्कार कर दिया जाना चाहिए। समाज एक ओर तो कहता है कि महिला-पुरुष एक समान है वहीं इस तरह महिलाओं पर अत्याचार के मामले सामने आते हैं।\nमजबूत केस बनाए\nहाईकोर्ट एडवोकेट अर्चला जोशी के मुताबिक पुलिस मजबूत सबूत और तथ्य जुटाना चाहिए। पुलिस मेडिकल के आधार पर ही धाराएं लगाएगी। इस तरह के कृत्य करने वाले को किसी भी सूरत में माफ नहीं किया जा सकता। अन्य स्थानों पर भी यदि महिला का पति या कोई अन्य पुरुष जुल्म करता है तो इसे किसी भी कीमत पर बर्दाश्त नहीं किया जाना चाहिए। संयोगितागंज पुलिस ने आरोपी सोहन के खिलाफ धारा 498 ए और 326 के तहत प्रकरण दर्ज किया है। महिला की मेडिकल रिपोर्ट अभी नहीं मिली है। इस मामले में हाईकोर्ट एडवोकेट प्रवीण रावल कहते हैं कि पति ने जो किया है वह बेहद शर्मनाक है। पुलिस को इस मामले में मेडिकल इनवेस्टीगेशन पर गंभीरता से ध्यान देना चाहिए।\nMore News शराबी पति से परेशान महिला ने...घूमने गए बिल्डर के घर चोरों ...महिला ने फांसी लगाकर जान दी...वोल्टेज के उतार चढ़ाव से रहव...दहेज लोभी दूल्हा पिता व भाई ...दो घंटे ज्यादा दौड़ेगी आई बस...विधायक और सभापति के प्रचार ब...दुष्कर्म के बाद महिला की गला...विवि परिसर में युवक ने जहर ख...बिचौलिए कमा रहे हैं मुनाफा...बहू की हत्या में जेठ को उम्र...बाथरुम में खुद पर घासलेट डाल...अय्याशी पर रोज उड़ा रहे थे 1...दिखावा बनकर रह गई मुहिम...महिला चोर गैंग को घेराबंदी क...बुजुर्ग ने की बच्ची से अश्ली...दुल्हन के गहने और कपड़े खाक...अब दोपहर में भी चलेगी आई-बस...नर्सिग होम संचालकों को दस दि...5000 पेज का चालान, 90 हजार प...हरिहरन की जादुई आवाज में खो ...'}
Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-633ec56fb30fb8ad/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-02f22e3248872ee0.arrow
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 7826410
})
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): Qwen2ForCausalMoeLM(
      (model): Qwen2Model(
        (embed_tokens): Embedding(151936, 2048)
        (layers): ModuleList(
          (0): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (1): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (2): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (3): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (4): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (5): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (6): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (7): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (8): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (9): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (10): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (11): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (12): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (13): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (14): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (15): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (16): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (17): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (18): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (19): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (20): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (21): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (22): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (23): Qwen2DecoderLayer(
            (self_attn): Qwen2FlashAttention2(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=512, out_features=4, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (1): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (2): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                  (3): FFN(
                    (gate_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=512, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=512, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
              (moe_mh_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
              (moe_merge_layer): ModuleDict(
                (default): Linear(in_features=2048, out_features=2048, bias=True)
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
        )
        (norm): Qwen2RMSNorm()
      )
      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
    )
  )
)
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:586] 2024-05-18 19:29:53,160 >> Using auto half precision backend
[2024-05-18 19:29:53,422] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 7826410
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 7826410
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 7826410
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 7826410
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 7826410
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 7826410
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 7826410
})
[2024-05-18 19:30:20,958] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-18 19:30:20,960] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-05-18 19:30:20,960] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-18 19:30:20,984] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-05-18 19:30:20,984] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-05-18 19:30:20,984] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-05-18 19:30:20,985] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-05-18 19:30:20,985] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-05-18 19:30:20,985] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: False
[2024-05-18 19:30:20,985] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-05-18 19:30:35,952] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-05-18 19:30:35,953] [INFO] [utils.py:803:see_memory_usage] MA 6.19 GB         Max_MA 6.19 GB         CA 6.48 GB         Max_CA 6 GB 
[2024-05-18 19:30:35,953] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 164.69 GB, percent = 16.3%
[2024-05-18 19:30:36,123] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-05-18 19:30:36,123] [INFO] [utils.py:803:see_memory_usage] MA 7.14 GB         Max_MA 8.56 GB         CA 8.84 GB         Max_CA 9 GB 
[2024-05-18 19:30:36,124] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 161.43 GB, percent = 16.0%
[2024-05-18 19:30:36,124] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-05-18 19:30:36,284] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-05-18 19:30:36,285] [INFO] [utils.py:803:see_memory_usage] MA 7.14 GB         Max_MA 7.14 GB         CA 8.84 GB         Max_CA 9 GB 
[2024-05-18 19:30:36,285] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 161.69 GB, percent = 16.0%
[2024-05-18 19:30:36,287] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-05-18 19:30:36,288] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-05-18 19:30:36,288] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-05-18 19:30:36,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
[2024-05-18 19:30:36,289] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-05-18 19:30:36,290] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-05-18 19:30:36,290] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-05-18 19:30:36,290] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-05-18 19:30:36,290] [INFO] [config.py:978:print]   amp_params ................... False
[2024-05-18 19:30:36,290] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-05-18 19:30:36,290] [INFO] [config.py:978:print]   bfloat16_enabled ............. True
[2024-05-18 19:30:36,290] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-05-18 19:30:36,290] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-05-18 19:30:36,290] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ef9d3d23220>
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   dump_state ................... False
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... None
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-05-18 19:30:36,291] [INFO] [config.py:978:print]   fp16_auto_cast ............... None
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   fp16_enabled ................. False
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 16
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 1
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   loss_scale ................... 1.0
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-05-18 19:30:36,292] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   pld_params ................... False
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   train_batch_size ............. 1024
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  8
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   world_size ................... 8
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-05-18 19:30:36,293] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-05-18 19:30:36,294] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 1.024000e+03, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 16, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1747] 2024-05-18 19:30:36,294 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-05-18 19:30:36,294 >>   Num examples = 7,826,410
[INFO|trainer.py:1749] 2024-05-18 19:30:36,294 >>   Num Epochs = 1
[INFO|trainer.py:1750] 2024-05-18 19:30:36,294 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-05-18 19:30:36,294 >>   Total train batch size (w. parallel, distributed & accumulation) = 1,024
[INFO|trainer.py:1754] 2024-05-18 19:30:36,294 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1755] 2024-05-18 19:30:36,294 >>   Total optimization steps = 7,643
[INFO|trainer.py:1756] 2024-05-18 19:30:36,296 >>   Number of trainable parameters = 1,013,071,872
[INFO|integration_utils.py:722] 2024-05-18 19:30:36,299 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: texi. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /mnt/vol1/huangxin/nanda/LLaMA-Factory/wandb/run-20240518_193038-dma4ofz4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-pyramid-2
wandb: ⭐️ View project at https://wandb.ai/texi/mh_moe_ardeishi8b
wandb: 🚀 View run at https://wandb.ai/texi/mh_moe_ardeishi8b/runs/dma4ofz4/workspace
05/18/2024 19:30:46 - WARNING - llmtuner.extras.callbacks - Previous log file in this folder will be deleted.
  0%|          | 0/7643 [00:00<?, ?it/s]  0%|          | 1/7643 [00:15<32:33:09, 15.33s/it]