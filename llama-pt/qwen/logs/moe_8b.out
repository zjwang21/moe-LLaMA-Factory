[2024-04-16 20:32:06,752] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-16 20:32:09,978] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-16 20:32:10,002] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/wzjsz/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/wangzj/LLaMA-Factory/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/nfs02/wangzj/models/Qwen1.5-1.8B --do_train --flash_attn --dataset de_2b,ar_2b,hi_2b,is_2b,slimpajam_1b --en_max_samples 10000 --preprocessing_num_workers 16 --mix_strategy concat --cutoff_len 1024 --cache_path /home/nfs03/wangzj/dataset/pretrain/ardehiis8b-en1w --finetuning_type moe --moe_every_k_layers 1 --moe_router_type top1 --moe_num_experts 2 --output_dir /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-ardehiis8ben1w-moe --overwrite_output_dir --per_device_train_batch_size 16 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 1 --save_only_model --save_steps 1000 --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-04-16 20:32:11,809] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-16 20:32:14,750] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-04-16 20:32:14,750] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-16 20:32:14,750] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-16 20:32:14,750] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-16 20:32:14,750] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
moe.sh: line 31: 59344 Killed                  deepspeed --num_gpus 4 --master_port=9902 src/train_bash.py --deepspeed /home/wangzj/LLaMA-Factory/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/nfs02/wangzj/models/Qwen1.5-1.8B --do_train --flash_attn --dataset de_2b,ar_2b,hi_2b,is_2b,slimpajam_1b --en_max_samples 10000 --preprocessing_num_workers 16 --mix_strategy concat --cutoff_len 1024 --cache_path /home/nfs03/wangzj/dataset/pretrain/ardehiis8b-en1w --finetuning_type moe --moe_every_k_layers 1 --moe_router_type top1 --moe_num_experts 2 --output_dir /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-ardehiis8ben1w-moe --overwrite_output_dir --per_device_train_batch_size 16 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 1 --save_only_model --save_steps 1000 --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
