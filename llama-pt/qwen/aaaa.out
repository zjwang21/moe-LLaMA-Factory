[2024-06-27 23:19:42,030] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:19:45,514] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-06-27 23:19:45,537] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/wzjsz/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/nfs04/wangzj/models/Qwen1.5-1.8B --adapter_name_or_path /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe --train_only_router --moe_router_type top2 --topk 2 --ce_loss_coef 0.05 --do_train --dataset ar_2b,de_2b --max_samples 50000 --preprocessing_num_workers 1 --cutoff_len 512 --finetuning_type moe --output_dir /home/nfs04/wangzj/checkpoints/moe/test --overwrite_output_dir --per_device_train_batch_size 8 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 100 --save_steps 10000 --save_only_model --learning_rate 2e-4 --num_train_epochs 1.0 --plot_loss --fp16
[2024-06-27 23:19:47,147] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:19:49,363] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-06-27 23:19:49,363] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-06-27 23:19:49,363] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-06-27 23:19:49,363] [INFO] [launch.py:163:main] dist_world_size=4
[2024-06-27 23:19:49,363] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-06-27 23:19:56,493] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:19:56,495] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:19:56,495] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:19:56,496] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-27 23:20:04,805] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-27 23:20:04,805] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-06-27 23:20:04,806] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-27 23:20:04,807] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-27 23:20:04,807] [INFO] [comm.py:637:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
06/27/2024 23:20:04 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
06/27/2024 23:20:04 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
06/27/2024 23:20:04 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
06/27/2024 23:20:04 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
06/27/2024 23:20:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-20-04_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
06/27/2024 23:20:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-20-04_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
06/27/2024 23:20:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-20-04_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
06/27/2024 23:20:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-20-04_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:20:04,836 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:20:04,837 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:20:04,837 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:20:04,837 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:20:04,837 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-06-27 23:20:04,837 >> loading file tokenizer.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-06-27 23:20:05,107 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:727] 2024-06-27 23:20:05,108 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/config.json
[INFO|configuration_utils.py:792] 2024-06-27 23:20:05,110 >> Model config Qwen2Config {
  "_name_or_path": "/home/nfs04/wangzj/models/Qwen1.5-1.8B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:3334] 2024-06-27 23:20:05,265 >> loading weights file /home/nfs04/wangzj/models/Qwen1.5-1.8B/model.safetensors
[INFO|modeling_utils.py:1459] 2024-06-27 23:20:05,285 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:827] 2024-06-27 23:20:05,287 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}

06/27/2024 23:20:10 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/27/2024 23:20:10 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
06/27/2024 23:20:10 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
[INFO|modeling_utils.py:4070] 2024-06-27 23:20:10,498 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4078] 2024-06-27 23:20:10,498 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/nfs04/wangzj/models/Qwen1.5-1.8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-06-27 23:20:10,501 >> loading configuration file /home/nfs04/wangzj/models/Qwen1.5-1.8B/generation_config.json
[INFO|configuration_utils.py:827] 2024-06-27 23:20:10,502 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "max_new_tokens": 2048
}

06/27/2024 23:20:10 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/27/2024 23:20:10 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
06/27/2024 23:20:10 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
06/27/2024 23:20:10 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/27/2024 23:20:10 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
06/27/2024 23:20:10 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
06/27/2024 23:20:10 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
06/27/2024 23:20:10 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
06/27/2024 23:20:10 - INFO - llmtuner.model.adapter - Resume training from moe adapter: /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe.
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
06/27/2024 23:20:12 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
06/27/2024 23:20:12 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
06/27/2024 23:20:12 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - MoeConfig(peft_type=<PeftType.MOE: 'MOE'>, auto_mapping=None, base_model_name_or_path='/home/nfs02/wangzj/models/Qwen1.5-1.8B', revision=None, task_type='CAUSAL_LM', inference_mode=False, init_moe_weights=True, router_type='top1', save_router_logits=False, num_experts=2, num_heads=4, topk=2, layers_to_transform=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], save_all_params=False, polarization_coef=2, router_aux_loss_coef=0.01, use_polarization_loss=False, polarization_func='powsum', use_load_balancing_loss=False, ce_loss_coef=0.05)
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - Mark only the moe router trainable.
06/27/2024 23:20:12 - INFO - llmtuner.model.adapter - Loaded adapter(s): /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe
06/27/2024 23:20:12 - INFO - llmtuner.model.loader - trainable params: 98304 || all params: 2648524800 || trainable%: 0.0037
06/27/2024 23:20:28 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
Using custom data configuration default-189877435147caf4
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-189877435147caf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-189877435147caf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-189877435147caf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-189877435147caf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4a50e9afc9c1c71b.arrow
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-189877435147caf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-37a230448f4661ad.arrow
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
06/27/2024 23:20:41 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Using custom data configuration default-29b185349b8b5563
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-90c09dad81efb3e2.arrow
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-29b185349b8b5563/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-7eb4e0f55f4b2ce0.arrow
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Running tokenizer on dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-189877435147caf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-4fa56a984594bcfb.arrow
Running tokenizer on dataset:   1%|          | 1000/100000 [00:08<13:51, 119.01 examples/s]06/27/2024 23:20:59 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
06/27/2024 23:20:59 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
06/27/2024 23:21:00 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/ar_2b.jsonl.
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Running tokenizer on dataset:   2%|▏         | 2000/100000 [00:14<11:52, 137.59 examples/s]Running tokenizer on dataset:   3%|▎         | 3000/100000 [00:21<11:01, 146.70 examples/s]06/27/2024 23:21:12 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
06/27/2024 23:21:12 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
06/27/2024 23:21:12 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/de_2b.jsonl.
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Running tokenizer on dataset:   4%|▍         | 4000/100000 [00:26<09:53, 161.86 examples/s]Dataset({
    features: ['prompt', 'response', 'system', 'tools', 'language'],
    num_rows: 50000
})
Running tokenizer on dataset:   5%|▌         | 5000/100000 [00:32<09:32, 165.99 examples/s]Running tokenizer on dataset:   6%|▌         | 6000/100000 [00:37<09:09, 171.06 examples/s][WARNING|tokenization_utils_base.py:3843] 2024-06-27 23:21:29,638 >> Token indices sequence length is longer than the specified maximum sequence length for this model (38249 > 32768). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset:   7%|▋         | 7000/100000 [00:43<08:52, 174.68 examples/s]Running tokenizer on dataset:   8%|▊         | 8000/100000 [00:48<08:29, 180.74 examples/s]Running tokenizer on dataset:   9%|▉         | 9000/100000 [00:53<08:27, 179.33 examples/s]Running tokenizer on dataset:  10%|█         | 10000/100000 [00:59<08:12, 182.77 examples/s]Running tokenizer on dataset:  11%|█         | 11000/100000 [01:04<08:01, 184.99 examples/s]Running tokenizer on dataset:  12%|█▏        | 12000/100000 [01:09<07:35, 193.30 examples/s]Running tokenizer on dataset:  13%|█▎        | 13000/100000 [01:13<07:22, 196.66 examples/s]Running tokenizer on dataset:  14%|█▍        | 14000/100000 [01:18<07:11, 199.43 examples/s]Running tokenizer on dataset:  15%|█▌        | 15000/100000 [01:23<06:58, 203.05 examples/s]Running tokenizer on dataset:  16%|█▌        | 16000/100000 [01:28<07:00, 199.99 examples/s]Running tokenizer on dataset:  17%|█▋        | 17000/100000 [01:33<06:52, 201.30 examples/s]Running tokenizer on dataset:  18%|█▊        | 18000/100000 [01:38<06:52, 198.69 examples/s]Running tokenizer on dataset:  19%|█▉        | 19000/100000 [01:43<06:44, 200.20 examples/s]Running tokenizer on dataset:  20%|██        | 20000/100000 [01:48<06:37, 201.01 examples/s]Running tokenizer on dataset:  21%|██        | 21000/100000 [01:53<06:23, 205.99 examples/s]Running tokenizer on dataset:  22%|██▏       | 22000/100000 [01:58<06:19, 205.62 examples/s]Running tokenizer on dataset:  23%|██▎       | 23000/100000 [02:02<06:12, 206.68 examples/s]Running tokenizer on dataset:  24%|██▍       | 24000/100000 [02:07<06:08, 205.98 examples/s]Running tokenizer on dataset:  25%|██▌       | 25000/100000 [02:12<06:01, 207.46 examples/s]Running tokenizer on dataset:  26%|██▌       | 26000/100000 [02:17<05:55, 207.96 examples/s]Running tokenizer on dataset:  27%|██▋       | 27000/100000 [02:22<05:52, 206.83 examples/s]Running tokenizer on dataset:  28%|██▊       | 28000/100000 [02:27<05:49, 205.86 examples/s]Running tokenizer on dataset:  29%|██▉       | 29000/100000 [02:31<05:40, 208.39 examples/s]Running tokenizer on dataset:  30%|███       | 30000/100000 [02:36<05:33, 210.21 examples/s]Running tokenizer on dataset:  31%|███       | 31000/100000 [02:41<05:29, 209.34 examples/s]Running tokenizer on dataset:  32%|███▏      | 32000/100000 [02:45<05:24, 209.72 examples/s]Running tokenizer on dataset:  33%|███▎      | 33000/100000 [02:50<05:22, 207.96 examples/s]Running tokenizer on dataset:  34%|███▍      | 34000/100000 [02:55<05:16, 208.35 examples/s]Running tokenizer on dataset:  35%|███▌      | 35000/100000 [03:00<05:11, 208.98 examples/s]Running tokenizer on dataset:  36%|███▌      | 36000/100000 [03:05<05:05, 209.37 examples/s]Running tokenizer on dataset:  37%|███▋      | 37000/100000 [03:09<04:57, 212.09 examples/s]Running tokenizer on dataset:  38%|███▊      | 38000/100000 [03:14<04:49, 213.89 examples/s]Running tokenizer on dataset:  39%|███▉      | 39000/100000 [03:18<04:43, 215.49 examples/s]Running tokenizer on dataset:  40%|████      | 40000/100000 [03:23<04:39, 214.51 examples/s]Running tokenizer on dataset:  41%|████      | 41000/100000 [03:27<04:30, 218.13 examples/s]Running tokenizer on dataset:  42%|████▏     | 42000/100000 [03:32<04:27, 216.79 examples/s]Running tokenizer on dataset:  43%|████▎     | 43000/100000 [03:36<04:17, 220.97 examples/s]Running tokenizer on dataset:  44%|████▍     | 44000/100000 [03:41<04:08, 225.21 examples/s]Running tokenizer on dataset:  45%|████▌     | 45000/100000 [03:45<04:02, 226.75 examples/s]Running tokenizer on dataset:  46%|████▌     | 46000/100000 [03:50<04:12, 213.68 examples/s]Running tokenizer on dataset:  47%|████▋     | 47000/100000 [03:55<04:07, 214.14 examples/s]Running tokenizer on dataset:  48%|████▊     | 48000/100000 [03:59<03:57, 218.74 examples/s]Running tokenizer on dataset:  49%|████▉     | 49000/100000 [04:04<03:50, 221.21 examples/s]Running tokenizer on dataset:  50%|█████     | 50000/100000 [04:08<03:44, 222.35 examples/s]Running tokenizer on dataset:  51%|█████     | 51000/100000 [04:15<04:13, 193.37 examples/s]Running tokenizer on dataset:  52%|█████▏    | 52000/100000 [04:21<04:15, 187.52 examples/s]Running tokenizer on dataset:  53%|█████▎    | 53000/100000 [04:25<04:03, 193.38 examples/s]Running tokenizer on dataset:  54%|█████▍    | 54000/100000 [04:31<04:00, 191.34 examples/s]Running tokenizer on dataset:  55%|█████▌    | 55000/100000 [04:36<03:50, 195.31 examples/s]Running tokenizer on dataset:  56%|█████▌    | 56000/100000 [04:41<03:53, 188.14 examples/s]Running tokenizer on dataset:  57%|█████▋    | 57000/100000 [04:47<03:50, 186.66 examples/s]Running tokenizer on dataset:  58%|█████▊    | 58000/100000 [04:51<03:34, 195.39 examples/s]Running tokenizer on dataset:  59%|█████▉    | 59000/100000 [04:56<03:28, 197.03 examples/s]Running tokenizer on dataset:  60%|██████    | 60000/100000 [05:01<03:15, 204.18 examples/s]Running tokenizer on dataset:  61%|██████    | 61000/100000 [05:05<03:06, 208.71 examples/s]Running tokenizer on dataset:  62%|██████▏   | 62000/100000 [05:10<03:01, 209.06 examples/s]Running tokenizer on dataset:  63%|██████▎   | 63000/100000 [05:15<02:57, 208.09 examples/s]Running tokenizer on dataset:  64%|██████▍   | 64000/100000 [05:20<02:55, 204.76 examples/s]Running tokenizer on dataset:  65%|██████▌   | 65000/100000 [05:25<02:52, 202.35 examples/s]Running tokenizer on dataset:  66%|██████▌   | 66000/100000 [05:30<02:47, 202.51 examples/s]Running tokenizer on dataset:  67%|██████▋   | 67000/100000 [05:35<02:43, 202.11 examples/s]Running tokenizer on dataset:  68%|██████▊   | 68000/100000 [05:40<02:35, 206.07 examples/s]Running tokenizer on dataset:  69%|██████▉   | 69000/100000 [05:44<02:28, 208.24 examples/s]Running tokenizer on dataset:  70%|███████   | 70000/100000 [05:49<02:24, 207.68 examples/s]Running tokenizer on dataset:  71%|███████   | 71000/100000 [05:54<02:17, 210.44 examples/s]Running tokenizer on dataset:  72%|███████▏  | 72000/100000 [05:58<02:08, 217.40 examples/s]Running tokenizer on dataset:  73%|███████▎  | 73000/100000 [06:03<02:06, 213.29 examples/s]Running tokenizer on dataset:  74%|███████▍  | 74000/100000 [06:08<02:04, 209.26 examples/s]Running tokenizer on dataset:  75%|███████▌  | 75000/100000 [06:12<01:56, 214.96 examples/s]Running tokenizer on dataset:  76%|███████▌  | 76000/100000 [06:17<01:48, 220.87 examples/s]Running tokenizer on dataset:  77%|███████▋  | 77000/100000 [06:21<01:44, 220.71 examples/s]Running tokenizer on dataset:  78%|███████▊  | 78000/100000 [06:26<01:41, 217.75 examples/s]Running tokenizer on dataset:  79%|███████▉  | 79000/100000 [06:31<01:37, 216.19 examples/s]Running tokenizer on dataset:  80%|████████  | 80000/100000 [06:35<01:31, 219.59 examples/s]Running tokenizer on dataset:  81%|████████  | 81000/100000 [06:40<01:27, 216.96 examples/s]Running tokenizer on dataset:  82%|████████▏ | 82000/100000 [06:45<01:24, 213.50 examples/s]Running tokenizer on dataset:  83%|████████▎ | 83000/100000 [06:49<01:18, 217.67 examples/s]Running tokenizer on dataset:  84%|████████▍ | 84000/100000 [06:53<01:12, 220.83 examples/s]Running tokenizer on dataset:  85%|████████▌ | 85000/100000 [06:58<01:06, 224.58 examples/s]Running tokenizer on dataset:  86%|████████▌ | 86000/100000 [07:02<01:03, 221.41 examples/s]Running tokenizer on dataset:  87%|████████▋ | 87000/100000 [07:07<00:58, 221.95 examples/s]Running tokenizer on dataset:  88%|████████▊ | 88000/100000 [07:12<00:55, 216.66 examples/s]Running tokenizer on dataset:  89%|████████▉ | 89000/100000 [07:16<00:51, 215.56 examples/s]Running tokenizer on dataset:  90%|█████████ | 90000/100000 [07:21<00:46, 217.11 examples/s]Running tokenizer on dataset:  91%|█████████ | 91000/100000 [07:26<00:41, 215.71 examples/s]Running tokenizer on dataset:  92%|█████████▏| 92000/100000 [07:30<00:35, 224.35 examples/s]Running tokenizer on dataset:  93%|█████████▎| 93000/100000 [07:34<00:30, 227.64 examples/s]Running tokenizer on dataset:  94%|█████████▍| 94000/100000 [07:38<00:26, 223.67 examples/s]Running tokenizer on dataset:  95%|█████████▌| 95000/100000 [07:43<00:22, 221.28 examples/s]Running tokenizer on dataset:  96%|█████████▌| 96000/100000 [07:48<00:18, 220.69 examples/s]Running tokenizer on dataset:  97%|█████████▋| 97000/100000 [07:52<00:13, 220.53 examples/s]Running tokenizer on dataset:  98%|█████████▊| 98000/100000 [07:57<00:08, 223.29 examples/s]Running tokenizer on dataset:  99%|█████████▉| 99000/100000 [08:01<00:04, 225.88 examples/s]Running tokenizer on dataset: 100%|██████████| 100000/100000 [08:05<00:00, 229.53 examples/s]Running tokenizer on dataset: 100%|██████████| 100000/100000 [08:05<00:00, 205.93 examples/s]
input_ids:
[81778, 125768, 17166, 125089, 135564, 124072, 21360, 124523, 27490, 73441, 442, 2100, 52275, 10531, 2034, 1959, 82168, 128583, 123877, 124388, 49388, 198, 81778, 125768, 17166, 125089, 135564, 124072, 21360, 124523, 27490, 73441, 442, 2100, 52275, 10531, 2034, 198, 31459, 220, 15, 23, 11, 220, 17, 15, 16, 19, 304, 29342, 21033, 11, 15762, 198, 134204, 128473, 92381, 53710, 124035, 124387, 63237, 59842, 126628, 16157, 128248, 92072, 128541, 141949, 124669, 127630, 55334, 35038, 13, 128702, 63237, 63415, 123860, 63415, 14293, 14293, 128332, 128259, 134842, 141949, 124669, 128342, 55334, 35038, 77273, 123877, 140806, 124114, 27490, 124872, 128261, 39434, 130510, 74315, 35038, 33090, 123961, 123829, 43635, 123894, 124641, 8532, 624, 91962, 14293, 130104, 56794, 135746, 124387, 68785, 128660, 77273, 131032, 126212, 124080, 123963, 132875, 128473, 124072, 128473, 92381, 25871, 124209, 126842, 68785, 125007, 141949, 124669, 127630, 55334, 35038, 128420, 92072, 128541, 63415, 125184, 129308, 129458, 128842, 510, 1, 135746, 124387, 4894, 124075, 53710, 124035, 124387, 4894, 27490, 125657, 0, 77703, 125657, 0, 77703, 41593, 6913, 125267, 125007, 131723, 127761, 125007, 39434, 31073, 124014, 126418, 53710, 124035, 124387, 86941, 126880, 124006, 128287, 59842, 65398, 124265, 126406, 128325, 125007, 37524, 130510, 128342, 20931, 132568, 126815, 133781, 68785, 63237, 59842, 124363, 16157, 128416, 73274, 133326, 125007, 125100, 39697, 73771, 25871, 128261, 74315, 124284, 124006, 37524, 124224, 14293, 12, 126805, 136178, 123894, 137577, 12, 53710, 70604, 23224, 123913, 124009, 124691, 123913, 124876, 624, 124703, 53710, 124035, 124387, 37524, 29825, 123980, 63415, 125184, 124766, 123938, 16157, 68785, 73274, 132137, 126196, 128920, 77273, 27846, 126229, 52157, 124454, 94957, 123920, 125704, 73274, 130510, 74315, 35038, 33090, 123961, 123829, 43635, 123894, 124641, 8532, 13, 128671, 55891, 11798, 31382, 31073, 52157, 124328, 25871, 125555, 126229, 123890, 39434, 130510, 138788, 124376, 130063, 50243, 95975, 55334, 25871, 44330, 125633, 53710, 124035, 124387, 68785, 128416, 125555, 129327, 124006, 63415, 21360, 124642, 128587, 63415, 137867, 53710, 124035, 124387, 129445, 85153, 46586, 124983, 129472, 125490, 77273, 128305, 141189, 68785, 128587, 73274, 130510, 77273, 128773, 133674, 124269, 126504, 53710, 123938, 125072, 124376, 624, 31073, 124519, 17166, 123832, 124210, 68785, 77273, 128280, 128438, 68785, 56794, 43635, 126756, 126436, 124267, 13, 129920, 69423, 128412, 39434, 130510, 128248, 125637, 124678, 133394, 135110, 55057, 123913, 124226, 68785, 128259, 133114, 126383, 128248, 123894, 126995, 128464, 73274, 27490, 130151, 129046, 17166, 124187, 11798, 624, 134204, 128473, 92381, 53710, 124035, 124387, 63237, 59842, 126628, 16157, 68785, 128641, 128305, 53479, 124653, 126214, 128280, 123913, 126628, 135306, 133534, 125633, 44330, 125633, 16157, 128248, 53710, 125592, 16157, 13, 39434, 140518, 63415, 125481, 98719, 82168, 124131, 16157, 125475, 143516, 130543, 128248, 126198, 73274, 126727, 68785, 63237, 124080, 92381, 124653, 133462, 128248, 123877, 124388, 13, 126208, 39434, 21360, 124015, 125490, 59842, 124181, 128264, 125447, 133398, 68785, 128464, 125555, 125962, 63415, 130181, 23224, 16157, 128288, 124006, 140674, 25871, 68785, 128523, 128261, 128248, 63415, 124042, 49388, 16157, 123961, 133176, 13, 126214, 124147, 124144, 143122, 123941, 77273, 128858, 143400, 82168, 124131, 16157, 68785, 73274, 124938, 12653, 129163, 12653, 124080, 124014, 25871, 63237, 77703, 124325, 53710, 125592, 16157, 128252, 134259, 63415, 124522, 95975, 63415, 130181, 23224, 77703, 124677, 126236, 13, 56794, 130059, 25871, 125007, 68238, 5703, 72804, 94957, 124925, 73771, 31073, 68785, 125555, 31382, 124147, 124144]
inputs:
غصن الزيتون والبندقية // ANONYMOUS — جفت الأقلام
غصن الزيتون والبندقية // ANONYMOUS
August 08, 2014 in INTERVIEW, TEXT
إستيقظ رؤوف من سباته على صوت صفارات الإنذار. ولكن من أين أتت؟ لا يوجد صفارات إنذار في الأحياء الفقيرة التي تقع خارج الحائط العازل.
إتضح لرؤوف، وهو في حالة بين النوم العميق واليقظة الشديدة، أن صفارات الإنذار هي صوت أمه وهي تنادي:
"رؤوف!
يا رؤوف!
قصف! قصف! قص-
قبل أن تستطيع أن تكمل أم رؤوف كلامها ، ساد الصمت بعد أن وقع إنفجار ضخم، من سمعه قد يعتقد أن الهزّة التي خلفها وصلت- بتقدير العاقل- رابع السماوات السبع.
كان رؤوف وحيد أمه وأبيه، يسكن معهما في بيت شديد التواضع يقع خارج الحائط العازل. كانت هنالك شجرة زيتون تقع تماماً أمام نافذة ديار رؤوف، قد زرعها أباه يوم أخذ رؤوف أول إستنشاقة له في هذه الدنيا، يوم يقع في أكثر أيام الربيع ربيعاً.
كانت الرياح، في هذا اليوم، لطيفة بشدة. برودتها تقع على المرء كالبشرى السارة، لا تقسو على العظام ولا يقشعر لها البدن.
إستيقظ رؤوف من سباته، وفي هذه المرة كان هذا السبات نتيجة إنهيار دياره على رأسه. تفقد أعضاء جسمه واكتشف أنها على ما يرام، من النظرة الخارجية على الأقل. لم تبتر له ساق أو ذراع، ولا زالت أصابعه كلها موجودة، حتى التي على أقدامه الحافية. كان الخدر منتشر في جميع أنحاء جسمه، يخطو خطو النملة من قمة رأسه إلى نهاية أطراف أصابع قدميه. لحظة أن حاول التحرّك، زال الخدر
Caching indices mapping at /home/wangzj/.cache/huggingface/datasets/json/default-189877435147caf4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-e399c5348240740e.arrow
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 216705
})
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): Qwen2ForCausalLM(
      (model): Qwen2Model(
        (embed_tokens): Embedding(151936, 2048)
        (layers): ModuleList(
          (0): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (1): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (2): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (3): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (4): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (5): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (6): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (7): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (8): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (9): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (10): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (11): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (12): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (13): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (14): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (15): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (16): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (17): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (18): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (19): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (20): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (21): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (22): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
          (23): Qwen2DecoderLayer(
            (self_attn): Qwen2Attention(
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
              (rotary_emb): Qwen2RotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): Qwen2MLP(
                (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=2048, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): Qwen2MLP(
                    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)
                    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): Qwen2RMSNorm()
            (post_attention_layernorm): Qwen2RMSNorm()
          )
        )
        (norm): Qwen2RMSNorm()
      )
      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
    )
  )
)
Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-20-04_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:586] 2024-06-27 23:28:52,707 >> Using auto half precision backend
[2024-06-27 23:28:52,932] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 216705
})
Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-20-04_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 216705
})
Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-20-04_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Dataset({
    features: ['input_ids', 'attention_mask', 'langs'],
    num_rows: 216705
})
Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs04/wangzj/checkpoints/moe/test/runs/Jun27_23-20-04_v100-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs04/wangzj/checkpoints/moe/test,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs04/wangzj/checkpoints/moe/test,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=10000,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[2024-06-27 23:28:57,767] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-06-27 23:28:57,768] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-06-27 23:28:57,768] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-27 23:28:57,770] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-06-27 23:28:57,770] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-06-27 23:28:57,771] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-06-27 23:28:57,771] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-06-27 23:28:57,771] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-06-27 23:28:57,771] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: False
[2024-06-27 23:28:57,771] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-06-27 23:28:58,099] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-06-27 23:28:58,100] [INFO] [utils.py:803:see_memory_usage] MA 5.38 GB         Max_MA 5.38 GB         CA 5.66 GB         Max_CA 6 GB 
[2024-06-27 23:28:58,100] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 30.04 GB, percent = 13.9%
[2024-06-27 23:28:58,289] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-06-27 23:28:58,290] [INFO] [utils.py:803:see_memory_usage] MA 5.38 GB         Max_MA 5.38 GB         CA 5.66 GB         Max_CA 6 GB 
[2024-06-27 23:28:58,290] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 30.04 GB, percent = 13.9%
[2024-06-27 23:28:58,290] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-06-27 23:28:58,474] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-06-27 23:28:58,474] [INFO] [utils.py:803:see_memory_usage] MA 5.38 GB         Max_MA 5.38 GB         CA 5.66 GB         Max_CA 6 GB 
[2024-06-27 23:28:58,474] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 30.04 GB, percent = 13.9%
[2024-06-27 23:28:58,475] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-06-27 23:28:58,475] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-06-27 23:28:58,475] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-06-27 23:28:58,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2024-06-27 23:28:58,476] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   amp_params ................... False
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   bfloat16_enabled ............. False
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff251d531c0>
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-27 23:28:58,477] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   dump_state ................... False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   fp16_auto_cast ............... False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   fp16_enabled ................. True
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 4
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 65536
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   loss_scale ................... 0
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-06-27 23:28:58,478] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   pld_params ................... False
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   train_batch_size ............. 128
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  8
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   world_size ................... 4
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-27 23:28:58,479] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-06-27 23:28:58,479] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": false
    }
}
[INFO|trainer.py:1748] 2024-06-27 23:28:58,479 >> ***** Running training *****
[INFO|trainer.py:1749] 2024-06-27 23:28:58,479 >>   Num examples = 216,705
[INFO|trainer.py:1750] 2024-06-27 23:28:58,479 >>   Num Epochs = 1
[INFO|trainer.py:1751] 2024-06-27 23:28:58,479 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1754] 2024-06-27 23:28:58,479 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:1755] 2024-06-27 23:28:58,479 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1756] 2024-06-27 23:28:58,479 >>   Total optimization steps = 1,693
[INFO|trainer.py:1757] 2024-06-27 23:28:58,481 >>   Number of trainable parameters = 98,304
  0%|          | 0/1693 [00:00<?, ?it/s]  0%|          | 1/1693 [00:04<2:17:39,  4.88s/it]  0%|          | 2/1693 [00:09<2:14:42,  4.78s/it]  0%|          | 3/1693 [00:14<2:13:45,  4.75s/it]  0%|          | 4/1693 [00:19<2:13:18,  4.74s/it]  0%|          | 5/1693 [00:23<2:13:00,  4.73s/it]  0%|          | 6/1693 [00:28<2:12:59,  4.73s/it]  0%|          | 7/1693 [00:33<2:12:54,  4.73s/it]  0%|          | 8/1693 [00:37<2:12:49,  4.73s/it]  1%|          | 9/1693 [00:42<2:12:45,  4.73s/it]  1%|          | 10/1693 [00:47<2:12:38,  4.73s/it]                                                   {'loss': 2.5484, 'learning_rate': 0.00019998278355928408, 'epoch': 0.01}
  1%|          | 10/1693 [00:47<2:12:38,  4.73s/it]                                                   {'router_ce_loss': 0.8088709712028503, 'old_lang_expert0_score': '0.75 0.75 0.17 0.17 0.08 0.82 0.26 0.08 0.04 0.96 0.01 0.96 0.12 0.34 0.98 0.96 0.35 0.81 0.94 0.09 0.9 0.0 1.0 0.0', 'epoch': 0.01}
  1%|          | 10/1693 [00:47<2:12:38,  4.73s/it]  1%|          | 11/1693 [00:52<2:12:53,  4.74s/it]  1%|          | 12/1693 [00:56<2:13:02,  4.75s/it]  1%|          | 13/1693 [01:01<2:13:03,  4.75s/it]  1%|          | 14/1693 [01:06<2:13:02,  4.75s/it]  1%|          | 15/1693 [01:11<2:12:58,  4.75s/it]  1%|          | 16/1693 [01:15<2:12:49,  4.75s/it]  1%|          | 17/1693 [01:20<2:12:49,  4.75s/it]  1%|          | 18/1693 [01:25<2:12:47,  4.76s/it]  1%|          | 19/1693 [01:30<2:12:43,  4.76s/it]  1%|          | 20/1693 [01:34<2:12:45,  4.76s/it]                                                   {'loss': 2.5533, 'learning_rate': 0.00019993114016525295, 'epoch': 0.01}
  1%|          | 20/1693 [01:34<2:12:45,  4.76s/it]                                                   {'router_ce_loss': 0.8082746863365173, 'old_lang_expert0_score': '0.77 0.78 0.14 0.19 0.07 0.83 0.27 0.06 0.09 0.97 0.03 0.97 0.1 0.33 0.99 0.96 0.31 0.81 0.95 0.07 0.9 0.01 1.0 0.0', 'epoch': 0.01}
  1%|          | 20/1693 [01:35<2:12:45,  4.76s/it]  1%|          | 21/1693 [01:39<2:12:56,  4.77s/it]  1%|▏         | 22/1693 [01:44<2:12:47,  4.77s/it]  1%|▏         | 23/1693 [01:49<2:12:39,  4.77s/it]  1%|▏         | 24/1693 [01:54<2:12:41,  4.77s/it]  1%|▏         | 25/1693 [01:58<2:12:40,  4.77s/it]  2%|▏         | 26/1693 [02:03<2:12:43,  4.78s/it]  2%|▏         | 27/1693 [02:08<2:12:34,  4.77s/it]  2%|▏         | 28/1693 [02:13<2:12:21,  4.77s/it]  2%|▏         | 29/1693 [02:17<2:12:21,  4.77s/it]  2%|▏         | 30/1693 [02:22<2:12:19,  4.77s/it]                                                   {'loss': 2.4915, 'learning_rate': 0.00019984508760021518, 'epoch': 0.02}
  2%|▏         | 30/1693 [02:22<2:12:19,  4.77s/it]                                                   {'router_ce_loss': 0.8059226274490356, 'old_lang_expert0_score': '0.8 0.78 0.13 0.18 0.07 0.84 0.28 0.08 0.04 0.98 0.01 0.98 0.14 0.33 0.99 0.97 0.35 0.82 0.95 0.07 0.91 0.01 1.0 0.0', 'epoch': 0.02}
  2%|▏         | 30/1693 [02:23<2:12:19,  4.77s/it]  2%|▏         | 31/1693 [02:27<2:12:18,  4.78s/it]  2%|▏         | 32/1693 [02:32<2:15:38,  4.90s/it]  2%|▏         | 33/1693 [02:37<2:14:30,  4.86s/it]  2%|▏         | 34/1693 [02:42<2:13:57,  4.84s/it]  2%|▏         | 35/1693 [02:47<2:13:24,  4.83s/it]  2%|▏         | 36/1693 [02:51<2:13:01,  4.82s/it]  2%|▏         | 37/1693 [02:56<2:12:36,  4.80s/it]  2%|▏         | 38/1693 [03:01<2:12:27,  4.80s/it]  2%|▏         | 39/1693 [03:06<2:12:10,  4.79s/it]  2%|▏         | 40/1693 [03:10<2:12:00,  4.79s/it]                                                   {'loss': 2.5653, 'learning_rate': 0.00019972465549454853, 'epoch': 0.02}
  2%|▏         | 40/1693 [03:11<2:12:00,  4.79s/it]                                                   {'router_ce_loss': 0.8069737553596497, 'old_lang_expert0_score': '0.81 0.78 0.12 0.19 0.07 0.83 0.31 0.08 0.04 0.98 0.02 0.97 0.14 0.29 0.98 0.94 0.3 0.83 0.96 0.09 0.93 0.01 0.99 0.0', 'epoch': 0.02}
  2%|▏         | 40/1693 [03:11<2:12:00,  4.79s/it]  2%|▏         | 41/1693 [03:15<2:11:59,  4.79s/it]  2%|▏         | 42/1693 [03:20<2:11:48,  4.79s/it]  3%|▎         | 43/1693 [03:25<2:11:45,  4.79s/it]  3%|▎         | 44/1693 [03:30<2:11:36,  4.79s/it]  3%|▎         | 45/1693 [03:34<2:11:36,  4.79s/it]  3%|▎         | 46/1693 [03:39<2:11:27,  4.79s/it]  3%|▎         | 47/1693 [03:44<2:11:25,  4.79s/it]  3%|▎         | 48/1693 [03:49<2:11:16,  4.79s/it]  3%|▎         | 49/1693 [03:54<2:11:14,  4.79s/it]  3%|▎         | 50/1693 [03:58<2:11:21,  4.80s/it]                                                   {'loss': 2.5108, 'learning_rate': 0.00019956988531649711, 'epoch': 0.03}
  3%|▎         | 50/1693 [03:58<2:11:21,  4.80s/it]                                                   {'router_ce_loss': 0.8056585192680359, 'old_lang_expert0_score': '0.82 0.81 0.13 0.18 0.07 0.85 0.3 0.09 0.05 0.98 0.02 0.98 0.13 0.3 0.99 0.97 0.31 0.82 0.96 0.09 0.9 0.01 1.0 0.0', 'epoch': 0.03}
  3%|▎         | 50/1693 [03:59<2:11:21,  4.80s/it]  3%|▎         | 51/1693 [04:03<2:11:14,  4.80s/it]  3%|▎         | 52/1693 [04:08<2:11:06,  4.79s/it]  3%|▎         | 53/1693 [04:13<2:11:03,  4.79s/it]  3%|▎         | 54/1693 [04:18<2:11:04,  4.80s/it]  3%|▎         | 55/1693 [04:22<2:10:54,  4.80s/it]  3%|▎         | 56/1693 [04:27<2:10:50,  4.80s/it]  3%|▎         | 57/1693 [04:32<2:10:39,  4.79s/it]  3%|▎         | 58/1693 [04:37<2:10:36,  4.79s/it]  3%|▎         | 59/1693 [04:42<2:10:32,  4.79s/it]  4%|▎         | 60/1693 [04:46<2:10:28,  4.79s/it]                                                   {'loss': 2.562, 'learning_rate': 0.00019938083035789288, 'epoch': 0.04}
  4%|▎         | 60/1693 [04:46<2:10:28,  4.79s/it]                                                   {'router_ce_loss': 0.8029229044914246, 'old_lang_expert0_score': '0.84 0.8 0.13 0.21 0.08 0.82 0.34 0.09 0.05 0.98 0.01 0.98 0.16 0.3 0.99 0.97 0.31 0.81 0.95 0.06 0.93 0.01 1.0 0.0', 'epoch': 0.04}
  4%|▎         | 60/1693 [04:47<2:10:28,  4.79s/it]  4%|▎         | 61/1693 [04:51<2:10:20,  4.79s/it]  4%|▎         | 62/1693 [04:56<2:10:23,  4.80s/it]  4%|▎         | 63/1693 [05:01<2:12:03,  4.86s/it]  4%|▍         | 64/1693 [05:06<2:13:26,  4.91s/it]  4%|▍         | 65/1693 [05:11<2:12:19,  4.88s/it]  4%|▍         | 66/1693 [05:16<2:11:39,  4.85s/it]  4%|▍         | 67/1693 [05:20<2:11:05,  4.84s/it]  4%|▍         | 68/1693 [05:25<2:10:40,  4.82s/it]  4%|▍         | 69/1693 [05:30<2:10:24,  4.82s/it]  4%|▍         | 70/1693 [05:35<2:10:06,  4.81s/it]                                                   {'loss': 2.4771, 'learning_rate': 0.00019915755571580548, 'epoch': 0.04}
  4%|▍         | 70/1693 [05:35<2:10:06,  4.81s/it]                                                   {'router_ce_loss': 0.8024283647537231, 'old_lang_expert0_score': '0.84 0.81 0.13 0.19 0.08 0.84 0.29 0.1 0.06 0.99 0.02 0.98 0.14 0.32 0.99 0.97 0.34 0.81 0.95 0.08 0.91 0.01 0.99 0.0', 'epoch': 0.04}
  4%|▍         | 70/1693 [05:35<2:10:06,  4.81s/it]  4%|▍         | 71/1693 [05:40<2:10:05,  4.81s/it]  4%|▍         | 72/1693 [05:44<2:09:44,  4.80s/it]  4%|▍         | 73/1693 [05:49<2:09:37,  4.80s/it]  4%|▍         | 74/1693 [05:54<2:09:28,  4.80s/it]  4%|▍         | 75/1693 [05:59<2:09:24,  4.80s/it]  4%|▍         | 76/1693 [06:04<2:09:18,  4.80s/it]  5%|▍         | 77/1693 [06:08<2:09:18,  4.80s/it]  5%|▍         | 78/1693 [06:13<2:09:10,  4.80s/it]  5%|▍         | 79/1693 [06:18<2:09:04,  4.80s/it]  5%|▍         | 80/1693 [06:23<2:08:57,  4.80s/it]                                                   {'loss': 2.5665, 'learning_rate': 0.00019890013827012775, 'epoch': 0.05}
  5%|▍         | 80/1693 [06:23<2:08:57,  4.80s/it]                                                   {'router_ce_loss': 0.7931252121925354, 'old_lang_expert0_score': '0.84 0.84 0.14 0.18 0.08 0.87 0.28 0.11 0.06 0.98 0.05 0.98 0.13 0.35 0.99 0.98 0.38 0.79 0.96 0.1 0.92 0.03 1.0 0.0', 'epoch': 0.05}
  5%|▍         | 80/1693 [06:23<2:08:57,  4.80s/it]  5%|▍         | 81/1693 [06:28<2:08:55,  4.80s/it]  5%|▍         | 82/1693 [06:32<2:08:54,  4.80s/it]  5%|▍         | 83/1693 [06:37<2:08:53,  4.80s/it]  5%|▍         | 84/1693 [06:42<2:08:44,  4.80s/it]  5%|▌         | 85/1693 [06:47<2:08:35,  4.80s/it]  5%|▌         | 86/1693 [06:52<2:08:32,  4.80s/it]  5%|▌         | 87/1693 [06:56<2:08:23,  4.80s/it]  5%|▌         | 88/1693 [07:01<2:08:27,  4.80s/it]  5%|▌         | 89/1693 [07:06<2:08:18,  4.80s/it]  5%|▌         | 90/1693 [07:11<2:08:21,  4.80s/it]                                                   {'loss': 2.4784, 'learning_rate': 0.00019860866665710356, 'epoch': 0.05}
  5%|▌         | 90/1693 [07:11<2:08:21,  4.80s/it]                                                   {'router_ce_loss': 0.7982103228569031, 'old_lang_expert0_score': '0.85 0.84 0.13 0.18 0.07 0.87 0.27 0.08 0.04 0.99 0.04 0.98 0.14 0.33 0.99 0.98 0.36 0.8 0.96 0.11 0.9 0.02 1.0 0.0', 'epoch': 0.05}
  5%|▌         | 90/1693 [07:11<2:08:21,  4.80s/it]  5%|▌         | 91/1693 [07:16<2:08:19,  4.81s/it]  5%|▌         | 92/1693 [07:20<2:08:15,  4.81s/it]aux_router.sh: line 30: 15802 Killed                  deepspeed --num_gpus 4 --master_port=9902 src/train_bash.py --deepspeed /home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json --stage pt --model_name_or_path /home/nfs04/wangzj/models/Qwen1.5-1.8B --adapter_name_or_path /home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe --train_only_router --moe_router_type top2 --topk 2 --ce_loss_coef 0.05 --do_train --dataset ar_2b,de_2b --max_samples 50000 --preprocessing_num_workers 1 --cutoff_len 512 --finetuning_type moe --output_dir /home/nfs04/wangzj/checkpoints/moe/test --overwrite_output_dir --per_device_train_batch_size 8 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 100 --save_steps 10000 --save_only_model --learning_rate 2e-4 --num_train_epochs 1.0 --plot_loss --fp16
  5%|▌         | 93/1693 [07:25<2:08:07,  4.80s/it][2024-06-27 23:36:26,480] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 15882
[2024-06-27 23:36:26,871] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 15883
[2024-06-27 23:36:27,262] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 15884
[2024-06-27 23:36:27,263] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 15885
[2024-06-27 23:36:27,613] [ERROR] [launch.py:321:sigkill_handler] ['/home/nfs02/anaconda3/envs/wzjsz/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--deepspeed', '/home/wangzj/aliyun/temp_data/LLaMA-Factory/llama-pt/config/ds_config.json', '--stage', 'pt', '--model_name_or_path', '/home/nfs04/wangzj/models/Qwen1.5-1.8B', '--adapter_name_or_path', '/home/nfs02/wangzj/checkpoints/llama-moe/qwen/1.8b-arderu6b-moe', '--train_only_router', '--moe_router_type', 'top2', '--topk', '2', '--ce_loss_coef', '0.05', '--do_train', '--dataset', 'ar_2b,de_2b', '--max_samples', '50000', '--preprocessing_num_workers', '1', '--cutoff_len', '512', '--finetuning_type', 'moe', '--output_dir', '/home/nfs04/wangzj/checkpoints/moe/test', '--overwrite_output_dir', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--save_total_limit', '100', '--save_steps', '10000', '--save_only_model', '--learning_rate', '2e-4', '--num_train_epochs', '1.0', '--plot_loss', '--fp16'] exits with return code = -9
