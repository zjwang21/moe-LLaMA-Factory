[2024-03-19 13:44:31,860] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-19 13:44:35,250] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-03-19 13:44:35,274] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/wzjsz/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9902 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json --stage pt --model_name_or_path /home/nfs02/model/llama2/hf/Llama-2-7b-hf --flash_attn --do_train --dataset is_1b --preprocessing_num_workers 4 --cutoff_len 2048 --finetuning_type lora --lora_target all --output_dir /home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b --overwrite_output_dir --per_device_train_batch_size 16 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 1 --save_only_model --save_steps 1000000 --learning_rate 2e-4 --num_train_epochs 1.0 --plot_loss --bf16
[2024-03-19 13:44:37,227] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-19 13:44:39,875] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-03-19 13:44:39,875] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-03-19 13:44:39,875] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-03-19 13:44:39,875] [INFO] [launch.py:163:main] dist_world_size=4
[2024-03-19 13:44:39,875] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-03-19 13:44:46,140] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-19 13:44:46,142] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-19 13:44:46,143] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-19 13:44:46,154] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-19 13:44:57,526] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-19 13:44:57,526] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-19 13:44:57,529] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-19 13:44:57,529] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-19 13:44:57,530] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
03/19/2024 13:44:58 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
03/19/2024 13:44:58 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1846] 2024-03-19 13:44:58,549 >> PyTorch: setting up devices
03/19/2024 13:44:58 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
03/19/2024 13:44:58 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/home/nfs02/wangzj/public_code/hitsz/transformers/src/transformers/training_args.py:1759: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/nfs02/wangzj/public_code/hitsz/transformers/src/transformers/training_args.py:1759: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/nfs02/wangzj/public_code/hitsz/transformers/src/transformers/training_args.py:1759: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
03/19/2024 13:44:58 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/19/2024 13:44:58 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/19/2024 13:44:58 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/19/2024 13:44:58 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b/runs/Mar19_13-44-57_Titan_RTX,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/19/2024 13:44:58 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b/runs/Mar19_13-44-57_Titan_RTX,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/19/2024 13:44:58 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b/runs/Mar19_13-44-57_Titan_RTX,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-03-19 13:44:58,564 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2027] 2024-03-19 13:44:58,565 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-03-19 13:44:58,565 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-03-19 13:44:58,565 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-03-19 13:44:58,565 >> loading file tokenizer.json
/home/nfs02/wangzj/public_code/hitsz/transformers/src/transformers/training_args.py:1759: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
03/19/2024 13:44:58 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/19/2024 13:44:58 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b/runs/Mar19_13-44-57_Titan_RTX,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:727] 2024-03-19 13:44:58,762 >> loading configuration file /home/nfs02/model/llama2/hf/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:792] 2024-03-19 13:44:58,763 >> Model config LlamaConfig {
  "_name_or_path": "/home/nfs02/model/llama2/hf/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

03/19/2024 13:44:58 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
03/19/2024 13:44:58 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
03/19/2024 13:44:58 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
03/19/2024 13:44:58 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
[INFO|modeling_utils.py:3334] 2024-03-19 13:44:58,858 >> loading weights file /home/nfs02/model/llama2/hf/Llama-2-7b-hf/model.safetensors.index.json
[INFO|modeling_utils.py:1459] 2024-03-19 13:44:58,859 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[WARNING|logging.py:329] 2024-03-19 13:44:58,859 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[WARNING|logging.py:329] 2024-03-19 13:44:58,862 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:827] 2024-03-19 13:44:58,865 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.12s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.34s/it]
03/19/2024 13:45:07 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/19/2024 13:45:07 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
03/19/2024 13:45:07 - INFO - llmtuner.model.utils - Found linear modules: gate_proj,o_proj,up_proj,q_proj,k_proj,down_proj,v_proj
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.85s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.85s/it]
[INFO|modeling_utils.py:4070] 2024-03-19 13:45:08,567 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4078] 2024-03-19 13:45:08,567 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/nfs02/model/llama2/hf/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.87s/it]
03/19/2024 13:45:08 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/19/2024 13:45:08 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
03/19/2024 13:45:08 - INFO - llmtuner.model.utils - Found linear modules: v_proj,gate_proj,q_proj,k_proj,up_proj,down_proj,o_proj
[INFO|configuration_utils.py:780] 2024-03-19 13:45:08,570 >> loading configuration file /home/nfs02/model/llama2/hf/Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:827] 2024-03-19 13:45:08,571 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

03/19/2024 13:45:08 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/19/2024 13:45:08 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
03/19/2024 13:45:08 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/19/2024 13:45:08 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
03/19/2024 13:45:08 - INFO - llmtuner.model.utils - Found linear modules: gate_proj,down_proj,q_proj,o_proj,v_proj,k_proj,up_proj
03/19/2024 13:45:08 - INFO - llmtuner.model.utils - Found linear modules: q_proj,o_proj,gate_proj,up_proj,down_proj,k_proj,v_proj
03/19/2024 13:45:10 - INFO - llmtuner.model.loader - trainable params: 19988480 || all params: 6758404096 || trainable%: 0.2958
03/19/2024 13:45:10 - INFO - llmtuner.data.template - Add pad token: </s>
03/19/2024 13:45:11 - INFO - llmtuner.model.loader - trainable params: 19988480 || all params: 6758404096 || trainable%: 0.2958
03/19/2024 13:45:11 - INFO - llmtuner.data.template - Add pad token: </s>
03/19/2024 13:45:11 - INFO - llmtuner.model.loader - trainable params: 19988480 || all params: 6758404096 || trainable%: 0.2958
03/19/2024 13:45:11 - INFO - llmtuner.data.template - Add pad token: </s>
03/19/2024 13:45:11 - INFO - llmtuner.model.loader - trainable params: 19988480 || all params: 6758404096 || trainable%: 0.2958
03/19/2024 13:45:11 - INFO - llmtuner.data.template - Add pad token: </s>
03/19/2024 13:45:17 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
Using custom data configuration default-b019e4271eec4c56
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
Found cached dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
Dataset({
    features: ['text'],
    num_rows: 619741
})
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00000_of_00004.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00001_of_00004.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00002_of_00004.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00003_of_00004.arrow
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_*_of_00004.arrow
Concatenating 4 shards
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-af876755afed9e29_00000_of_00004.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-af876755afed9e29_00001_of_00004.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-af876755afed9e29_00002_of_00004.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-af876755afed9e29_00003_of_00004.arrow
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-af876755afed9e29_*_of_00004.arrow
Concatenating 4 shards
input_ids:
[435, 30178, 4375, 29954, 29967, 30094, 29943, 341, 30175, 29940, 438, 29954, 476, 30232, 29934, 30178, 323, 6227, 379, 5348, 29903, 319, 10262, 1718, 29903, 448, 1605, 355, 1212, 13, 29953, 29953, 30073, 29940, 272, 30189, 332, 29928, 2190, 29924, 30094, 29934, 29968, 29924, 1430, 29915, 29903, 6850, 29979, 1307, 28577, 2672, 13171, 3094, 1964, 1254, 29979, 1307, 13, 30232, 29873, 492, 3516, 30189, 7019, 398, 14921, 1984, 289, 2518, 4677, 29876, 381, 3976, 29871, 30340, 2108, 273, 9523, 30189, 29892, 289, 7012, 381, 263, 30189, 1147, 29874, 3514, 273, 14468, 29871, 29945, 28786, 3671, 29871, 30340, 29976, 3602, 398, 3516, 30189, 263, 30189, 2377, 1764, 376, 30232, 15761, 398, 3516, 30189, 14921, 1984, 289, 2518, 263, 30189, 10235, 29874, 298, 369, 3963, 30189, 5848, 19615, 5444, 14468, 432, 29980, 3110, 12382, 29888, 29892, 321, 30189, 29874, 302, 30052, 1764, 24721, 2039, 688, 29884, 29892, 1999, 284, 2883, 29908, 785, 16210, 321, 30189, 29874, 289, 2518, 2215, 29874, 14468, 301, 30030, 29916, 375, 1341, 29983, 29889, 9444, 604, 16210, 394, 29122, 28683, 29983, 808, 29873, 29892, 3671, 27442, 1436, 29876, 303, 321, 6859, 814, 408, 29876, 744, 4141, 29871, 30340, 387, 279, 263, 30189, 12416, 330, 1572, 29871, 30340, 29874, 30189, 29889, 3067, 29887, 658, 5444, 30189, 29875, 289, 2518, 19421, 2464, 29888, 398, 27442, 263, 30189, 7779, 343, 29878, 30189, 29875, 14921, 1984, 28316, 16301, 30189, 275, 29892, 7779, 560, 4621, 263, 30189, 10235, 29874, 29892, 7779, 560, 4621, 263, 30189, 282, 30078, 433, 3671, 3731, 2681, 29892, 29871, 30078, 29871, 30340, 29875, 30189, 325, 4812, 30189, 29892, 599, 29873, 14468, 413, 5393, 398, 330, 1764, 29888, 20860, 30189, 1099, 1922, 432, 29980, 1915, 29892, 427, 298, 1064, 604, 7779, 29889, 3067, 29887, 9814, 30189, 29875, 29871, 30340, 11300, 14468, 28786, 29889, 29871, 30452, 11300, 722, 24883, 299, 279, 298, 29894, 548, 29875, 24721, 2039, 26281, 29892, 321, 30189, 29874, 286, 431, 433, 29892, 4934, 332, 12979, 1984, 29889, 16462, 1984, 3052, 16757, 1764, 4934, 332, 29892, 7779, 604, 343, 28034, 29542, 29871, 1715, 30078, 29887, 30189, 332, 592, 30189, 29871, 30340, 11300, 3671, 7779, 592, 3055, 2377, 1764, 380, 557, 29895, 10282, 29976, 29871, 30340, 404, 29884, 29889, 10630, 30189, 330, 29976, 29888, 398, 298, 369, 3963, 30189, 5848, 323, 513, 332, 3720, 22833, 4347, 1424, 29976, 29871, 29953, 29953, 30073, 29940, 272, 30189, 332, 785, 427, 285, 4316, 303, 722, 1820, 415, 1153, 29884, 30189, 29899, 932, 1379, 3642, 688, 2497, 3671, 29871, 30340, 29874, 30189, 4677, 1652, 29926, 29980, 698, 14468, 301, 29926, 7173, 263, 30189, 29871, 30340, 29874, 30189, 722, 14921, 1984, 269, 1690, 29879, 263, 30189, 3516, 30189, 330, 30078, 29873, 398, 316, 2782, 3720, 22833, 348, 1240, 29889, 10630, 30189, 3764, 398, 2215, 29876, 381, 14468, 1589, 407, 1240, 298, 369, 325, 557, 1056, 30189, 29875, 285, 4316, 29878, 3976, 286, 990, 29876, 1648, 6928, 263, 30189, 16511, 4207, 398, 14468, 325, 2559, 4347, 29892, 427, 3516, 30189, 286, 30078, 29873, 398, 3976, 269, 3304, 260, 29983, 655, 269, 1567, 21416, 29889, 13, 29903, 1365, 3514, 804, 290, 352, 17698, 30189, 722, 263, 30189, 3516, 30189, 286, 870, 398, 316, 4233, 23911, 1056, 30189, 29875, 3976, 302, 30052, 29878, 374, 3720, 29880, 3746, 29892, 321, 30189, 29874, 298, 812, 14468, 1153, 348, 262, 1240, 376, 1989, 415, 29875, 4365, 3720, 29878, 29908, 623, 1379, 3642, 688, 21528, 3671, 7779, 302, 30052, 698, 29875, 29871, 30340, 812, 6584, 292, 10282, 29983, 3731, 1340, 9161, 29889, 317, 1365, 10442, 1147, 30189, 398, 3516, 30189, 298, 11500, 18597, 314, 381, 6928, 29871, 30078, 17191, 17029, 29889, 1174, 3720, 29880, 8357, 604, 592, 3055, 3671, 1375, 1056, 10282, 29879, 2495, 7779, 29892, 427, 29871, 30340, 585, 3976, 698, 29884, 1011, 29874, 3976, 15012, 17930, 371, 29887, 1943, 298, 1064, 14468, 9879, 1785, 29892, 3031, 722, 10263, 1335, 289, 2518, 2071, 309, 370, 29877, 30189, 1424, 29976, 394, 6391, 262, 398, 263, 30189, 7779, 3976, 698, 29875, 263, 30189, 867, 29983, 3274, 298, 1648, 29889, 3067, 29887, 604, 1075, 2559, 29880, 361, 392, 29875, 592, 30189, 298, 1648, 29892, 3671, 12979, 21474, 3127, 2559, 3031, 7779, 604, 29892, 29871, 30340, 29976, 10263, 30189, 332, 27442, 1011, 29879, 3671, 7779, 540, 29888, 16511, 30189, 1011, 29882, 369, 29926, 398, 304, 407, 29889, 3067, 29887, 330, 24900, 13445, 1397, 29874, 560, 4621, 29871, 30340, 9297, 3720, 29880, 3746, 29889, 13, 25639, 913, 1137, 414, 14166, 3976, 269, 5963, 566, 810, 29885, 990, 1240, 785, 9083, 2574, 330, 13533, 263, 30189, 316, 4233, 29871, 30340, 9584, 592, 30189, 343, 6859, 332, 263, 30189, 29871, 30340, 404, 279, 289, 1314, 332, 604, 29884, 302, 30052, 1989, 415, 279, 3671, 29871, 30340, 17930, 3764, 29884, 28316, 29871, 30340, 29878, 29997, 865, 279, 263, 30189, 7779, 22500, 1896, 29871, 30340, 17930, 29871, 30078, 29873, 6092, 30189, 29884, 263, 30189, 7689, 996, 1764, 3976, 27442, 413, 25293, 595, 279, 29871, 30340, 387, 279, 7779, 1820, 415, 29875, 29871, 30340, 17930, 29892, 28316, 330, 29976, 21154, 29871, 30340, 17930, 4365, 29871, 30340, 29874, 30189, 286, 10058, 30189, 263, 30189, 7779, 330, 30078, 2034, 470, 30189, 29875, 30189, 29871, 5471, 29948, 698, 332, 3671, 451, 29874, 30189, 29871, 30340, 17930, 3976, 592, 30189, 29887, 29997, 865, 29884, 29889, 379, 3228, 335, 29871, 30340, 29878, 996, 29875, 7779, 289, 1314, 332, 29973, 317, 29926, 29980, 30189, 29874, 29871, 30340, 17930, 14468, 29871, 29929, 29900, 30073, 29973, 22305, 343, 29915, 497, 29889, 13, 29967, 30178, 4375, 30178, 16033, 1964, 9047, 1177, 29940, 341, 1177, 29940, 29871, 29906, 29900, 29896, 29955, 448, 13, 30178, 6059, 17896, 29923, 29902, 402, 1307, 30633, 29902, 1307, 29954, 2190, 383, 29979, 29934, 1254, 29909, 5012, 1660, 9486, 1001, 6824, 6824, 8982, 365, 30175, 3738, 30633, 382, 29968, 29968, 29902, 360, 30102, 29903, 5194, 1307, 23799, 476, 4717, 29968, 29968, 1718, 6824, 3067, 29887, 604, 289, 30030, 2559, 263, 30189, 413, 585, 3274, 599, 517, 29888, 15276, 279, 432, 29980, 3110, 1764, 28034, 3671, 27442, 1436, 29876, 303, 7779, 20642, 321, 4324, 2989, 29873, 321, 615, 381, 29889, 360, 12382, 8159, 560, 4621, 7779, 29871, 30340, 2108, 273, 260, 29983, 655, 3976, 2288, 29889, 3067, 29887, 604, 289, 15948, 303, 2142, 1397, 29874, 491, 29878, 1764, 30189, 332, 263, 30189, 13547, 1764, 6836, 30189, 332, 9489, 2679, 29895, 329, 29983, 25096, 29892, 3671, 604, 289, 30030, 2559, 263, 30189, 731, 1764, 2989, 29873, 14468, 25364, 1581, 29875, 30189, 22219, 28316, 7779, 30098, 2, 18574, 1049, 448, 14109, 29892, 1424, 29926, 2464, 4977, 394, 1341, 30078, 30189, 381, 4812, 30189, 13, 11746, 262, 2151, 260, 686, 398, 2464, 29901, 14859, 30340, 30052, 4621, 29892, 7319, 29890, 29983, 4621, 13, 29950, 29997, 21154, 30189, 5173, 30189, 332, 29901, 25132, 13, 20754, 9144, 348, 29901, 29871, 29941, 29889, 23882, 29871, 29896, 29929, 29929, 29900, 13, 29943, 5066, 2817, 2464, 29901, 29871, 29896, 29947, 29889, 29946, 29906, 29900, 29892, 29896, 29945, 2383, 30088, 13, 29924, 812, 29888, 12382, 430, 29875, 29901, 29871, 29946, 29892, 29900, 29945, 29900, 286, 14042, 29980, 29889, 313, 29941, 29900, 29889, 18251, 29871, 29906, 29900, 29896, 29946, 29897, 13, 30452, 29948, 698, 280, 10058, 491, 1505, 30189, 279, 29901, 29871, 29906, 29906, 29900, 29914, 8848, 30088, 13, 29963, 1389, 29879, 29983, 30189, 29874, 29901, 269, 14145, 29889, 311, 13, 29943, 943, 30078, 28898, 6135, 30189, 2276, 336, 29901, 5765, 476, 2267, 816, 1050, 313, 6530, 29965, 29897, 13, 29903, 1165, 1049, 313, 30340, 30052, 4621, 29901, 3878, 2079, 271, 23783, 29936, 7319, 29890, 29983, 4621, 29901, 3925, 711, 397, 1460, 1002, 19736, 4621, 29897, 604, 260, 29983, 8917, 380, 17930, 5173, 269, 1117, 4167, 1049, 29871, 30452, 30052, 808, 284, 4167, 29892, 29871, 29896, 29947, 29889, 29946, 29906, 29900, 2383, 11298, 29871, 30452, 29874, 30189, 604, 263, 30189, 269, 3304, 2071, 2754, 29871, 30340, 29874, 30189, 269, 1117, 4167, 1049, 3031, 302, 17930, 28537, 303, 6928, 21996, 1295, 29889, 27614, 18574, 1049, 29875, 289, 30030, 29874, 29871, 29946, 3533, 29926, 888, 381, 767, 1056, 313, 29941, 29900, 29889, 18251, 29871, 29906, 29900, 29896, 29946, 29897, 3671, 604, 29871, 30340, 11300, 29871, 30340, 9584, 19421, 8705, 29874, 285, 29926, 5873, 1527, 29876, 5427, 269, 1117, 4167, 1049, 29871, 30452, 30052, 808, 284, 4167, 29889, 11623, 21154, 30189, 14203, 262, 604, 25132, 29889, 2191, 30189, 284, 2791, 369, 30189, 336, 16511, 698, 30030, 26822, 29891, 374, 6050, 30078, 336, 14468, 18574, 1049, 29875, 10269, 452, 29888, 1056, 1652, 29926, 29980, 2034, 30189, 18574, 761, 29875, 29892, 285, 29926, 497, 5397, 30189, 2559, 17390, 29888, 12382, 645, 313, 2110, 29920, 4310, 381, 479, 29897, 3671, 8188, 698, 25934, 299, 381, 24156, 14468, 18574, 4515, 1335, 15012, 790, 313, 29903, 29437, 4070, 22703, 467, 13, 29903, 1165, 1049, 14172, 29887, 332, 21996, 579, 14468, 29871, 30452, 30052, 808, 11627, 29875, 3671, 3976, 301, 29997, 865, 2982, 314, 30078, 374, 263, 30189, 323, 5859, 29895, 1049, 29875, 29892, 427, 1011, 22234, 263, 30189, 349, 29980, 645, 392, 29875, 29889, 383, 29891, 12416, 3643, 30189, 273, 604, 269, 1117, 4167, 1049, 29875, 30189, 1771, 7257, 14203, 29892, 285, 29891, 12416, 3643, 30189, 10147, 273, 604, 23783, 29899, 2744, 10647, 3671, 285, 29891, 12416, 16779, 273, 604, 29884, 29871, 30452, 30052, 5393, 11627, 313, 1349, 1276, 4289, 29897, 3671, 350, 30078, 4758, 11627, 29889, 13, 29943, 29976, 1240, 18574, 5252, 604, 592, 30189, 260, 345, 326, 332, 301, 3054, 29948, 698, 398, 367, 6859, 29926, 398, 29892, 298, 29894, 2468, 398, 263, 30189, 310, 273, 3671, 867, 30078, 1949, 263, 30189, 452, 30189, 273, 29889, 4971, 29926, 2741, 279, 1050, 1984, 30189, 604, 29871, 30340, 369, 29878, 29997, 299, 29980, 698, 29892, 3731, 442, 3671, 3671, 330, 499, 29892, 592, 30189, 867, 30078, 1949, 9820, 30189, 29874, 3976, 2071, 29976, 343, 28034, 29889, 402, 21528, 3671, 3731, 1340, 9161, 7697, 595, 279, 604, 29884, 10282, 3389, 24156, 1424, 29976, 26579, 273, 29983, 261, 29899, 30078, 698, 262, 1240, 3976, 29871, 29896, 29906, 29889, 3963, 430, 29889, 1632, 30078, 1056, 2071, 29976, 4089, 30189, 273, 398, 722, 289, 30078, 698, 3516, 30189, 29871, 29896, 29906, 29953, 29900, 29889, 29871, 30452, 387, 279, 26579, 273, 29983, 261, 29899, 30078, 698, 262, 270, 29980, 3720, 29873, 260, 15948, 2791, 7979, 361, 2559, 1424, 29976, 2191, 13119, 269, 1064, 29871, 30340, 11300, 260, 29976, 3959, 3671, 540, 22613, 29871, 30340, 29874, 30189, 298, 2741, 391, 16296, 30189, 273, 29889, 13, 2816, 30189, 29875, 30189, 23783, 337, 23843, 364, 15827, 16296, 1056, 6928, 22593, 19041, 29871, 30078, 698, 29890, 2464, 2039, 1144, 18574, 29874, 29889, 1394, 30189, 29875, 30189, 872, 5389, 604, 270, 1727, 29875, 30189, 2511, 24988, 433, 22593, 19041, 470, 30189, 8675, 26024, 29879, 29892, 3031, 2778, 14166, 269, 369, 30189, 321, 30189, 29874, 6361, 332, 298, 2213, 22613, 313, 29879, 1182, 29889, 872, 29916, 3976, 14468, 2536, 575, 2120, 467, 518, 29896, 29962, 13, 29903, 1165, 1049, 16511, 30189, 29875, 3976, 30189, 332, 285, 4316, 29878, 343, 28034, 15207, 30189, 29976, 698, 398, 10058, 30189, 3731, 30078, 30189, 29875, 14468, 3643, 30189, 332, 4415, 6637, 10442, 369, 392, 29875, 29871, 30452, 30052, 808, 284, 4167, 29889, 3467, 5288, 4211, 7381, 298, 814, 15948, 22243, 273, 298, 29880, 6637, 29871, 30340, 404, 3731, 30078, 30189, 275, 16296, 30189, 433, 3976, 29871, 29947, 29889, 3963, 430, 3671, 14468, 10282, 29882, 2142, 29875, 29871, 30340, 29872, 381, 13678, 302, 29983, 870, 29884, 29889, 15012, 30078, 30189, 29875, 30189, 3031, 590, 299, 279, 10442, 369, 392, 29875, 18574, 1049, 722, 29871, 30340, 29980, 14921, 1984, 263, 30189, 2989, 29884, 954, 29875, 30189, 2511, 22593, 29997, 1949, 285, 4316, 29878, 427, 592, 30189, 2982, 13980, 1195, 29884, 286, 638, 433, 3976, 29871, 29896, 29906, 29889, 3963, 430, 29889, 1174, 29876, 14468, 12136, 24964, 29878, 269, 4112, 4515, 23843, 1375, 1240, 4415, 11321, 14468, 298, 6947, 30189, 8675, 29889, 2379, 342, 289, 30078, 1764, 29899, 3671, 289, 990, 29874, 354, 4812, 29871, 30340, 279, 604, 29884, 269, 4112, 4515, 29895, 263, 30189, 10282, 3389, 29874, 29889, 6162, 5173, 659, 1388, 30078, 698, 262, 14468, 298, 6947, 30189, 8675, 722, 26579, 273, 29983, 261, 29899, 30078, 698, 262, 29889, 7509, 303, 2559, 14468, 18574, 1049, 29875, 722, 413, 24900, 22613, 303, 29875, 14468, 29871, 30340, 30052, 4621, 364, 23576, 8675, 29889, 7260, 374, 30189, 29871, 29896, 29946, 29947, 29945, 413, 417, 29888, 3433, 30189, 29884, 22500, 582, 30189, 262, 18574, 1049, 3671, 29871, 30452, 30052, 5393, 11627, 14468, 9631, 29997, 22500, 582, 30189, 29889, 27614, 29871, 29941, 29900, 3976]
inputs:
JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet
66°NorðurDANMÖRKMEN'S STYLENEW INPERSONALSTYLE
Ætli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.
Svo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.
Drulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y'all.
JÓLAÓSKALISTINN MINN 2017 -
ÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…</s> Saxland - Wikipedia, frjálsa alfræðiritið
Opinbert tungumál: háþýska, sorbíska
Höfuðstaður: Dresden
Stofnun: 3. október 1990
Flatarmál: 18.420,15 km²
Mannfjöldi: 4,050 mljó. (30. september 2014)
Þéttleiki byggðar: 220/km²
Vefsíða: sachsen.de
Forsætisráðherra: Michael Kretschmer (CDU)
Saxland (þýska: Freistaat Sachsen; sorbíska: Swobodny stat Sakska) er tíunda stærsta sambandsland Þýskalands, 18.420 km². Það er að sama skapi það sambandsland sem nær lengst til austurs. Í Saxlandi búa 4 milljónir manna (30. september 2014) og er þetta því sjötta fjölmennasta sambandsland Þýskalands. Höfuðborgin er Dresden. Meðal markverðra náttúrufyrirbæra í Saxlandi má nefna fljótið Saxelfi, fjallgarðinn Erzfjöll (Erzgebirge) og klettamyndirnar í Saxneska Sviss (Sächsiche Schweiz).
Saxland liggur austast í Þýskalandi og á löng landamæri að Tékklandi, en einnig að Póllandi. Fyrir norðan er sambandslandið Brandenborg, fyrir norðvestan er Sachsen-Anhalt og fyrir vestan eru Þýringaland (Thüringen) og Bæjaraland.
Fáni Saxlands er með tveimur láréttum bekkjum, hvítum að ofan og grænum að neðan. Skjaldarmerkið er þverröndótt, svart og og gult, með grænum borða á ská yfir. Gulu og svörtu rendurnar eru upprunnar frá Askaníer-ættinni á 12. öld. Græna skáborðanum var bætt við 1260. Þegar Askaníer-ættin dó út tók markgreifinn frá Meissen sér þetta tákn og hefur það haldist síðan.
Orðið Sachsen rekur rót sína til germanska ættbálksins Saxa. Orðið saxi er dregið af gamla germanska orðinu sahs, sem merkir sverð eða langur hnífur (sbr. sax á íslensku). [1]
Saxland náði áður fyrr yfir víðáttumikið svæði í norðurhluta núverandi Þýskalands. Karlamagnús hertók mestan hluta þess svæðis síðla á 8. öld og í upphafi þeirrar níundu. Svæðið sem myndar núverandi Saxland var þó ekki að fullu numið af germönum fyrr en með landnáminu mikla á 12. öld. Enn í dag býr slavneskur minnihluti í héraðinu. Flest bæja- og borgaheiti þar eru slavnesk að uppruna. Helsta valdaættin í héraðinu var Askaníer-ættin. Furstinn í Saxlandi var kjörfursti í þýska ríkinu. Árið 1485 klofnuðu héruðin Saxland og Þýringaland í tvö héruð. Í 30 á
03/19/2024 13:46:45 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
03/19/2024 13:46:45 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
03/19/2024 13:46:45 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
Dataset({
    features: ['text'],
    num_rows: 619741
})
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
Dataset({
    features: ['text'],
    num_rows: 619741
})
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
Dataset({
    features: ['text'],
    num_rows: 619741
})
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
Loading cached shuffled indices for dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-202f0fb1439d1a88.arrow
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 487977
})
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:586] 2024-03-19 13:48:06,180 >> Using auto half precision backend
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 487977
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 487977
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 487977
})
[INFO|deepspeed.py:325] 2024-03-19 13:48:06,414 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/wangzj/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.0329625606536865 seconds
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.049065589904785 seconds
Time to load cpu_adam op: 3.0484023094177246 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.1299378871917725 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000200, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-03-19 13:48:13,564] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
[2024-03-19 13:48:23,201] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-03-19 13:48:23,206] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-03-19 13:48:23,207] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-03-19 13:48:23,254] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-03-19 13:48:23,254] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-03-19 13:48:23,255] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-03-19 13:48:23,255] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-03-19 13:48:23,255] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-03-19 13:48:23,255] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: True
[2024-03-19 13:48:23,255] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-03-19 13:48:26,394] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-03-19 13:48:26,394] [INFO] [utils.py:803:see_memory_usage] MA 12.65 GB         Max_MA 12.65 GB         CA 12.68 GB         Max_CA 13 GB 
[2024-03-19 13:48:26,395] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 100.99 GB, percent = 40.1%
[2024-03-19 13:48:26,592] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-03-19 13:48:26,592] [INFO] [utils.py:803:see_memory_usage] MA 12.65 GB         Max_MA 12.65 GB         CA 12.68 GB         Max_CA 13 GB 
[2024-03-19 13:48:26,592] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 101.1 GB, percent = 40.2%
[2024-03-19 13:48:26,593] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-03-19 13:48:26,757] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-03-19 13:48:26,758] [INFO] [utils.py:803:see_memory_usage] MA 12.65 GB         Max_MA 12.65 GB         CA 12.68 GB         Max_CA 13 GB 
[2024-03-19 13:48:26,758] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 101.1 GB, percent = 40.2%
[2024-03-19 13:48:26,765] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-03-19 13:48:26,765] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-03-19 13:48:26,765] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-03-19 13:48:26,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]
[2024-03-19 13:48:26,770] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-03-19 13:48:26,770] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-03-19 13:48:26,770] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-03-19 13:48:26,770] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-03-19 13:48:26,770] [INFO] [config.py:978:print]   amp_params ................... False
[2024-03-19 13:48:26,770] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-03-19 13:48:26,770] [INFO] [config.py:978:print]   bfloat16_enabled ............. True
[2024-03-19 13:48:26,770] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-03-19 13:48:26,770] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-03-19 13:48:26,770] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f23d837a320>
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   dump_state ................... False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... None
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   fp16_auto_cast ............... None
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   fp16_enabled ................. False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 4
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 1
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   loss_scale ................... 1.0
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-03-19 13:48:26,771] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   pld_params ................... False
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   train_batch_size ............. 256
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  16
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   world_size ................... 4
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-03-19 13:48:26,772] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-03-19 13:48:26,772] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1747] 2024-03-19 13:48:26,772 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-03-19 13:48:26,773 >>   Num examples = 487,977
[INFO|trainer.py:1749] 2024-03-19 13:48:26,773 >>   Num Epochs = 1
[INFO|trainer.py:1750] 2024-03-19 13:48:26,773 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1753] 2024-03-19 13:48:26,773 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:1754] 2024-03-19 13:48:26,773 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1755] 2024-03-19 13:48:26,773 >>   Total optimization steps = 1,906
[INFO|trainer.py:1756] 2024-03-19 13:48:26,777 >>   Number of trainable parameters = 19,988,480
  0%|          | 0/1906 [00:00<?, ?it/s]  0%|          | 1/1906 [01:11<37:58:49, 71.77s/it]  0%|          | 2/1906 [02:24<38:14:44, 72.31s/it]  0%|          | 3/1906 [03:37<38:28:02, 72.77s/it]  0%|          | 4/1906 [04:51<38:33:12, 72.97s/it]  0%|          | 5/1906 [06:04<38:35:12, 73.07s/it]  0%|          | 6/1906 [07:17<38:37:43, 73.19s/it]  0%|          | 7/1906 [08:31<38:37:42, 73.23s/it]  0%|          | 8/1906 [09:44<38:37:55, 73.27s/it]  0%|          | 9/1906 [10:57<38:37:32, 73.30s/it]  1%|          | 10/1906 [12:11<38:35:43, 73.28s/it]                                                    {'loss': 2.7832, 'learning_rate': 0.00019998641642375657, 'epoch': 0.01}
  1%|          | 10/1906 [12:11<38:35:43, 73.28s/it]  1%|          | 11/1906 [13:24<38:34:54, 73.30s/it]  1%|          | 12/1906 [14:37<38:32:45, 73.27s/it]  1%|          | 13/1906 [15:50<38:31:39, 73.27s/it]  1%|          | 14/1906 [17:03<38:29:05, 73.23s/it]  1%|          | 15/1906 [18:17<38:26:19, 73.18s/it]  1%|          | 16/1906 [19:30<38:24:57, 73.17s/it]  1%|          | 17/1906 [20:43<38:22:19, 73.13s/it]  1%|          | 18/1906 [21:56<38:21:34, 73.14s/it]  1%|          | 19/1906 [23:09<38:20:26, 73.15s/it]  1%|          | 20/1906 [24:22<38:20:00, 73.17s/it]                                                    {'loss': 2.6811, 'learning_rate': 0.00019994566938529712, 'epoch': 0.01}
  1%|          | 20/1906 [24:22<38:20:00, 73.17s/it]  1%|          | 21/1906 [25:36<38:19:40, 73.20s/it]  1%|          | 22/1906 [26:49<38:18:33, 73.20s/it]  1%|          | 23/1906 [28:02<38:16:43, 73.18s/it]  1%|▏         | 24/1906 [29:15<38:14:29, 73.15s/it]  1%|▏         | 25/1906 [30:28<38:12:19, 73.12s/it]  1%|▏         | 26/1906 [31:41<38:11:34, 73.14s/it]  1%|▏         | 27/1906 [32:54<38:10:42, 73.15s/it]  1%|▏         | 28/1906 [34:07<38:08:32, 73.12s/it]  2%|▏         | 29/1906 [35:20<38:07:20, 73.12s/it]  2%|▏         | 30/1906 [36:33<38:04:59, 73.08s/it]                                                    {'loss': 2.5563, 'learning_rate': 0.00019987776995443178, 'epoch': 0.02}
  2%|▏         | 30/1906 [36:34<38:04:59, 73.08s/it]  2%|▏         | 31/1906 [37:47<38:04:37, 73.11s/it]  2%|▏         | 32/1906 [39:00<38:02:59, 73.09s/it]  2%|▏         | 33/1906 [40:13<38:00:29, 73.05s/it]  2%|▏         | 34/1906 [41:26<38:00:01, 73.08s/it]  2%|▏         | 35/1906 [42:39<37:58:27, 73.07s/it]  2%|▏         | 36/1906 [43:52<37:57:44, 73.08s/it]  2%|▏         | 37/1906 [45:05<37:58:16, 73.14s/it]  2%|▏         | 38/1906 [46:18<37:57:11, 73.14s/it]  2%|▏         | 39/1906 [47:31<37:54:53, 73.11s/it]  2%|▏         | 40/1906 [48:45<37:54:03, 73.12s/it]                                                    {'loss': 2.475, 'learning_rate': 0.00019978273657750238, 'epoch': 0.02}
  2%|▏         | 40/1906 [48:45<37:54:03, 73.12s/it]  2%|▏         | 41/1906 [49:58<37:55:16, 73.20s/it]  2%|▏         | 42/1906 [51:11<37:52:30, 73.15s/it]  2%|▏         | 43/1906 [52:24<37:50:11, 73.11s/it]  2%|▏         | 44/1906 [53:37<37:48:17, 73.09s/it]  2%|▏         | 45/1906 [54:50<37:46:30, 73.07s/it]  2%|▏         | 46/1906 [56:03<37:46:44, 73.12s/it]  2%|▏         | 47/1906 [57:16<37:44:57, 73.10s/it]  3%|▎         | 48/1906 [58:29<37:43:01, 73.08s/it]  3%|▎         | 49/1906 [59:42<37:41:42, 73.08s/it]  3%|▎         | 50/1906 [1:00:56<37:41:04, 73.09s/it]                                                      {'loss': 2.4021, 'learning_rate': 0.0001996605950723714, 'epoch': 0.03}
  3%|▎         | 50/1906 [1:00:56<37:41:04, 73.09s/it]  3%|▎         | 51/1906 [1:02:09<37:42:37, 73.18s/it]  3%|▎         | 52/1906 [1:03:22<37:41:03, 73.17s/it]  3%|▎         | 53/1906 [1:04:35<37:39:19, 73.16s/it]  3%|▎         | 54/1906 [1:05:48<37:37:14, 73.13s/it]  3%|▎         | 55/1906 [1:07:01<37:35:38, 73.12s/it]  3%|▎         | 56/1906 [1:08:15<37:34:20, 73.11s/it]  3%|▎         | 57/1906 [1:09:28<37:32:57, 73.11s/it]  3%|▎         | 58/1906 [1:10:41<37:30:50, 73.08s/it]  3%|▎         | 59/1906 [1:11:54<37:29:23, 73.07s/it]  3%|▎         | 60/1906 [1:13:07<37:27:05, 73.04s/it]                                                      {'loss': 2.349, 'learning_rate': 0.00019951137862140778, 'epoch': 0.03}
  3%|▎         | 60/1906 [1:13:07<37:27:05, 73.04s/it]  3%|▎         | 61/1906 [1:14:20<37:26:20, 73.05s/it]  3%|▎         | 62/1906 [1:15:33<37:25:45, 73.07s/it]  3%|▎         | 63/1906 [1:16:46<37:23:59, 73.05s/it]  3%|▎         | 64/1906 [1:17:59<37:23:05, 73.06s/it]  3%|▎         | 65/1906 [1:19:12<37:22:26, 73.08s/it]  3%|▎         | 66/1906 [1:20:25<37:20:36, 73.06s/it]  4%|▎         | 67/1906 [1:21:38<37:19:48, 73.08s/it]  4%|▎         | 68/1906 [1:22:51<37:17:42, 73.05s/it]  4%|▎         | 69/1906 [1:24:04<37:16:00, 73.03s/it]  4%|▎         | 70/1906 [1:25:17<37:15:10, 73.04s/it]                                                      {'loss': 2.3049, 'learning_rate': 0.0001993351277624723, 'epoch': 0.04}
  4%|▎         | 70/1906 [1:25:17<37:15:10, 73.04s/it]  4%|▎         | 71/1906 [1:26:30<37:15:09, 73.08s/it]  4%|▍         | 72/1906 [1:27:43<37:13:16, 73.06s/it]  4%|▍         | 73/1906 [1:28:57<37:12:25, 73.07s/it]  4%|▍         | 74/1906 [1:30:10<37:10:18, 73.04s/it]  4%|▍         | 75/1906 [1:31:23<37:08:58, 73.04s/it]  4%|▍         | 76/1906 [1:32:36<37:08:27, 73.06s/it]  4%|▍         | 77/1906 [1:33:49<37:07:07, 73.06s/it]  4%|▍         | 78/1906 [1:35:02<37:05:57, 73.06s/it]  4%|▍         | 79/1906 [1:36:15<37:03:43, 73.03s/it]  4%|▍         | 80/1906 [1:37:28<37:02:22, 73.02s/it]                                                      {'loss': 2.266, 'learning_rate': 0.00019913189037790456, 'epoch': 0.04}
  4%|▍         | 80/1906 [1:37:28<37:02:22, 73.02s/it]  4%|▍         | 81/1906 [1:38:41<37:01:55, 73.05s/it]  4%|▍         | 82/1906 [1:39:54<36:59:56, 73.02s/it]  4%|▍         | 83/1906 [1:41:07<36:58:07, 73.00s/it]  4%|▍         | 84/1906 [1:42:20<36:56:28, 72.99s/it]  4%|▍         | 85/1906 [1:43:33<36:54:42, 72.97s/it]  5%|▍         | 86/1906 [1:44:46<36:53:46, 72.98s/it]  5%|▍         | 87/1906 [1:45:59<36:53:23, 73.01s/it]  5%|▍         | 88/1906 [1:47:12<36:52:33, 73.02s/it]  5%|▍         | 89/1906 [1:48:25<36:51:40, 73.03s/it]  5%|▍         | 90/1906 [1:49:38<36:50:08, 73.02s/it]                                                      {'loss': 2.234, 'learning_rate': 0.00019890172168151473, 'epoch': 0.05}
  5%|▍         | 90/1906 [1:49:38<36:50:08, 73.02s/it]  5%|▍         | 91/1906 [1:50:51<36:49:14, 73.03s/it]  5%|▍         | 92/1906 [1:52:04<36:48:22, 73.04s/it]  5%|▍         | 93/1906 [1:53:17<36:47:16, 73.05s/it]  5%|▍         | 94/1906 [1:54:30<36:46:10, 73.05s/it]  5%|▍         | 95/1906 [1:55:43<36:45:45, 73.08s/it]  5%|▌         | 96/1906 [1:56:56<36:43:46, 73.05s/it]  5%|▌         | 97/1906 [1:58:09<36:41:37, 73.02s/it]  5%|▌         | 98/1906 [1:59:22<36:40:00, 73.01s/it]  5%|▌         | 99/1906 [2:00:35<36:38:25, 73.00s/it]  5%|▌         | 100/1906 [2:01:48<36:37:42, 73.01s/it]                                                       {'loss': 2.1976, 'learning_rate': 0.00019864468420358354, 'epoch': 0.05}
  5%|▌         | 100/1906 [2:01:48<36:37:42, 73.01s/it]  5%|▌         | 101/1906 [2:03:01<36:37:20, 73.04s/it]  5%|▌         | 102/1906 [2:04:14<36:36:05, 73.04s/it]  5%|▌         | 103/1906 [2:05:27<36:34:46, 73.04s/it]  5%|▌         | 104/1906 [2:06:40<36:32:57, 73.02s/it]  6%|▌         | 105/1906 [2:07:53<36:32:26, 73.04s/it]  6%|▌         | 106/1906 [2:09:06<36:30:38, 73.02s/it]  6%|▌         | 107/1906 [2:10:19<36:28:55, 73.00s/it]  6%|▌         | 108/1906 [2:11:32<36:28:07, 73.02s/it]  6%|▌         | 109/1906 [2:12:45<36:26:56, 73.02s/it]  6%|▌         | 110/1906 [2:13:58<36:24:56, 72.99s/it]                                                       {'loss': 2.1881, 'learning_rate': 0.00019836084777387458, 'epoch': 0.06}
  6%|▌         | 110/1906 [2:13:58<36:24:56, 72.99s/it]  6%|▌         | 111/1906 [2:15:11<36:24:19, 73.01s/it]  6%|▌         | 112/1906 [2:16:24<36:22:48, 73.00s/it]  6%|▌         | 113/1906 [2:17:37<36:20:44, 72.98s/it]  6%|▌         | 114/1906 [2:18:50<36:18:54, 72.95s/it]  6%|▌         | 115/1906 [2:20:03<36:18:21, 72.98s/it]  6%|▌         | 116/1906 [2:21:16<36:16:51, 72.97s/it]  6%|▌         | 117/1906 [2:22:29<36:15:25, 72.96s/it]  6%|▌         | 118/1906 [2:23:42<36:14:37, 72.97s/it]  6%|▌         | 119/1906 [2:24:55<36:12:40, 72.95s/it]  6%|▋         | 120/1906 [2:26:08<36:10:57, 72.93s/it]                                                       {'loss': 2.1604, 'learning_rate': 0.00019805028950266348, 'epoch': 0.06}
  6%|▋         | 120/1906 [2:26:08<36:10:57, 72.93s/it]  6%|▋         | 121/1906 [2:27:21<36:09:56, 72.94s/it]  6%|▋         | 122/1906 [2:28:34<36:08:42, 72.94s/it]  6%|▋         | 123/1906 [2:29:47<36:07:08, 72.93s/it]  7%|▋         | 124/1906 [2:31:00<36:06:22, 72.94s/it]  7%|▋         | 125/1906 [2:32:13<36:05:05, 72.94s/it]  7%|▋         | 126/1906 [2:33:26<36:03:36, 72.93s/it]  7%|▋         | 127/1906 [2:34:38<36:02:20, 72.93s/it]  7%|▋         | 128/1906 [2:35:51<36:01:45, 72.95s/it]  7%|▋         | 129/1906 [2:37:04<36:00:49, 72.96s/it]  7%|▋         | 130/1906 [2:38:17<35:59:30, 72.96s/it]                                                       {'loss': 2.1371, 'learning_rate': 0.0001977130937597894, 'epoch': 0.07}
  7%|▋         | 130/1906 [2:38:17<35:59:30, 72.96s/it]  7%|▋         | 131/1906 [2:39:30<35:58:39, 72.97s/it]  7%|▋         | 132/1906 [2:40:43<35:57:20, 72.97s/it]  7%|▋         | 133/1906 [2:41:56<35:55:51, 72.96s/it]  7%|▋         | 134/1906 [2:43:09<35:54:45, 72.96s/it]  7%|▋         | 135/1906 [2:44:22<35:53:46, 72.97s/it]  7%|▋         | 136/1906 [2:45:35<35:52:14, 72.96s/it]  7%|▋         | 137/1906 [2:46:48<35:51:33, 72.98s/it]  7%|▋         | 138/1906 [2:48:01<35:50:37, 72.99s/it]  7%|▋         | 139/1906 [2:49:14<35:49:20, 72.98s/it]  7%|▋         | 140/1906 [2:50:27<35:47:28, 72.96s/it]                                                       {'loss': 2.1239, 'learning_rate': 0.00019734935215173392, 'epoch': 0.07}
  7%|▋         | 140/1906 [2:50:27<35:47:28, 72.96s/it]  7%|▋         | 141/1906 [2:51:40<35:46:42, 72.98s/it]  7%|▋         | 142/1906 [2:52:53<35:45:46, 72.99s/it]  8%|▊         | 143/1906 [2:54:06<35:44:15, 72.98s/it]  8%|▊         | 144/1906 [2:55:19<35:42:49, 72.97s/it]  8%|▊         | 145/1906 [2:56:32<35:41:03, 72.95s/it]  8%|▊         | 146/1906 [2:57:45<35:40:03, 72.96s/it]  8%|▊         | 147/1906 [2:58:58<35:39:43, 72.99s/it]  8%|▊         | 148/1906 [3:00:11<35:38:04, 72.97s/it]  8%|▊         | 149/1906 [3:01:24<35:36:49, 72.97s/it]  8%|▊         | 150/1906 [3:02:37<35:35:50, 72.98s/it]                                                       {'loss': 2.1056, 'learning_rate': 0.0001969591634967344, 'epoch': 0.08}
  8%|▊         | 150/1906 [3:02:37<35:35:50, 72.98s/it]  8%|▊         | 151/1906 [3:03:50<35:35:32, 73.01s/it]  8%|▊         | 152/1906 [3:05:03<35:34:10, 73.00s/it]  8%|▊         | 153/1906 [3:06:16<35:32:59, 73.01s/it]  8%|▊         | 154/1906 [3:07:29<35:31:31, 73.00s/it]  8%|▊         | 155/1906 [3:08:42<35:29:07, 72.96s/it]  8%|▊         | 156/1906 [3:09:55<35:27:39, 72.95s/it]  8%|▊         | 157/1906 [3:11:08<35:25:37, 72.92s/it]  8%|▊         | 158/1906 [3:12:21<35:25:09, 72.95s/it]  8%|▊         | 159/1906 [3:13:33<35:22:37, 72.90s/it]  8%|▊         | 160/1906 [3:14:46<35:21:58, 72.92s/it]                                                       {'loss': 2.0957, 'learning_rate': 0.00019654263379793773, 'epoch': 0.08}
  8%|▊         | 160/1906 [3:14:46<35:21:58, 72.92s/it]  8%|▊         | 161/1906 [3:15:59<35:21:30, 72.95s/it]  8%|▊         | 162/1906 [3:17:12<35:20:30, 72.95s/it]  9%|▊         | 163/1906 [3:18:25<35:18:29, 72.93s/it]  9%|▊         | 164/1906 [3:19:38<35:16:44, 72.91s/it]  9%|▊         | 165/1906 [3:20:51<35:15:22, 72.90s/it]  9%|▊         | 166/1906 [3:22:04<35:13:09, 72.87s/it]  9%|▉         | 167/1906 [3:23:17<35:11:42, 72.86s/it]  9%|▉         | 168/1906 [3:24:29<35:10:39, 72.87s/it]  9%|▉         | 169/1906 [3:25:42<35:08:37, 72.84s/it]  9%|▉         | 170/1906 [3:26:55<35:08:06, 72.86s/it]                                                       {'loss': 2.0705, 'learning_rate': 0.00019609987621460232, 'epoch': 0.09}
  9%|▉         | 170/1906 [3:26:55<35:08:06, 72.86s/it]  9%|▉         | 171/1906 [3:28:08<35:07:26, 72.88s/it]  9%|▉         | 172/1906 [3:29:21<35:06:16, 72.88s/it]  9%|▉         | 173/1906 [3:30:34<35:05:26, 72.89s/it]  9%|▉         | 174/1906 [3:31:47<35:04:10, 72.89s/it]  9%|▉         | 175/1906 [3:33:00<35:03:20, 72.91s/it]  9%|▉         | 176/1906 [3:34:13<35:01:49, 72.90s/it]  9%|▉         | 177/1906 [3:35:25<35:00:54, 72.91s/it]  9%|▉         | 178/1906 [3:36:38<34:59:05, 72.88s/it]  9%|▉         | 179/1906 [3:37:51<34:58:21, 72.90s/it]  9%|▉         | 180/1906 [3:39:04<34:57:15, 72.91s/it]                                                       {'loss': 2.0506, 'learning_rate': 0.00019563101103135602, 'epoch': 0.09}
  9%|▉         | 180/1906 [3:39:04<34:57:15, 72.91s/it]  9%|▉         | 181/1906 [3:40:17<34:56:30, 72.92s/it] 10%|▉         | 182/1906 [3:41:30<34:55:05, 72.91s/it] 10%|▉         | 183/1906 [3:42:43<34:53:12, 72.89s/it] 10%|▉         | 184/1906 [3:43:56<34:51:19, 72.87s/it] 10%|▉         | 185/1906 [3:45:09<34:50:28, 72.88s/it] 10%|▉         | 186/1906 [3:46:22<34:49:39, 72.89s/it] 10%|▉         | 187/1906 [3:47:35<34:49:03, 72.92s/it] 10%|▉         | 188/1906 [3:48:47<34:47:07, 72.89s/it] 10%|▉         | 189/1906 [3:50:00<34:45:56, 72.89s/it] 10%|▉         | 190/1906 [3:51:13<34:44:58, 72.90s/it]                                                       {'loss': 2.0385, 'learning_rate': 0.00019513616562551807, 'epoch': 0.1}
 10%|▉         | 190/1906 [3:51:13<34:44:58, 72.90s/it] 10%|█         | 191/1906 [3:52:26<34:44:25, 72.92s/it] 10%|█         | 192/1906 [3:53:39<34:43:20, 72.93s/it] 10%|█         | 193/1906 [3:54:52<34:42:45, 72.95s/it] 10%|█         | 194/1906 [3:56:05<34:41:33, 72.95s/it] 10%|█         | 195/1906 [3:57:18<34:39:45, 72.93s/it] 10%|█         | 196/1906 [3:58:31<34:38:30, 72.93s/it] 10%|█         | 197/1906 [3:59:44<34:37:21, 72.93s/it] 10%|█         | 198/1906 [4:00:57<34:36:33, 72.95s/it] 10%|█         | 199/1906 [4:02:10<34:34:42, 72.92s/it] 10%|█         | 200/1906 [4:03:23<34:34:05, 72.95s/it]                                                       {'loss': 2.0337, 'learning_rate': 0.0001946154744324945, 'epoch': 0.1}
 10%|█         | 200/1906 [4:03:23<34:34:05, 72.95s/it] 11%|█         | 201/1906 [4:04:36<34:32:50, 72.94s/it] 11%|█         | 202/1906 [4:05:49<34:31:35, 72.94s/it] 11%|█         | 203/1906 [4:07:01<34:30:05, 72.93s/it] 11%|█         | 204/1906 [4:08:14<34:28:40, 72.93s/it] 11%|█         | 205/1906 [4:09:27<34:27:36, 72.93s/it] 11%|█         | 206/1906 [4:10:40<34:26:32, 72.94s/it] 11%|█         | 207/1906 [4:11:53<34:25:21, 72.94s/it] 11%|█         | 208/1906 [4:13:06<34:23:58, 72.93s/it] 11%|█         | 209/1906 [4:14:19<34:22:33, 72.93s/it] 11%|█         | 210/1906 [4:15:32<34:21:43, 72.94s/it]                                                       {'loss': 2.0223, 'learning_rate': 0.00019406907890925562, 'epoch': 0.11}
 11%|█         | 210/1906 [4:15:32<34:21:43, 72.94s/it] 11%|█         | 211/1906 [4:16:45<34:20:27, 72.94s/it] 11%|█         | 212/1906 [4:17:58<34:18:55, 72.93s/it] 11%|█         | 213/1906 [4:19:11<34:18:04, 72.94s/it] 11%|█         | 214/1906 [4:20:24<34:16:45, 72.93s/it] 11%|█▏        | 215/1906 [4:21:37<34:15:57, 72.95s/it] 11%|█▏        | 216/1906 [4:22:50<34:14:28, 72.94s/it] 11%|█▏        | 217/1906 [4:24:02<34:12:29, 72.91s/it] 11%|█▏        | 218/1906 [4:25:15<34:11:02, 72.90s/it] 11%|█▏        | 219/1906 [4:26:28<34:09:30, 72.89s/it] 12%|█▏        | 220/1906 [4:27:41<34:08:41, 72.91s/it]                                                       {'loss': 2.0179, 'learning_rate': 0.00019349712749590649, 'epoch': 0.12}
 12%|█▏        | 220/1906 [4:27:41<34:08:41, 72.91s/it] 12%|█▏        | 221/1906 [4:28:54<34:07:45, 72.92s/it] 12%|█▏        | 222/1906 [4:30:07<34:06:00, 72.90s/it] 12%|█▏        | 223/1906 [4:31:20<34:04:46, 72.90s/it] 12%|█▏        | 224/1906 [4:32:33<34:03:50, 72.91s/it] 12%|█▏        | 225/1906 [4:33:46<34:02:27, 72.90s/it] 12%|█▏        | 226/1906 [4:34:58<34:00:39, 72.88s/it] 12%|█▏        | 227/1906 [4:36:11<33:59:05, 72.87s/it] 12%|█▏        | 228/1906 [4:37:24<33:57:51, 72.87s/it] 12%|█▏        | 229/1906 [4:38:37<33:56:17, 72.85s/it] 12%|█▏        | 230/1906 [4:39:50<33:55:23, 72.87s/it]                                                       {'loss': 2.0008, 'learning_rate': 0.0001928997755753597, 'epoch': 0.12}
 12%|█▏        | 230/1906 [4:39:50<33:55:23, 72.87s/it] 12%|█▏        | 231/1906 [4:41:03<33:54:40, 72.88s/it] 12%|█▏        | 232/1906 [4:42:16<33:53:33, 72.89s/it] 12%|█▏        | 233/1906 [4:43:29<33:52:11, 72.88s/it] 12%|█▏        | 234/1906 [4:44:41<33:50:54, 72.88s/it] 12%|█▏        | 235/1906 [4:45:54<33:50:17, 72.90s/it] 12%|█▏        | 236/1906 [4:47:07<33:49:12, 72.91s/it] 12%|█▏        | 237/1906 [4:48:20<33:47:40, 72.89s/it] 12%|█▏        | 238/1906 [4:49:33<33:46:53, 72.91s/it] 13%|█▎        | 239/1906 [4:50:46<33:45:44, 72.91s/it] 13%|█▎        | 240/1906 [4:51:59<33:44:06, 72.90s/it]                                                       {'loss': 1.9804, 'learning_rate': 0.00019227718543112236, 'epoch': 0.13}
 13%|█▎        | 240/1906 [4:51:59<33:44:06, 72.90s/it] 13%|█▎        | 241/1906 [4:53:12<33:43:07, 72.91s/it] 13%|█▎        | 242/1906 [4:54:25<33:41:50, 72.90s/it] 13%|█▎        | 243/1906 [4:55:38<33:40:30, 72.90s/it] 13%|█▎        | 244/1906 [4:56:51<33:39:27, 72.90s/it] 13%|█▎        | 245/1906 [4:58:03<33:38:11, 72.90s/it] 13%|█▎        | 246/1906 [4:59:16<33:37:35, 72.93s/it] 13%|█▎        | 247/1906 [5:00:29<33:35:46, 72.90s/it] 13%|█▎        | 248/1906 [5:01:42<33:34:09, 72.89s/it] 13%|█▎        | 249/1906 [5:02:55<33:33:11, 72.90s/it] 13%|█▎        | 250/1906 [5:04:08<33:32:09, 72.90s/it]                                                       {'loss': 1.9783, 'learning_rate': 0.0001916295262032084, 'epoch': 0.13}
 13%|█▎        | 250/1906 [5:04:08<33:32:09, 72.90s/it] 13%|█▎        | 251/1906 [5:05:21<33:30:58, 72.91s/it] 13%|█▎        | 252/1906 [5:06:34<33:30:05, 72.92s/it] 13%|█▎        | 253/1906 [5:07:47<33:29:09, 72.93s/it] 13%|█▎        | 254/1906 [5:09:00<33:27:33, 72.91s/it] 13%|█▎        | 255/1906 [5:10:13<33:26:35, 72.92s/it] 13%|█▎        | 256/1906 [5:11:26<33:25:22, 72.92s/it] 13%|█▎        | 257/1906 [5:12:38<33:24:22, 72.93s/it] 14%|█▎        | 258/1906 [5:13:51<33:22:46, 72.92s/it] 14%|█▎        | 259/1906 [5:15:04<33:21:11, 72.90s/it] 14%|█▎        | 260/1906 [5:16:17<33:19:31, 72.89s/it]                                                       {'loss': 1.9723, 'learning_rate': 0.0001909569738421878, 'epoch': 0.14}
 14%|█▎        | 260/1906 [5:16:17<33:19:31, 72.89s/it] 14%|█▎        | 261/1906 [5:17:30<33:18:07, 72.88s/it] 14%|█▎        | 262/1906 [5:18:43<33:17:35, 72.90s/it] 14%|█▍        | 263/1906 [5:19:56<33:15:44, 72.88s/it] 14%|█▍        | 264/1906 [5:21:09<33:14:14, 72.87s/it] 14%|█▍        | 265/1906 [5:22:21<33:13:21, 72.88s/it] 14%|█▍        | 266/1906 [5:23:34<33:11:35, 72.86s/it] 14%|█▍        | 267/1906 [5:24:47<33:10:34, 72.87s/it] 14%|█▍        | 268/1906 [5:26:00<33:09:20, 72.87s/it] 14%|█▍        | 269/1906 [5:27:13<33:08:15, 72.87s/it] 14%|█▍        | 270/1906 [5:28:26<33:07:15, 72.88s/it]                                                       {'loss': 1.9647, 'learning_rate': 0.000190259711061386, 'epoch': 0.14}
 14%|█▍        | 270/1906 [5:28:26<33:07:15, 72.88s/it] 14%|█▍        | 271/1906 [5:29:39<33:06:57, 72.92s/it] 14%|█▍        | 272/1906 [5:30:52<33:05:35, 72.91s/it] 14%|█▍        | 273/1906 [5:32:05<33:04:07, 72.90s/it] 14%|█▍        | 274/1906 [5:33:18<33:02:56, 72.90s/it] 14%|█▍        | 275/1906 [5:34:30<33:01:19, 72.89s/it] 14%|█▍        | 276/1906 [5:35:43<32:59:34, 72.87s/it] 15%|█▍        | 277/1906 [5:36:56<32:58:51, 72.89s/it] 15%|█▍        | 278/1906 [5:38:09<32:57:54, 72.90s/it] 15%|█▍        | 279/1906 [5:39:22<32:56:26, 72.89s/it] 15%|█▍        | 280/1906 [5:40:35<32:55:44, 72.91s/it]                                                       {'loss': 1.9647, 'learning_rate': 0.000189537927287246, 'epoch': 0.15}
 15%|█▍        | 280/1906 [5:40:35<32:55:44, 72.91s/it] 15%|█▍        | 281/1906 [5:41:48<32:55:23, 72.94s/it] 15%|█▍        | 282/1906 [5:43:01<32:54:18, 72.94s/it] 15%|█▍        | 283/1906 [5:44:14<32:52:21, 72.92s/it] 15%|█▍        | 284/1906 [5:45:27<32:51:11, 72.92s/it] 15%|█▍        | 285/1906 [5:46:40<32:49:53, 72.91s/it] 15%|█▌        | 286/1906 [5:47:52<32:48:23, 72.90s/it] 15%|█▌        | 287/1906 [5:49:05<32:47:02, 72.90s/it] 15%|█▌        | 288/1906 [5:50:18<32:46:19, 72.92s/it] 15%|█▌        | 289/1906 [5:51:31<32:44:42, 72.90s/it] 15%|█▌        | 290/1906 [5:52:44<32:43:36, 72.91s/it]                                                       {'loss': 1.9457, 'learning_rate': 0.00018879181860786623, 'epoch': 0.15}
 15%|█▌        | 290/1906 [5:52:44<32:43:36, 72.91s/it] 15%|█▌        | 291/1906 [5:53:57<32:42:47, 72.92s/it] 15%|█▌        | 292/1906 [5:55:10<32:41:22, 72.91s/it] 15%|█▌        | 293/1906 [5:56:23<32:40:06, 72.91s/it] 15%|█▌        | 294/1906 [5:57:36<32:38:47, 72.91s/it] 15%|█▌        | 295/1906 [5:58:49<32:37:04, 72.89s/it] 16%|█▌        | 296/1906 [6:00:01<32:36:17, 72.91s/it] 16%|█▌        | 297/1906 [6:01:14<32:34:29, 72.88s/it] 16%|█▌        | 298/1906 [6:02:27<32:33:29, 72.89s/it] 16%|█▌        | 299/1906 [6:03:40<32:32:50, 72.91s/it] 16%|█▌        | 300/1906 [6:04:53<32:31:53, 72.92s/it]                                                       {'loss': 1.9417, 'learning_rate': 0.00018802158771972943, 'epoch': 0.16}
 16%|█▌        | 300/1906 [6:04:53<32:31:53, 72.92s/it] 16%|█▌        | 301/1906 [6:06:06<32:30:51, 72.93s/it] 16%|█▌        | 302/1906 [6:07:19<32:29:43, 72.93s/it] 16%|█▌        | 303/1906 [6:08:32<32:28:06, 72.92s/it] 16%|█▌        | 304/1906 [6:09:45<32:26:52, 72.92s/it] 16%|█▌        | 305/1906 [6:10:58<32:25:41, 72.92s/it] 16%|█▌        | 306/1906 [6:12:11<32:24:31, 72.92s/it] 16%|█▌        | 307/1906 [6:13:23<32:22:42, 72.90s/it] 16%|█▌        | 308/1906 [6:14:36<32:21:12, 72.89s/it] 16%|█▌        | 309/1906 [6:15:49<32:19:52, 72.88s/it] 16%|█▋        | 310/1906 [6:17:02<32:18:10, 72.86s/it]                                                       {'loss': 1.9334, 'learning_rate': 0.00018722744387263544, 'epoch': 0.16}
 16%|█▋        | 310/1906 [6:17:02<32:18:10, 72.86s/it] 16%|█▋        | 311/1906 [6:18:15<32:17:23, 72.88s/it] 16%|█▋        | 312/1906 [6:19:28<32:15:39, 72.86s/it] 16%|█▋        | 313/1906 [6:20:41<32:14:28, 72.86s/it] 16%|█▋        | 314/1906 [6:21:54<32:13:29, 72.87s/it] 17%|█▋        | 315/1906 [6:23:06<32:12:00, 72.86s/it] 17%|█▋        | 316/1906 [6:24:19<32:10:36, 72.85s/it] 17%|█▋        | 317/1906 [6:25:32<32:09:08, 72.84s/it] 17%|█▋        | 318/1906 [6:26:45<32:08:04, 72.85s/it] 17%|█▋        | 319/1906 [6:27:58<32:06:03, 72.82s/it] 17%|█▋        | 320/1906 [6:29:10<32:04:36, 72.81s/it]                                                       {'loss': 1.9243, 'learning_rate': 0.00018640960281285417, 'epoch': 0.17}
 17%|█▋        | 320/1906 [6:29:10<32:04:36, 72.81s/it] 17%|█▋        | 321/1906 [6:30:23<32:03:29, 72.81s/it] 17%|█▋        | 322/1906 [6:31:36<32:01:53, 72.80s/it] 17%|█▋        | 323/1906 [6:32:49<32:00:12, 72.78s/it] 17%|█▋        | 324/1906 [6:34:02<31:58:50, 72.78s/it] 17%|█▋        | 325/1906 [6:35:14<31:57:08, 72.76s/it] 17%|█▋        | 326/1906 [6:36:27<31:56:17, 72.77s/it] 17%|█▋        | 327/1906 [6:37:40<31:54:54, 72.76s/it] 17%|█▋        | 328/1906 [6:38:53<31:54:08, 72.78s/it] 17%|█▋        | 329/1906 [6:40:05<31:53:32, 72.80s/it] 17%|█▋        | 330/1906 [6:41:18<31:52:25, 72.81s/it]                                                       {'loss': 1.9197, 'learning_rate': 0.0001855682867245134, 'epoch': 0.17}
 17%|█▋        | 330/1906 [6:41:18<31:52:25, 72.81s/it] 17%|█▋        | 331/1906 [6:42:31<31:52:03, 72.84s/it] 17%|█▋        | 332/1906 [6:43:44<31:50:22, 72.82s/it] 17%|█▋        | 333/1906 [6:44:57<31:48:57, 72.81s/it] 18%|█▊        | 334/1906 [6:46:10<31:47:42, 72.81s/it] 18%|█▊        | 335/1906 [6:47:22<31:47:10, 72.84s/it] 18%|█▊        | 336/1906 [6:48:35<31:46:35, 72.86s/it] 18%|█▊        | 337/1906 [6:49:48<31:45:18, 72.86s/it] 18%|█▊        | 338/1906 [6:51:01<31:44:13, 72.87s/it] 18%|█▊        | 339/1906 [6:52:14<31:42:43, 72.85s/it] 18%|█▊        | 340/1906 [6:53:27<31:41:15, 72.85s/it]                                                       {'loss': 1.9217, 'learning_rate': 0.0001847037241692378, 'epoch': 0.18}
 18%|█▊        | 340/1906 [6:53:27<31:41:15, 72.85s/it] 18%|█▊        | 341/1906 [6:54:40<31:39:52, 72.84s/it] 18%|█▊        | 342/1906 [6:55:53<31:39:18, 72.86s/it] 18%|█▊        | 343/1906 [6:57:05<31:38:19, 72.87s/it] 18%|█▊        | 344/1906 [6:58:18<31:36:54, 72.86s/it] 18%|█▊        | 345/1906 [6:59:31<31:35:26, 72.85s/it] 18%|█▊        | 346/1906 [7:00:44<31:34:13, 72.85s/it] 18%|█▊        | 347/1906 [7:01:57<31:33:20, 72.87s/it] 18%|█▊        | 348/1906 [7:03:10<31:32:01, 72.86s/it] 18%|█▊        | 349/1906 [7:04:23<31:30:47, 72.86s/it] 18%|█▊        | 350/1906 [7:05:35<31:29:47, 72.87s/it]                                                       {'loss': 1.9144, 'learning_rate': 0.00018381615002405509, 'epoch': 0.18}
 18%|█▊        | 350/1906 [7:05:35<31:29:47, 72.87s/it] 18%|█▊        | 351/1906 [7:06:48<31:28:35, 72.87s/it] 18%|█▊        | 352/1906 [7:08:01<31:27:37, 72.88s/it] 19%|█▊        | 353/1906 [7:09:14<31:25:59, 72.86s/it] 19%|█▊        | 354/1906 [7:10:27<31:24:27, 72.85s/it] 19%|█▊        | 355/1906 [7:11:40<31:23:04, 72.85s/it] 19%|█▊        | 356/1906 [7:12:53<31:22:12, 72.86s/it] 19%|█▊        | 357/1906 [7:14:05<31:20:48, 72.85s/it] 19%|█▉        | 358/1906 [7:15:18<31:20:01, 72.87s/it] 19%|█▉        | 359/1906 [7:16:31<31:18:41, 72.86s/it] 19%|█▉        | 360/1906 [7:17:44<31:17:44, 72.87s/it]                                                       {'loss': 1.9036, 'learning_rate': 0.00018290580541758668, 'epoch': 0.19}
 19%|█▉        | 360/1906 [7:17:44<31:17:44, 72.87s/it] 19%|█▉        | 361/1906 [7:18:57<31:17:21, 72.91s/it] 19%|█▉        | 362/1906 [7:20:10<31:16:19, 72.91s/it] 19%|█▉        | 363/1906 [7:21:23<31:14:43, 72.90s/it] 19%|█▉        | 364/1906 [7:22:36<31:13:27, 72.90s/it] 19%|█▉        | 365/1906 [7:23:49<31:12:11, 72.89s/it] 19%|█▉        | 366/1906 [7:25:02<31:11:10, 72.90s/it] 19%|█▉        | 367/1906 [7:26:15<31:10:16, 72.91s/it] 19%|█▉        | 368/1906 [7:27:27<31:09:16, 72.92s/it] 19%|█▉        | 369/1906 [7:28:40<31:08:29, 72.94s/it] 19%|█▉        | 370/1906 [7:29:53<31:07:33, 72.95s/it]                                                       {'loss': 1.8915, 'learning_rate': 0.00018197293766454003, 'epoch': 0.19}
 19%|█▉        | 370/1906 [7:29:53<31:07:33, 72.95s/it] 19%|█▉        | 371/1906 [7:31:06<31:06:57, 72.98s/it] 20%|█▉        | 372/1906 [7:32:19<31:06:17, 73.00s/it] 20%|█▉        | 373/1906 [7:33:33<31:05:09, 73.00s/it] 20%|█▉        | 374/1906 [7:34:46<31:04:02, 73.00s/it] 20%|█▉        | 375/1906 [7:35:59<31:03:29, 73.03s/it] 20%|█▉        | 376/1906 [7:37:12<31:02:43, 73.05s/it] 20%|█▉        | 377/1906 [7:38:25<31:01:25, 73.04s/it] 20%|█▉        | 378/1906 [7:39:38<31:00:13, 73.05s/it] 20%|█▉        | 379/1906 [7:40:51<30:59:07, 73.05s/it] 20%|█▉        | 380/1906 [7:42:04<30:57:47, 73.05s/it]                                                       {'loss': 1.8814, 'learning_rate': 0.00018101780019852008, 'epoch': 0.2}
 20%|█▉        | 380/1906 [7:42:04<30:57:47, 73.05s/it] 20%|█▉        | 381/1906 [7:43:17<30:56:34, 73.05s/it] 20%|██        | 382/1906 [7:44:30<30:55:20, 73.04s/it] 20%|██        | 383/1906 [7:45:43<30:54:22, 73.05s/it] 20%|██        | 384/1906 [7:46:56<30:52:45, 73.04s/it] 20%|██        | 385/1906 [7:48:09<30:50:51, 73.01s/it] 20%|██        | 386/1906 [7:49:22<30:49:29, 73.01s/it] 20%|██        | 387/1906 [7:50:35<30:47:55, 72.99s/it] 20%|██        | 388/1906 [7:51:48<30:46:14, 72.97s/it] 20%|██        | 389/1906 [7:53:01<30:44:56, 72.97s/it] 20%|██        | 390/1906 [7:54:14<30:44:05, 72.98s/it]                                                       {'loss': 1.8783, 'learning_rate': 0.00018004065250317868, 'epoch': 0.2}
 20%|██        | 390/1906 [7:54:17<30:44:05, 72.98s/it] 21%|██        | 391/1906 [7:55:29<31:02:52, 73.78s/it] 21%|██        | 392/1906 [7:56:42<30:54:46, 73.50s/it] 21%|██        | 393/1906 [7:57:55<30:48:46, 73.32s/it] 21%|██        | 394/1906 [7:59:08<30:44:27, 73.19s/it] 21%|██        | 395/1906 [8:00:21<30:41:41, 73.13s/it] 21%|██        | 396/1906 [8:01:34<30:39:01, 73.07s/it] 21%|██        | 397/1906 [8:02:47<30:36:55, 73.04s/it] 21%|██        | 398/1906 [8:04:00<30:34:53, 73.01s/it] 21%|██        | 399/1906 [8:05:13<30:32:53, 72.98s/it] 21%|██        | 400/1906 [8:06:26<30:30:54, 72.94s/it]                                                       {'loss': 1.875, 'learning_rate': 0.00017904176004172027, 'epoch': 0.21}
 21%|██        | 400/1906 [8:06:26<30:30:54, 72.94s/it] 21%|██        | 401/1906 [8:07:39<30:29:30, 72.94s/it] 21%|██        | 402/1906 [8:08:52<30:28:18, 72.94s/it] 21%|██        | 403/1906 [8:10:05<30:27:35, 72.96s/it] 21%|██        | 404/1906 [8:11:18<30:26:31, 72.96s/it] 21%|██        | 405/1906 [8:12:31<30:25:31, 72.97s/it] 21%|██▏       | 406/1906 [8:13:44<30:24:21, 72.97s/it] 21%|██▏       | 407/1906 [8:14:56<30:22:53, 72.96s/it] 21%|██▏       | 408/1906 [8:16:09<30:21:16, 72.95s/it] 21%|██▏       | 409/1906 [8:17:22<30:20:27, 72.96s/it] 22%|██▏       | 410/1906 [8:18:35<30:18:55, 72.95s/it]                                                       {'loss': 1.8607, 'learning_rate': 0.00017802139418478298, 'epoch': 0.22}
 22%|██▏       | 410/1906 [8:18:35<30:18:55, 72.95s/it] 22%|██▏       | 411/1906 [8:19:48<30:17:26, 72.94s/it] 22%|██▏       | 412/1906 [8:21:01<30:16:07, 72.94s/it] 22%|██▏       | 413/1906 [8:22:14<30:14:25, 72.92s/it] 22%|██▏       | 414/1906 [8:23:27<30:13:07, 72.91s/it] 22%|██▏       | 415/1906 [8:24:40<30:11:53, 72.91s/it] 22%|██▏       | 416/1906 [8:25:53<30:10:35, 72.91s/it] 22%|██▏       | 417/1906 [8:27:06<30:09:33, 72.92s/it] 22%|██▏       | 418/1906 [8:28:19<30:07:51, 72.90s/it] 22%|██▏       | 419/1906 [8:29:31<30:06:48, 72.90s/it] 22%|██▏       | 420/1906 [8:30:44<30:05:56, 72.92s/it]                                                       {'loss': 1.8635, 'learning_rate': 0.00017697983213671515, 'epoch': 0.22}
 22%|██▏       | 420/1906 [8:30:44<30:05:56, 72.92s/it] 22%|██▏       | 421/1906 [8:31:57<30:05:04, 72.93s/it] 22%|██▏       | 422/1906 [8:33:10<30:03:48, 72.93s/it] 22%|██▏       | 423/1906 [8:34:23<30:02:38, 72.93s/it] 22%|██▏       | 424/1906 [8:35:36<30:01:53, 72.95s/it] 22%|██▏       | 425/1906 [8:36:49<30:00:39, 72.95s/it] 22%|██▏       | 426/1906 [8:38:02<29:59:31, 72.95s/it] 22%|██▏       | 427/1906 [8:39:15<29:58:32, 72.96s/it] 22%|██▏       | 428/1906 [8:40:28<29:56:52, 72.94s/it] 23%|██▎       | 429/1906 [8:41:41<29:55:45, 72.95s/it] 23%|██▎       | 430/1906 [8:42:54<29:53:48, 72.92s/it]                                                       {'loss': 1.8675, 'learning_rate': 0.00017591735686026661, 'epoch': 0.23}
 23%|██▎       | 430/1906 [8:42:54<29:53:48, 72.92s/it] 23%|██▎       | 431/1906 [8:44:07<29:53:15, 72.95s/it] 23%|██▎       | 432/1906 [8:45:20<29:51:48, 72.94s/it] 23%|██▎       | 433/1906 [8:46:33<29:50:08, 72.92s/it] 23%|██▎       | 434/1906 [8:47:46<29:49:24, 72.94s/it] 23%|██▎       | 435/1906 [8:48:59<29:48:08, 72.94s/it] 23%|██▎       | 436/1906 [8:50:11<29:46:20, 72.91s/it] 23%|██▎       | 437/1906 [8:51:24<29:44:59, 72.91s/it] 23%|██▎       | 438/1906 [8:52:37<29:43:37, 72.90s/it] 23%|██▎       | 439/1906 [8:53:50<29:42:41, 72.91s/it] 23%|██▎       | 440/1906 [8:55:03<29:41:34, 72.92s/it]                                                       {'loss': 1.8553, 'learning_rate': 0.0001748342569997158, 'epoch': 0.23}
 23%|██▎       | 440/1906 [8:55:03<29:41:34, 72.92s/it] 23%|██▎       | 441/1906 [8:56:16<29:40:21, 72.92s/it] 23%|██▎       | 442/1906 [8:57:29<29:39:07, 72.91s/it] 23%|██▎       | 443/1906 [8:58:42<29:37:59, 72.92s/it] 23%|██▎       | 444/1906 [8:59:55<29:36:53, 72.92s/it] 23%|██▎       | 445/1906 [9:01:08<29:35:26, 72.91s/it] 23%|██▎       | 446/1906 [9:02:21<29:33:55, 72.90s/it] 23%|██▎       | 447/1906 [9:03:33<29:32:52, 72.91s/it] 24%|██▎       | 448/1906 [9:04:46<29:31:25, 72.90s/it] 24%|██▎       | 449/1906 [9:05:59<29:29:53, 72.89s/it] 24%|██▎       | 450/1906 [9:07:12<29:28:29, 72.88s/it]                                                       {'loss': 1.8566, 'learning_rate': 0.00017373082680245347, 'epoch': 0.24}
 24%|██▎       | 450/1906 [9:07:12<29:28:29, 72.88s/it] 24%|██▎       | 451/1906 [9:08:25<29:27:47, 72.90s/it] 24%|██▎       | 452/1906 [9:09:38<29:26:20, 72.89s/it] 24%|██▍       | 453/1906 [9:10:51<29:24:57, 72.88s/it] 24%|██▍       | 454/1906 [9:12:04<29:23:36, 72.88s/it] 24%|██▍       | 455/1906 [9:13:16<29:22:19, 72.87s/it] 24%|██▍       | 456/1906 [9:14:29<29:21:05, 72.87s/it] 24%|██▍       | 457/1906 [9:15:42<29:19:48, 72.87s/it] 24%|██▍       | 458/1906 [9:16:55<29:18:32, 72.87s/it] 24%|██▍       | 459/1906 [9:18:08<29:17:12, 72.86s/it] 24%|██▍       | 460/1906 [9:19:21<29:15:43, 72.85s/it]                                                       {'loss': 1.8451, 'learning_rate': 0.0001726073660390439, 'epoch': 0.24}
 24%|██▍       | 460/1906 [9:19:21<29:15:43, 72.85s/it] 24%|██▍       | 461/1906 [9:20:34<29:14:46, 72.86s/it] 24%|██▍       | 462/1906 [9:21:46<29:12:54, 72.84s/it] 24%|██▍       | 463/1906 [9:22:59<29:11:52, 72.84s/it] 24%|██▍       | 464/1906 [9:24:12<29:09:59, 72.82s/it] 24%|██▍       | 465/1906 [9:25:25<29:08:28, 72.80s/it] 24%|██▍       | 466/1906 [9:26:38<29:06:59, 72.79s/it] 25%|██▍       | 467/1906 [9:27:50<29:05:33, 72.78s/it] 25%|██▍       | 468/1906 [9:29:03<29:03:48, 72.76s/it] 25%|██▍       | 469/1906 [9:30:16<29:02:35, 72.76s/it] 25%|██▍       | 470/1906 [9:31:28<29:01:11, 72.75s/it]                                                       {'loss': 1.8415, 'learning_rate': 0.0001714641799217858, 'epoch': 0.25}
 25%|██▍       | 470/1906 [9:31:29<29:01:11, 72.75s/it] 25%|██▍       | 471/1906 [9:32:41<29:00:40, 72.78s/it] 25%|██▍       | 472/1906 [9:33:54<28:59:34, 72.79s/it] 25%|██▍       | 473/1906 [9:35:07<28:58:35, 72.80s/it] 25%|██▍       | 474/1906 [9:36:20<28:57:22, 72.80s/it] 25%|██▍       | 475/1906 [9:37:33<28:56:14, 72.80s/it] 25%|██▍       | 476/1906 [9:38:45<28:55:19, 72.81s/it] 25%|██▌       | 477/1906 [9:39:58<28:54:27, 72.83s/it] 25%|██▌       | 478/1906 [9:41:11<28:53:18, 72.83s/it] 25%|██▌       | 479/1906 [9:42:24<28:51:56, 72.82s/it] 25%|██▌       | 480/1906 [9:43:37<28:51:02, 72.83s/it]                                                       {'loss': 1.8323, 'learning_rate': 0.00017030157902179485, 'epoch': 0.25}
 25%|██▌       | 480/1906 [9:43:37<28:51:02, 72.83s/it] 25%|██▌       | 481/1906 [9:44:50<28:49:52, 72.84s/it] 25%|██▌       | 482/1906 [9:46:02<28:48:39, 72.84s/it] 25%|██▌       | 483/1906 [9:47:15<28:47:40, 72.85s/it] 25%|██▌       | 484/1906 [9:48:28<28:46:16, 72.84s/it] 25%|██▌       | 485/1906 [9:49:41<28:44:58, 72.83s/it] 25%|██▌       | 486/1906 [9:50:54<28:44:11, 72.85s/it] 26%|██▌       | 487/1906 [9:52:07<28:42:52, 72.85s/it] 26%|██▌       | 488/1906 [9:53:20<28:41:40, 72.85s/it] 26%|██▌       | 489/1906 [9:54:32<28:40:13, 72.84s/it] 26%|██▌       | 490/1906 [9:55:45<28:39:00, 72.84s/it]                                                       {'loss': 1.8304, 'learning_rate': 0.00016911987918463034, 'epoch': 0.26}
 26%|██▌       | 490/1906 [9:55:45<28:39:00, 72.84s/it] 26%|██▌       | 491/1906 [9:56:58<28:38:25, 72.87s/it] 26%|██▌       | 492/1906 [9:58:11<28:37:00, 72.86s/it] 26%|██▌       | 493/1906 [9:59:24<28:35:48, 72.86s/it] 26%|██▌       | 494/1906 [10:00:37<28:34:08, 72.84s/it] 26%|██▌       | 495/1906 [10:01:49<28:32:48, 72.83s/it] 26%|██▌       | 496/1906 [10:03:02<28:31:31, 72.83s/it] 26%|██▌       | 497/1906 [10:04:15<28:31:13, 72.87s/it] 26%|██▌       | 498/1906 [10:05:28<28:30:09, 72.88s/it] 26%|██▌       | 499/1906 [10:06:41<28:28:43, 72.87s/it] 26%|██▌       | 500/1906 [10:07:54<28:27:17, 72.86s/it]                                                        {'loss': 1.8271, 'learning_rate': 0.00016791940144448902, 'epoch': 0.26}
 26%|██▌       | 500/1906 [10:07:54<28:27:17, 72.86s/it] 26%|██▋       | 501/1906 [10:09:07<28:26:20, 72.87s/it] 26%|██▋       | 502/1906 [10:10:19<28:24:50, 72.86s/it] 26%|██▋       | 503/1906 [10:11:32<28:23:38, 72.86s/it] 26%|██▋       | 504/1906 [10:12:45<28:22:26, 72.86s/it] 26%|██▋       | 505/1906 [10:13:58<28:21:09, 72.85s/it] 27%|██▋       | 506/1906 [10:15:11<28:20:00, 72.86s/it] 27%|██▋       | 507/1906 [10:16:24<28:18:17, 72.84s/it] 27%|██▋       | 508/1906 [10:17:37<28:17:33, 72.86s/it] 27%|██▋       | 509/1906 [10:18:49<28:16:26, 72.86s/it] 27%|██▋       | 510/1906 [10:20:02<28:15:32, 72.87s/it]                                                        {'loss': 1.8226, 'learning_rate': 0.00016670047193698912, 'epoch': 0.27}
 27%|██▋       | 510/1906 [10:20:02<28:15:32, 72.87s/it] 27%|██▋       | 511/1906 [10:21:15<28:14:30, 72.88s/it] 27%|██▋       | 512/1906 [10:22:28<28:13:29, 72.89s/it] 27%|██▋       | 513/1906 [10:23:41<28:11:47, 72.87s/it] 27%|██▋       | 514/1906 [10:24:54<28:10:26, 72.86s/it] 27%|██▋       | 515/1906 [10:26:07<28:09:36, 72.88s/it] 27%|██▋       | 516/1906 [10:27:20<28:08:39, 72.89s/it] 27%|██▋       | 517/1906 [10:28:33<28:07:33, 72.90s/it] 27%|██▋       | 518/1906 [10:29:46<28:06:23, 72.90s/it] 27%|██▋       | 519/1906 [10:30:58<28:05:06, 72.90s/it] 27%|██▋       | 520/1906 [10:32:11<28:03:54, 72.90s/it]                                                        {'loss': 1.81, 'learning_rate': 0.0001654634218105686, 'epoch': 0.27}
 27%|██▋       | 520/1906 [10:32:11<28:03:54, 72.90s/it] 27%|██▋       | 521/1906 [10:33:24<28:03:09, 72.92s/it] 27%|██▋       | 522/1906 [10:34:37<28:01:40, 72.91s/it] 27%|██▋       | 523/1906 [10:35:50<28:00:34, 72.91s/it] 27%|██▋       | 524/1906 [10:37:03<27:59:11, 72.90s/it] 28%|██▊       | 525/1906 [10:38:16<27:57:38, 72.89s/it] 28%|██▊       | 526/1906 [10:39:29<27:56:56, 72.91s/it] 28%|██▊       | 527/1906 [10:40:42<27:55:58, 72.92s/it] 28%|██▊       | 528/1906 [10:41:55<27:54:27, 72.91s/it] 28%|██▊       | 529/1906 [10:43:08<27:53:15, 72.91s/it] 28%|██▊       | 530/1906 [10:44:20<27:51:56, 72.90s/it]                                                        {'loss': 1.8281, 'learning_rate': 0.0001642085871365217, 'epoch': 0.28}
 28%|██▊       | 530/1906 [10:44:20<27:51:56, 72.90s/it] 28%|██▊       | 531/1906 [10:45:33<27:50:30, 72.90s/it] 28%|██▊       | 532/1906 [10:46:46<27:49:07, 72.89s/it] 28%|██▊       | 533/1906 [10:47:59<27:47:29, 72.87s/it] 28%|██▊       | 534/1906 [10:49:12<27:46:47, 72.89s/it] 28%|██▊       | 535/1906 [10:50:25<27:45:11, 72.87s/it] 28%|██▊       | 536/1906 [10:51:38<27:43:48, 72.87s/it] 28%|██▊       | 537/1906 [10:52:50<27:42:48, 72.88s/it] 28%|██▊       | 538/1906 [10:54:03<27:41:47, 72.89s/it] 28%|██▊       | 539/1906 [10:55:16<27:40:39, 72.89s/it] 28%|██▊       | 540/1906 [10:56:29<27:39:14, 72.88s/it]                                                        {'loss': 1.8143, 'learning_rate': 0.00016293630881769773, 'epoch': 0.28}
 28%|██▊       | 540/1906 [10:56:29<27:39:14, 72.88s/it] 28%|██▊       | 541/1906 [10:57:42<27:38:12, 72.89s/it] 28%|██▊       | 542/1906 [10:58:55<27:36:25, 72.86s/it] 28%|██▊       | 543/1906 [11:00:08<27:35:13, 72.86s/it] 29%|██▊       | 544/1906 [11:01:21<27:34:01, 72.86s/it] 29%|██▊       | 545/1906 [11:02:33<27:32:54, 72.87s/it] 29%|██▊       | 546/1906 [11:03:46<27:31:53, 72.88s/it] 29%|██▊       | 547/1906 [11:04:59<27:30:22, 72.86s/it] 29%|██▉       | 548/1906 [11:06:12<27:29:11, 72.87s/it] 29%|██▉       | 549/1906 [11:07:25<27:27:43, 72.85s/it] 29%|██▉       | 550/1906 [11:08:38<27:26:12, 72.84s/it]                                                        {'loss': 1.8153, 'learning_rate': 0.00016164693249588768, 'epoch': 0.29}
 29%|██▉       | 550/1906 [11:08:38<27:26:12, 72.84s/it] 29%|██▉       | 551/1906 [11:09:51<27:25:10, 72.85s/it] 29%|██▉       | 552/1906 [11:11:03<27:23:55, 72.85s/it] 29%|██▉       | 553/1906 [11:12:16<27:22:37, 72.84s/it] 29%|██▉       | 554/1906 [11:13:29<27:22:09, 72.88s/it] 29%|██▉       | 555/1906 [11:14:42<27:21:01, 72.88s/it] 29%|██▉       | 556/1906 [11:15:55<27:19:42, 72.88s/it] 29%|██▉       | 557/1906 [11:17:08<27:18:12, 72.86s/it] 29%|██▉       | 558/1906 [11:18:21<27:17:16, 72.88s/it] 29%|██▉       | 559/1906 [11:19:34<27:16:15, 72.88s/it] 29%|██▉       | 560/1906 [11:20:46<27:14:40, 72.87s/it]                                                        {'loss': 1.8038, 'learning_rate': 0.00016034080845792295, 'epoch': 0.29}
 29%|██▉       | 560/1906 [11:20:46<27:14:40, 72.87s/it] 29%|██▉       | 561/1906 [11:21:59<27:14:12, 72.90s/it] 29%|██▉       | 562/1906 [11:23:12<27:12:55, 72.90s/it] 30%|██▉       | 563/1906 [11:24:25<27:11:51, 72.90s/it] 30%|██▉       | 564/1906 [11:25:38<27:10:30, 72.90s/it] 30%|██▉       | 565/1906 [11:26:51<27:09:34, 72.91s/it] 30%|██▉       | 566/1906 [11:28:04<27:08:02, 72.90s/it] 30%|██▉       | 567/1906 [11:29:17<27:06:58, 72.90s/it] 30%|██▉       | 568/1906 [11:30:30<27:05:30, 72.89s/it] 30%|██▉       | 569/1906 [11:31:43<27:04:07, 72.89s/it] 30%|██▉       | 570/1906 [11:32:55<27:02:28, 72.87s/it]                                                        {'loss': 1.8012, 'learning_rate': 0.00015901829154051265, 'epoch': 0.3}
 30%|██▉       | 570/1906 [11:32:55<27:02:28, 72.87s/it] 30%|██▉       | 571/1906 [11:34:08<27:01:26, 72.87s/it] 30%|███       | 572/1906 [11:35:21<27:00:04, 72.87s/it] 30%|███       | 573/1906 [11:36:34<26:58:46, 72.86s/it] 30%|███       | 574/1906 [11:37:47<26:57:32, 72.86s/it] 30%|███       | 575/1906 [11:39:00<26:56:14, 72.86s/it] 30%|███       | 576/1906 [11:40:13<26:54:52, 72.85s/it] 30%|███       | 577/1906 [11:41:25<26:53:26, 72.84s/it] 30%|███       | 578/1906 [11:42:38<26:52:07, 72.84s/it] 30%|███       | 579/1906 [11:43:51<26:50:54, 72.84s/it] 30%|███       | 580/1906 [11:45:04<26:50:08, 72.86s/it]                                                        {'loss': 1.7897, 'learning_rate': 0.00015767974103384443, 'epoch': 0.3}
 30%|███       | 580/1906 [11:45:04<26:50:08, 72.86s/it] 30%|███       | 581/1906 [11:46:17<26:48:44, 72.85s/it] 31%|███       | 582/1906 [11:47:30<26:47:48, 72.86s/it] 31%|███       | 583/1906 [11:48:42<26:46:14, 72.85s/it] 31%|███       | 584/1906 [11:49:55<26:45:01, 72.85s/it] 31%|███       | 585/1906 [11:51:08<26:43:40, 72.84s/it] 31%|███       | 586/1906 [11:52:21<26:42:58, 72.86s/it] 31%|███       | 587/1906 [11:53:34<26:42:02, 72.88s/it] 31%|███       | 588/1906 [11:54:47<26:40:45, 72.87s/it] 31%|███       | 589/1906 [11:56:00<26:39:26, 72.87s/it] 31%|███       | 590/1906 [11:57:13<26:38:02, 72.86s/it]                                                        {'loss': 1.7985, 'learning_rate': 0.00015632552058397544, 'epoch': 0.31}
 31%|███       | 590/1906 [11:57:13<26:38:02, 72.86s/it] 31%|███       | 591/1906 [11:58:25<26:36:51, 72.86s/it] 31%|███       | 592/1906 [11:59:38<26:35:29, 72.85s/it] 31%|███       | 593/1906 [12:00:51<26:34:21, 72.86s/it] 31%|███       | 594/1906 [12:02:04<26:33:07, 72.86s/it] 31%|███       | 595/1906 [12:03:17<26:31:54, 72.86s/it] 31%|███▏      | 596/1906 [12:04:30<26:30:20, 72.84s/it] 31%|███▏      | 597/1906 [12:05:42<26:29:09, 72.84s/it] 31%|███▏      | 598/1906 [12:06:55<26:27:40, 72.83s/it] 31%|███▏      | 599/1906 [12:08:08<26:26:14, 72.82s/it] 31%|███▏      | 600/1906 [12:09:21<26:24:55, 72.81s/it]                                                        {'loss': 1.7959, 'learning_rate': 0.00015495599809404044, 'epoch': 0.31}
 31%|███▏      | 600/1906 [12:09:21<26:24:55, 72.81s/it] 32%|███▏      | 601/1906 [12:10:34<26:24:04, 72.83s/it] 32%|███▏      | 602/1906 [12:11:47<26:23:02, 72.84s/it] 32%|███▏      | 603/1906 [12:12:59<26:21:41, 72.83s/it] 32%|███▏      | 604/1906 [12:14:12<26:20:38, 72.84s/it] 32%|███▏      | 605/1906 [12:15:25<26:19:02, 72.82s/it] 32%|███▏      | 606/1906 [12:16:38<26:17:36, 72.81s/it] 32%|███▏      | 607/1906 [12:17:51<26:16:29, 72.82s/it] 32%|███▏      | 608/1906 [12:19:03<26:15:03, 72.81s/it] 32%|███▏      | 609/1906 [12:20:16<26:14:04, 72.82s/it] 32%|███▏      | 610/1906 [12:21:29<26:12:47, 72.81s/it]                                                        {'loss': 1.7851, 'learning_rate': 0.00015357154562430252, 'epoch': 0.32}
 32%|███▏      | 610/1906 [12:21:29<26:12:47, 72.81s/it] 32%|███▏      | 611/1906 [12:22:42<26:11:21, 72.80s/it] 32%|███▏      | 612/1906 [12:23:55<26:10:03, 72.80s/it] 32%|███▏      | 613/1906 [12:25:07<26:09:00, 72.81s/it] 32%|███▏      | 614/1906 [12:26:20<26:07:59, 72.82s/it] 32%|███▏      | 615/1906 [12:27:33<26:06:17, 72.79s/it] 32%|███▏      | 616/1906 [12:28:46<26:04:59, 72.79s/it] 32%|███▏      | 617/1906 [12:29:59<26:03:52, 72.79s/it] 32%|███▏      | 618/1906 [12:31:11<26:02:23, 72.78s/it] 32%|███▏      | 619/1906 [12:32:24<26:00:57, 72.77s/it] 33%|███▎      | 620/1906 [12:33:37<25:59:49, 72.78s/it]                                                        {'loss': 1.7847, 'learning_rate': 0.0001521725392910753, 'epoch': 0.33}
 33%|███▎      | 620/1906 [12:33:37<25:59:49, 72.78s/it] 33%|███▎      | 621/1906 [12:34:50<25:58:38, 72.78s/it] 33%|███▎      | 622/1906 [12:36:02<25:57:17, 72.77s/it] 33%|███▎      | 623/1906 [12:37:15<25:55:43, 72.75s/it] 33%|███▎      | 624/1906 [12:38:28<25:54:32, 72.76s/it] 33%|███▎      | 625/1906 [12:39:41<25:53:21, 72.76s/it] 33%|███▎      | 626/1906 [12:40:53<25:52:07, 72.76s/it] 33%|███▎      | 627/1906 [12:42:06<25:50:38, 72.74s/it] 33%|███▎      | 628/1906 [12:43:19<25:49:33, 72.75s/it] 33%|███▎      | 629/1906 [12:44:32<25:48:27, 72.75s/it] 33%|███▎      | 630/1906 [12:45:44<25:47:26, 72.76s/it]                                                        {'loss': 1.7765, 'learning_rate': 0.00015075935916454255, 'epoch': 0.33}
 33%|███▎      | 630/1906 [12:45:45<25:47:26, 72.76s/it] 33%|███▎      | 631/1906 [12:46:57<25:46:35, 72.78s/it] 33%|███▎      | 632/1906 [12:48:10<25:45:53, 72.81s/it] 33%|███▎      | 633/1906 [12:49:23<25:44:08, 72.78s/it] 33%|███▎      | 634/1906 [12:50:36<25:43:07, 72.79s/it] 33%|███▎      | 635/1906 [12:51:48<25:41:37, 72.78s/it] 33%|███▎      | 636/1906 [12:53:01<25:40:19, 72.77s/it] 33%|███▎      | 637/1906 [12:54:14<25:38:58, 72.76s/it] 33%|███▎      | 638/1906 [12:55:27<25:37:56, 72.77s/it] 34%|███▎      | 639/1906 [12:56:39<25:36:36, 72.77s/it] 34%|███▎      | 640/1906 [12:57:52<25:35:30, 72.77s/it]                                                        {'loss': 1.7763, 'learning_rate': 0.00014933238916550425, 'epoch': 0.34}
 34%|███▎      | 640/1906 [12:57:52<25:35:30, 72.77s/it] 34%|███▎      | 641/1906 [12:59:05<25:35:05, 72.81s/it] 34%|███▎      | 642/1906 [13:00:18<25:33:42, 72.80s/it] 34%|███▎      | 643/1906 [13:01:31<25:32:31, 72.80s/it] 34%|███▍      | 644/1906 [13:02:44<25:31:28, 72.81s/it] 34%|███▍      | 645/1906 [13:03:56<25:30:20, 72.82s/it] 34%|███▍      | 646/1906 [13:05:09<25:28:55, 72.81s/it] 34%|███▍      | 647/1906 [13:06:22<25:27:31, 72.80s/it] 34%|███▍      | 648/1906 [13:07:35<25:26:10, 72.79s/it] 34%|███▍      | 649/1906 [13:08:48<25:24:59, 72.79s/it] 34%|███▍      | 650/1906 [13:10:00<25:23:38, 72.79s/it]                                                        {'loss': 1.7715, 'learning_rate': 0.00014789201696107594, 'epoch': 0.34}
 34%|███▍      | 650/1906 [13:10:00<25:23:38, 72.79s/it] 34%|███▍      | 651/1906 [13:11:13<25:22:20, 72.78s/it] 34%|███▍      | 652/1906 [13:12:26<25:21:04, 72.78s/it] 34%|███▍      | 653/1906 [13:13:39<25:19:46, 72.77s/it] 34%|███▍      | 654/1906 [13:14:51<25:18:22, 72.77s/it] 34%|███▍      | 655/1906 [13:16:04<25:17:19, 72.77s/it] 34%|███▍      | 656/1906 [13:17:17<25:16:20, 72.78s/it] 34%|███▍      | 657/1906 [13:18:30<25:15:25, 72.80s/it] 35%|███▍      | 658/1906 [13:19:43<25:14:28, 72.81s/it] 35%|███▍      | 659/1906 [13:20:55<25:13:12, 72.81s/it] 35%|███▍      | 660/1906 [13:22:08<25:11:47, 72.80s/it]                                                        {'loss': 1.7721, 'learning_rate': 0.00014643863385937076, 'epoch': 0.35}
 35%|███▍      | 660/1906 [13:22:08<25:11:47, 72.80s/it] 35%|███▍      | 661/1906 [13:23:21<25:10:33, 72.80s/it] 35%|███▍      | 662/1906 [13:24:34<25:09:29, 72.80s/it] 35%|███▍      | 663/1906 [13:25:47<25:07:51, 72.78s/it] 35%|███▍      | 664/1906 [13:26:59<25:06:17, 72.77s/it] 35%|███▍      | 665/1906 [13:28:12<25:05:08, 72.77s/it] 35%|███▍      | 666/1906 [13:29:25<25:03:53, 72.77s/it] 35%|███▍      | 667/1906 [13:30:38<25:02:58, 72.78s/it] 35%|███▌      | 668/1906 [13:31:50<25:01:49, 72.79s/it] 35%|███▌      | 669/1906 [13:33:03<25:00:46, 72.79s/it] 35%|███▌      | 670/1906 [13:34:16<24:59:38, 72.80s/it]                                                        {'loss': 1.7764, 'learning_rate': 0.00014497263470319215, 'epoch': 0.35}
 35%|███▌      | 670/1906 [13:34:16<24:59:38, 72.80s/it] 35%|███▌      | 671/1906 [13:35:29<24:58:46, 72.82s/it] 35%|███▌      | 672/1906 [13:36:42<24:57:36, 72.82s/it] 35%|███▌      | 673/1906 [13:37:55<24:56:25, 72.82s/it] 35%|███▌      | 674/1906 [13:39:07<24:55:08, 72.82s/it] 35%|███▌      | 675/1906 [13:40:20<24:54:17, 72.83s/it] 35%|███▌      | 676/1906 [13:41:33<24:53:09, 72.84s/it] 36%|███▌      | 677/1906 [13:42:46<24:51:53, 72.83s/it] 36%|███▌      | 678/1906 [13:43:59<24:50:48, 72.84s/it] 36%|███▌      | 679/1906 [13:45:12<24:49:54, 72.86s/it] 36%|███▌      | 680/1906 [13:46:25<24:48:33, 72.85s/it]                                                        {'loss': 1.7728, 'learning_rate': 0.0001434944177627664, 'epoch': 0.36}
 36%|███▌      | 680/1906 [13:46:25<24:48:33, 72.85s/it] 36%|███▌      | 681/1906 [13:47:37<24:47:44, 72.87s/it] 36%|███▌      | 682/1906 [13:48:50<24:46:23, 72.86s/it] 36%|███▌      | 683/1906 [13:50:03<24:44:52, 72.85s/it] 36%|███▌      | 684/1906 [13:51:16<24:43:28, 72.84s/it] 36%|███▌      | 685/1906 [13:52:29<24:42:30, 72.85s/it] 36%|███▌      | 686/1906 [13:53:42<24:41:13, 72.85s/it] 36%|███▌      | 687/1906 [13:54:54<24:40:03, 72.85s/it] 36%|███▌      | 688/1906 [13:56:07<24:38:38, 72.84s/it] 36%|███▌      | 689/1906 [13:57:20<24:37:42, 72.85s/it] 36%|███▌      | 690/1906 [13:58:33<24:36:43, 72.86s/it]                                                        {'loss': 1.7763, 'learning_rate': 0.00014200438462754373, 'epoch': 0.36}
 36%|███▌      | 690/1906 [13:58:33<24:36:43, 72.86s/it] 36%|███▋      | 691/1906 [13:59:46<24:35:42, 72.87s/it] 36%|███▋      | 692/1906 [14:00:59<24:34:18, 72.87s/it] 36%|███▋      | 693/1906 [14:02:12<24:32:43, 72.85s/it] 36%|███▋      | 694/1906 [14:03:24<24:31:25, 72.84s/it] 36%|███▋      | 695/1906 [14:04:37<24:29:46, 72.82s/it] 37%|███▋      | 696/1906 [14:05:50<24:28:29, 72.82s/it] 37%|███▋      | 697/1906 [14:07:03<24:27:06, 72.81s/it] 37%|███▋      | 698/1906 [14:08:16<24:25:42, 72.80s/it] 37%|███▋      | 699/1906 [14:09:28<24:24:58, 72.82s/it] 37%|███▋      | 700/1906 [14:10:41<24:23:30, 72.81s/it]                                                        {'loss': 1.7637, 'learning_rate': 0.00014050294009709813, 'epoch': 0.37}
 37%|███▋      | 700/1906 [14:10:41<24:23:30, 72.81s/it] 37%|███▋      | 701/1906 [14:11:54<24:22:44, 72.83s/it] 37%|███▋      | 702/1906 [14:13:07<24:21:26, 72.83s/it] 37%|███▋      | 703/1906 [14:14:20<24:20:12, 72.83s/it] 37%|███▋      | 704/1906 [14:15:33<24:19:02, 72.83s/it] 37%|███▋      | 705/1906 [14:16:46<24:18:16, 72.85s/it] 37%|███▋      | 706/1906 [14:17:58<24:16:44, 72.84s/it] 37%|███▋      | 707/1906 [14:19:11<24:15:55, 72.86s/it] 37%|███▋      | 708/1906 [14:20:24<24:14:34, 72.85s/it] 37%|███▋      | 709/1906 [14:21:37<24:13:10, 72.84s/it] 37%|███▋      | 710/1906 [14:22:50<24:11:49, 72.83s/it]                                                        {'loss': 1.7624, 'learning_rate': 0.0001389904920711547, 'epoch': 0.37}
 37%|███▋      | 710/1906 [14:22:50<24:11:49, 72.83s/it] 37%|███▋      | 711/1906 [14:24:03<24:10:49, 72.84s/it] 37%|███▋      | 712/1906 [14:25:15<24:09:13, 72.83s/it] 37%|███▋      | 713/1906 [14:26:28<24:07:47, 72.81s/it] 37%|███▋      | 714/1906 [14:27:41<24:06:57, 72.83s/it] 38%|███▊      | 715/1906 [14:28:54<24:05:55, 72.84s/it] 38%|███▊      | 716/1906 [14:30:07<24:04:29, 72.83s/it] 38%|███▊      | 717/1906 [14:31:20<24:03:21, 72.84s/it] 38%|███▊      | 718/1906 [14:32:32<24:01:56, 72.83s/it] 38%|███▊      | 719/1906 [14:33:45<24:00:35, 72.82s/it] 38%|███▊      | 720/1906 [14:34:58<23:59:18, 72.81s/it]                                                        {'loss': 1.7647, 'learning_rate': 0.0001374674514387749, 'epoch': 0.38}
 38%|███▊      | 720/1906 [14:34:58<23:59:18, 72.81s/it] 38%|███▊      | 721/1906 [14:36:11<23:58:48, 72.85s/it] 38%|███▊      | 722/1906 [14:37:24<23:57:27, 72.84s/it] 38%|███▊      | 723/1906 [14:38:37<23:55:59, 72.83s/it] 38%|███▊      | 724/1906 [14:39:49<23:54:47, 72.83s/it] 38%|███▊      | 725/1906 [14:41:02<23:53:42, 72.84s/it] 38%|███▊      | 726/1906 [14:42:15<23:52:21, 72.83s/it] 38%|███▊      | 727/1906 [14:43:28<23:50:57, 72.82s/it] 38%|███▊      | 728/1906 [14:44:41<23:49:43, 72.82s/it] 38%|███▊      | 729/1906 [14:45:53<23:48:24, 72.82s/it] 38%|███▊      | 730/1906 [14:47:06<23:47:39, 72.84s/it]                                                        {'loss': 1.7547, 'learning_rate': 0.0001359342319667298, 'epoch': 0.38}
 38%|███▊      | 730/1906 [14:47:06<23:47:39, 72.84s/it] 38%|███▊      | 731/1906 [14:48:19<23:46:24, 72.84s/it] 38%|███▊      | 732/1906 [14:49:32<23:44:54, 72.82s/it] 38%|███▊      | 733/1906 [14:50:45<23:43:35, 72.82s/it] 39%|███▊      | 734/1906 [14:51:58<23:42:34, 72.83s/it] 39%|███▊      | 735/1906 [14:53:10<23:41:17, 72.82s/it] 39%|███▊      | 736/1906 [14:54:23<23:40:07, 72.83s/it] 39%|███▊      | 737/1906 [14:55:36<23:39:06, 72.84s/it] 39%|███▊      | 738/1906 [14:56:49<23:37:42, 72.83s/it] 39%|███▉      | 739/1906 [14:58:02<23:36:28, 72.83s/it] 39%|███▉      | 740/1906 [14:59:15<23:35:13, 72.82s/it]                                                        {'loss': 1.7486, 'learning_rate': 0.0001343912501870913, 'epoch': 0.39}
 39%|███▉      | 740/1906 [14:59:15<23:35:13, 72.82s/it] 39%|███▉      | 741/1906 [15:00:27<23:34:19, 72.84s/it] 39%|███▉      | 742/1906 [15:01:40<23:33:02, 72.84s/it] 39%|███▉      | 743/1906 [15:02:53<23:31:21, 72.81s/it] 39%|███▉      | 744/1906 [15:04:06<23:29:49, 72.80s/it] 39%|███▉      | 745/1906 [15:05:19<23:28:16, 72.78s/it] 39%|███▉      | 746/1906 [15:06:31<23:27:14, 72.79s/it] 39%|███▉      | 747/1906 [15:07:44<23:25:59, 72.79s/it] 39%|███▉      | 748/1906 [15:08:57<23:24:26, 72.77s/it] 39%|███▉      | 749/1906 [15:10:10<23:23:08, 72.76s/it] 39%|███▉      | 750/1906 [15:11:22<23:22:23, 72.79s/it]                                                        {'loss': 1.7462, 'learning_rate': 0.00013283892528407235, 'epoch': 0.39}
 39%|███▉      | 750/1906 [15:11:23<23:22:23, 72.79s/it] 39%|███▉      | 751/1906 [15:12:35<23:21:33, 72.81s/it] 39%|███▉      | 752/1906 [15:13:48<23:20:26, 72.81s/it] 40%|███▉      | 753/1906 [15:15:01<23:19:14, 72.81s/it] 40%|███▉      | 754/1906 [15:16:14<23:18:20, 72.83s/it] 40%|███▉      | 755/1906 [15:17:27<23:17:06, 72.83s/it] 40%|███▉      | 756/1906 [15:18:39<23:15:55, 72.83s/it] 40%|███▉      | 757/1906 [15:19:52<23:14:31, 72.82s/it] 40%|███▉      | 758/1906 [15:21:05<23:13:37, 72.84s/it] 40%|███▉      | 759/1906 [15:22:18<23:12:10, 72.83s/it] 40%|███▉      | 760/1906 [15:23:31<23:10:51, 72.82s/it]                                                        {'loss': 1.7519, 'learning_rate': 0.00013127767898014637, 'epoch': 0.4}
 40%|███▉      | 760/1906 [15:23:31<23:10:51, 72.82s/it] 40%|███▉      | 761/1906 [15:24:44<23:09:52, 72.83s/it] 40%|███▉      | 762/1906 [15:25:56<23:08:29, 72.82s/it] 40%|████      | 763/1906 [15:27:09<23:07:13, 72.82s/it] 40%|████      | 764/1906 [15:28:22<23:05:51, 72.81s/it] 40%|████      | 765/1906 [15:29:35<23:04:39, 72.81s/it] 40%|████      | 766/1906 [15:30:48<23:03:11, 72.80s/it] 40%|████      | 767/1906 [15:32:00<23:01:45, 72.79s/it] 40%|████      | 768/1906 [15:33:13<23:00:40, 72.79s/it] 40%|████      | 769/1906 [15:34:26<22:59:18, 72.79s/it] 40%|████      | 770/1906 [15:35:39<22:58:00, 72.78s/it]                                                        {'loss': 1.7539, 'learning_rate': 0.00012970793542147756, 'epoch': 0.4}
 40%|████      | 770/1906 [15:35:39<22:58:00, 72.78s/it] 40%|████      | 771/1906 [15:36:52<22:56:59, 72.79s/it] 41%|████      | 772/1906 [15:38:04<22:55:45, 72.79s/it] 41%|████      | 773/1906 [15:39:17<22:54:28, 72.79s/it] 41%|████      | 774/1906 [15:40:30<22:53:14, 72.79s/it] 41%|████      | 775/1906 [15:41:43<22:51:41, 72.77s/it] 41%|████      | 776/1906 [15:42:55<22:50:25, 72.77s/it] 41%|████      | 777/1906 [15:44:08<22:49:06, 72.76s/it] 41%|████      | 778/1906 [15:45:21<22:47:55, 72.76s/it] 41%|████      | 779/1906 [15:46:34<22:46:46, 72.76s/it] 41%|████      | 780/1906 [15:47:46<22:45:32, 72.76s/it]                                                        {'loss': 1.7413, 'learning_rate': 0.00012813012106269208, 'epoch': 0.41}
 41%|████      | 780/1906 [15:47:46<22:45:32, 72.76s/it] 41%|████      | 781/1906 [15:48:59<22:44:58, 72.80s/it] 41%|████      | 782/1906 [15:50:12<22:43:54, 72.81s/it] 41%|████      | 783/1906 [15:51:25<22:42:26, 72.79s/it] 41%|████      | 784/1906 [15:52:38<22:41:12, 72.79s/it] 41%|████      | 785/1906 [15:53:50<22:39:50, 72.78s/it] 41%|████      | 786/1906 [15:55:03<22:38:17, 72.77s/it] 41%|████▏     | 787/1906 [15:56:16<22:36:57, 72.76s/it] 41%|████▏     | 788/1906 [15:57:29<22:35:29, 72.75s/it] 41%|████▏     | 789/1906 [15:58:41<22:34:08, 72.74s/it] 41%|████▏     | 790/1906 [15:59:54<22:32:55, 72.74s/it]                                                        {'loss': 1.7419, 'learning_rate': 0.00012654466455102272, 'epoch': 0.41}
 41%|████▏     | 790/1906 [15:59:54<22:32:55, 72.74s/it] 42%|████▏     | 791/1906 [16:01:07<22:31:55, 72.75s/it] 42%|████▏     | 792/1906 [16:02:20<22:30:29, 72.74s/it] 42%|████▏     | 793/1906 [16:03:32<22:29:37, 72.76s/it] 42%|████▏     | 794/1906 [16:04:45<22:28:02, 72.74s/it] 42%|████▏     | 795/1906 [16:05:58<22:26:47, 72.73s/it] 42%|████▏     | 796/1906 [16:07:11<22:25:32, 72.73s/it] 42%|████▏     | 797/1906 [16:08:23<22:24:08, 72.72s/it] 42%|████▏     | 798/1906 [16:09:36<22:22:45, 72.71s/it] 42%|████▏     | 799/1906 [16:10:49<22:21:39, 72.72s/it] 42%|████▏     | 800/1906 [16:12:01<22:20:25, 72.72s/it]                                                        {'loss': 1.7516, 'learning_rate': 0.00012495199660985767, 'epoch': 0.42}
 42%|████▏     | 800/1906 [16:12:01<22:20:25, 72.72s/it] 42%|████▏     | 801/1906 [16:13:14<22:19:24, 72.73s/it] 42%|████▏     | 802/1906 [16:14:27<22:18:02, 72.72s/it] 42%|████▏     | 803/1906 [16:15:40<22:16:59, 72.73s/it] 42%|████▏     | 804/1906 [16:16:52<22:15:48, 72.73s/it] 42%|████▏     | 805/1906 [16:18:05<22:14:39, 72.73s/it] 42%|████▏     | 806/1906 [16:19:18<22:13:29, 72.74s/it] 42%|████▏     | 807/1906 [16:20:31<22:12:24, 72.74s/it] 42%|████▏     | 808/1906 [16:21:43<22:11:08, 72.74s/it] 42%|████▏     | 809/1906 [16:22:56<22:09:49, 72.73s/it] 42%|████▏     | 810/1906 [16:24:09<22:08:36, 72.73s/it]                                                        {'loss': 1.741, 'learning_rate': 0.00012335254992172512, 'epoch': 0.42}
 42%|████▏     | 810/1906 [16:24:09<22:08:36, 72.73s/it] 43%|████▎     | 811/1906 [16:25:21<22:07:33, 72.74s/it] 43%|████▎     | 812/1906 [16:26:34<22:06:18, 72.74s/it] 43%|████▎     | 813/1906 [16:27:47<22:04:57, 72.73s/it] 43%|████▎     | 814/1906 [16:29:00<22:03:42, 72.73s/it] 43%|████▎     | 815/1906 [16:30:12<22:02:36, 72.74s/it] 43%|████▎     | 816/1906 [16:31:25<22:01:26, 72.74s/it] 43%|████▎     | 817/1906 [16:32:38<22:00:14, 72.74s/it] 43%|████▎     | 818/1906 [16:33:51<21:59:00, 72.74s/it] 43%|████▎     | 819/1906 [16:35:03<21:57:48, 72.74s/it] 43%|████▎     | 820/1906 [16:36:16<21:56:45, 72.75s/it]                                                        {'loss': 1.7323, 'learning_rate': 0.00012174675901074577, 'epoch': 0.43}
 43%|████▎     | 820/1906 [16:36:16<21:56:45, 72.75s/it] 43%|████▎     | 821/1906 [16:37:29<21:55:50, 72.77s/it] 43%|████▎     | 822/1906 [16:38:42<21:54:31, 72.76s/it] 43%|████▎     | 823/1906 [16:39:54<21:53:05, 72.75s/it] 43%|████▎     | 824/1906 [16:41:07<21:52:10, 72.76s/it] 43%|████▎     | 825/1906 [16:42:20<21:50:56, 72.76s/it] 43%|████▎     | 826/1906 [16:43:33<21:49:53, 72.77s/it] 43%|████▎     | 827/1906 [16:44:46<21:48:51, 72.78s/it] 43%|████▎     | 828/1906 [16:45:58<21:47:41, 72.78s/it] 43%|████▎     | 829/1906 [16:47:11<21:46:17, 72.77s/it] 44%|████▎     | 830/1906 [16:48:24<21:44:53, 72.76s/it]                                                        {'loss': 1.7352, 'learning_rate': 0.000120135060124585, 'epoch': 0.44}
 44%|████▎     | 830/1906 [16:48:24<21:44:53, 72.76s/it] 44%|████▎     | 831/1906 [16:49:37<21:43:41, 72.76s/it] 44%|████▎     | 832/1906 [16:50:49<21:42:26, 72.76s/it] 44%|████▎     | 833/1906 [16:52:02<21:41:06, 72.76s/it] 44%|████▍     | 834/1906 [16:53:15<21:39:58, 72.76s/it] 44%|████▍     | 835/1906 [16:54:28<21:38:50, 72.76s/it] 44%|████▍     | 836/1906 [16:55:40<21:37:41, 72.77s/it] 44%|████▍     | 837/1906 [16:56:53<21:36:44, 72.78s/it] 44%|████▍     | 838/1906 [16:58:06<21:35:22, 72.77s/it] 44%|████▍     | 839/1906 [16:59:19<21:34:03, 72.77s/it] 44%|████▍     | 840/1906 [17:00:32<21:32:52, 72.77s/it]                                                        {'loss': 1.7405, 'learning_rate': 0.00011851789111593688, 'epoch': 0.44}
 44%|████▍     | 840/1906 [17:00:32<21:32:52, 72.77s/it] 44%|████▍     | 841/1906 [17:01:44<21:31:53, 72.78s/it] 44%|████▍     | 842/1906 [17:02:57<21:30:30, 72.77s/it] 44%|████▍     | 843/1906 [17:04:10<21:29:18, 72.77s/it] 44%|████▍     | 844/1906 [17:05:23<21:28:23, 72.79s/it] 44%|████▍     | 845/1906 [17:06:36<21:27:12, 72.79s/it] 44%|████▍     | 846/1906 [17:07:48<21:26:13, 72.80s/it] 44%|████▍     | 847/1906 [17:09:01<21:24:54, 72.80s/it] 44%|████▍     | 848/1906 [17:10:14<21:23:39, 72.80s/it] 45%|████▍     | 849/1906 [17:11:27<21:22:42, 72.81s/it] 45%|████▍     | 850/1906 [17:12:40<21:21:32, 72.81s/it]                                                        {'loss': 1.7281, 'learning_rate': 0.0001168956913235719, 'epoch': 0.45}
 45%|████▍     | 850/1906 [17:12:40<21:21:32, 72.81s/it] 45%|████▍     | 851/1906 [17:13:52<21:20:37, 72.83s/it] 45%|████▍     | 852/1906 [17:15:05<21:19:14, 72.82s/it] 45%|████▍     | 853/1906 [17:16:18<21:17:53, 72.81s/it] 45%|████▍     | 854/1906 [17:17:31<21:16:57, 72.83s/it] 45%|████▍     | 855/1906 [17:18:44<21:15:34, 72.82s/it] 45%|████▍     | 856/1906 [17:19:57<21:14:21, 72.82s/it] 45%|████▍     | 857/1906 [17:21:09<21:13:10, 72.82s/it] 45%|████▌     | 858/1906 [17:22:22<21:11:53, 72.82s/it] 45%|████▌     | 859/1906 [17:23:35<21:10:47, 72.82s/it] 45%|████▌     | 860/1906 [17:24:48<21:09:34, 72.82s/it]                                                        {'loss': 1.7378, 'learning_rate': 0.00011526890145298137, 'epoch': 0.45}
 45%|████▌     | 860/1906 [17:24:48<21:09:34, 72.82s/it] 45%|████▌     | 861/1906 [17:26:01<21:08:33, 72.84s/it] 45%|████▌     | 862/1906 [17:27:14<21:07:09, 72.83s/it] 45%|████▌     | 863/1906 [17:28:26<21:05:40, 72.81s/it] 45%|████▌     | 864/1906 [17:29:39<21:04:34, 72.82s/it] 45%|████▌     | 865/1906 [17:30:52<21:03:03, 72.80s/it] 45%|████▌     | 866/1906 [17:32:05<21:01:47, 72.80s/it] 45%|████▌     | 867/1906 [17:33:17<21:00:28, 72.79s/it] 46%|████▌     | 868/1906 [17:34:30<20:59:05, 72.78s/it] 46%|████▌     | 869/1906 [17:35:43<20:57:41, 72.77s/it] 46%|████▌     | 870/1906 [17:36:56<20:56:26, 72.77s/it]                                                        {'loss': 1.7216, 'learning_rate': 0.00011363796345665001, 'epoch': 0.46}
 46%|████▌     | 870/1906 [17:36:56<20:56:26, 72.77s/it] 46%|████▌     | 871/1906 [17:38:09<20:55:34, 72.79s/it] 46%|████▌     | 872/1906 [17:39:21<20:54:11, 72.78s/it] 46%|████▌     | 873/1906 [17:40:34<20:52:59, 72.78s/it] 46%|████▌     | 874/1906 [17:41:47<20:51:39, 72.77s/it] 46%|████▌     | 875/1906 [17:43:00<20:50:21, 72.77s/it] 46%|████▌     | 876/1906 [17:44:12<20:49:02, 72.76s/it] 46%|████▌     | 877/1906 [17:45:25<20:47:41, 72.75s/it] 46%|████▌     | 878/1906 [17:46:38<20:46:25, 72.75s/it] 46%|████▌     | 879/1906 [17:47:51<20:45:01, 72.74s/it] 46%|████▌     | 880/1906 [17:49:03<20:43:50, 72.74s/it]                                                        {'loss': 1.7273, 'learning_rate': 0.00011200332041399027, 'epoch': 0.46}
 46%|████▌     | 880/1906 [17:49:03<20:43:50, 72.74s/it] 46%|████▌     | 881/1906 [17:50:16<20:43:02, 72.76s/it] 46%|████▋     | 882/1906 [17:51:29<20:41:57, 72.77s/it] 46%|████▋     | 883/1906 [17:52:42<20:40:40, 72.77s/it] 46%|████▋     | 884/1906 [17:53:54<20:39:21, 72.76s/it] 46%|████▋     | 885/1906 [17:55:07<20:37:58, 72.75s/it] 46%|████▋     | 886/1906 [17:56:20<20:36:39, 72.74s/it] 47%|████▋     | 887/1906 [17:57:33<20:35:13, 72.73s/it] 47%|████▋     | 888/1906 [17:58:45<20:34:09, 72.74s/it] 47%|████▋     | 889/1906 [17:59:58<20:33:13, 72.76s/it] 47%|████▋     | 890/1906 [18:01:11<20:31:58, 72.75s/it]                                                        {'loss': 1.728, 'learning_rate': 0.0001103654164109702, 'epoch': 0.47}
 47%|████▋     | 890/1906 [18:01:11<20:31:58, 72.75s/it] 47%|████▋     | 891/1906 [18:02:24<20:30:49, 72.76s/it] 47%|████▋     | 892/1906 [18:03:36<20:29:42, 72.76s/it] 47%|████▋     | 893/1906 [18:04:49<20:28:33, 72.77s/it] 47%|████▋     | 894/1906 [18:06:02<20:27:30, 72.78s/it] 47%|████▋     | 895/1906 [18:07:15<20:26:24, 72.78s/it] 47%|████▋     | 896/1906 [18:08:28<20:25:11, 72.78s/it] 47%|████▋     | 897/1906 [18:09:40<20:24:05, 72.79s/it] 47%|████▋     | 898/1906 [18:10:53<20:23:00, 72.80s/it] 47%|████▋     | 899/1906 [18:12:06<20:21:40, 72.79s/it] 47%|████▋     | 900/1906 [18:13:19<20:20:17, 72.78s/it]                                                        {'loss': 1.7276, 'learning_rate': 0.00010872469641946783, 'epoch': 0.47}
 47%|████▋     | 900/1906 [18:13:19<20:20:17, 72.78s/it] 47%|████▋     | 901/1906 [18:14:31<20:19:11, 72.79s/it] 47%|████▋     | 902/1906 [18:15:44<20:17:43, 72.77s/it] 47%|████▋     | 903/1906 [18:16:57<20:16:21, 72.76s/it] 47%|████▋     | 904/1906 [18:18:10<20:15:10, 72.77s/it] 47%|████▋     | 905/1906 [18:19:23<20:14:09, 72.78s/it] 48%|████▊     | 906/1906 [18:20:35<20:13:37, 72.82s/it] 48%|████▊     | 907/1906 [18:21:48<20:12:24, 72.82s/it] 48%|████▊     | 908/1906 [18:23:01<20:10:48, 72.79s/it] 48%|████▊     | 909/1906 [18:24:14<20:09:21, 72.78s/it] 48%|████▊     | 910/1906 [18:25:27<20:08:12, 72.78s/it]                                                        {'loss': 1.7183, 'learning_rate': 0.00010708160617638521, 'epoch': 0.48}
 48%|████▊     | 910/1906 [18:25:27<20:08:12, 72.78s/it] 48%|████▊     | 911/1906 [18:26:39<20:07:03, 72.79s/it] 48%|████▊     | 912/1906 [18:27:52<20:05:39, 72.78s/it] 48%|████▊     | 913/1906 [18:29:05<20:04:24, 72.77s/it] 48%|████▊     | 914/1906 [18:30:18<20:03:09, 72.77s/it] 48%|████▊     | 915/1906 [18:31:30<20:01:55, 72.77s/it] 48%|████▊     | 916/1906 [18:32:43<20:00:40, 72.77s/it] 48%|████▊     | 917/1906 [18:33:56<19:59:33, 72.77s/it] 48%|████▊     | 918/1906 [18:35:09<19:58:26, 72.78s/it] 48%|████▊     | 919/1906 [18:36:22<19:57:13, 72.78s/it] 48%|████▊     | 920/1906 [18:37:34<19:56:00, 72.78s/it]                                                        {'loss': 1.7265, 'learning_rate': 0.00010543659206255409, 'epoch': 0.48}
 48%|████▊     | 920/1906 [18:37:34<19:56:00, 72.78s/it] 48%|████▊     | 921/1906 [18:38:47<19:54:44, 72.78s/it] 48%|████▊     | 922/1906 [18:40:00<19:53:17, 72.76s/it] 48%|████▊     | 923/1906 [18:41:13<19:52:09, 72.77s/it] 48%|████▊     | 924/1906 [18:42:25<19:50:56, 72.77s/it] 49%|████▊     | 925/1906 [18:43:38<19:49:46, 72.77s/it] 49%|████▊     | 926/1906 [18:44:51<19:48:29, 72.77s/it] 49%|████▊     | 927/1906 [18:46:04<19:47:08, 72.76s/it] 49%|████▊     | 928/1906 [18:47:16<19:46:16, 72.78s/it] 49%|████▊     | 929/1906 [18:48:29<19:45:17, 72.79s/it] 49%|████▉     | 930/1906 [18:49:42<19:43:54, 72.78s/it]                                                        {'loss': 1.7108, 'learning_rate': 0.00010379010098146728, 'epoch': 0.49}
 49%|████▉     | 930/1906 [18:49:42<19:43:54, 72.78s/it] 49%|████▉     | 931/1906 [18:50:55<19:42:39, 72.78s/it] 49%|████▉     | 932/1906 [18:52:08<19:41:18, 72.77s/it] 49%|████▉     | 933/1906 [18:53:20<19:39:57, 72.76s/it] 49%|████▉     | 934/1906 [18:54:33<19:38:37, 72.75s/it] 49%|████▉     | 935/1906 [18:55:46<19:37:16, 72.75s/it] 49%|████▉     | 936/1906 [18:56:58<19:36:03, 72.75s/it] 49%|████▉     | 937/1906 [18:58:11<19:34:45, 72.74s/it] 49%|████▉     | 938/1906 [18:59:24<19:33:29, 72.74s/it] 49%|████▉     | 939/1906 [19:00:37<19:32:23, 72.74s/it] 49%|████▉     | 940/1906 [19:01:49<19:31:07, 72.74s/it]                                                        {'loss': 1.7128, 'learning_rate': 0.0001021425802378674, 'epoch': 0.49}
 49%|████▉     | 940/1906 [19:01:49<19:31:07, 72.74s/it] 49%|████▉     | 941/1906 [19:03:02<19:30:19, 72.77s/it] 49%|████▉     | 942/1906 [19:04:15<19:29:01, 72.76s/it] 49%|████▉     | 943/1906 [19:05:28<19:27:38, 72.75s/it] 50%|████▉     | 944/1906 [19:06:40<19:26:18, 72.74s/it] 50%|████▉     | 945/1906 [19:07:53<19:25:06, 72.74s/it] 50%|████▉     | 946/1906 [19:09:06<19:23:42, 72.73s/it] 50%|████▉     | 947/1906 [19:10:19<19:22:23, 72.72s/it] 50%|████▉     | 948/1906 [19:11:31<19:21:01, 72.72s/it] 50%|████▉     | 949/1906 [19:12:44<19:19:35, 72.70s/it] 50%|████▉     | 950/1906 [19:13:57<19:18:29, 72.71s/it]                                                        {'loss': 1.7207, 'learning_rate': 0.00010049447741622717, 'epoch': 0.5}
 50%|████▉     | 950/1906 [19:13:57<19:18:29, 72.71s/it] 50%|████▉     | 951/1906 [19:15:09<19:17:25, 72.72s/it] 50%|████▉     | 952/1906 [19:16:22<19:16:05, 72.71s/it] 50%|█████     | 953/1906 [19:17:35<19:14:48, 72.71s/it] 50%|█████     | 954/1906 [19:18:48<19:13:26, 72.70s/it] 50%|█████     | 955/1906 [19:20:00<19:12:18, 72.70s/it] 50%|█████     | 956/1906 [19:21:13<19:11:06, 72.70s/it] 50%|█████     | 957/1906 [19:22:26<19:09:54, 72.70s/it] 50%|█████     | 958/1906 [19:23:38<19:08:43, 72.70s/it] 50%|█████     | 959/1906 [19:24:51<19:07:23, 72.70s/it] 50%|█████     | 960/1906 [19:26:04<19:06:07, 72.69s/it]                                                        {'loss': 1.716, 'learning_rate': 9.884624025915328e-05, 'epoch': 0.5}
 50%|█████     | 960/1906 [19:26:04<19:06:07, 72.69s/it] 50%|█████     | 961/1906 [19:27:16<19:05:09, 72.71s/it] 50%|█████     | 962/1906 [19:28:29<19:03:41, 72.69s/it] 51%|█████     | 963/1906 [19:29:42<19:02:33, 72.70s/it] 51%|█████     | 964/1906 [19:30:54<19:01:16, 72.69s/it] 51%|█████     | 965/1906 [19:32:07<19:00:09, 72.70s/it] 51%|█████     | 966/1906 [19:33:20<18:58:55, 72.70s/it] 51%|█████     | 967/1906 [19:34:33<18:57:55, 72.71s/it] 51%|█████     | 968/1906 [19:35:45<18:56:40, 72.71s/it] 51%|█████     | 969/1906 [19:36:58<18:55:27, 72.71s/it] 51%|█████     | 970/1906 [19:38:11<18:54:10, 72.70s/it]                                                        {'loss': 1.7167, 'learning_rate': 9.719831654574745e-05, 'epoch': 0.51}
 51%|█████     | 970/1906 [19:38:11<18:54:10, 72.70s/it] 51%|█████     | 971/1906 [19:39:24<18:53:15, 72.72s/it] 51%|█████     | 972/1906 [19:40:36<18:51:56, 72.72s/it] 51%|█████     | 973/1906 [19:41:49<18:50:41, 72.71s/it] 51%|█████     | 974/1906 [19:43:02<18:49:35, 72.72s/it] 51%|█████     | 975/1906 [19:44:14<18:48:25, 72.72s/it] 51%|█████     | 976/1906 [19:45:27<18:47:13, 72.72s/it] 51%|█████▏    | 977/1906 [19:46:40<18:46:05, 72.73s/it] 51%|█████▏    | 978/1906 [19:47:53<18:44:47, 72.72s/it] 51%|█████▏    | 979/1906 [19:49:05<18:43:29, 72.72s/it] 51%|█████▏    | 980/1906 [19:50:18<18:42:18, 72.72s/it]                                                        {'loss': 1.7125, 'learning_rate': 9.555115396995788e-05, 'epoch': 0.51}
 51%|█████▏    | 980/1906 [19:50:18<18:42:18, 72.72s/it] 51%|█████▏    | 981/1906 [19:51:31<18:41:19, 72.73s/it] 52%|█████▏    | 982/1906 [19:52:44<18:40:16, 72.75s/it] 52%|█████▏    | 983/1906 [19:53:56<18:39:00, 72.74s/it] 52%|█████▏    | 984/1906 [19:55:09<18:37:53, 72.75s/it] 52%|█████▏    | 985/1906 [19:56:22<18:36:36, 72.74s/it] 52%|█████▏    | 986/1906 [19:57:34<18:35:21, 72.74s/it] 52%|█████▏    | 987/1906 [19:58:47<18:34:07, 72.74s/it] 52%|█████▏    | 988/1906 [20:00:00<18:33:04, 72.75s/it] 52%|█████▏    | 989/1906 [20:01:13<18:31:47, 72.75s/it] 52%|█████▏    | 990/1906 [20:02:25<18:30:23, 72.73s/it]                                                        {'loss': 1.71, 'learning_rate': 9.390520001895321e-05, 'epoch': 0.52}
 52%|█████▏    | 990/1906 [20:02:25<18:30:23, 72.73s/it] 52%|█████▏    | 991/1906 [20:03:38<18:29:26, 72.75s/it] 52%|█████▏    | 992/1906 [20:04:51<18:28:10, 72.75s/it] 52%|█████▏    | 993/1906 [20:06:04<18:26:53, 72.74s/it] 52%|█████▏    | 994/1906 [20:07:16<18:25:36, 72.74s/it] 52%|█████▏    | 995/1906 [20:08:29<18:24:32, 72.75s/it] 52%|█████▏    | 996/1906 [20:09:42<18:23:12, 72.74s/it] 52%|█████▏    | 997/1906 [20:10:55<18:22:05, 72.75s/it] 52%|█████▏    | 998/1906 [20:12:07<18:21:00, 72.75s/it] 52%|█████▏    | 999/1906 [20:13:20<18:19:53, 72.76s/it] 52%|█████▏    | 1000/1906 [20:14:33<18:18:41, 72.76s/it]                                                         {'loss': 1.7109, 'learning_rate': 9.226090185155314e-05, 'epoch': 0.52}
 52%|█████▏    | 1000/1906 [20:14:33<18:18:41, 72.76s/it] 53%|█████▎    | 1001/1906 [20:15:46<18:17:34, 72.77s/it] 53%|█████▎    | 1002/1906 [20:16:59<18:16:14, 72.76s/it] 53%|█████▎    | 1003/1906 [20:18:11<18:15:05, 72.76s/it] 53%|█████▎    | 1004/1906 [20:19:24<18:13:42, 72.75s/it] 53%|█████▎    | 1005/1906 [20:20:37<18:12:26, 72.75s/it] 53%|█████▎    | 1006/1906 [20:21:50<18:11:17, 72.75s/it] 53%|█████▎    | 1007/1906 [20:23:02<18:10:08, 72.76s/it] 53%|█████▎    | 1008/1906 [20:24:15<18:08:57, 72.76s/it] 53%|█████▎    | 1009/1906 [20:25:28<18:07:49, 72.76s/it] 53%|█████▎    | 1010/1906 [20:26:41<18:06:32, 72.76s/it]                                                         {'loss': 1.7108, 'learning_rate': 9.061870617674817e-05, 'epoch': 0.53}
 53%|█████▎    | 1010/1906 [20:26:41<18:06:32, 72.76s/it] 53%|█████▎    | 1011/1906 [20:27:53<18:05:31, 72.77s/it] 53%|█████▎    | 1012/1906 [20:29:06<18:04:17, 72.77s/it] 53%|█████▎    | 1013/1906 [20:30:19<18:03:03, 72.77s/it] 53%|█████▎    | 1014/1906 [20:31:32<18:01:57, 72.78s/it] 53%|█████▎    | 1015/1906 [20:32:45<18:00:51, 72.79s/it] 53%|█████▎    | 1016/1906 [20:33:57<17:59:37, 72.78s/it] 53%|█████▎    | 1017/1906 [20:35:10<17:58:33, 72.79s/it] 53%|█████▎    | 1018/1906 [20:36:23<17:57:23, 72.80s/it] 53%|█████▎    | 1019/1906 [20:37:36<17:56:03, 72.79s/it] 54%|█████▎    | 1020/1906 [20:38:48<17:54:48, 72.79s/it]                                                         {'loss': 1.7082, 'learning_rate': 8.897905913234143e-05, 'epoch': 0.54}
 54%|█████▎    | 1020/1906 [20:38:49<17:54:48, 72.79s/it] 54%|█████▎    | 1021/1906 [20:40:01<17:53:39, 72.79s/it] 54%|█████▎    | 1022/1906 [20:41:14<17:52:21, 72.78s/it] 54%|█████▎    | 1023/1906 [20:42:27<17:51:05, 72.78s/it] 54%|█████▎    | 1024/1906 [20:43:40<17:49:47, 72.77s/it] 54%|█████▍    | 1025/1906 [20:44:52<17:48:27, 72.77s/it] 54%|█████▍    | 1026/1906 [20:46:05<17:47:11, 72.76s/it] 54%|█████▍    | 1027/1906 [20:47:18<17:46:02, 72.77s/it] 54%|█████▍    | 1028/1906 [20:48:31<17:44:52, 72.77s/it] 54%|█████▍    | 1029/1906 [20:49:43<17:43:37, 72.77s/it] 54%|█████▍    | 1030/1906 [20:50:56<17:42:13, 72.76s/it]                                                         {'loss': 1.7041, 'learning_rate': 8.734240616374565e-05, 'epoch': 0.54}
 54%|█████▍    | 1030/1906 [20:50:56<17:42:13, 72.76s/it] 54%|█████▍    | 1031/1906 [20:52:09<17:41:12, 72.77s/it] 54%|█████▍    | 1032/1906 [20:53:22<17:39:56, 72.77s/it] 54%|█████▍    | 1033/1906 [20:54:34<17:38:39, 72.76s/it] 54%|█████▍    | 1034/1906 [20:55:47<17:37:34, 72.77s/it] 54%|█████▍    | 1035/1906 [20:57:00<17:36:23, 72.77s/it] 54%|█████▍    | 1036/1906 [20:58:13<17:35:13, 72.77s/it] 54%|█████▍    | 1037/1906 [20:59:26<17:34:00, 72.77s/it] 54%|█████▍    | 1038/1906 [21:00:38<17:32:39, 72.76s/it] 55%|█████▍    | 1039/1906 [21:01:51<17:31:17, 72.75s/it] 55%|█████▍    | 1040/1906 [21:03:04<17:29:57, 72.75s/it]                                                         {'loss': 1.7024, 'learning_rate': 8.570919190296855e-05, 'epoch': 0.55}
 55%|█████▍    | 1040/1906 [21:03:04<17:29:57, 72.75s/it] 55%|█████▍    | 1041/1906 [21:04:17<17:28:52, 72.75s/it] 55%|█████▍    | 1042/1906 [21:05:29<17:27:45, 72.76s/it] 55%|█████▍    | 1043/1906 [21:06:42<17:26:34, 72.76s/it] 55%|█████▍    | 1044/1906 [21:07:55<17:25:21, 72.76s/it] 55%|█████▍    | 1045/1906 [21:09:08<17:24:02, 72.76s/it] 55%|█████▍    | 1046/1906 [21:10:20<17:22:46, 72.75s/it] 55%|█████▍    | 1047/1906 [21:11:33<17:21:36, 72.75s/it] 55%|█████▍    | 1048/1906 [21:12:46<17:20:25, 72.76s/it] 55%|█████▌    | 1049/1906 [21:13:59<17:19:19, 72.76s/it] 55%|█████▌    | 1050/1906 [21:15:11<17:18:14, 72.77s/it]                                                         {'loss': 1.6999, 'learning_rate': 8.407986004781879e-05, 'epoch': 0.55}
 55%|█████▌    | 1050/1906 [21:15:11<17:18:14, 72.77s/it] 55%|█████▌    | 1051/1906 [21:16:24<17:17:21, 72.80s/it] 55%|█████▌    | 1052/1906 [21:17:37<17:16:10, 72.80s/it] 55%|█████▌    | 1053/1906 [21:18:50<17:14:53, 72.79s/it] 55%|█████▌    | 1054/1906 [21:20:03<17:13:35, 72.79s/it] 55%|█████▌    | 1055/1906 [21:21:15<17:12:25, 72.79s/it] 55%|█████▌    | 1056/1906 [21:22:28<17:11:11, 72.79s/it] 55%|█████▌    | 1057/1906 [21:23:41<17:10:00, 72.79s/it] 56%|█████▌    | 1058/1906 [21:24:54<17:08:46, 72.79s/it] 56%|█████▌    | 1059/1906 [21:26:07<17:07:47, 72.81s/it] 56%|█████▌    | 1060/1906 [21:27:19<17:06:35, 72.81s/it]                                                         {'loss': 1.7094, 'learning_rate': 8.245485324136597e-05, 'epoch': 0.56}
 56%|█████▌    | 1060/1906 [21:27:19<17:06:35, 72.81s/it] 56%|█████▌    | 1061/1906 [21:28:32<17:05:37, 72.82s/it] 56%|█████▌    | 1062/1906 [21:29:45<17:04:13, 72.81s/it] 56%|█████▌    | 1063/1906 [21:30:58<17:02:49, 72.80s/it] 56%|█████▌    | 1064/1906 [21:32:11<17:01:25, 72.79s/it] 56%|█████▌    | 1065/1906 [21:33:23<17:00:02, 72.77s/it] 56%|█████▌    | 1066/1906 [21:34:36<16:58:45, 72.77s/it] 56%|█████▌    | 1067/1906 [21:35:49<16:57:33, 72.77s/it] 56%|█████▌    | 1068/1906 [21:37:02<16:56:18, 72.77s/it] 56%|█████▌    | 1069/1906 [21:38:14<16:55:02, 72.76s/it] 56%|█████▌    | 1070/1906 [21:39:27<16:53:54, 72.77s/it]                                                         {'loss': 1.7037, 'learning_rate': 8.083461295168707e-05, 'epoch': 0.56}
 56%|█████▌    | 1070/1906 [21:39:27<16:53:54, 72.77s/it] 56%|█████▌    | 1071/1906 [21:40:40<16:52:55, 72.79s/it] 56%|█████▌    | 1072/1906 [21:41:53<16:51:46, 72.79s/it] 56%|█████▋    | 1073/1906 [21:43:06<16:50:31, 72.79s/it] 56%|█████▋    | 1074/1906 [21:44:18<16:49:07, 72.77s/it] 56%|█████▋    | 1075/1906 [21:45:31<16:47:50, 72.77s/it] 56%|█████▋    | 1076/1906 [21:46:44<16:46:35, 72.77s/it] 57%|█████▋    | 1077/1906 [21:47:57<16:45:23, 72.77s/it] 57%|█████▋    | 1078/1906 [21:49:09<16:44:22, 72.78s/it] 57%|█████▋    | 1079/1906 [21:50:22<16:43:03, 72.77s/it] 57%|█████▋    | 1080/1906 [21:51:35<16:41:56, 72.78s/it]                                                         {'loss': 1.7002, 'learning_rate': 7.921957935193232e-05, 'epoch': 0.57}
 57%|█████▋    | 1080/1906 [21:51:35<16:41:56, 72.78s/it] 57%|█████▋    | 1081/1906 [21:52:48<16:40:50, 72.79s/it] 57%|█████▋    | 1082/1906 [21:54:01<16:39:38, 72.79s/it] 57%|█████▋    | 1083/1906 [21:55:13<16:38:16, 72.78s/it] 57%|█████▋    | 1084/1906 [21:56:26<16:36:50, 72.76s/it] 57%|█████▋    | 1085/1906 [21:57:39<16:35:30, 72.75s/it] 57%|█████▋    | 1086/1906 [21:58:51<16:34:02, 72.73s/it] 57%|█████▋    | 1087/1906 [22:00:04<16:32:54, 72.74s/it] 57%|█████▋    | 1088/1906 [22:01:17<16:31:51, 72.75s/it] 57%|█████▋    | 1089/1906 [22:02:30<16:30:41, 72.76s/it] 57%|█████▋    | 1090/1906 [22:03:42<16:29:19, 72.74s/it]                                                         {'loss': 1.6962, 'learning_rate': 7.761019120074245e-05, 'epoch': 0.57}
 57%|█████▋    | 1090/1906 [22:03:43<16:29:19, 72.74s/it] 57%|█████▋    | 1091/1906 [22:04:55<16:28:18, 72.76s/it] 57%|█████▋    | 1092/1906 [22:06:08<16:26:55, 72.75s/it] 57%|█████▋    | 1093/1906 [22:07:21<16:25:39, 72.74s/it] 57%|█████▋    | 1094/1906 [22:08:33<16:24:18, 72.73s/it] 57%|█████▋    | 1095/1906 [22:09:46<16:22:55, 72.72s/it] 58%|█████▊    | 1096/1906 [22:10:59<16:21:40, 72.72s/it] 58%|█████▊    | 1097/1906 [22:12:12<16:20:32, 72.72s/it] 58%|█████▊    | 1098/1906 [22:13:24<16:19:11, 72.71s/it] 58%|█████▊    | 1099/1906 [22:14:37<16:17:55, 72.71s/it] 58%|█████▊    | 1100/1906 [22:15:50<16:16:45, 72.71s/it]                                                         {'loss': 1.6997, 'learning_rate': 7.60068857230506e-05, 'epoch': 0.58}
 58%|█████▊    | 1100/1906 [22:15:50<16:16:45, 72.71s/it] 58%|█████▊    | 1101/1906 [22:17:02<16:15:55, 72.74s/it] 58%|█████▊    | 1102/1906 [22:18:15<16:14:49, 72.75s/it] 58%|█████▊    | 1103/1906 [22:19:28<16:13:50, 72.76s/it] 58%|█████▊    | 1104/1906 [22:20:41<16:12:47, 72.78s/it] 58%|█████▊    | 1105/1906 [22:21:54<16:11:23, 72.76s/it] 58%|█████▊    | 1106/1906 [22:23:06<16:10:03, 72.75s/it] 58%|█████▊    | 1107/1906 [22:24:19<16:08:44, 72.75s/it] 58%|█████▊    | 1108/1906 [22:25:32<16:07:26, 72.74s/it] 58%|█████▊    | 1109/1906 [22:26:45<16:06:19, 72.75s/it] 58%|█████▊    | 1110/1906 [22:27:57<16:04:58, 72.74s/it]                                                         {'loss': 1.7007, 'learning_rate': 7.441009849130067e-05, 'epoch': 0.58}
 58%|█████▊    | 1110/1906 [22:27:57<16:04:58, 72.74s/it] 58%|█████▊    | 1111/1906 [22:29:10<16:03:49, 72.74s/it] 58%|█████▊    | 1112/1906 [22:30:23<16:02:24, 72.73s/it] 58%|█████▊    | 1113/1906 [22:31:35<16:01:19, 72.74s/it] 58%|█████▊    | 1114/1906 [22:32:48<16:00:08, 72.74s/it] 58%|█████▊    | 1115/1906 [22:34:01<15:58:51, 72.73s/it] 59%|█████▊    | 1116/1906 [22:35:14<15:57:44, 72.74s/it] 59%|█████▊    | 1117/1906 [22:36:26<15:56:29, 72.74s/it] 59%|█████▊    | 1118/1906 [22:37:39<15:55:16, 72.74s/it] 59%|█████▊    | 1119/1906 [22:38:52<15:54:09, 72.74s/it] 59%|█████▉    | 1120/1906 [22:40:05<15:52:58, 72.75s/it]                                                         {'loss': 1.6909, 'learning_rate': 7.282026330711489e-05, 'epoch': 0.59}
 59%|█████▉    | 1120/1906 [22:40:05<15:52:58, 72.75s/it] 59%|█████▉    | 1121/1906 [22:41:17<15:52:02, 72.77s/it] 59%|█████▉    | 1122/1906 [22:42:30<15:50:43, 72.76s/it] 59%|█████▉    | 1123/1906 [22:43:43<15:49:27, 72.76s/it] 59%|█████▉    | 1124/1906 [22:44:56<15:48:11, 72.75s/it] 59%|█████▉    | 1125/1906 [22:46:08<15:46:54, 72.75s/it] 59%|█████▉    | 1126/1906 [22:47:21<15:45:45, 72.75s/it] 59%|█████▉    | 1127/1906 [22:48:34<15:44:37, 72.76s/it] 59%|█████▉    | 1128/1906 [22:49:47<15:43:19, 72.75s/it] 59%|█████▉    | 1129/1906 [22:51:00<15:42:21, 72.77s/it] 59%|█████▉    | 1130/1906 [22:52:12<15:41:13, 72.77s/it]                                                         {'loss': 1.6882, 'learning_rate': 7.1237812083442e-05, 'epoch': 0.59}
 59%|█████▉    | 1130/1906 [22:52:12<15:41:13, 72.77s/it] 59%|█████▉    | 1131/1906 [22:53:25<15:40:08, 72.78s/it] 59%|█████▉    | 1132/1906 [22:54:38<15:38:51, 72.78s/it] 59%|█████▉    | 1133/1906 [22:55:51<15:37:44, 72.79s/it] 59%|█████▉    | 1134/1906 [22:57:03<15:36:25, 72.78s/it] 60%|█████▉    | 1135/1906 [22:58:16<15:35:06, 72.77s/it] 60%|█████▉    | 1136/1906 [22:59:29<15:33:45, 72.76s/it] 60%|█████▉    | 1137/1906 [23:00:42<15:32:32, 72.76s/it] 60%|█████▉    | 1138/1906 [23:01:54<15:31:20, 72.76s/it] 60%|█████▉    | 1139/1906 [23:03:07<15:30:06, 72.76s/it] 60%|█████▉    | 1140/1906 [23:04:20<15:28:56, 72.76s/it]                                                         {'loss': 1.6981, 'learning_rate': 6.966317472721897e-05, 'epoch': 0.6}
 60%|█████▉    | 1140/1906 [23:04:20<15:28:56, 72.76s/it] 60%|█████▉    | 1141/1906 [23:05:33<15:27:57, 72.78s/it] 60%|█████▉    | 1142/1906 [23:06:46<15:26:44, 72.78s/it] 60%|█████▉    | 1143/1906 [23:07:58<15:25:43, 72.80s/it] 60%|██████    | 1144/1906 [23:09:11<15:24:18, 72.78s/it] 60%|██████    | 1145/1906 [23:10:24<15:22:58, 72.77s/it] 60%|██████    | 1146/1906 [23:11:37<15:21:53, 72.78s/it] 60%|██████    | 1147/1906 [23:12:50<15:20:44, 72.79s/it] 60%|██████    | 1148/1906 [23:14:02<15:19:25, 72.78s/it] 60%|██████    | 1149/1906 [23:15:15<15:18:07, 72.77s/it] 60%|██████    | 1150/1906 [23:16:28<15:16:57, 72.77s/it]                                                         {'loss': 1.6869, 'learning_rate': 6.809677902257742e-05, 'epoch': 0.6}
 60%|██████    | 1150/1906 [23:16:28<15:16:57, 72.77s/it] 60%|██████    | 1151/1906 [23:17:41<15:15:57, 72.79s/it] 60%|██████    | 1152/1906 [23:18:53<15:14:47, 72.80s/it] 60%|██████    | 1153/1906 [23:20:06<15:13:29, 72.79s/it] 61%|██████    | 1154/1906 [23:21:19<15:12:19, 72.79s/it] 61%|██████    | 1155/1906 [23:22:32<15:11:17, 72.81s/it] 61%|██████    | 1156/1906 [23:23:45<15:10:05, 72.81s/it] 61%|██████    | 1157/1906 [23:24:57<15:08:53, 72.81s/it] 61%|██████    | 1158/1906 [23:26:10<15:07:49, 72.82s/it] 61%|██████    | 1159/1906 [23:27:23<15:06:32, 72.81s/it] 61%|██████    | 1160/1906 [23:28:36<15:05:21, 72.82s/it]                                                         {'loss': 1.6981, 'learning_rate': 6.653905051462708e-05, 'epoch': 0.61}
 61%|██████    | 1160/1906 [23:28:36<15:05:21, 72.82s/it] 61%|██████    | 1161/1906 [23:29:49<15:04:26, 72.84s/it] 61%|██████    | 1162/1906 [23:31:02<15:03:13, 72.84s/it] 61%|██████    | 1163/1906 [23:32:14<15:01:55, 72.83s/it] 61%|██████    | 1164/1906 [23:33:27<15:00:32, 72.82s/it] 61%|██████    | 1165/1906 [23:34:40<14:59:16, 72.82s/it] 61%|██████    | 1166/1906 [23:35:53<14:58:00, 72.81s/it] 61%|██████    | 1167/1906 [23:37:06<14:56:47, 72.81s/it] 61%|██████▏   | 1168/1906 [23:38:18<14:55:24, 72.80s/it] 61%|██████▏   | 1169/1906 [23:39:31<14:54:07, 72.79s/it] 61%|██████▏   | 1170/1906 [23:40:44<14:52:52, 72.79s/it]                                                         {'loss': 1.6901, 'learning_rate': 6.499041239384698e-05, 'epoch': 0.61}
 61%|██████▏   | 1170/1906 [23:40:44<14:52:52, 72.79s/it] 61%|██████▏   | 1171/1906 [23:41:57<14:51:53, 72.81s/it] 61%|██████▏   | 1172/1906 [23:43:10<14:50:32, 72.80s/it] 62%|██████▏   | 1173/1906 [23:44:22<14:49:15, 72.79s/it] 62%|██████▏   | 1174/1906 [23:45:35<14:48:01, 72.79s/it] 62%|██████▏   | 1175/1906 [23:46:48<14:46:48, 72.79s/it] 62%|██████▏   | 1176/1906 [23:48:01<14:45:43, 72.80s/it] 62%|██████▏   | 1177/1906 [23:49:14<14:44:24, 72.79s/it] 62%|██████▏   | 1178/1906 [23:50:26<14:42:59, 72.77s/it] 62%|██████▏   | 1179/1906 [23:51:39<14:41:31, 72.75s/it] 62%|██████▏   | 1180/1906 [23:52:52<14:40:12, 72.74s/it]                                                         {'loss': 1.6887, 'learning_rate': 6.345128538111685e-05, 'epoch': 0.62}
 62%|██████▏   | 1180/1906 [23:52:52<14:40:12, 72.74s/it] 62%|██████▏   | 1181/1906 [23:54:05<14:39:08, 72.76s/it] 62%|██████▏   | 1182/1906 [23:55:17<14:37:48, 72.75s/it] 62%|██████▏   | 1183/1906 [23:56:30<14:36:35, 72.75s/it] 62%|██████▏   | 1184/1906 [23:57:43<14:35:23, 72.75s/it] 62%|██████▏   | 1185/1906 [23:58:56<14:34:17, 72.76s/it] 62%|██████▏   | 1186/1906 [24:00:08<14:33:01, 72.75s/it] 62%|██████▏   | 1187/1906 [24:01:21<14:31:52, 72.76s/it] 62%|██████▏   | 1188/1906 [24:02:34<14:30:35, 72.75s/it] 62%|██████▏   | 1189/1906 [24:03:47<14:29:23, 72.75s/it] 62%|██████▏   | 1190/1906 [24:04:59<14:28:17, 72.76s/it]                                                         {'loss': 1.6959, 'learning_rate': 6.192208761341925e-05, 'epoch': 0.62}
 62%|██████▏   | 1190/1906 [24:04:59<14:28:17, 72.76s/it] 62%|██████▏   | 1191/1906 [24:06:12<14:27:18, 72.78s/it] 63%|██████▎   | 1192/1906 [24:07:25<14:26:06, 72.78s/it] 63%|██████▎   | 1193/1906 [24:08:38<14:24:50, 72.78s/it] 63%|██████▎   | 1194/1906 [24:09:50<14:23:36, 72.78s/it] 63%|██████▎   | 1195/1906 [24:11:03<14:22:31, 72.79s/it] 63%|██████▎   | 1196/1906 [24:12:16<14:21:08, 72.77s/it] 63%|██████▎   | 1197/1906 [24:13:29<14:19:56, 72.77s/it] 63%|██████▎   | 1198/1906 [24:14:42<14:18:47, 72.78s/it] 63%|██████▎   | 1199/1906 [24:15:54<14:17:30, 72.77s/it] 63%|██████▎   | 1200/1906 [24:17:07<14:16:09, 72.76s/it]                                                         {'loss': 1.688, 'learning_rate': 6.040323453024351e-05, 'epoch': 0.63}
 63%|██████▎   | 1200/1906 [24:17:07<14:16:09, 72.76s/it] 63%|██████▎   | 1201/1906 [24:18:20<14:15:00, 72.77s/it] 63%|██████▎   | 1202/1906 [24:19:33<14:13:49, 72.77s/it] 63%|██████▎   | 1203/1906 [24:20:45<14:12:33, 72.76s/it] 63%|██████▎   | 1204/1906 [24:21:58<14:11:18, 72.76s/it] 63%|██████▎   | 1205/1906 [24:23:11<14:10:01, 72.75s/it] 63%|██████▎   | 1206/1906 [24:24:24<14:08:40, 72.74s/it] 63%|██████▎   | 1207/1906 [24:25:36<14:07:36, 72.76s/it] 63%|██████▎   | 1208/1906 [24:26:49<14:06:22, 72.75s/it] 63%|██████▎   | 1209/1906 [24:28:02<14:05:07, 72.75s/it] 63%|██████▎   | 1210/1906 [24:29:15<14:03:57, 72.76s/it]                                                         {'loss': 1.6953, 'learning_rate': 5.889513876072283e-05, 'epoch': 0.63}
 63%|██████▎   | 1210/1906 [24:29:15<14:03:57, 72.76s/it] 64%|██████▎   | 1211/1906 [24:30:28<14:03:17, 72.80s/it] 64%|██████▎   | 1212/1906 [24:31:40<14:01:58, 72.79s/it] 64%|██████▎   | 1213/1906 [24:32:53<14:00:42, 72.79s/it] 64%|██████▎   | 1214/1906 [24:34:06<13:59:26, 72.78s/it] 64%|██████▎   | 1215/1906 [24:35:19<13:58:17, 72.79s/it] 64%|██████▍   | 1216/1906 [24:36:31<13:57:06, 72.79s/it] 64%|██████▍   | 1217/1906 [24:37:44<13:55:53, 72.79s/it] 64%|██████▍   | 1218/1906 [24:38:57<13:54:43, 72.80s/it] 64%|██████▍   | 1219/1906 [24:40:10<13:53:39, 72.81s/it] 64%|██████▍   | 1220/1906 [24:41:23<13:52:30, 72.81s/it]                                                         {'loss': 1.6875, 'learning_rate': 5.739821001153451e-05, 'epoch': 0.64}
 64%|██████▍   | 1220/1906 [24:41:23<13:52:30, 72.81s/it] 64%|██████▍   | 1221/1906 [24:42:36<13:51:15, 72.81s/it] 64%|██████▍   | 1222/1906 [24:43:48<13:50:03, 72.81s/it] 64%|██████▍   | 1223/1906 [24:45:01<13:48:45, 72.80s/it] 64%|██████▍   | 1224/1906 [24:46:14<13:47:32, 72.80s/it] 64%|██████▍   | 1225/1906 [24:47:27<13:46:24, 72.81s/it] 64%|██████▍   | 1226/1906 [24:48:40<13:45:12, 72.81s/it] 64%|██████▍   | 1227/1906 [24:49:52<13:43:59, 72.81s/it] 64%|██████▍   | 1228/1906 [24:51:05<13:42:39, 72.80s/it] 64%|██████▍   | 1229/1906 [24:52:18<13:41:25, 72.80s/it] 65%|██████▍   | 1230/1906 [24:53:31<13:40:13, 72.80s/it]                                                         {'loss': 1.6887, 'learning_rate': 5.591285495559453e-05, 'epoch': 0.65}
 65%|██████▍   | 1230/1906 [24:53:31<13:40:13, 72.80s/it] 65%|██████▍   | 1231/1906 [24:54:44<13:39:23, 72.83s/it] 65%|██████▍   | 1232/1906 [24:55:56<13:38:00, 72.82s/it] 65%|██████▍   | 1233/1906 [24:57:09<13:36:58, 72.84s/it] 65%|██████▍   | 1234/1906 [24:58:22<13:35:35, 72.82s/it] 65%|██████▍   | 1235/1906 [24:59:35<13:34:16, 72.81s/it] 65%|██████▍   | 1236/1906 [25:00:48<13:33:06, 72.82s/it] 65%|██████▍   | 1237/1906 [25:02:01<13:31:51, 72.81s/it] 65%|██████▍   | 1238/1906 [25:03:13<13:30:32, 72.80s/it] 65%|██████▌   | 1239/1906 [25:04:26<13:29:13, 72.79s/it] 65%|██████▌   | 1240/1906 [25:05:39<13:27:59, 72.79s/it]                                                         {'loss': 1.6851, 'learning_rate': 5.443947712157587e-05, 'epoch': 0.65}
 65%|██████▌   | 1240/1906 [25:05:39<13:27:59, 72.79s/it] 65%|██████▌   | 1241/1906 [25:06:52<13:26:51, 72.80s/it] 65%|██████▌   | 1242/1906 [25:08:04<13:25:26, 72.78s/it] 65%|██████▌   | 1243/1906 [25:09:17<13:24:00, 72.76s/it] 65%|██████▌   | 1244/1906 [25:10:30<13:22:46, 72.76s/it] 65%|██████▌   | 1245/1906 [25:11:43<13:21:33, 72.76s/it] 65%|██████▌   | 1246/1906 [25:12:55<13:20:09, 72.74s/it] 65%|██████▌   | 1247/1906 [25:14:08<13:18:50, 72.73s/it] 65%|██████▌   | 1248/1906 [25:15:21<13:17:42, 72.74s/it] 66%|██████▌   | 1249/1906 [25:16:34<13:16:23, 72.73s/it] 66%|██████▌   | 1250/1906 [25:17:46<13:15:12, 72.73s/it]                                                         {'loss': 1.6863, 'learning_rate': 5.297847678428141e-05, 'epoch': 0.66}
 66%|██████▌   | 1250/1906 [25:17:46<13:15:12, 72.73s/it] 66%|██████▌   | 1251/1906 [25:18:59<13:14:19, 72.76s/it] 66%|██████▌   | 1252/1906 [25:20:12<13:12:59, 72.75s/it] 66%|██████▌   | 1253/1906 [25:21:25<13:11:37, 72.74s/it] 66%|██████▌   | 1254/1906 [25:22:37<13:10:16, 72.72s/it] 66%|██████▌   | 1255/1906 [25:23:50<13:08:59, 72.72s/it] 66%|██████▌   | 1256/1906 [25:25:03<13:07:52, 72.73s/it] 66%|██████▌   | 1257/1906 [25:26:15<13:06:40, 72.73s/it] 66%|██████▌   | 1258/1906 [25:27:28<13:05:28, 72.73s/it] 66%|██████▌   | 1259/1906 [25:28:41<13:04:20, 72.74s/it] 66%|██████▌   | 1260/1906 [25:29:54<13:03:15, 72.75s/it]                                                         {'loss': 1.6812, 'learning_rate': 5.1530250855900576e-05, 'epoch': 0.66}
 66%|██████▌   | 1260/1906 [25:29:54<13:03:15, 72.75s/it] 66%|██████▌   | 1261/1906 [25:31:07<13:02:13, 72.77s/it] 66%|██████▌   | 1262/1906 [25:32:19<13:01:05, 72.77s/it] 66%|██████▋   | 1263/1906 [25:33:32<12:59:51, 72.77s/it] 66%|██████▋   | 1264/1906 [25:34:45<12:58:40, 72.77s/it] 66%|██████▋   | 1265/1906 [25:35:58<12:57:25, 72.77s/it] 66%|██████▋   | 1266/1906 [25:37:10<12:56:09, 72.77s/it] 66%|██████▋   | 1267/1906 [25:38:23<12:54:59, 72.77s/it] 67%|██████▋   | 1268/1906 [25:39:36<12:53:48, 72.77s/it] 67%|██████▋   | 1269/1906 [25:40:49<12:52:28, 72.76s/it] 67%|██████▋   | 1270/1906 [25:42:01<12:51:17, 72.76s/it]                                                         {'loss': 1.6787, 'learning_rate': 5.009519277817976e-05, 'epoch': 0.67}
 67%|██████▋   | 1270/1906 [25:42:01<12:51:17, 72.76s/it] 67%|██████▋   | 1271/1906 [25:43:14<12:50:24, 72.79s/it] 67%|██████▋   | 1272/1906 [25:44:27<12:49:09, 72.79s/it] 67%|██████▋   | 1273/1906 [25:45:40<12:47:53, 72.79s/it] 67%|██████▋   | 1274/1906 [25:46:53<12:46:44, 72.79s/it] 67%|██████▋   | 1275/1906 [25:48:05<12:45:37, 72.80s/it] 67%|██████▋   | 1276/1906 [25:49:18<12:44:32, 72.81s/it] 67%|██████▋   | 1277/1906 [25:50:31<12:43:17, 72.81s/it] 67%|██████▋   | 1278/1906 [25:51:44<12:42:04, 72.81s/it] 67%|██████▋   | 1279/1906 [25:52:57<12:40:52, 72.81s/it] 67%|██████▋   | 1280/1906 [25:54:10<12:39:42, 72.82s/it]                                                         {'loss': 1.6722, 'learning_rate': 4.8673692415535186e-05, 'epoch': 0.67}
 67%|██████▋   | 1280/1906 [25:54:10<12:39:42, 72.82s/it] 67%|██████▋   | 1281/1906 [25:55:22<12:38:37, 72.83s/it] 67%|██████▋   | 1282/1906 [25:56:35<12:37:26, 72.83s/it] 67%|██████▋   | 1283/1906 [25:57:48<12:36:05, 72.82s/it] 67%|██████▋   | 1284/1906 [25:59:01<12:34:46, 72.81s/it] 67%|██████▋   | 1285/1906 [26:00:14<12:33:28, 72.80s/it] 67%|██████▋   | 1286/1906 [26:01:26<12:32:10, 72.79s/it] 68%|██████▊   | 1287/1906 [26:02:39<12:30:59, 72.79s/it] 68%|██████▊   | 1288/1906 [26:03:52<12:29:52, 72.80s/it] 68%|██████▊   | 1289/1906 [26:05:05<12:28:43, 72.81s/it] 68%|██████▊   | 1290/1906 [26:06:18<12:27:38, 72.82s/it]                                                         {'loss': 1.6814, 'learning_rate': 4.726613594913796e-05, 'epoch': 0.68}
 68%|██████▊   | 1290/1906 [26:06:18<12:27:38, 72.82s/it] 68%|██████▊   | 1291/1906 [26:07:31<12:26:37, 72.84s/it] 68%|██████▊   | 1292/1906 [26:08:43<12:25:24, 72.84s/it] 68%|██████▊   | 1293/1906 [26:09:56<12:24:19, 72.85s/it] 68%|██████▊   | 1294/1906 [26:11:09<12:23:07, 72.86s/it] 68%|██████▊   | 1295/1906 [26:12:22<12:21:57, 72.86s/it] 68%|██████▊   | 1296/1906 [26:13:35<12:20:40, 72.85s/it] 68%|██████▊   | 1297/1906 [26:14:48<12:19:39, 72.87s/it] 68%|██████▊   | 1298/1906 [26:16:01<12:18:23, 72.87s/it] 68%|██████▊   | 1299/1906 [26:17:13<12:17:03, 72.86s/it] 68%|██████▊   | 1300/1906 [26:18:26<12:15:49, 72.85s/it]                                                         {'loss': 1.6774, 'learning_rate': 4.587290577199965e-05, 'epoch': 0.68}
 68%|██████▊   | 1300/1906 [26:18:26<12:15:49, 72.85s/it] 68%|██████▊   | 1301/1906 [26:19:39<12:14:43, 72.87s/it] 68%|██████▊   | 1302/1906 [26:20:52<12:13:23, 72.85s/it] 68%|██████▊   | 1303/1906 [26:22:05<12:11:58, 72.83s/it] 68%|██████▊   | 1304/1906 [26:23:18<12:10:55, 72.85s/it] 68%|██████▊   | 1305/1906 [26:24:31<12:09:41, 72.85s/it] 69%|██████▊   | 1306/1906 [26:25:43<12:08:24, 72.84s/it] 69%|██████▊   | 1307/1906 [26:26:56<12:07:05, 72.83s/it] 69%|██████▊   | 1308/1906 [26:28:09<12:05:52, 72.83s/it] 69%|██████▊   | 1309/1906 [26:29:22<12:04:43, 72.84s/it] 69%|██████▊   | 1310/1906 [26:30:35<12:03:29, 72.84s/it]                                                         {'loss': 1.678, 'learning_rate': 4.4494380385086986e-05, 'epoch': 0.69}
 69%|██████▊   | 1310/1906 [26:30:35<12:03:29, 72.84s/it] 69%|██████▉   | 1311/1906 [26:31:48<12:02:24, 72.85s/it] 69%|██████▉   | 1312/1906 [26:33:00<12:01:15, 72.85s/it] 69%|██████▉   | 1313/1906 [26:34:13<12:00:03, 72.86s/it] 69%|██████▉   | 1314/1906 [26:35:26<11:58:41, 72.84s/it] 69%|██████▉   | 1315/1906 [26:36:39<11:57:23, 72.83s/it] 69%|██████▉   | 1316/1906 [26:37:52<11:56:01, 72.82s/it] 69%|██████▉   | 1317/1906 [26:39:05<11:54:50, 72.82s/it] 69%|██████▉   | 1318/1906 [26:40:17<11:53:39, 72.82s/it] 69%|██████▉   | 1319/1906 [26:41:30<11:52:31, 72.83s/it] 69%|██████▉   | 1320/1906 [26:42:43<11:51:17, 72.83s/it]                                                         {'loss': 1.6756, 'learning_rate': 4.3130934294493885e-05, 'epoch': 0.69}
 69%|██████▉   | 1320/1906 [26:42:43<11:51:17, 72.83s/it] 69%|██████▉   | 1321/1906 [26:43:56<11:50:09, 72.84s/it] 69%|██████▉   | 1322/1906 [26:45:09<11:48:50, 72.83s/it] 69%|██████▉   | 1323/1906 [26:46:22<11:47:39, 72.83s/it] 69%|██████▉   | 1324/1906 [26:47:34<11:46:30, 72.84s/it] 70%|██████▉   | 1325/1906 [26:48:47<11:45:21, 72.84s/it] 70%|██████▉   | 1326/1906 [26:50:00<11:44:04, 72.83s/it] 70%|██████▉   | 1327/1906 [26:51:13<11:42:53, 72.84s/it] 70%|██████▉   | 1328/1906 [26:52:26<11:41:44, 72.85s/it] 70%|██████▉   | 1329/1906 [26:53:39<11:40:39, 72.86s/it] 70%|██████▉   | 1330/1906 [26:54:51<11:39:27, 72.86s/it]                                                         {'loss': 1.6845, 'learning_rate': 4.178293790969883e-05, 'epoch': 0.7}
 70%|██████▉   | 1330/1906 [26:54:52<11:39:27, 72.86s/it] 70%|██████▉   | 1331/1906 [26:56:04<11:38:20, 72.87s/it] 70%|██████▉   | 1332/1906 [26:57:17<11:36:58, 72.85s/it] 70%|██████▉   | 1333/1906 [26:58:30<11:35:42, 72.85s/it] 70%|██████▉   | 1334/1906 [26:59:43<11:34:28, 72.85s/it] 70%|███████   | 1335/1906 [27:00:56<11:33:13, 72.84s/it] 70%|███████   | 1336/1906 [27:02:09<11:32:00, 72.84s/it] 70%|███████   | 1337/1906 [27:03:21<11:30:47, 72.84s/it] 70%|███████   | 1338/1906 [27:04:34<11:29:30, 72.84s/it] 70%|███████   | 1339/1906 [27:05:47<11:28:19, 72.84s/it] 70%|███████   | 1340/1906 [27:07:00<11:27:01, 72.83s/it]                                                         {'loss': 1.6718, 'learning_rate': 4.045075744293525e-05, 'epoch': 0.7}
 70%|███████   | 1340/1906 [27:07:00<11:27:01, 72.83s/it] 70%|███████   | 1341/1906 [27:08:13<11:26:00, 72.85s/it] 70%|███████   | 1342/1906 [27:09:26<11:24:41, 72.84s/it] 70%|███████   | 1343/1906 [27:10:38<11:23:27, 72.84s/it] 71%|███████   | 1344/1906 [27:11:51<11:22:11, 72.83s/it] 71%|███████   | 1345/1906 [27:13:04<11:20:51, 72.82s/it] 71%|███████   | 1346/1906 [27:14:17<11:19:38, 72.82s/it] 71%|███████   | 1347/1906 [27:15:30<11:18:22, 72.81s/it] 71%|███████   | 1348/1906 [27:16:42<11:17:09, 72.81s/it] 71%|███████   | 1349/1906 [27:17:55<11:15:58, 72.82s/it] 71%|███████   | 1350/1906 [27:19:08<11:14:52, 72.83s/it]                                                         {'loss': 1.6748, 'learning_rate': 3.913475480970193e-05, 'epoch': 0.71}
 71%|███████   | 1350/1906 [27:19:08<11:14:52, 72.83s/it] 71%|███████   | 1351/1906 [27:20:21<11:13:48, 72.84s/it] 71%|███████   | 1352/1906 [27:21:34<11:12:28, 72.83s/it] 71%|███████   | 1353/1906 [27:22:47<11:11:07, 72.82s/it] 71%|███████   | 1354/1906 [27:23:59<11:09:51, 72.81s/it] 71%|███████   | 1355/1906 [27:25:12<11:08:39, 72.81s/it] 71%|███████   | 1356/1906 [27:26:25<11:07:19, 72.80s/it] 71%|███████   | 1357/1906 [27:27:38<11:06:04, 72.79s/it] 71%|███████   | 1358/1906 [27:28:51<11:04:50, 72.79s/it] 71%|███████▏  | 1359/1906 [27:30:03<11:03:37, 72.79s/it] 71%|███████▏  | 1360/1906 [27:31:16<11:02:27, 72.80s/it]                                                         {'loss': 1.6828, 'learning_rate': 3.783528753044093e-05, 'epoch': 0.71}
 71%|███████▏  | 1360/1906 [27:31:16<11:02:27, 72.80s/it] 71%|███████▏  | 1361/1906 [27:32:29<11:01:31, 72.83s/it] 71%|███████▏  | 1362/1906 [27:33:42<11:00:09, 72.81s/it] 72%|███████▏  | 1363/1906 [27:34:55<10:58:54, 72.81s/it] 72%|███████▏  | 1364/1906 [27:36:07<10:57:37, 72.80s/it] 72%|███████▏  | 1365/1906 [27:37:20<10:56:23, 72.80s/it] 72%|███████▏  | 1366/1906 [27:38:33<10:55:07, 72.79s/it] 72%|███████▏  | 1367/1906 [27:39:46<10:53:57, 72.80s/it] 72%|███████▏  | 1368/1906 [27:40:59<10:52:37, 72.78s/it] 72%|███████▏  | 1369/1906 [27:42:11<10:51:24, 72.78s/it] 72%|███████▏  | 1370/1906 [27:43:24<10:50:02, 72.77s/it]                                                         {'loss': 1.6648, 'learning_rate': 3.6552708633409613e-05, 'epoch': 0.72}
 72%|███████▏  | 1370/1906 [27:43:24<10:50:02, 72.77s/it] 72%|███████▏  | 1371/1906 [27:44:37<10:49:25, 72.83s/it] 72%|███████▏  | 1372/1906 [27:45:50<10:47:56, 72.80s/it] 72%|███████▏  | 1373/1906 [27:47:03<10:46:40, 72.80s/it] 72%|███████▏  | 1374/1906 [27:48:15<10:45:26, 72.80s/it] 72%|███████▏  | 1375/1906 [27:49:28<10:44:05, 72.78s/it] 72%|███████▏  | 1376/1906 [27:50:41<10:42:47, 72.77s/it] 72%|███████▏  | 1377/1906 [27:51:54<10:41:31, 72.76s/it] 72%|███████▏  | 1378/1906 [27:53:06<10:40:16, 72.76s/it] 72%|███████▏  | 1379/1906 [27:54:19<10:39:03, 72.76s/it] 72%|███████▏  | 1380/1906 [27:55:32<10:37:49, 72.76s/it]                                                         {'loss': 1.6732, 'learning_rate': 3.528736655877264e-05, 'epoch': 0.72}
 72%|███████▏  | 1380/1906 [27:55:32<10:37:49, 72.76s/it] 72%|███████▏  | 1381/1906 [27:56:45<10:36:47, 72.78s/it] 73%|███████▎  | 1382/1906 [27:57:57<10:35:29, 72.77s/it] 73%|███████▎  | 1383/1906 [27:59:10<10:34:13, 72.76s/it] 73%|███████▎  | 1384/1906 [28:00:23<10:32:56, 72.75s/it] 73%|███████▎  | 1385/1906 [28:01:36<10:31:41, 72.75s/it] 73%|███████▎  | 1386/1906 [28:02:48<10:30:29, 72.75s/it] 73%|███████▎  | 1387/1906 [28:04:01<10:29:10, 72.74s/it] 73%|███████▎  | 1388/1906 [28:05:14<10:27:54, 72.73s/it] 73%|███████▎  | 1389/1906 [28:06:27<10:26:44, 72.74s/it] 73%|███████▎  | 1390/1906 [28:07:39<10:25:40, 72.75s/it]                                                         {'loss': 1.6771, 'learning_rate': 3.403960506394092e-05, 'epoch': 0.73}
 73%|███████▎  | 1390/1906 [28:07:39<10:25:40, 72.75s/it] 73%|███████▎  | 1391/1906 [28:08:52<10:24:33, 72.76s/it] 73%|███████▎  | 1392/1906 [28:10:05<10:23:16, 72.76s/it] 73%|███████▎  | 1393/1906 [28:11:18<10:21:59, 72.75s/it] 73%|███████▎  | 1394/1906 [28:12:30<10:20:45, 72.75s/it] 73%|███████▎  | 1395/1906 [28:13:43<10:19:35, 72.75s/it] 73%|███████▎  | 1396/1906 [28:14:56<10:18:21, 72.75s/it] 73%|███████▎  | 1397/1906 [28:16:09<10:17:08, 72.75s/it] 73%|███████▎  | 1398/1906 [28:17:21<10:15:58, 72.75s/it] 73%|███████▎  | 1399/1906 [28:18:34<10:14:43, 72.75s/it] 73%|███████▎  | 1400/1906 [28:19:47<10:13:37, 72.76s/it]                                                         {'loss': 1.6782, 'learning_rate': 3.280976313018239e-05, 'epoch': 0.73}
 73%|███████▎  | 1400/1906 [28:19:47<10:13:37, 72.76s/it] 74%|███████▎  | 1401/1906 [28:21:00<10:12:32, 72.78s/it] 74%|███████▎  | 1402/1906 [28:22:12<10:11:17, 72.77s/it] 74%|███████▎  | 1403/1906 [28:23:25<10:10:01, 72.77s/it] 74%|███████▎  | 1404/1906 [28:24:38<10:08:49, 72.77s/it] 74%|███████▎  | 1405/1906 [28:25:51<10:07:33, 72.76s/it] 74%|███████▍  | 1406/1906 [28:27:03<10:06:20, 72.76s/it] 74%|███████▍  | 1407/1906 [28:28:16<10:05:12, 72.77s/it] 74%|███████▍  | 1408/1906 [28:29:29<10:03:59, 72.77s/it] 74%|███████▍  | 1409/1906 [28:30:42<10:02:46, 72.77s/it] 74%|███████▍  | 1410/1906 [28:31:55<10:01:34, 72.77s/it]                                                         {'loss': 1.6826, 'learning_rate': 3.1598174870530604e-05, 'epoch': 0.74}
 74%|███████▍  | 1410/1906 [28:31:55<10:01:34, 72.77s/it] 74%|███████▍  | 1411/1906 [28:33:07<10:00:31, 72.79s/it] 74%|███████▍  | 1412/1906 [28:34:20<9:59:15, 72.79s/it]  74%|███████▍  | 1413/1906 [28:35:33<9:57:57, 72.77s/it] 74%|███████▍  | 1414/1906 [28:36:46<9:56:42, 72.77s/it] 74%|███████▍  | 1415/1906 [28:37:58<9:55:25, 72.76s/it] 74%|███████▍  | 1416/1906 [28:39:11<9:54:15, 72.77s/it] 74%|███████▍  | 1417/1906 [28:40:24<9:53:07, 72.78s/it] 74%|███████▍  | 1418/1906 [28:41:37<9:51:52, 72.77s/it] 74%|███████▍  | 1419/1906 [28:42:50<9:50:42, 72.78s/it] 75%|███████▍  | 1420/1906 [28:44:02<9:49:29, 72.78s/it]                                                        {'loss': 1.6753, 'learning_rate': 3.0405169439015557e-05, 'epoch': 0.74}
 75%|███████▍  | 1420/1906 [28:44:02<9:49:29, 72.78s/it] 75%|███████▍  | 1421/1906 [28:45:15<9:48:22, 72.79s/it] 75%|███████▍  | 1422/1906 [28:46:28<9:47:06, 72.78s/it] 75%|███████▍  | 1423/1906 [28:47:41<9:45:52, 72.78s/it] 75%|███████▍  | 1424/1906 [28:48:53<9:44:35, 72.77s/it] 75%|███████▍  | 1425/1906 [28:50:06<9:43:27, 72.78s/it] 75%|███████▍  | 1426/1906 [28:51:19<9:42:15, 72.78s/it] 75%|███████▍  | 1427/1906 [28:52:32<9:41:09, 72.80s/it] 75%|███████▍  | 1428/1906 [28:53:45<9:39:56, 72.80s/it] 75%|███████▍  | 1429/1906 [28:54:57<9:38:45, 72.80s/it] 75%|███████▌  | 1430/1906 [28:56:10<9:37:30, 72.80s/it]                                                        {'loss': 1.6684, 'learning_rate': 2.9231070941241988e-05, 'epoch': 0.75}
 75%|███████▌  | 1430/1906 [28:56:10<9:37:30, 72.80s/it] 75%|███████▌  | 1431/1906 [28:57:23<9:36:23, 72.81s/it] 75%|███████▌  | 1432/1906 [28:58:36<9:35:03, 72.79s/it] 75%|███████▌  | 1433/1906 [28:59:49<9:33:45, 72.78s/it] 75%|███████▌  | 1434/1906 [29:01:01<9:32:29, 72.77s/it] 75%|███████▌  | 1435/1906 [29:02:14<9:31:15, 72.77s/it] 75%|███████▌  | 1436/1906 [29:03:27<9:30:00, 72.77s/it] 75%|███████▌  | 1437/1906 [29:04:40<9:28:54, 72.78s/it] 75%|███████▌  | 1438/1906 [29:05:52<9:27:39, 72.78s/it] 75%|███████▌  | 1439/1906 [29:07:05<9:26:28, 72.78s/it] 76%|███████▌  | 1440/1906 [29:08:18<9:25:16, 72.78s/it]                                                        {'loss': 1.6755, 'learning_rate': 2.8076198346339113e-05, 'epoch': 0.76}
 76%|███████▌  | 1440/1906 [29:08:18<9:25:16, 72.78s/it] 76%|███████▌  | 1441/1906 [29:09:31<9:24:23, 72.82s/it] 76%|███████▌  | 1442/1906 [29:10:44<9:23:06, 72.82s/it] 76%|███████▌  | 1443/1906 [29:11:57<9:21:49, 72.81s/it] 76%|███████▌  | 1444/1906 [29:13:09<9:20:39, 72.81s/it] 76%|███████▌  | 1445/1906 [29:14:22<9:19:30, 72.82s/it] 76%|███████▌  | 1446/1906 [29:15:35<9:18:16, 72.82s/it] 76%|███████▌  | 1447/1906 [29:16:48<9:17:05, 72.82s/it] 76%|███████▌  | 1448/1906 [29:18:01<9:15:55, 72.83s/it] 76%|███████▌  | 1449/1906 [29:19:14<9:14:40, 72.82s/it] 76%|███████▌  | 1450/1906 [29:20:26<9:13:23, 72.81s/it]                                                        {'loss': 1.672, 'learning_rate': 2.694086540030587e-05, 'epoch': 0.76}
 76%|███████▌  | 1450/1906 [29:20:26<9:13:23, 72.81s/it] 76%|███████▌  | 1451/1906 [29:21:39<9:12:14, 72.82s/it] 76%|███████▌  | 1452/1906 [29:22:52<9:10:55, 72.81s/it] 76%|███████▌  | 1453/1906 [29:24:05<9:09:42, 72.81s/it] 76%|███████▋  | 1454/1906 [29:25:18<9:08:26, 72.80s/it] 76%|███████▋  | 1455/1906 [29:26:30<9:07:12, 72.80s/it] 76%|███████▋  | 1456/1906 [29:27:43<9:05:58, 72.80s/it] 76%|███████▋  | 1457/1906 [29:28:56<9:04:42, 72.79s/it] 76%|███████▋  | 1458/1906 [29:30:09<9:03:24, 72.78s/it] 77%|███████▋  | 1459/1906 [29:31:21<9:02:06, 72.77s/it] 77%|███████▋  | 1460/1906 [29:32:34<9:00:46, 72.75s/it]                                                        {'loss': 1.6698, 'learning_rate': 2.5825380540774914e-05, 'epoch': 0.77}
 77%|███████▋  | 1460/1906 [29:32:34<9:00:46, 72.75s/it] 77%|███████▋  | 1461/1906 [29:33:47<8:59:39, 72.76s/it] 77%|███████▋  | 1462/1906 [29:35:00<8:58:28, 72.77s/it] 77%|███████▋  | 1463/1906 [29:36:12<8:57:11, 72.76s/it] 77%|███████▋  | 1464/1906 [29:37:25<8:55:59, 72.76s/it] 77%|███████▋  | 1465/1906 [29:38:38<8:54:45, 72.76s/it] 77%|███████▋  | 1466/1906 [29:39:51<8:53:38, 72.77s/it] 77%|███████▋  | 1467/1906 [29:41:03<8:52:25, 72.77s/it] 77%|███████▋  | 1468/1906 [29:42:16<8:51:06, 72.76s/it] 77%|███████▋  | 1469/1906 [29:43:29<8:49:53, 72.75s/it] 77%|███████▋  | 1470/1906 [29:44:42<8:48:42, 72.76s/it]                                                        {'loss': 1.6696, 'learning_rate': 2.4730046813218987e-05, 'epoch': 0.77}
 77%|███████▋  | 1470/1906 [29:44:42<8:48:42, 72.76s/it] 77%|███████▋  | 1471/1906 [29:45:54<8:47:32, 72.76s/it] 77%|███████▋  | 1472/1906 [29:47:07<8:46:13, 72.75s/it] 77%|███████▋  | 1473/1906 [29:48:20<8:44:59, 72.75s/it] 77%|███████▋  | 1474/1906 [29:49:33<8:43:46, 72.75s/it] 77%|███████▋  | 1475/1906 [29:50:45<8:42:39, 72.76s/it] 77%|███████▋  | 1476/1906 [29:51:58<8:41:35, 72.78s/it] 77%|███████▋  | 1477/1906 [29:53:11<8:40:20, 72.77s/it] 78%|███████▊  | 1478/1906 [29:54:24<8:39:11, 72.78s/it] 78%|███████▊  | 1479/1906 [29:55:37<8:37:53, 72.77s/it] 78%|███████▊  | 1480/1906 [29:56:49<8:36:41, 72.77s/it]                                                        {'loss': 1.6672, 'learning_rate': 2.3655161788622138e-05, 'epoch': 0.78}
 78%|███████▊  | 1480/1906 [29:56:49<8:36:41, 72.77s/it] 78%|███████▊  | 1481/1906 [29:58:02<8:35:36, 72.79s/it] 78%|███████▊  | 1482/1906 [29:59:15<8:34:17, 72.78s/it] 78%|███████▊  | 1483/1906 [30:00:28<8:33:06, 72.78s/it] 78%|███████▊  | 1484/1906 [30:01:41<8:31:54, 72.78s/it] 78%|███████▊  | 1485/1906 [30:02:53<8:30:44, 72.79s/it] 78%|███████▊  | 1486/1906 [30:04:06<8:29:31, 72.79s/it] 78%|███████▊  | 1487/1906 [30:05:19<8:28:21, 72.80s/it] 78%|███████▊  | 1488/1906 [30:06:32<8:27:03, 72.78s/it] 78%|███████▊  | 1489/1906 [30:07:45<8:25:53, 72.79s/it] 78%|███████▊  | 1490/1906 [30:08:57<8:24:41, 72.79s/it]                                                        {'loss': 1.6771, 'learning_rate': 2.260101748263803e-05, 'epoch': 0.78}
 78%|███████▊  | 1490/1906 [30:08:57<8:24:41, 72.79s/it] 78%|███████▊  | 1491/1906 [30:10:10<8:23:34, 72.81s/it] 78%|███████▊  | 1492/1906 [30:11:23<8:22:23, 72.81s/it] 78%|███████▊  | 1493/1906 [30:12:36<8:21:11, 72.81s/it] 78%|███████▊  | 1494/1906 [30:13:49<8:19:57, 72.81s/it] 78%|███████▊  | 1495/1906 [30:15:01<8:18:47, 72.82s/it] 78%|███████▊  | 1496/1906 [30:16:14<8:17:29, 72.80s/it] 79%|███████▊  | 1497/1906 [30:17:27<8:16:11, 72.79s/it] 79%|███████▊  | 1498/1906 [30:18:40<8:14:54, 72.78s/it] 79%|███████▊  | 1499/1906 [30:19:53<8:13:44, 72.79s/it] 79%|███████▊  | 1500/1906 [30:21:05<8:12:28, 72.78s/it]                                                        {'loss': 1.6677, 'learning_rate': 2.1567900276257703e-05, 'epoch': 0.79}
 79%|███████▊  | 1500/1906 [30:21:06<8:12:28, 72.78s/it] 79%|███████▉  | 1501/1906 [30:22:18<8:11:48, 72.86s/it] 79%|███████▉  | 1502/1906 [30:23:31<8:10:24, 72.83s/it] 79%|███████▉  | 1503/1906 [30:24:44<8:09:03, 72.81s/it] 79%|███████▉  | 1504/1906 [30:25:57<8:07:47, 72.81s/it] 79%|███████▉  | 1505/1906 [30:27:09<8:06:35, 72.81s/it] 79%|███████▉  | 1506/1906 [30:28:22<8:05:16, 72.79s/it] 79%|███████▉  | 1507/1906 [30:29:35<8:04:02, 72.79s/it] 79%|███████▉  | 1508/1906 [30:30:48<8:02:48, 72.79s/it] 79%|███████▉  | 1509/1906 [30:32:01<8:01:30, 72.77s/it] 79%|███████▉  | 1510/1906 [30:33:13<8:00:17, 72.77s/it]                                                        {'loss': 1.6828, 'learning_rate': 2.0556090838007957e-05, 'epoch': 0.79}
 79%|███████▉  | 1510/1906 [30:33:13<8:00:17, 72.77s/it] 79%|███████▉  | 1511/1906 [30:34:26<7:59:11, 72.79s/it] 79%|███████▉  | 1512/1906 [30:35:39<7:57:52, 72.77s/it] 79%|███████▉  | 1513/1906 [30:36:52<7:56:34, 72.76s/it] 79%|███████▉  | 1514/1906 [30:38:04<7:55:23, 72.76s/it] 79%|███████▉  | 1515/1906 [30:39:17<7:54:14, 72.77s/it] 80%|███████▉  | 1516/1906 [30:40:30<7:52:58, 72.76s/it] 80%|███████▉  | 1517/1906 [30:41:43<7:51:43, 72.76s/it] 80%|███████▉  | 1518/1906 [30:42:55<7:50:32, 72.76s/it] 80%|███████▉  | 1519/1906 [30:44:08<7:49:19, 72.76s/it] 80%|███████▉  | 1520/1906 [30:45:21<7:48:05, 72.76s/it]                                                        {'loss': 1.6646, 'learning_rate': 1.956586404770182e-05, 'epoch': 0.8}
 80%|███████▉  | 1520/1906 [30:45:21<7:48:05, 72.76s/it] 80%|███████▉  | 1521/1906 [30:46:34<7:46:54, 72.77s/it] 80%|███████▉  | 1522/1906 [30:47:46<7:45:40, 72.76s/it] 80%|███████▉  | 1523/1906 [30:48:59<7:44:31, 72.77s/it] 80%|███████▉  | 1524/1906 [30:50:12<7:43:15, 72.76s/it] 80%|████████  | 1525/1906 [30:51:25<7:42:04, 72.77s/it] 80%|████████  | 1526/1906 [30:52:38<7:40:44, 72.75s/it] 80%|████████  | 1527/1906 [30:53:50<7:39:27, 72.74s/it] 80%|████████  | 1528/1906 [30:55:03<7:38:17, 72.75s/it] 80%|████████  | 1529/1906 [30:56:16<7:37:01, 72.74s/it] 80%|████████  | 1530/1906 [30:57:28<7:35:39, 72.71s/it]                                                        {'loss': 1.6708, 'learning_rate': 1.859748892176133e-05, 'epoch': 0.8}
 80%|████████  | 1530/1906 [30:57:28<7:35:39, 72.71s/it] 80%|████████  | 1531/1906 [30:58:41<7:34:28, 72.72s/it] 80%|████████  | 1532/1906 [30:59:54<7:33:11, 72.70s/it] 80%|████████  | 1533/1906 [31:01:06<7:31:59, 72.71s/it] 80%|████████  | 1534/1906 [31:02:19<7:30:45, 72.70s/it] 81%|████████  | 1535/1906 [31:03:32<7:29:27, 72.69s/it] 81%|████████  | 1536/1906 [31:04:44<7:28:13, 72.68s/it] 81%|████████  | 1537/1906 [31:05:57<7:27:01, 72.69s/it] 81%|████████  | 1538/1906 [31:07:10<7:25:49, 72.69s/it] 81%|████████  | 1539/1906 [31:08:23<7:24:40, 72.70s/it] 81%|████████  | 1540/1906 [31:09:35<7:23:35, 72.72s/it]                                                        {'loss': 1.6628, 'learning_rate': 1.7651228540133623e-05, 'epoch': 0.81}
 81%|████████  | 1540/1906 [31:09:35<7:23:35, 72.72s/it] 81%|████████  | 1541/1906 [31:10:48<7:22:24, 72.72s/it] 81%|████████  | 1542/1906 [31:12:01<7:21:11, 72.72s/it] 81%|████████  | 1543/1906 [31:13:14<7:19:54, 72.71s/it] 81%|████████  | 1544/1906 [31:14:26<7:18:39, 72.71s/it] 81%|████████  | 1545/1906 [31:15:39<7:17:26, 72.71s/it] 81%|████████  | 1546/1906 [31:16:52<7:16:27, 72.74s/it] 81%|████████  | 1547/1906 [31:18:04<7:15:12, 72.74s/it] 81%|████████  | 1548/1906 [31:19:17<7:13:59, 72.73s/it] 81%|████████▏ | 1549/1906 [31:20:30<7:12:47, 72.74s/it] 81%|████████▏ | 1550/1906 [31:21:43<7:11:33, 72.73s/it]                                                        {'loss': 1.6699, 'learning_rate': 1.6727339974819456e-05, 'epoch': 0.81}
 81%|████████▏ | 1550/1906 [31:21:43<7:11:33, 72.73s/it] 81%|████████▏ | 1551/1906 [31:22:55<7:10:27, 72.75s/it] 81%|████████▏ | 1552/1906 [31:24:08<7:09:12, 72.75s/it] 81%|████████▏ | 1553/1906 [31:25:21<7:07:57, 72.74s/it] 82%|████████▏ | 1554/1906 [31:26:34<7:06:47, 72.75s/it] 82%|████████▏ | 1555/1906 [31:27:46<7:05:36, 72.75s/it] 82%|████████▏ | 1556/1906 [31:28:59<7:04:18, 72.74s/it] 82%|████████▏ | 1557/1906 [31:30:12<7:03:03, 72.73s/it] 82%|████████▏ | 1558/1906 [31:31:25<7:01:53, 72.74s/it] 82%|████████▏ | 1559/1906 [31:32:37<7:00:43, 72.75s/it] 82%|████████▏ | 1560/1906 [31:33:50<6:59:31, 72.75s/it]                                                        {'loss': 1.6716, 'learning_rate': 1.5826074220034226e-05, 'epoch': 0.82}
 82%|████████▏ | 1560/1906 [31:33:50<6:59:31, 72.75s/it] 82%|████████▏ | 1561/1906 [31:35:03<6:58:22, 72.76s/it] 82%|████████▏ | 1562/1906 [31:36:16<6:57:04, 72.75s/it] 82%|████████▏ | 1563/1906 [31:37:28<6:55:50, 72.74s/it] 82%|████████▏ | 1564/1906 [31:38:41<6:54:35, 72.74s/it] 82%|████████▏ | 1565/1906 [31:39:54<6:53:24, 72.74s/it] 82%|████████▏ | 1566/1906 [31:41:07<6:52:11, 72.74s/it] 82%|████████▏ | 1567/1906 [31:42:19<6:50:56, 72.73s/it] 82%|████████▏ | 1568/1906 [31:43:32<6:49:46, 72.74s/it] 82%|████████▏ | 1569/1906 [31:44:45<6:48:38, 72.76s/it] 82%|████████▏ | 1570/1906 [31:45:58<6:47:24, 72.75s/it]                                                        {'loss': 1.6698, 'learning_rate': 1.4947676124019839e-05, 'epoch': 0.82}
 82%|████████▏ | 1570/1906 [31:45:58<6:47:24, 72.75s/it] 82%|████████▏ | 1571/1906 [31:47:10<6:46:15, 72.76s/it] 82%|████████▏ | 1572/1906 [31:48:23<6:45:02, 72.76s/it] 83%|████████▎ | 1573/1906 [31:49:36<6:43:51, 72.77s/it] 83%|████████▎ | 1574/1906 [31:50:49<6:42:39, 72.77s/it] 83%|████████▎ | 1575/1906 [31:52:01<6:41:24, 72.76s/it] 83%|████████▎ | 1576/1906 [31:53:14<6:40:11, 72.76s/it] 83%|████████▎ | 1577/1906 [31:54:27<6:39:00, 72.77s/it] 83%|████████▎ | 1578/1906 [31:55:40<6:37:55, 72.79s/it] 83%|████████▎ | 1579/1906 [31:56:53<6:36:41, 72.79s/it] 83%|████████▎ | 1580/1906 [31:58:05<6:35:26, 72.78s/it]                                                        {'loss': 1.6703, 'learning_rate': 1.4092384322526442e-05, 'epoch': 0.83}
 83%|████████▎ | 1580/1906 [31:58:05<6:35:26, 72.78s/it] 83%|████████▎ | 1581/1906 [31:59:18<6:34:21, 72.81s/it] 83%|████████▎ | 1582/1906 [32:00:31<6:33:12, 72.82s/it] 83%|████████▎ | 1583/1906 [32:01:44<6:32:02, 72.83s/it] 83%|████████▎ | 1584/1906 [32:02:57<6:30:51, 72.83s/it] 83%|████████▎ | 1585/1906 [32:04:10<6:29:41, 72.84s/it] 83%|████████▎ | 1586/1906 [32:05:22<6:28:30, 72.84s/it] 83%|████████▎ | 1587/1906 [32:06:35<6:27:17, 72.84s/it] 83%|████████▎ | 1588/1906 [32:07:48<6:26:04, 72.84s/it] 83%|████████▎ | 1589/1906 [32:09:01<6:24:51, 72.84s/it] 83%|████████▎ | 1590/1906 [32:10:14<6:23:35, 72.83s/it]                                                        {'loss': 1.671, 'learning_rate': 1.3260431173982001e-05, 'epoch': 0.83}
 83%|████████▎ | 1590/1906 [32:10:14<6:23:35, 72.83s/it] 83%|████████▎ | 1591/1906 [32:11:27<6:22:24, 72.84s/it] 84%|████████▎ | 1592/1906 [32:12:39<6:21:08, 72.83s/it] 84%|████████▎ | 1593/1906 [32:13:52<6:19:55, 72.83s/it] 84%|████████▎ | 1594/1906 [32:15:05<6:18:42, 72.83s/it] 84%|████████▎ | 1595/1906 [32:16:18<6:17:28, 72.83s/it] 84%|████████▎ | 1596/1906 [32:17:31<6:16:13, 72.82s/it] 84%|████████▍ | 1597/1906 [32:18:44<6:15:00, 72.82s/it] 84%|████████▍ | 1598/1906 [32:19:56<6:13:46, 72.81s/it] 84%|████████▍ | 1599/1906 [32:21:09<6:12:31, 72.81s/it] 84%|████████▍ | 1600/1906 [32:22:22<6:11:17, 72.80s/it]                                                        {'loss': 1.6739, 'learning_rate': 1.2452042696366984e-05, 'epoch': 0.84}
 84%|████████▍ | 1600/1906 [32:22:22<6:11:17, 72.80s/it] 84%|████████▍ | 1601/1906 [32:23:35<6:10:06, 72.81s/it] 84%|████████▍ | 1602/1906 [32:24:48<6:08:50, 72.80s/it] 84%|████████▍ | 1603/1906 [32:26:00<6:07:31, 72.78s/it] 84%|████████▍ | 1604/1906 [32:27:13<6:06:15, 72.77s/it] 84%|████████▍ | 1605/1906 [32:28:26<6:05:04, 72.77s/it] 84%|████████▍ | 1606/1906 [32:29:39<6:03:52, 72.78s/it] 84%|████████▍ | 1607/1906 [32:30:51<6:02:34, 72.76s/it] 84%|████████▍ | 1608/1906 [32:32:04<6:01:17, 72.74s/it] 84%|████████▍ | 1609/1906 [32:33:17<6:00:03, 72.74s/it] 84%|████████▍ | 1610/1906 [32:34:29<5:58:48, 72.73s/it]                                                        {'loss': 1.6527, 'learning_rate': 1.1667438505811801e-05, 'epoch': 0.84}
 84%|████████▍ | 1610/1906 [32:34:30<5:58:48, 72.73s/it] 85%|████████▍ | 1611/1906 [32:35:42<5:57:44, 72.76s/it] 85%|████████▍ | 1612/1906 [32:36:55<5:56:29, 72.75s/it] 85%|████████▍ | 1613/1906 [32:38:08<5:55:18, 72.76s/it] 85%|████████▍ | 1614/1906 [32:39:21<5:54:06, 72.76s/it] 85%|████████▍ | 1615/1906 [32:40:33<5:52:52, 72.76s/it] 85%|████████▍ | 1616/1906 [32:41:46<5:51:39, 72.76s/it] 85%|████████▍ | 1617/1906 [32:42:59<5:50:25, 72.75s/it] 85%|████████▍ | 1618/1906 [32:44:12<5:49:14, 72.76s/it] 85%|████████▍ | 1619/1906 [32:45:24<5:48:03, 72.76s/it] 85%|████████▍ | 1620/1906 [32:46:37<5:46:49, 72.76s/it]                                                        {'loss': 1.6657, 'learning_rate': 1.0906831756933267e-05, 'epoch': 0.85}
 85%|████████▍ | 1620/1906 [32:46:37<5:46:49, 72.76s/it] 85%|████████▌ | 1621/1906 [32:47:50<5:45:41, 72.78s/it] 85%|████████▌ | 1622/1906 [32:49:03<5:44:26, 72.77s/it] 85%|████████▌ | 1623/1906 [32:50:15<5:43:14, 72.77s/it] 85%|████████▌ | 1624/1906 [32:51:28<5:42:00, 72.77s/it] 85%|████████▌ | 1625/1906 [32:52:41<5:40:49, 72.77s/it] 85%|████████▌ | 1626/1906 [32:53:54<5:39:34, 72.77s/it] 85%|████████▌ | 1627/1906 [32:55:07<5:38:21, 72.76s/it] 85%|████████▌ | 1628/1906 [32:56:19<5:37:07, 72.76s/it] 85%|████████▌ | 1629/1906 [32:57:32<5:35:56, 72.77s/it] 86%|████████▌ | 1630/1906 [32:58:45<5:34:41, 72.76s/it]                                                        {'loss': 1.6621, 'learning_rate': 1.0170429084926746e-05, 'epoch': 0.86}
 86%|████████▌ | 1630/1906 [32:58:45<5:34:41, 72.76s/it] 86%|████████▌ | 1631/1906 [32:59:58<5:33:34, 72.78s/it] 86%|████████▌ | 1632/1906 [33:01:10<5:32:22, 72.78s/it] 86%|████████▌ | 1633/1906 [33:02:23<5:31:10, 72.78s/it] 86%|████████▌ | 1634/1906 [33:03:36<5:29:57, 72.78s/it] 86%|████████▌ | 1635/1906 [33:04:49<5:28:46, 72.79s/it] 86%|████████▌ | 1636/1906 [33:06:02<5:27:32, 72.79s/it] 86%|████████▌ | 1637/1906 [33:07:14<5:26:19, 72.79s/it] 86%|████████▌ | 1638/1906 [33:08:27<5:25:05, 72.78s/it] 86%|████████▌ | 1639/1906 [33:09:40<5:23:53, 72.79s/it] 86%|████████▌ | 1640/1906 [33:10:53<5:22:37, 72.77s/it]                                                        {'loss': 1.6624, 'learning_rate': 9.458430549429032e-06, 'epoch': 0.86}
 86%|████████▌ | 1640/1906 [33:10:53<5:22:37, 72.77s/it] 86%|████████▌ | 1641/1906 [33:12:05<5:21:28, 72.78s/it] 86%|████████▌ | 1642/1906 [33:13:18<5:20:13, 72.78s/it] 86%|████████▌ | 1643/1906 [33:14:31<5:19:03, 72.79s/it] 86%|████████▋ | 1644/1906 [33:15:44<5:17:47, 72.78s/it] 86%|████████▋ | 1645/1906 [33:16:57<5:16:33, 72.77s/it] 86%|████████▋ | 1646/1906 [33:18:09<5:15:22, 72.78s/it] 86%|████████▋ | 1647/1906 [33:19:22<5:14:12, 72.79s/it] 86%|████████▋ | 1648/1906 [33:20:35<5:12:58, 72.79s/it] 87%|████████▋ | 1649/1906 [33:21:48<5:11:45, 72.78s/it] 87%|████████▋ | 1650/1906 [33:23:00<5:10:30, 72.78s/it]                                                        {'loss': 1.6649, 'learning_rate': 8.771029580167967e-06, 'epoch': 0.87}
 87%|████████▋ | 1650/1906 [33:23:01<5:10:30, 72.78s/it] 87%|████████▋ | 1651/1906 [33:24:13<5:09:19, 72.78s/it] 87%|████████▋ | 1652/1906 [33:25:26<5:08:05, 72.78s/it] 87%|████████▋ | 1653/1906 [33:26:39<5:06:55, 72.79s/it] 87%|████████▋ | 1654/1906 [33:27:52<5:05:41, 72.78s/it] 87%|████████▋ | 1655/1906 [33:29:04<5:04:28, 72.78s/it] 87%|████████▋ | 1656/1906 [33:30:17<5:03:14, 72.78s/it] 87%|████████▋ | 1657/1906 [33:31:30<5:01:58, 72.76s/it] 87%|████████▋ | 1658/1906 [33:32:43<5:00:42, 72.75s/it] 87%|████████▋ | 1659/1906 [33:33:55<4:59:28, 72.75s/it] 87%|████████▋ | 1660/1906 [33:35:08<4:58:12, 72.73s/it]                                                        {'loss': 1.663, 'learning_rate': 8.108412924413056e-06, 'epoch': 0.87}
 87%|████████▋ | 1660/1906 [33:35:08<4:58:12, 72.73s/it] 87%|████████▋ | 1661/1906 [33:36:21<4:57:02, 72.74s/it] 87%|████████▋ | 1662/1906 [33:37:34<4:55:45, 72.73s/it] 87%|████████▋ | 1663/1906 [33:38:46<4:54:30, 72.72s/it] 87%|████████▋ | 1664/1906 [33:39:59<4:53:15, 72.71s/it] 87%|████████▋ | 1665/1906 [33:41:12<4:52:00, 72.70s/it] 87%|████████▋ | 1666/1906 [33:42:24<4:50:49, 72.71s/it] 87%|████████▋ | 1667/1906 [33:43:37<4:49:35, 72.70s/it] 88%|████████▊ | 1668/1906 [33:44:50<4:48:23, 72.70s/it] 88%|████████▊ | 1669/1906 [33:46:02<4:47:13, 72.72s/it] 88%|████████▊ | 1670/1906 [33:47:15<4:46:01, 72.72s/it]                                                        {'loss': 1.6601, 'learning_rate': 7.4707605962415775e-06, 'epoch': 0.88}
 88%|████████▊ | 1670/1906 [33:47:15<4:46:01, 72.72s/it] 88%|████████▊ | 1671/1906 [33:48:28<4:44:52, 72.73s/it] 88%|████████▊ | 1672/1906 [33:49:41<4:43:36, 72.72s/it] 88%|████████▊ | 1673/1906 [33:50:53<4:42:25, 72.73s/it] 88%|████████▊ | 1674/1906 [33:52:06<4:41:14, 72.74s/it] 88%|████████▊ | 1675/1906 [33:53:19<4:40:02, 72.74s/it] 88%|████████▊ | 1676/1906 [33:54:32<4:38:47, 72.73s/it] 88%|████████▊ | 1677/1906 [33:55:44<4:37:38, 72.74s/it] 88%|████████▊ | 1678/1906 [33:56:57<4:36:27, 72.75s/it] 88%|████████▊ | 1679/1906 [33:58:10<4:35:14, 72.75s/it] 88%|████████▊ | 1680/1906 [33:59:23<4:34:02, 72.75s/it]                                                        {'loss': 1.6555, 'learning_rate': 6.858245827633869e-06, 'epoch': 0.88}
 88%|████████▊ | 1680/1906 [33:59:23<4:34:02, 72.75s/it] 88%|████████▊ | 1681/1906 [34:00:35<4:32:52, 72.77s/it] 88%|████████▊ | 1682/1906 [34:01:48<4:31:38, 72.76s/it] 88%|████████▊ | 1683/1906 [34:03:01<4:30:28, 72.77s/it] 88%|████████▊ | 1684/1906 [34:04:14<4:29:14, 72.77s/it] 88%|████████▊ | 1685/1906 [34:05:26<4:27:59, 72.76s/it] 88%|████████▊ | 1686/1906 [34:06:39<4:26:48, 72.77s/it] 89%|████████▊ | 1687/1906 [34:07:52<4:25:34, 72.76s/it] 89%|████████▊ | 1688/1906 [34:09:05<4:24:20, 72.75s/it] 89%|████████▊ | 1689/1906 [34:10:18<4:23:07, 72.75s/it] 89%|████████▊ | 1690/1906 [34:11:30<4:21:52, 72.74s/it]                                                        {'loss': 1.6683, 'learning_rate': 6.271035021411098e-06, 'epoch': 0.89}
 89%|████████▊ | 1690/1906 [34:11:30<4:21:52, 72.74s/it] 89%|████████▊ | 1691/1906 [34:12:43<4:20:41, 72.75s/it] 89%|████████▉ | 1692/1906 [34:13:56<4:19:29, 72.75s/it] 89%|████████▉ | 1693/1906 [34:15:09<4:18:16, 72.75s/it] 89%|████████▉ | 1694/1906 [34:16:21<4:17:05, 72.76s/it] 89%|████████▉ | 1695/1906 [34:17:34<4:15:54, 72.77s/it] 89%|████████▉ | 1696/1906 [34:18:47<4:14:43, 72.78s/it] 89%|████████▉ | 1697/1906 [34:20:00<4:13:31, 72.78s/it] 89%|████████▉ | 1698/1906 [34:21:13<4:12:21, 72.79s/it] 89%|████████▉ | 1699/1906 [34:22:25<4:11:09, 72.80s/it] 89%|████████▉ | 1700/1906 [34:23:38<4:09:56, 72.80s/it]                                                        {'loss': 1.6723, 'learning_rate': 5.709287706028454e-06, 'epoch': 0.89}
 89%|████████▉ | 1700/1906 [34:23:38<4:09:56, 72.80s/it] 89%|████████▉ | 1701/1906 [34:24:51<4:08:45, 72.81s/it] 89%|████████▉ | 1702/1906 [34:26:04<4:07:34, 72.82s/it] 89%|████████▉ | 1703/1906 [34:27:17<4:06:20, 72.81s/it] 89%|████████▉ | 1704/1906 [34:28:29<4:05:06, 72.80s/it] 89%|████████▉ | 1705/1906 [34:29:42<4:03:53, 72.80s/it] 90%|████████▉ | 1706/1906 [34:30:55<4:02:40, 72.80s/it] 90%|████████▉ | 1707/1906 [34:32:08<4:01:29, 72.81s/it] 90%|████████▉ | 1708/1906 [34:33:21<4:00:16, 72.81s/it] 90%|████████▉ | 1709/1906 [34:34:33<3:59:03, 72.81s/it] 90%|████████▉ | 1710/1906 [34:35:46<3:57:52, 72.82s/it]                                                        {'loss': 1.676, 'learning_rate': 5.173156492235665e-06, 'epoch': 0.9}
 90%|████████▉ | 1710/1906 [34:35:46<3:57:52, 72.82s/it] 90%|████████▉ | 1711/1906 [34:36:59<3:56:43, 72.84s/it] 90%|████████▉ | 1712/1906 [34:38:12<3:55:31, 72.84s/it] 90%|████████▉ | 1713/1906 [34:39:25<3:54:18, 72.84s/it] 90%|████████▉ | 1714/1906 [34:40:38<3:53:03, 72.83s/it] 90%|████████▉ | 1715/1906 [34:41:50<3:51:49, 72.82s/it] 90%|█████████ | 1716/1906 [34:43:03<3:50:35, 72.82s/it] 90%|█████████ | 1717/1906 [34:44:16<3:49:21, 72.81s/it] 90%|█████████ | 1718/1906 [34:45:29<3:48:09, 72.82s/it] 90%|█████████ | 1719/1906 [34:46:42<3:46:57, 72.82s/it] 90%|█████████ | 1720/1906 [34:47:54<3:45:43, 72.81s/it]                                                        {'loss': 1.657, 'learning_rate': 4.662787031617122e-06, 'epoch': 0.9}
 90%|█████████ | 1720/1906 [34:47:55<3:45:43, 72.81s/it] 90%|█████████ | 1721/1906 [34:49:07<3:44:33, 72.83s/it] 90%|█████████ | 1722/1906 [34:50:20<3:43:20, 72.83s/it] 90%|█████████ | 1723/1906 [34:51:33<3:42:06, 72.82s/it] 90%|█████████ | 1724/1906 [34:52:46<3:40:52, 72.82s/it] 91%|█████████ | 1725/1906 [34:53:59<3:39:40, 72.82s/it] 91%|█████████ | 1726/1906 [34:55:11<3:38:25, 72.81s/it] 91%|█████████ | 1727/1906 [34:56:24<3:37:11, 72.80s/it] 91%|█████████ | 1728/1906 [34:57:37<3:35:57, 72.79s/it] 91%|█████████ | 1729/1906 [34:58:50<3:34:44, 72.79s/it] 91%|█████████ | 1730/1906 [35:00:03<3:33:31, 72.80s/it]                                                        {'loss': 1.668, 'learning_rate': 4.1783179770224275e-06, 'epoch': 0.91}
 91%|█████████ | 1730/1906 [35:00:03<3:33:31, 72.80s/it] 91%|█████████ | 1731/1906 [35:01:15<3:32:19, 72.80s/it] 91%|█████████ | 1732/1906 [35:02:28<3:31:06, 72.79s/it] 91%|█████████ | 1733/1906 [35:03:41<3:29:51, 72.78s/it] 91%|█████████ | 1734/1906 [35:04:54<3:28:38, 72.78s/it] 91%|█████████ | 1735/1906 [35:06:06<3:27:25, 72.78s/it] 91%|█████████ | 1736/1906 [35:07:19<3:26:12, 72.78s/it] 91%|█████████ | 1737/1906 [35:08:32<3:25:00, 72.78s/it] 91%|█████████ | 1738/1906 [35:09:45<3:23:47, 72.78s/it] 91%|█████████ | 1739/1906 [35:10:58<3:22:33, 72.78s/it] 91%|█████████▏| 1740/1906 [35:12:10<3:21:21, 72.78s/it]                                                        {'loss': 1.666, 'learning_rate': 3.7198809448984128e-06, 'epoch': 0.91}
 91%|█████████▏| 1740/1906 [35:12:10<3:21:21, 72.78s/it] 91%|█████████▏| 1741/1906 [35:13:23<3:20:10, 72.79s/it] 91%|█████████▏| 1742/1906 [35:14:36<3:18:57, 72.79s/it] 91%|█████████▏| 1743/1906 [35:15:49<3:17:45, 72.80s/it] 92%|█████████▏| 1744/1906 [35:17:02<3:16:33, 72.80s/it] 92%|█████████▏| 1745/1906 [35:18:14<3:15:19, 72.79s/it] 92%|█████████▏| 1746/1906 [35:19:27<3:14:06, 72.79s/it] 92%|█████████▏| 1747/1906 [35:20:40<3:12:54, 72.79s/it] 92%|█████████▏| 1748/1906 [35:21:53<3:11:41, 72.79s/it] 92%|█████████▏| 1749/1906 [35:23:06<3:10:28, 72.79s/it] 92%|█████████▏| 1750/1906 [35:24:18<3:09:16, 72.80s/it]                                                        {'loss': 1.6583, 'learning_rate': 3.287600479532649e-06, 'epoch': 0.92}
 92%|█████████▏| 1750/1906 [35:24:18<3:09:16, 72.80s/it] 92%|█████████▏| 1751/1906 [35:25:31<3:08:07, 72.82s/it] 92%|█████████▏| 1752/1906 [35:26:44<3:06:55, 72.83s/it] 92%|█████████▏| 1753/1906 [35:27:57<3:05:41, 72.82s/it] 92%|█████████▏| 1754/1906 [35:29:10<3:04:27, 72.81s/it] 92%|█████████▏| 1755/1906 [35:30:22<3:03:14, 72.81s/it] 92%|█████████▏| 1756/1906 [35:31:35<3:02:02, 72.82s/it] 92%|█████████▏| 1757/1906 [35:32:48<3:00:50, 72.82s/it] 92%|█████████▏| 1758/1906 [35:34:01<2:59:38, 72.83s/it] 92%|█████████▏| 1759/1906 [35:35:14<2:58:23, 72.81s/it] 92%|█████████▏| 1760/1906 [35:36:27<2:57:10, 72.81s/it]                                                        {'loss': 1.669, 'learning_rate': 2.8815940192183033e-06, 'epoch': 0.92}
 92%|█████████▏| 1760/1906 [35:36:27<2:57:10, 72.81s/it] 92%|█████████▏| 1761/1906 [35:37:39<2:55:59, 72.82s/it] 92%|█████████▏| 1762/1906 [35:38:52<2:54:44, 72.81s/it] 92%|█████████▏| 1763/1906 [35:40:05<2:53:31, 72.81s/it] 93%|█████████▎| 1764/1906 [35:41:18<2:52:16, 72.79s/it] 93%|█████████▎| 1765/1906 [35:42:31<2:51:02, 72.78s/it] 93%|█████████▎| 1766/1906 [35:43:43<2:49:49, 72.78s/it] 93%|█████████▎| 1767/1906 [35:44:56<2:48:37, 72.79s/it] 93%|█████████▎| 1768/1906 [35:46:09<2:47:24, 72.79s/it] 93%|█████████▎| 1769/1906 [35:47:22<2:46:10, 72.78s/it] 93%|█████████▎| 1770/1906 [35:48:34<2:44:56, 72.77s/it]                                                        {'loss': 1.661, 'learning_rate': 2.501971864349606e-06, 'epoch': 0.93}
 93%|█████████▎| 1770/1906 [35:48:34<2:44:56, 72.77s/it] 93%|█████████▎| 1771/1906 [35:49:47<2:43:45, 72.78s/it] 93%|█████████▎| 1772/1906 [35:51:00<2:42:33, 72.78s/it] 93%|█████████▎| 1773/1906 [35:52:13<2:41:20, 72.78s/it] 93%|█████████▎| 1774/1906 [35:53:26<2:40:07, 72.79s/it] 93%|█████████▎| 1775/1906 [35:54:38<2:38:54, 72.78s/it] 93%|█████████▎| 1776/1906 [35:55:51<2:37:41, 72.78s/it] 93%|█████████▎| 1777/1906 [35:57:04<2:36:29, 72.79s/it] 93%|█████████▎| 1778/1906 [35:58:17<2:35:17, 72.79s/it] 93%|█████████▎| 1779/1906 [35:59:29<2:34:03, 72.78s/it] 93%|█████████▎| 1780/1906 [36:00:42<2:32:48, 72.77s/it]                                                        {'loss': 1.664, 'learning_rate': 2.1488371474562063e-06, 'epoch': 0.93}
 93%|█████████▎| 1780/1906 [36:00:42<2:32:48, 72.77s/it] 93%|█████████▎| 1781/1906 [36:01:55<2:31:36, 72.77s/it] 93%|█████████▎| 1782/1906 [36:03:08<2:30:24, 72.78s/it] 94%|█████████▎| 1783/1906 [36:04:21<2:29:11, 72.78s/it] 94%|█████████▎| 1784/1906 [36:05:33<2:27:58, 72.78s/it] 94%|█████████▎| 1785/1906 [36:06:46<2:26:45, 72.78s/it] 94%|█████████▎| 1786/1906 [36:07:59<2:25:32, 72.77s/it] 94%|█████████▍| 1787/1906 [36:09:12<2:24:17, 72.75s/it] 94%|█████████▍| 1788/1906 [36:10:24<2:23:05, 72.76s/it] 94%|█████████▍| 1789/1906 [36:11:37<2:21:52, 72.76s/it] 94%|█████████▍| 1790/1906 [36:12:50<2:20:40, 72.77s/it]                                                        {'loss': 1.674, 'learning_rate': 1.822285805185142e-06, 'epoch': 0.94}
 94%|█████████▍| 1790/1906 [36:12:50<2:20:40, 72.77s/it] 94%|█████████▍| 1791/1906 [36:14:03<2:19:28, 72.77s/it] 94%|█████████▍| 1792/1906 [36:15:15<2:18:15, 72.77s/it] 94%|█████████▍| 1793/1906 [36:16:28<2:17:02, 72.76s/it] 94%|█████████▍| 1794/1906 [36:17:41<2:15:48, 72.76s/it] 94%|█████████▍| 1795/1906 [36:18:54<2:14:36, 72.76s/it] 94%|█████████▍| 1796/1906 [36:20:06<2:13:23, 72.76s/it] 94%|█████████▍| 1797/1906 [36:21:19<2:12:09, 72.75s/it] 94%|█████████▍| 1798/1906 [36:22:32<2:10:56, 72.75s/it] 94%|█████████▍| 1799/1906 [36:23:45<2:09:43, 72.75s/it] 94%|█████████▍| 1800/1906 [36:24:57<2:08:30, 72.74s/it]                                                        {'loss': 1.6683, 'learning_rate': 1.5224065522374986e-06, 'epoch': 0.94}
 94%|█████████▍| 1800/1906 [36:24:57<2:08:30, 72.74s/it] 94%|█████████▍| 1801/1906 [36:26:10<2:07:19, 72.75s/it] 95%|█████████▍| 1802/1906 [36:27:23<2:06:06, 72.76s/it] 95%|█████████▍| 1803/1906 [36:28:36<2:04:53, 72.75s/it] 95%|█████████▍| 1804/1906 [36:29:48<2:03:39, 72.74s/it] 95%|█████████▍| 1805/1906 [36:31:01<2:02:26, 72.73s/it] 95%|█████████▍| 1806/1906 [36:32:14<2:01:12, 72.73s/it] 95%|█████████▍| 1807/1906 [36:33:27<1:59:58, 72.72s/it] 95%|█████████▍| 1808/1906 [36:34:39<1:58:45, 72.71s/it] 95%|█████████▍| 1809/1906 [36:35:52<1:57:32, 72.70s/it] 95%|█████████▍| 1810/1906 [36:37:05<1:56:19, 72.70s/it]                                                        {'loss': 1.6637, 'learning_rate': 1.249280857267221e-06, 'epoch': 0.95}
 95%|█████████▍| 1810/1906 [36:37:05<1:56:19, 72.70s/it] 95%|█████████▌| 1811/1906 [36:38:17<1:55:07, 72.71s/it] 95%|█████████▌| 1812/1906 [36:39:30<1:53:54, 72.71s/it] 95%|█████████▌| 1813/1906 [36:40:43<1:52:41, 72.71s/it] 95%|█████████▌| 1814/1906 [36:41:55<1:51:29, 72.71s/it] 95%|█████████▌| 1815/1906 [36:43:08<1:50:16, 72.71s/it] 95%|█████████▌| 1816/1906 [36:44:21<1:49:03, 72.71s/it] 95%|█████████▌| 1817/1906 [36:45:34<1:47:50, 72.71s/it] 95%|█████████▌| 1818/1906 [36:46:46<1:46:38, 72.71s/it] 95%|█████████▌| 1819/1906 [36:47:59<1:45:25, 72.71s/it] 95%|█████████▌| 1820/1906 [36:49:12<1:44:12, 72.70s/it]                                                        {'loss': 1.6657, 'learning_rate': 1.002982920748341e-06, 'epoch': 0.95}
 95%|█████████▌| 1820/1906 [36:49:12<1:44:12, 72.70s/it] 96%|█████████▌| 1821/1906 [36:50:24<1:43:01, 72.72s/it] 96%|█████████▌| 1822/1906 [36:51:37<1:41:48, 72.73s/it] 96%|█████████▌| 1823/1906 [36:52:50<1:40:35, 72.72s/it] 96%|█████████▌| 1824/1906 [36:54:03<1:39:22, 72.72s/it] 96%|█████████▌| 1825/1906 [36:55:15<1:38:10, 72.72s/it] 96%|█████████▌| 1826/1906 [36:56:28<1:36:57, 72.72s/it] 96%|█████████▌| 1827/1906 [36:57:41<1:35:45, 72.73s/it] 96%|█████████▌| 1828/1906 [36:58:54<1:34:32, 72.73s/it] 96%|█████████▌| 1829/1906 [37:00:06<1:33:20, 72.74s/it] 96%|█████████▌| 1830/1906 [37:01:19<1:32:07, 72.74s/it]                                                        {'loss': 1.6673, 'learning_rate': 7.835796548168351e-07, 'epoch': 0.96}
 96%|█████████▌| 1830/1906 [37:01:19<1:32:07, 72.74s/it] 96%|█████████▌| 1831/1906 [37:02:32<1:30:56, 72.75s/it] 96%|█████████▌| 1832/1906 [37:03:45<1:29:43, 72.75s/it] 96%|█████████▌| 1833/1906 [37:04:57<1:28:30, 72.74s/it] 96%|█████████▌| 1834/1906 [37:06:10<1:27:16, 72.73s/it] 96%|█████████▋| 1835/1906 [37:07:23<1:26:04, 72.74s/it] 96%|█████████▋| 1836/1906 [37:08:35<1:24:52, 72.75s/it] 96%|█████████▋| 1837/1906 [37:09:48<1:23:39, 72.74s/it] 96%|█████████▋| 1838/1906 [37:11:01<1:22:26, 72.74s/it] 96%|█████████▋| 1839/1906 [37:12:14<1:21:13, 72.74s/it] 97%|█████████▋| 1840/1906 [37:13:26<1:20:01, 72.75s/it]                                                        {'loss': 1.6654, 'learning_rate': 5.911306650925208e-07, 'epoch': 0.97}
 97%|█████████▋| 1840/1906 [37:13:27<1:20:01, 72.75s/it] 97%|█████████▋| 1841/1906 [37:14:39<1:18:49, 72.76s/it] 97%|█████████▋| 1842/1906 [37:15:52<1:17:36, 72.75s/it] 97%|█████████▋| 1843/1906 [37:17:05<1:16:23, 72.75s/it] 97%|█████████▋| 1844/1906 [37:18:17<1:15:10, 72.75s/it] 97%|█████████▋| 1845/1906 [37:19:30<1:13:58, 72.75s/it] 97%|█████████▋| 1846/1906 [37:20:43<1:12:44, 72.75s/it] 97%|█████████▋| 1847/1906 [37:21:56<1:11:32, 72.75s/it] 97%|█████████▋| 1848/1906 [37:23:08<1:10:19, 72.75s/it] 97%|█████████▋| 1849/1906 [37:24:21<1:09:07, 72.76s/it] 97%|█████████▋| 1850/1906 [37:25:34<1:07:54, 72.76s/it]                                                        {'loss': 1.6627, 'learning_rate': 4.2568823448591034e-07, 'epoch': 0.97}
 97%|█████████▋| 1850/1906 [37:25:34<1:07:54, 72.76s/it] 97%|█████████▋| 1851/1906 [37:26:47<1:06:42, 72.78s/it] 97%|█████████▋| 1852/1906 [37:28:00<1:05:30, 72.78s/it] 97%|█████████▋| 1853/1906 [37:29:12<1:04:18, 72.79s/it] 97%|█████████▋| 1854/1906 [37:30:25<1:03:05, 72.80s/it] 97%|█████████▋| 1855/1906 [37:31:38<1:01:52, 72.80s/it] 97%|█████████▋| 1856/1906 [37:32:51<1:00:40, 72.81s/it] 97%|█████████▋| 1857/1906 [37:34:04<59:27, 72.81s/it]   97%|█████████▋| 1858/1906 [37:35:17<58:15, 72.83s/it] 98%|█████████▊| 1859/1906 [37:36:29<57:02, 72.83s/it] 98%|█████████▊| 1860/1906 [37:37:42<55:50, 72.83s/it]                                                      {'loss': 1.6582, 'learning_rate': 2.8729730899438313e-07, 'epoch': 0.98}
 98%|█████████▊| 1860/1906 [37:37:42<55:50, 72.83s/it] 98%|█████████▊| 1861/1906 [37:38:55<54:38, 72.86s/it] 98%|█████████▊| 1862/1906 [37:40:08<53:25, 72.85s/it] 98%|█████████▊| 1863/1906 [37:41:21<52:12, 72.84s/it] 98%|█████████▊| 1864/1906 [37:42:34<50:59, 72.84s/it] 98%|█████████▊| 1865/1906 [37:43:46<49:46, 72.83s/it] 98%|█████████▊| 1866/1906 [37:44:59<48:32, 72.81s/it] 98%|█████████▊| 1867/1906 [37:46:12<47:19, 72.81s/it] 98%|█████████▊| 1868/1906 [37:47:25<46:06, 72.80s/it] 98%|█████████▊| 1869/1906 [37:48:38<44:53, 72.80s/it] 98%|█████████▊| 1870/1906 [37:49:50<43:40, 72.79s/it]                                                      {'loss': 1.6723, 'learning_rate': 1.7599548549170897e-07, 'epoch': 0.98}
 98%|█████████▊| 1870/1906 [37:49:50<43:40, 72.79s/it] 98%|█████████▊| 1871/1906 [37:51:03<42:27, 72.80s/it] 98%|█████████▊| 1872/1906 [37:52:16<41:15, 72.80s/it] 98%|█████████▊| 1873/1906 [37:53:29<40:01, 72.79s/it] 98%|█████████▊| 1874/1906 [37:54:42<38:49, 72.78s/it] 98%|█████████▊| 1875/1906 [37:55:54<37:35, 72.77s/it] 98%|█████████▊| 1876/1906 [37:57:07<36:22, 72.77s/it] 98%|█████████▊| 1877/1906 [37:58:20<35:09, 72.75s/it] 99%|█████████▊| 1878/1906 [37:59:32<33:56, 72.75s/it] 99%|█████████▊| 1879/1906 [38:00:45<32:44, 72.74s/it] 99%|█████████▊| 1880/1906 [38:01:58<31:31, 72.75s/it]                                                      {'loss': 1.664, 'learning_rate': 9.181300151399618e-08, 'epoch': 0.99}
 99%|█████████▊| 1880/1906 [38:01:58<31:31, 72.75s/it] 99%|█████████▊| 1881/1906 [38:03:11<30:18, 72.76s/it] 99%|█████████▊| 1882/1906 [38:04:24<29:06, 72.76s/it] 99%|█████████▉| 1883/1906 [38:05:36<27:53, 72.76s/it] 99%|█████████▉| 1884/1906 [38:06:49<26:41, 72.78s/it] 99%|█████████▉| 1885/1906 [38:08:02<25:28, 72.77s/it] 99%|█████████▉| 1886/1906 [38:09:15<24:15, 72.77s/it] 99%|█████████▉| 1887/1906 [38:10:27<23:02, 72.77s/it] 99%|█████████▉| 1888/1906 [38:11:40<21:49, 72.77s/it] 99%|█████████▉| 1889/1906 [38:12:53<20:36, 72.75s/it] 99%|█████████▉| 1890/1906 [38:14:06<19:23, 72.74s/it]                                                      {'loss': 1.6568, 'learning_rate': 3.477272704504042e-08, 'epoch': 0.99}
 99%|█████████▉| 1890/1906 [38:14:06<19:23, 72.74s/it] 99%|█████████▉| 1891/1906 [38:15:18<18:11, 72.76s/it] 99%|█████████▉| 1892/1906 [38:16:31<16:58, 72.76s/it] 99%|█████████▉| 1893/1906 [38:17:44<15:45, 72.76s/it] 99%|█████████▉| 1894/1906 [38:18:57<14:33, 72.77s/it] 99%|█████████▉| 1895/1906 [38:20:10<13:20, 72.78s/it] 99%|█████████▉| 1896/1906 [38:21:22<12:07, 72.78s/it]100%|█████████▉| 1897/1906 [38:22:35<10:55, 72.78s/it]100%|█████████▉| 1898/1906 [38:23:48<09:42, 72.78s/it]100%|█████████▉| 1899/1906 [38:25:01<08:29, 72.78s/it]100%|█████████▉| 1900/1906 [38:26:13<07:16, 72.78s/it]                                                      {'loss': 1.6599, 'learning_rate': 4.89015830317241e-09, 'epoch': 1.0}
100%|█████████▉| 1900/1906 [38:26:13<07:16, 72.78s/it]100%|█████████▉| 1901/1906 [38:27:26<06:03, 72.79s/it]100%|█████████▉| 1902/1906 [38:28:39<04:51, 72.79s/it]100%|█████████▉| 1903/1906 [38:29:52<03:38, 72.78s/it]100%|█████████▉| 1904/1906 [38:31:05<02:25, 72.78s/it]100%|█████████▉| 1905/1906 [38:32:17<01:12, 72.78s/it]100%|██████████| 1906/1906 [38:33:30<00:00, 72.78s/it][INFO|trainer.py:1988] 2024-03-21 04:21:57,435 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                      {'train_runtime': 138810.6577, 'train_samples_per_second': 3.515, 'train_steps_per_second': 0.014, 'train_loss': 1.7959253034961937, 'epoch': 1.0}
100%|██████████| 1906/1906 [38:33:30<00:00, 72.78s/it]100%|██████████| 1906/1906 [38:33:30<00:00, 72.83s/it]
[INFO|trainer.py:2979] 2024-03-21 04:22:06,850 >> Saving model checkpoint to /home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b
/home/nfs02/wangzj/public_code/hitsz/peft/src/peft/utils/save_and_load.py:151: UserWarning: Could not find a config file in /home/nfs02/model/llama2/hf/Llama-2-7b-hf - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2435] 2024-03-21 04:22:08,391 >> tokenizer config file saved in /home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-03-21 04:22:08,508 >> Special tokens file saved in /home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b/special_tokens_map.json
***** train metrics *****
  epoch                    =                1.0
  train_loss               =             1.7959
  train_runtime            = 1 day, 14:33:30.65
  train_samples_per_second =              3.515
  train_steps_per_second   =              0.014
Figure saved: /home/nfs02/wangzj/checkpoints/calm/llama-lora-is1b/training_loss.png
03/21/2024 04:22:13 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-03-21 04:22:13,025 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-03-21 04:22:14,941] [INFO] [launch.py:347:main] Process 19777 exits successfully.
[2024-03-21 04:22:14,942] [INFO] [launch.py:347:main] Process 19775 exits successfully.
[2024-03-21 04:22:14,942] [INFO] [launch.py:347:main] Process 19776 exits successfully.
[2024-03-21 04:22:15,944] [INFO] [launch.py:347:main] Process 19774 exits successfully.
