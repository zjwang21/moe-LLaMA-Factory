[2024-03-06 17:29:19,869] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-06 17:29:30,121] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-03-06 17:29:30,169] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/wzjsz/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json --stage pt --model_name_or_path /home/nfs02/model/llama2/hf/Llama-2-7b-hf --flash_attn --do_train --dataset skypile_1b,slimpajam_1b --en_max_samples 10000 --preprocessing_num_workers 20 --mix_strategy concat --cutoff_len 2048 --finetuning_type moe --moe_every_k_layers 1 --moe_router_type top1 --moe_num_experts 2 --output_dir /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1 --overwrite_output_dir --per_device_train_batch_size 8 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 1 --save_only_model --save_steps 1000 --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-03-06 17:29:31,916] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-06 17:29:34,588] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-03-06 17:29:34,588] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-03-06 17:29:34,588] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-03-06 17:29:34,588] [INFO] [launch.py:163:main] dist_world_size=4
[2024-03-06 17:29:34,588] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-03-06 17:29:48,882] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-06 17:29:48,882] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-06 17:29:48,882] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-06 17:29:48,883] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-06 17:30:04,811] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-06 17:30:04,811] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-06 17:30:04,811] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-06 17:30:04,813] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-06 17:30:04,818] [INFO] [comm.py:637:init_distributed] cdb=None
03/06/2024 17:30:04 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/06/2024 17:30:04 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/06/2024 17:30:04 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/06/2024 17:30:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1/runs/Mar06_17-30-04_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/06/2024 17:30:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1/runs/Mar06_17-30-04_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/06/2024 17:30:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1/runs/Mar06_17-30-04_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/06/2024 17:30:04 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/06/2024 17:30:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1/runs/Mar06_17-30-04_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-03-06 17:30:04,841 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2027] 2024-03-06 17:30:04,842 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-03-06 17:30:04,842 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-03-06 17:30:04,842 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-03-06 17:30:04,842 >> loading file tokenizer.json
[INFO|configuration_utils.py:727] 2024-03-06 17:30:05,190 >> loading configuration file /home/nfs02/model/llama2/hf/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:792] 2024-03-06 17:30:05,191 >> Model config LlamaConfig {
  "_name_or_path": "/home/nfs02/model/llama2/hf/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

03/06/2024 17:30:05 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
03/06/2024 17:30:05 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
03/06/2024 17:30:05 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
03/06/2024 17:30:05 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
[INFO|modeling_utils.py:3334] 2024-03-06 17:30:05,347 >> loading weights file /home/nfs02/model/llama2/hf/Llama-2-7b-hf/model.safetensors.index.json
[INFO|modeling_utils.py:1459] 2024-03-06 17:30:05,349 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[WARNING|logging.py:329] 2024-03-06 17:30:05,349 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[WARNING|logging.py:329] 2024-03-06 17:30:05,352 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:827] 2024-03-06 17:30:05,355 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [01:26<01:26, 86.91s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [01:28<01:28, 88.29s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [01:28<01:28, 88.29s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [01:28<01:28, 88.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:57<00:00, 53.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:57<00:00, 58.84s/it]
[INFO|modeling_utils.py:4070] 2024-03-06 17:32:06,725 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4078] 2024-03-06 17:32:06,725 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/nfs02/model/llama2/hf/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-03-06 17:32:06,729 >> loading configuration file /home/nfs02/model/llama2/hf/Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:827] 2024-03-06 17:32:06,729 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

03/06/2024 17:32:06 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/06/2024 17:32:06 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE

Loading checkpoint shards: 100%|██████████| 2/2 [01:58<00:00, 53.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:58<00:00, 59.03s/it]
03/06/2024 17:32:07 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/06/2024 17:32:07 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE

Loading checkpoint shards: 100%|██████████| 2/2 [01:58<00:00, 53.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:58<00:00, 59.05s/it]
03/06/2024 17:32:07 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/06/2024 17:32:07 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE

Loading checkpoint shards: 100%|██████████| 2/2 [01:58<00:00, 53.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:58<00:00, 59.06s/it]
03/06/2024 17:32:07 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/06/2024 17:32:07 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
03/06/2024 17:32:09 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
03/06/2024 17:32:09 - INFO - llmtuner.data.template - Add pad token: </s>
03/06/2024 17:32:10 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
03/06/2024 17:32:10 - INFO - llmtuner.data.template - Add pad token: </s>
03/06/2024 17:32:10 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
03/06/2024 17:32:10 - INFO - llmtuner.data.template - Add pad token: </s>
03/06/2024 17:32:10 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
03/06/2024 17:32:10 - INFO - llmtuner.data.template - Add pad token: </s>
03/06/2024 17:32:58 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
Using custom data configuration default-e04fdd9113403d69
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Downloading and preparing dataset json/default to /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...

Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]
Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 6384.02it/s]
Downloading took 0.0 min
Checksum Computation took 0.0 min

Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]
Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 228.55it/s]
Generating train split

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 1758 examples [00:00, 11202.16 examples/s]
Generating train split: 9002 examples [00:00, 35223.10 examples/s]
Generating train split: 17886 examples [00:00, 49778.29 examples/s]
Generating train split: 28828 examples [00:00, 59905.56 examples/s]
Generating train split: 35962 examples [00:00, 62632.89 examples/s]
Generating train split: 43032 examples [00:00, 64034.85 examples/s]
Generating train split: 50128 examples [00:00, 65563.03 examples/s]
Generating train split: 58891 examples [00:01, 66210.15 examples/s]
Generating train split: 65984 examples [00:01, 67196.49 examples/s]
Generating train split: 75061 examples [00:01, 68413.85 examples/s]
Generating train split: 82241 examples [00:01, 68977.74 examples/s]
Generating train split: 91255 examples [00:01, 69858.98 examples/s]
Generating train split: 98379 examples [00:01, 69599.06 examples/s]
Generating train split: 105480 examples [00:01, 68166.77 examples/s]
Generating train split: 112701 examples [00:01, 68549.55 examples/s]
Generating train split: 119968 examples [00:01, 68529.01 examples/s]
Generating train split: 128849 examples [00:02, 69466.26 examples/s]
Generating train split: 135877 examples [00:02, 68956.17 examples/s]
Generating train split: 143138 examples [00:02, 68461.26 examples/s]
Generating train split: 150271 examples [00:02, 67658.81 examples/s]
Generating train split: 157446 examples [00:02, 67928.38 examples/s]
Generating train split: 164509 examples [00:02, 67560.67 examples/s]
Generating train split: 171751 examples [00:02, 67801.38 examples/s]
Generating train split: 180609 examples [00:02, 68411.63 examples/s]
Generating train split: 187785 examples [00:02, 67391.70 examples/s]
Generating train split: 194930 examples [00:02, 67090.53 examples/s]
Generating train split: 202028 examples [00:03, 67581.38 examples/s]
Generating train split: 209177 examples [00:03, 67309.40 examples/s]
Generating train split: 216349 examples [00:03, 67436.82 examples/s]
Generating train split: 223471 examples [00:03, 67755.50 examples/s]
Generating train split: 232398 examples [00:03, 68261.19 examples/s]
Generating train split: 239606 examples [00:03, 67682.43 examples/s]
Generating train split: 246767 examples [00:03, 67633.44 examples/s]
Generating train split: 253992 examples [00:03, 68289.81 examples/s]
Generating train split: 261190 examples [00:03, 68241.86 examples/s]
Generating train split: 270088 examples [00:04, 68894.01 examples/s]
Generating train split: 277206 examples [00:04, 69028.73 examples/s]
Generating train split: 286066 examples [00:04, 69436.50 examples/s]
Generating train split: 293197 examples [00:04, 69361.55 examples/s]
Generating train split: 300214 examples [00:04, 68108.50 examples/s]
Generating train split: 307317 examples [00:04, 67068.35 examples/s]
Generating train split: 314477 examples [00:04, 66671.81 examples/s]
Generating train split: 321695 examples [00:04, 66703.33 examples/s]
Generating train split: 328929 examples [00:04, 67850.45 examples/s]
Generating train split: 336042 examples [00:05, 67651.93 examples/s]
Generating train split: 343193 examples [00:05, 67970.61 examples/s]
Generating train split: 350301 examples [00:05, 67168.09 examples/s]
Generating train split: 359332 examples [00:05, 67850.74 examples/s]
Generating train split: 366514 examples [00:05, 68152.79 examples/s]
Generating train split: 373772 examples [00:05, 68013.53 examples/s]
Generating train split: 380903 examples [00:05, 67654.29 examples/s]
Generating train split: 388040 examples [00:05, 67592.93 examples/s]
Generating train split: 395142 examples [00:05, 67834.97 examples/s]
Generating train split: 402253 examples [00:06, 68035.18 examples/s]
Generating train split: 409284 examples [00:06, 67672.49 examples/s]
Generating train split: 416558 examples [00:06, 68469.40 examples/s]
Generating train split: 423810 examples [00:06, 68271.94 examples/s]
Generating train split: 432834 examples [00:06, 69588.65 examples/s]
Generating train split: 439918 examples [00:06, 68419.57 examples/s]
Generating train split: 447196 examples [00:06, 67330.35 examples/s]
Generating train split: 454279 examples [00:06, 66704.93 examples/s]
Generating train split: 461523 examples [00:06, 66839.04 examples/s]
Generating train split: 468669 examples [00:07, 67229.06 examples/s]
Generating train split: 476095 examples [00:07, 68567.41 examples/s]
Generating train split: 484966 examples [00:07, 69379.41 examples/s]
Generating train split: 492213 examples [00:07, 69576.08 examples/s]
Generating train split: 499418 examples [00:07, 68081.65 examples/s]
Generating train split: 506594 examples [00:07, 68239.96 examples/s]
Generating train split: 513783 examples [00:07, 68418.33 examples/s]
Generating train split: 521002 examples [00:07, 68463.35 examples/s]
Generating train split: 528029 examples [00:07, 67491.15 examples/s]
Generating train split: 537002 examples [00:08, 68872.78 examples/s]
Generating train split: 544198 examples [00:08, 69412.55 examples/s]
Generating train split: 551349 examples [00:08, 69281.75 examples/s]
Generating train split: 560313 examples [00:08, 69850.25 examples/s]
Generating train split: 567510 examples [00:08, 69785.48 examples/s]
Generating train split: 574676 examples [00:08, 68521.52 examples/s]
Generating train split: 581811 examples [00:08, 65845.89 examples/s]
Generating train split: 589013 examples [00:08, 66804.44 examples/s]
Generating train split: 596170 examples [00:08, 67493.40 examples/s]
Generating train split: 605161 examples [00:09, 68290.64 examples/s]
Generating train split: 614180 examples [00:09, 69217.37 examples/s]
Generating train split: 625097 examples [00:09, 71309.36 examples/s]
Generating train split: 632271 examples [00:09, 69949.28 examples/s]
Generating train split: 639479 examples [00:09, 69542.69 examples/s]
Generating train split: 646696 examples [00:09, 69128.34 examples/s]
Generating train split: 655615 examples [00:09, 68841.38 examples/s]
Generating train split: 661595 examples [00:09, 67338.22 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
Dataset({
    features: ['text'],
    num_rows: 661595
})
{'text': '一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁\n所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！\n隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。\n'}
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00000_of_00020.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00001_of_00020.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00002_of_00020.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00003_of_00020.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00004_of_00020.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00005_of_00020.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00006_of_00020.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00007_of_00020.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00008_of_00020.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00009_of_00020.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00010_of_00020.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00011_of_00020.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00012_of_00020.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00013_of_00020.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00014_of_00020.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00015_of_00020.arrow
Process #16 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00016_of_00020.arrow
Process #17 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00017_of_00020.arrow
Process #18 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00018_of_00020.arrow
Process #19 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00019_of_00020.arrow
Spawning 20 processes

Converting format of dataset (num_proc=20):   0%|          | 0/661595 [00:00<?, ? examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00005_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00002_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00000_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00004_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00007_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00001_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00006_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00003_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00013_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00010_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00011_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00012_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00009_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00008_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00014_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00017_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00019_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00016_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00018_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00015_of_00020.arrow

Converting format of dataset (num_proc=20):   0%|          | 1000/661595 [00:00<03:39, 3013.03 examples/s]
Converting format of dataset (num_proc=20):   4%|▍         | 28000/661595 [00:00<00:07, 83286.23 examples/s]
Converting format of dataset (num_proc=20):  12%|█▏        | 82000/661595 [00:00<00:02, 218524.58 examples/s]
Converting format of dataset (num_proc=20):  22%|██▏       | 148000/661595 [00:00<00:01, 349454.62 examples/s]
Converting format of dataset (num_proc=20):  34%|███▍      | 225000/661595 [00:00<00:00, 466846.00 examples/s]
Converting format of dataset (num_proc=20):  45%|████▌     | 298000/661595 [00:00<00:00, 540947.52 examples/s]
Converting format of dataset (num_proc=20):  57%|█████▋    | 375000/661595 [00:00<00:00, 606943.31 examples/s]
Converting format of dataset (num_proc=20):  68%|██████▊   | 450000/661595 [00:01<00:00, 648209.78 examples/s]
Converting format of dataset (num_proc=20):  79%|███████▊  | 520000/661595 [00:01<00:00, 662488.66 examples/s]
Converting format of dataset (num_proc=20):  90%|█████████ | 598160/661595 [00:01<00:00, 671979.41 examples/s]
Converting format of dataset (num_proc=20):  99%|█████████▊| 652318/661595 [00:17<00:00, 671979.41 examples/s]
Converting format of dataset (num_proc=20):  99%|█████████▉| 653398/661595 [00:23<00:00, 9557.25 examples/s]  
Converting format of dataset (num_proc=20):  99%|█████████▉| 655637/661595 [00:24<00:00, 9363.16 examples/s]
Converting format of dataset (num_proc=20): 100%|██████████| 661595/661595 [00:25<00:00, 26210.08 examples/s]
Concatenating 20 shards
03/06/2024 17:34:29 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Using custom data configuration default-fd8d5f83c6d4fc15
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Downloading and preparing dataset json/default to /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...

Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]
Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 6842.26it/s]
Downloading took 0.0 min
Checksum Computation took 0.0 min

Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]
Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 207.55it/s]
Generating train split

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 7174 examples [00:00, 56640.20 examples/s]
Generating train split: 13955 examples [00:00, 52552.98 examples/s]
Generating train split: 22470 examples [00:00, 37669.33 examples/s]
Generating train split: 29541 examples [00:00, 39727.05 examples/s]
Generating train split: 34106 examples [00:00, 40450.19 examples/s]
Generating train split: 40614 examples [00:00, 42182.41 examples/s]
Generating train split: 49643 examples [00:01, 45574.63 examples/s]
Generating train split: 56115 examples [00:01, 40624.09 examples/s]
Generating train split: 65496 examples [00:01, 45251.39 examples/s]
Generating train split: 74619 examples [00:01, 48193.01 examples/s]
Generating train split: 81662 examples [00:01, 49400.19 examples/s]
Generating train split: 90803 examples [00:01, 52352.88 examples/s]
Generating train split: 99643 examples [00:02, 51779.61 examples/s]
Generating train split: 109274 examples [00:02, 55210.14 examples/s]
Generating train split: 118785 examples [00:02, 55307.99 examples/s]
Generating train split: 127838 examples [00:02, 53611.21 examples/s]
Generating train split: 136959 examples [00:02, 54513.02 examples/s]
Generating train split: 146214 examples [00:02, 54736.70 examples/s]
Generating train split: 155681 examples [00:03, 55858.40 examples/s]
Generating train split: 164869 examples [00:03, 55410.74 examples/s]
Generating train split: 174668 examples [00:03, 58204.31 examples/s]
Generating train split: 183916 examples [00:03, 57142.16 examples/s]
Generating train split: 193184 examples [00:03, 56694.82 examples/s]
Generating train split: 202452 examples [00:03, 57680.70 examples/s]
Generating train split: 209120 examples [00:04, 55903.06 examples/s]
Generating train split: 218464 examples [00:04, 55511.46 examples/s]
Generating train split: 227861 examples [00:04, 53402.75 examples/s]
Generating train split: 234478 examples [00:04, 48844.58 examples/s]
Generating train split: 243600 examples [00:04, 49657.45 examples/s]
Generating train split: 250163 examples [00:04, 46864.17 examples/s]
Generating train split: 259230 examples [00:05, 47478.67 examples/s]
Generating train split: 268399 examples [00:05, 48379.43 examples/s]
Generating train split: 277541 examples [00:05, 48016.11 examples/s]
Generating train split: 286820 examples [00:05, 48599.29 examples/s]
Generating train split: 293719 examples [00:05, 48169.04 examples/s]
Generating train split: 302352 examples [00:06, 48580.65 examples/s]
Generating train split: 311391 examples [00:06, 50349.64 examples/s]
Generating train split: 318280 examples [00:06, 50479.54 examples/s]
Generating train split: 327315 examples [00:06, 51278.75 examples/s]
Generating train split: 333994 examples [00:06, 50884.68 examples/s]
Generating train split: 343112 examples [00:06, 53503.70 examples/s]
Generating train split: 352346 examples [00:06, 54308.49 examples/s]
Generating train split: 361518 examples [00:07, 55577.72 examples/s]
Generating train split: 368512 examples [00:07, 54874.91 examples/s]
Generating train split: 378015 examples [00:07, 57920.08 examples/s]
Generating train split: 387194 examples [00:07, 58103.02 examples/s]
Generating train split: 394001 examples [00:07, 54372.23 examples/s]
Generating train split: 403789 examples [00:07, 52191.35 examples/s]
Generating train split: 413221 examples [00:08, 51701.51 examples/s]
Generating train split: 420156 examples [00:08, 50213.33 examples/s]
Generating train split: 429135 examples [00:08, 49691.07 examples/s]
Generating train split: 438695 examples [00:08, 50532.54 examples/s]
Generating train split: 448290 examples [00:08, 51039.32 examples/s]
Generating train split: 454725 examples [00:08, 50034.69 examples/s]
Generating train split: 464366 examples [00:09, 50079.78 examples/s]
Generating train split: 471442 examples [00:09, 50113.74 examples/s]
Generating train split: 478170 examples [00:09, 48195.03 examples/s]
Generating train split: 485235 examples [00:09, 48675.36 examples/s]
Generating train split: 494684 examples [00:09, 49147.45 examples/s]
Generating train split: 504198 examples [00:09, 52207.29 examples/s]
Generating train split: 513388 examples [00:10, 54057.37 examples/s]
Generating train split: 523137 examples [00:10, 57380.37 examples/s]
Generating train split: 532880 examples [00:10, 59909.96 examples/s]
Generating train split: 542669 examples [00:10, 61538.65 examples/s]
Generating train split: 549135 examples [00:10, 57467.70 examples/s]
Generating train split: 558576 examples [00:10, 58406.34 examples/s]
Generating train split: 568426 examples [00:10, 58836.44 examples/s]
Generating train split: 577876 examples [00:11, 59173.26 examples/s]
Generating train split: 584692 examples [00:11, 56116.64 examples/s]
Generating train split: 594291 examples [00:11, 58433.14 examples/s]
Generating train split: 604182 examples [00:11, 60282.56 examples/s]
Generating train split: 613065 examples [00:11, 57173.83 examples/s]
Generating train split: 622314 examples [00:11, 58510.47 examples/s]
Generating train split: 631495 examples [00:12, 57737.42 examples/s]
Generating train split: 638738 examples [00:12, 56330.72 examples/s]
Generating train split: 647661 examples [00:12, 56508.00 examples/s]
Generating train split: 656980 examples [00:12, 56383.21 examples/s]
Generating train split: 666152 examples [00:12, 56897.42 examples/s]
Generating train split: 675569 examples [00:12, 57710.46 examples/s]
Generating train split: 682663 examples [00:12, 56996.46 examples/s]
Generating train split: 691932 examples [00:13, 57253.19 examples/s]
Generating train split: 701527 examples [00:13, 59386.92 examples/s]
Generating train split: 711144 examples [00:13, 60242.34 examples/s]
Generating train split: 720762 examples [00:13, 61302.28 examples/s]
Generating train split: 729696 examples [00:13, 59417.73 examples/s]
Generating train split: 738610 examples [00:13, 57616.92 examples/s]
Generating train split: 748330 examples [00:14, 59373.62 examples/s]
Generating train split: 757231 examples [00:14, 58124.78 examples/s]
Generating train split: 766477 examples [00:14, 57956.00 examples/s]
Generating train split: 775844 examples [00:14, 57978.96 examples/s]
Generating train split: 784587 examples [00:14, 55741.71 examples/s]
Generating train split: 793526 examples [00:14, 55994.41 examples/s]
Generating train split: 803063 examples [00:14, 58461.57 examples/s]
Generating train split: 810299 examples [00:15, 55056.87 examples/s]
Generating train split: 817029 examples [00:15, 54258.92 examples/s]
Generating train split: 823785 examples [00:15, 53232.46 examples/s]
Generating train split: 833181 examples [00:15, 50820.01 examples/s]
Generating train split: 841848 examples [00:15, 47109.49 examples/s]
Generating train split: 850994 examples [00:15, 49922.32 examples/s]
Generating train split: 855529 examples [00:16, 53210.93 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
Sample en data 10000 lines
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 10000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00000_of_00020.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00001_of_00020.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00002_of_00020.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00003_of_00020.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00004_of_00020.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00005_of_00020.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00006_of_00020.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00007_of_00020.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00008_of_00020.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00009_of_00020.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00010_of_00020.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00011_of_00020.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00012_of_00020.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00013_of_00020.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00014_of_00020.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00015_of_00020.arrow
Process #16 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00016_of_00020.arrow
Process #17 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00017_of_00020.arrow
Process #18 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00018_of_00020.arrow
Process #19 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00019_of_00020.arrow
Spawning 20 processes

Converting format of dataset (num_proc=20):   0%|          | 0/10000 [00:00<?, ? examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00000_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00001_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00006_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00007_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00002_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00003_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00005_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00004_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00009_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00010_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00008_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00011_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00012_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00013_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00014_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00016_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00015_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00018_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00017_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00019_of_00020.arrow

Converting format of dataset (num_proc=20):   5%|▌         | 500/10000 [00:00<00:05, 1807.48 examples/s]
Converting format of dataset (num_proc=20):  30%|███       | 3000/10000 [00:00<00:00, 9107.73 examples/s]
Converting format of dataset (num_proc=20): 100%|██████████| 10000/10000 [00:00<00:00, 10822.36 examples/s]
Concatenating 20 shards
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00000_of_00020.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00001_of_00020.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00002_of_00020.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00003_of_00020.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00004_of_00020.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00005_of_00020.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00006_of_00020.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00007_of_00020.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00008_of_00020.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00009_of_00020.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00010_of_00020.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00011_of_00020.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00012_of_00020.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00013_of_00020.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00014_of_00020.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00015_of_00020.arrow
Process #16 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00016_of_00020.arrow
Process #17 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00017_of_00020.arrow
Process #18 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00018_of_00020.arrow
Process #19 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00019_of_00020.arrow
Spawning 20 processes

Running tokenizer on dataset (num_proc=20):   0%|          | 0/671595 [00:00<?, ? examples/s]03/06/2024 17:35:09 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
03/06/2024 17:35:10 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
03/06/2024 17:35:10 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00006_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00019_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00001_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00013_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00005_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00011_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00000_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00003_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00016_of_00020.arrow

Running tokenizer on dataset (num_proc=20):   0%|          | 1000/671595 [00:06<1:08:02, 164.25 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00008_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00017_of_00020.arrow

Running tokenizer on dataset (num_proc=20):   0%|          | 2000/671595 [00:06<28:42, 388.71 examples/s]  Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00004_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00002_of_00020.arrow

Running tokenizer on dataset (num_proc=20):   1%|          | 6000/671595 [00:06<06:47, 1632.06 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00007_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00009_of_00020.arrow

Running tokenizer on dataset (num_proc=20):   1%|▏         | 9000/671595 [00:06<03:53, 2833.39 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00012_of_00020.arrow

Running tokenizer on dataset (num_proc=20):   2%|▏         | 12000/671595 [00:06<02:42, 4064.41 examples/s]
Running tokenizer on dataset (num_proc=20):   2%|▏         | 16000/671595 [00:06<01:46, 6138.67 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00015_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00018_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00010_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5801a1115a779409_00014_of_00020.arrow

Running tokenizer on dataset (num_proc=20):   3%|▎         | 18000/671595 [00:07<02:02, 5318.10 examples/s]
Running tokenizer on dataset (num_proc=20):   3%|▎         | 20000/671595 [00:07<01:43, 6307.41 examples/s]
Running tokenizer on dataset (num_proc=20):   3%|▎         | 22000/671595 [00:11<06:58, 1553.32 examples/s]
Running tokenizer on dataset (num_proc=20):   3%|▎         | 23000/671595 [00:11<06:12, 1742.93 examples/s]
Running tokenizer on dataset (num_proc=20):   4%|▎         | 25000/671595 [00:11<04:34, 2356.37 examples/s]
Running tokenizer on dataset (num_proc=20):   4%|▍         | 28000/671595 [00:12<02:53, 3698.83 examples/s]
Running tokenizer on dataset (num_proc=20):   4%|▍         | 30000/671595 [00:12<02:19, 4607.45 examples/s]
Running tokenizer on dataset (num_proc=20):   5%|▍         | 32000/671595 [00:12<02:02, 5205.56 examples/s]
Running tokenizer on dataset (num_proc=20):   5%|▌         | 34000/671595 [00:12<01:39, 6398.40 examples/s]Dataset({
    features: ['text'],
    num_rows: 661595
})
{'text': '一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁\n所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！\n隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。\n'}
Dataset({
    features: ['text'],
    num_rows: 661595
})
{'text': '一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁\n所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！\n隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。\n'}

Running tokenizer on dataset (num_proc=20):   5%|▌         | 36000/671595 [00:13<01:47, 5894.07 examples/s]
Running tokenizer on dataset (num_proc=20):   6%|▌         | 39000/671595 [00:13<01:32, 6833.71 examples/s]Dataset({
    features: ['text'],
    num_rows: 661595
})
{'text': '一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁\n所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！\n隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。\n'}

Running tokenizer on dataset (num_proc=20):   6%|▌         | 41000/671595 [00:17<06:16, 1676.09 examples/s]
Running tokenizer on dataset (num_proc=20):   6%|▋         | 43000/671595 [00:17<05:02, 2077.19 examples/s]
Running tokenizer on dataset (num_proc=20):   7%|▋         | 45000/671595 [00:17<03:50, 2718.75 examples/s]
Running tokenizer on dataset (num_proc=20):   7%|▋         | 46000/671595 [00:17<03:22, 3083.63 examples/s]
Running tokenizer on dataset (num_proc=20):   7%|▋         | 48000/671595 [00:17<02:41, 3850.52 examples/s]
Running tokenizer on dataset (num_proc=20):   7%|▋         | 50000/671595 [00:18<02:01, 5102.14 examples/s]
Running tokenizer on dataset (num_proc=20):   8%|▊         | 52000/671595 [00:18<01:53, 5456.57 examples/s]
Running tokenizer on dataset (num_proc=20):   8%|▊         | 53000/671595 [00:18<02:05, 4920.58 examples/s]
Running tokenizer on dataset (num_proc=20):   8%|▊         | 55000/671595 [00:19<02:38, 3889.29 examples/s]
Running tokenizer on dataset (num_proc=20):   8%|▊         | 56000/671595 [00:19<02:35, 3950.23 examples/s]
Running tokenizer on dataset (num_proc=20):   9%|▊         | 58000/671595 [00:20<03:11, 3207.11 examples/s]
Running tokenizer on dataset (num_proc=20):   9%|▉         | 59000/671595 [00:20<03:06, 3292.44 examples/s]03/06/2024 17:35:27 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.

Running tokenizer on dataset (num_proc=20):   9%|▉         | 60000/671595 [00:21<04:06, 2481.10 examples/s]
Running tokenizer on dataset (num_proc=20):   9%|▉         | 61000/671595 [00:22<05:14, 1940.99 examples/s]
Running tokenizer on dataset (num_proc=20):   9%|▉         | 62000/671595 [00:22<05:31, 1841.33 examples/s]03/06/2024 17:35:30 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.

Running tokenizer on dataset (num_proc=20):   9%|▉         | 63000/671595 [00:23<04:36, 2199.93 examples/s]
Running tokenizer on dataset (num_proc=20):  10%|▉         | 66000/671595 [00:23<02:28, 4074.88 examples/s]
Running tokenizer on dataset (num_proc=20):  10%|█         | 68000/671595 [00:23<01:58, 5101.98 examples/s]
Running tokenizer on dataset (num_proc=20):  10%|█         | 69000/671595 [00:23<02:09, 4657.22 examples/s]
Running tokenizer on dataset (num_proc=20):  10%|█         | 70000/671595 [00:24<02:17, 4365.88 examples/s]03/06/2024 17:35:31 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.

Running tokenizer on dataset (num_proc=20):  11%|█         | 72000/671595 [00:24<02:30, 3995.89 examples/s]
Running tokenizer on dataset (num_proc=20):  11%|█         | 73000/671595 [00:25<03:09, 3165.81 examples/s]
Running tokenizer on dataset (num_proc=20):  11%|█         | 74000/671595 [00:25<03:21, 2973.08 examples/s]
Running tokenizer on dataset (num_proc=20):  11%|█▏        | 76000/671595 [00:25<02:46, 3585.82 examples/s]
Running tokenizer on dataset (num_proc=20):  11%|█▏        | 77000/671595 [00:26<03:04, 3224.87 examples/s]
Running tokenizer on dataset (num_proc=20):  12%|█▏        | 78000/671595 [00:26<03:41, 2683.25 examples/s]
Running tokenizer on dataset (num_proc=20):  12%|█▏        | 80000/671595 [00:27<02:55, 3363.09 examples/s]
Running tokenizer on dataset (num_proc=20):  12%|█▏        | 81000/671595 [00:27<03:00, 3268.60 examples/s]
Running tokenizer on dataset (num_proc=20):  12%|█▏        | 82000/671595 [00:28<03:53, 2527.62 examples/s]
Running tokenizer on dataset (num_proc=20):  13%|█▎        | 85000/671595 [00:28<02:06, 4622.02 examples/s]
Running tokenizer on dataset (num_proc=20):  13%|█▎        | 86000/671595 [00:28<02:00, 4876.42 examples/s]
Running tokenizer on dataset (num_proc=20):  13%|█▎        | 88000/671595 [00:28<01:51, 5213.36 examples/s]
Running tokenizer on dataset (num_proc=20):  13%|█▎        | 89000/671595 [00:29<02:05, 4656.47 examples/s]
Running tokenizer on dataset (num_proc=20):  13%|█▎        | 90000/671595 [00:29<02:15, 4282.75 examples/s]
Running tokenizer on dataset (num_proc=20):  14%|█▎        | 92000/671595 [00:30<02:31, 3816.85 examples/s]
Running tokenizer on dataset (num_proc=20):  14%|█▍        | 93000/671595 [00:30<03:05, 3113.80 examples/s]Sample en data 10000 lines
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 10000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}

Running tokenizer on dataset (num_proc=20):  14%|█▍        | 94000/671595 [00:30<02:40, 3599.99 examples/s]
Running tokenizer on dataset (num_proc=20):  14%|█▍        | 95000/671595 [00:31<02:54, 3306.18 examples/s]
Running tokenizer on dataset (num_proc=20):  14%|█▍        | 96000/671595 [00:31<02:29, 3861.43 examples/s]
Running tokenizer on dataset (num_proc=20):  14%|█▍        | 97000/671595 [00:32<04:59, 1916.91 examples/s]
Running tokenizer on dataset (num_proc=20):  15%|█▍        | 98000/671595 [00:32<03:53, 2458.10 examples/s]Sample en data 10000 lines
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 10000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}

Running tokenizer on dataset (num_proc=20):  15%|█▍        | 99000/671595 [00:33<05:22, 1777.91 examples/s]
Running tokenizer on dataset (num_proc=20):  15%|█▍        | 100000/671595 [00:33<04:22, 2173.71 examples/s]
Running tokenizer on dataset (num_proc=20):  15%|█▌        | 101000/671595 [00:34<03:29, 2723.58 examples/s]
Running tokenizer on dataset (num_proc=20):  15%|█▌        | 104000/671595 [00:34<01:49, 5188.38 examples/s]Sample en data 10000 lines
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 10000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}

Running tokenizer on dataset (num_proc=20):  16%|█▌        | 105000/671595 [00:34<01:41, 5575.62 examples/s]
Running tokenizer on dataset (num_proc=20):  16%|█▌        | 106000/671595 [00:34<01:55, 4894.86 examples/s]
Running tokenizer on dataset (num_proc=20):  16%|█▌        | 107000/671595 [00:35<02:42, 3482.58 examples/s]
Running tokenizer on dataset (num_proc=20):  16%|█▌        | 109000/671595 [00:35<02:25, 3879.69 examples/s]
Running tokenizer on dataset (num_proc=20):  16%|█▋        | 110000/671595 [00:36<02:50, 3294.37 examples/s]
Running tokenizer on dataset (num_proc=20):  17%|█▋        | 112000/671595 [00:36<02:00, 4659.24 examples/s]
Running tokenizer on dataset (num_proc=20):  17%|█▋        | 113000/671595 [00:36<03:08, 2961.89 examples/s]
Running tokenizer on dataset (num_proc=20):  17%|█▋        | 114000/671595 [00:37<02:54, 3195.48 examples/s]
Running tokenizer on dataset (num_proc=20):  17%|█▋        | 115000/671595 [00:37<02:52, 3220.03 examples/s]
Running tokenizer on dataset (num_proc=20):  17%|█▋        | 116000/671595 [00:38<04:04, 2270.97 examples/s]
Running tokenizer on dataset (num_proc=20):  18%|█▊        | 118000/671595 [00:38<02:39, 3467.16 examples/s]
Running tokenizer on dataset (num_proc=20):  18%|█▊        | 119000/671595 [00:39<03:53, 2366.25 examples/s]
Running tokenizer on dataset (num_proc=20):  18%|█▊        | 123000/671595 [00:39<01:51, 4936.34 examples/s]
Running tokenizer on dataset (num_proc=20):  19%|█▊        | 125000/671595 [00:39<01:32, 5914.45 examples/s]
Running tokenizer on dataset (num_proc=20):  19%|█▉        | 127000/671595 [00:40<02:38, 3426.98 examples/s]
Running tokenizer on dataset (num_proc=20):  19%|█▉        | 128000/671595 [00:41<02:53, 3130.95 examples/s]
Running tokenizer on dataset (num_proc=20):  19%|█▉        | 129000/671595 [00:41<02:38, 3415.34 examples/s]
Running tokenizer on dataset (num_proc=20):  19%|█▉        | 130000/671595 [00:41<02:59, 3025.03 examples/s]
Running tokenizer on dataset (num_proc=20):  20%|█▉        | 131000/671595 [00:42<03:31, 2557.69 examples/s]
Running tokenizer on dataset (num_proc=20):  20%|█▉        | 132000/671595 [00:43<05:38, 1595.80 examples/s]
Running tokenizer on dataset (num_proc=20):  20%|█▉        | 133000/671595 [00:44<04:56, 1815.17 examples/s]
Running tokenizer on dataset (num_proc=20):  20%|█▉        | 134000/671595 [00:44<04:40, 1917.20 examples/s]
Running tokenizer on dataset (num_proc=20):  20%|██        | 136000/671595 [00:44<02:57, 3024.43 examples/s]
Running tokenizer on dataset (num_proc=20):  21%|██        | 140000/671595 [00:45<02:07, 4172.34 examples/s]
Running tokenizer on dataset (num_proc=20):  21%|██        | 141000/671595 [00:45<02:27, 3589.61 examples/s]
Running tokenizer on dataset (num_proc=20):  21%|██▏       | 143000/671595 [00:46<01:48, 4849.82 examples/s]
Running tokenizer on dataset (num_proc=20):  21%|██▏       | 144000/671595 [00:46<02:05, 4214.66 examples/s]
Running tokenizer on dataset (num_proc=20):  22%|██▏       | 146000/671595 [00:46<01:52, 4654.22 examples/s]
Running tokenizer on dataset (num_proc=20):  22%|██▏       | 148000/671595 [00:46<01:27, 5964.25 examples/s]
Running tokenizer on dataset (num_proc=20):  22%|██▏       | 149000/671595 [00:47<02:11, 3963.90 examples/s]
Running tokenizer on dataset (num_proc=20):  22%|██▏       | 150000/671595 [00:47<02:17, 3788.26 examples/s]
Running tokenizer on dataset (num_proc=20):  22%|██▏       | 151000/671595 [00:47<02:04, 4190.81 examples/s]
Running tokenizer on dataset (num_proc=20):  23%|██▎       | 152000/671595 [00:49<04:34, 1894.51 examples/s]
Running tokenizer on dataset (num_proc=20):  23%|██▎       | 153000/671595 [00:49<03:56, 2193.11 examples/s]
Running tokenizer on dataset (num_proc=20):  23%|██▎       | 154000/671595 [00:50<03:59, 2160.29 examples/s]
Running tokenizer on dataset (num_proc=20):  23%|██▎       | 157000/671595 [00:50<02:07, 4027.95 examples/s]
Running tokenizer on dataset (num_proc=20):  24%|██▎       | 159000/671595 [00:51<02:49, 3030.19 examples/s]
Running tokenizer on dataset (num_proc=20):  24%|██▍       | 160000/671595 [00:51<03:08, 2716.30 examples/s]
Running tokenizer on dataset (num_proc=20):  24%|██▍       | 162000/671595 [00:51<02:12, 3844.99 examples/s]
Running tokenizer on dataset (num_proc=20):  24%|██▍       | 163000/671595 [00:52<02:00, 4218.44 examples/s]
Running tokenizer on dataset (num_proc=20):  24%|██▍       | 164000/671595 [00:52<01:53, 4484.91 examples/s]
Running tokenizer on dataset (num_proc=20):  25%|██▍       | 166000/671595 [00:52<02:19, 3620.30 examples/s]
Running tokenizer on dataset (num_proc=20):  25%|██▌       | 168000/671595 [00:53<02:10, 3861.93 examples/s]
Running tokenizer on dataset (num_proc=20):  25%|██▌       | 169000/671595 [00:54<03:38, 2302.61 examples/s]
Running tokenizer on dataset (num_proc=20):  25%|██▌       | 170000/671595 [00:54<03:14, 2575.97 examples/s]
Running tokenizer on dataset (num_proc=20):  26%|██▌       | 172000/671595 [00:55<02:46, 3003.07 examples/s]
Running tokenizer on dataset (num_proc=20):  26%|██▌       | 173000/671595 [00:55<02:54, 2856.06 examples/s]
Running tokenizer on dataset (num_proc=20):  26%|██▌       | 175000/671595 [00:56<02:27, 3374.06 examples/s]
Running tokenizer on dataset (num_proc=20):  26%|██▌       | 176000/671595 [00:56<02:49, 2929.04 examples/s]
Running tokenizer on dataset (num_proc=20):  26%|██▋       | 177000/671595 [00:57<03:13, 2553.72 examples/s]
Running tokenizer on dataset (num_proc=20):  27%|██▋       | 178000/671595 [00:57<02:39, 3085.26 examples/s]
Running tokenizer on dataset (num_proc=20):  27%|██▋       | 180000/671595 [00:57<01:44, 4704.33 examples/s]
Running tokenizer on dataset (num_proc=20):  27%|██▋       | 181000/671595 [00:57<01:47, 4579.98 examples/s]
Running tokenizer on dataset (num_proc=20):  27%|██▋       | 183000/671595 [00:58<02:14, 3631.87 examples/s]
Running tokenizer on dataset (num_proc=20):  27%|██▋       | 184000/671595 [00:58<02:01, 4024.37 examples/s]
Running tokenizer on dataset (num_proc=20):  28%|██▊       | 185000/671595 [00:58<02:23, 3389.51 examples/s]
Running tokenizer on dataset (num_proc=20):  28%|██▊       | 186000/671595 [00:59<03:51, 2099.29 examples/s]
Running tokenizer on dataset (num_proc=20):  28%|██▊       | 187000/671595 [01:00<03:18, 2440.29 examples/s]
Running tokenizer on dataset (num_proc=20):  28%|██▊       | 188000/671595 [01:00<02:47, 2885.56 examples/s]
Running tokenizer on dataset (num_proc=20):  28%|██▊       | 190000/671595 [01:01<02:41, 2987.23 examples/s]
Running tokenizer on dataset (num_proc=20):  28%|██▊       | 191000/671595 [01:01<02:21, 3384.64 examples/s]
Running tokenizer on dataset (num_proc=20):  29%|██▊       | 193000/671595 [01:01<02:30, 3176.67 examples/s]
Running tokenizer on dataset (num_proc=20):  29%|██▉       | 194000/671595 [01:02<02:36, 3049.97 examples/s]
Running tokenizer on dataset (num_proc=20):  29%|██▉       | 195000/671595 [01:02<02:48, 2826.22 examples/s]
Running tokenizer on dataset (num_proc=20):  29%|██▉       | 197000/671595 [01:02<02:07, 3726.46 examples/s]
Running tokenizer on dataset (num_proc=20):  30%|██▉       | 199000/671595 [01:03<01:44, 4503.08 examples/s]
Running tokenizer on dataset (num_proc=20):  30%|██▉       | 200000/671595 [01:03<02:10, 3621.27 examples/s]
Running tokenizer on dataset (num_proc=20):  30%|██▉       | 201000/671595 [01:04<02:59, 2628.71 examples/s]
Running tokenizer on dataset (num_proc=20):  30%|███       | 202000/671595 [01:04<03:01, 2584.66 examples/s]
Running tokenizer on dataset (num_proc=20):  30%|███       | 203000/671595 [01:05<02:48, 2785.75 examples/s]
Running tokenizer on dataset (num_proc=20):  31%|███       | 205000/671595 [01:05<01:55, 4035.92 examples/s]
Running tokenizer on dataset (num_proc=20):  31%|███       | 206000/671595 [01:05<02:09, 3604.24 examples/s]
Running tokenizer on dataset (num_proc=20):  31%|███       | 207000/671595 [01:06<02:51, 2712.11 examples/s]
Running tokenizer on dataset (num_proc=20):  31%|███       | 208000/671595 [01:06<02:23, 3224.14 examples/s]
Running tokenizer on dataset (num_proc=20):  31%|███▏      | 210000/671595 [01:06<01:47, 4282.92 examples/s]
Running tokenizer on dataset (num_proc=20):  31%|███▏      | 211000/671595 [01:06<01:44, 4405.97 examples/s]
Running tokenizer on dataset (num_proc=20):  32%|███▏      | 212000/671595 [01:07<03:01, 2533.52 examples/s]
Running tokenizer on dataset (num_proc=20):  32%|███▏      | 213000/671595 [01:08<02:51, 2669.99 examples/s]
Running tokenizer on dataset (num_proc=20):  32%|███▏      | 215000/671595 [01:08<02:22, 3214.38 examples/s]
Running tokenizer on dataset (num_proc=20):  32%|███▏      | 216000/671595 [01:09<02:42, 2811.42 examples/s]
Running tokenizer on dataset (num_proc=20):  32%|███▏      | 218000/671595 [01:09<02:16, 3315.15 examples/s]
Running tokenizer on dataset (num_proc=20):  33%|███▎      | 219000/671595 [01:09<02:18, 3262.69 examples/s]
Running tokenizer on dataset (num_proc=20):  33%|███▎      | 221000/671595 [01:10<01:38, 4569.87 examples/s]
Running tokenizer on dataset (num_proc=20):  33%|███▎      | 222000/671595 [01:10<02:16, 3287.66 examples/s]
Running tokenizer on dataset (num_proc=20):  33%|███▎      | 223000/671595 [01:11<02:24, 3098.53 examples/s]
Running tokenizer on dataset (num_proc=20):  33%|███▎      | 224000/671595 [01:11<02:04, 3588.50 examples/s]
Running tokenizer on dataset (num_proc=20):  34%|███▎      | 225000/671595 [01:11<02:36, 2861.52 examples/s]
Running tokenizer on dataset (num_proc=20):  34%|███▎      | 226000/671595 [01:12<02:28, 3009.46 examples/s]
Running tokenizer on dataset (num_proc=20):  34%|███▍      | 228000/671595 [01:12<01:52, 3938.77 examples/s]
Running tokenizer on dataset (num_proc=20):  34%|███▍      | 229000/671595 [01:13<03:10, 2321.36 examples/s]
Running tokenizer on dataset (num_proc=20):  34%|███▍      | 230000/671595 [01:13<02:46, 2656.14 examples/s]
Running tokenizer on dataset (num_proc=20):  34%|███▍      | 231000/671595 [01:13<02:33, 2870.37 examples/s]
Running tokenizer on dataset (num_proc=20):  35%|███▍      | 233000/671595 [01:14<01:43, 4218.89 examples/s]
Running tokenizer on dataset (num_proc=20):  35%|███▍      | 234000/671595 [01:14<01:57, 3711.80 examples/s]
Running tokenizer on dataset (num_proc=20):  35%|███▍      | 235000/671595 [01:14<02:12, 3291.20 examples/s]
Running tokenizer on dataset (num_proc=20):  35%|███▌      | 236000/671595 [01:15<02:08, 3402.10 examples/s]
Running tokenizer on dataset (num_proc=20):  35%|███▌      | 238000/671595 [01:15<01:25, 5048.09 examples/s]
Running tokenizer on dataset (num_proc=20):  36%|███▌      | 239000/671595 [01:16<02:40, 2689.08 examples/s]
Running tokenizer on dataset (num_proc=20):  36%|███▌      | 240000/671595 [01:16<02:22, 3025.79 examples/s]
Running tokenizer on dataset (num_proc=20):  36%|███▌      | 241000/671595 [01:16<02:36, 2749.51 examples/s]
Running tokenizer on dataset (num_proc=20):  36%|███▌      | 242000/671595 [01:17<02:18, 3105.95 examples/s]
Running tokenizer on dataset (num_proc=20):  36%|███▌      | 243000/671595 [01:17<02:32, 2811.69 examples/s]
Running tokenizer on dataset (num_proc=20):  36%|███▋      | 244000/671595 [01:17<02:18, 3090.49 examples/s]
Running tokenizer on dataset (num_proc=20):  36%|███▋      | 245000/671595 [01:17<02:01, 3513.57 examples/s]
Running tokenizer on dataset (num_proc=20):  37%|███▋      | 246000/671595 [01:18<02:55, 2424.00 examples/s]
Running tokenizer on dataset (num_proc=20):  37%|███▋      | 247000/671595 [01:18<02:46, 2544.87 examples/s]
Running tokenizer on dataset (num_proc=20):  37%|███▋      | 248000/671595 [01:19<02:27, 2862.84 examples/s]
Running tokenizer on dataset (num_proc=20):  37%|███▋      | 249000/671595 [01:19<02:21, 2985.30 examples/s]
Running tokenizer on dataset (num_proc=20):  37%|███▋      | 250000/671595 [01:19<01:58, 3566.70 examples/s]
Running tokenizer on dataset (num_proc=20):  37%|███▋      | 251000/671595 [01:19<01:49, 3840.39 examples/s]
Running tokenizer on dataset (num_proc=20):  38%|███▊      | 253000/671595 [01:20<01:16, 5457.23 examples/s]
Running tokenizer on dataset (num_proc=20):  38%|███▊      | 254000/671595 [01:20<02:05, 3332.78 examples/s]
Running tokenizer on dataset (num_proc=20):  38%|███▊      | 256000/671595 [01:21<01:36, 4313.00 examples/s]
Running tokenizer on dataset (num_proc=20):  38%|███▊      | 257000/671595 [01:21<02:20, 2957.39 examples/s]
Running tokenizer on dataset (num_proc=20):  38%|███▊      | 258000/671595 [01:21<02:07, 3239.05 examples/s]
Running tokenizer on dataset (num_proc=20):  39%|███▊      | 259000/671595 [01:22<02:18, 2986.49 examples/s]
Running tokenizer on dataset (num_proc=20):  39%|███▉      | 261000/671595 [01:22<02:05, 3274.00 examples/s]
Running tokenizer on dataset (num_proc=20):  39%|███▉      | 262000/671595 [01:23<01:57, 3483.64 examples/s]
Running tokenizer on dataset (num_proc=20):  39%|███▉      | 263000/671595 [01:23<01:53, 3585.75 examples/s]
Running tokenizer on dataset (num_proc=20):  39%|███▉      | 264000/671595 [01:24<03:09, 2155.95 examples/s]
Running tokenizer on dataset (num_proc=20):  40%|███▉      | 266000/671595 [01:24<02:40, 2531.19 examples/s]
Running tokenizer on dataset (num_proc=20):  40%|███▉      | 268000/671595 [01:25<02:10, 3089.95 examples/s]
Running tokenizer on dataset (num_proc=20):  40%|████      | 269000/671595 [01:25<01:52, 3574.92 examples/s]
Running tokenizer on dataset (num_proc=20):  40%|████      | 270000/671595 [01:25<01:41, 3950.99 examples/s]
Running tokenizer on dataset (num_proc=20):  40%|████      | 271000/671595 [01:26<02:06, 3179.17 examples/s]
Running tokenizer on dataset (num_proc=20):  41%|████      | 272000/671595 [01:26<01:50, 3604.74 examples/s]
Running tokenizer on dataset (num_proc=20):  41%|████      | 274000/671595 [01:26<01:35, 4175.85 examples/s]
Running tokenizer on dataset (num_proc=20):  41%|████      | 275000/671595 [01:27<01:47, 3702.42 examples/s]
Running tokenizer on dataset (num_proc=20):  41%|████      | 276000/671595 [01:27<02:19, 2831.41 examples/s]
Running tokenizer on dataset (num_proc=20):  41%|████      | 277000/671595 [01:27<02:05, 3138.51 examples/s]
Running tokenizer on dataset (num_proc=20):  41%|████▏     | 278000/671595 [01:28<01:56, 3386.22 examples/s]
Running tokenizer on dataset (num_proc=20):  42%|████▏     | 279000/671595 [01:28<01:46, 3686.71 examples/s]
Running tokenizer on dataset (num_proc=20):  42%|████▏     | 280000/671595 [01:28<01:43, 3788.61 examples/s]
Running tokenizer on dataset (num_proc=20):  42%|████▏     | 281000/671595 [01:28<02:00, 3251.42 examples/s]
Running tokenizer on dataset (num_proc=20):  42%|████▏     | 282000/671595 [01:29<02:49, 2302.39 examples/s]
Running tokenizer on dataset (num_proc=20):  42%|████▏     | 283000/671595 [01:30<03:21, 1925.37 examples/s]
Running tokenizer on dataset (num_proc=20):  42%|████▏     | 284000/671595 [01:30<02:33, 2525.10 examples/s]
Running tokenizer on dataset (num_proc=20):  42%|████▏     | 285000/671595 [01:30<02:10, 2957.29 examples/s]
Running tokenizer on dataset (num_proc=20):  43%|████▎     | 286000/671595 [01:30<01:48, 3550.14 examples/s]
Running tokenizer on dataset (num_proc=20):  43%|████▎     | 289000/671595 [01:31<01:03, 6049.28 examples/s]
Running tokenizer on dataset (num_proc=20):  43%|████▎     | 290000/671595 [01:31<01:43, 3691.85 examples/s]
Running tokenizer on dataset (num_proc=20):  43%|████▎     | 291000/671595 [01:32<01:50, 3440.37 examples/s]
Running tokenizer on dataset (num_proc=20):  44%|████▎     | 293000/671595 [01:32<01:26, 4390.46 examples/s]
Running tokenizer on dataset (num_proc=20):  44%|████▍     | 294000/671595 [01:33<02:25, 2603.22 examples/s]
Running tokenizer on dataset (num_proc=20):  44%|████▍     | 295000/671595 [01:33<02:02, 3076.23 examples/s]
Running tokenizer on dataset (num_proc=20):  44%|████▍     | 296000/671595 [01:33<02:15, 2776.52 examples/s]
Running tokenizer on dataset (num_proc=20):  44%|████▍     | 298000/671595 [01:34<02:12, 2811.73 examples/s]
Running tokenizer on dataset (num_proc=20):  45%|████▍     | 299000/671595 [01:35<02:23, 2605.33 examples/s]
Running tokenizer on dataset (num_proc=20):  45%|████▍     | 300000/671595 [01:35<02:24, 2571.35 examples/s]
Running tokenizer on dataset (num_proc=20):  45%|████▍     | 301000/671595 [01:35<02:26, 2523.52 examples/s]
Running tokenizer on dataset (num_proc=20):  45%|████▌     | 303000/671595 [01:36<01:32, 3973.17 examples/s]
Running tokenizer on dataset (num_proc=20):  45%|████▌     | 304000/671595 [01:36<01:37, 3789.17 examples/s]
Running tokenizer on dataset (num_proc=20):  45%|████▌     | 305000/671595 [01:36<01:42, 3571.60 examples/s]
Running tokenizer on dataset (num_proc=20):  46%|████▌     | 307000/671595 [01:37<01:36, 3769.75 examples/s]
Running tokenizer on dataset (num_proc=20):  46%|████▌     | 308000/671595 [01:37<01:36, 3769.91 examples/s]
Running tokenizer on dataset (num_proc=20):  46%|████▌     | 310000/671595 [01:37<01:06, 5451.63 examples/s]
Running tokenizer on dataset (num_proc=20):  46%|████▋     | 311000/671595 [01:38<02:01, 2963.12 examples/s]
Running tokenizer on dataset (num_proc=20):  47%|████▋     | 313000/671595 [01:38<01:48, 3317.27 examples/s]
Running tokenizer on dataset (num_proc=20):  47%|████▋     | 314000/671595 [01:39<02:21, 2529.70 examples/s]
Running tokenizer on dataset (num_proc=20):  47%|████▋     | 315000/671595 [01:40<02:24, 2466.22 examples/s]
Running tokenizer on dataset (num_proc=20):  47%|████▋     | 317000/671595 [01:40<02:02, 2905.97 examples/s]
Running tokenizer on dataset (num_proc=20):  47%|████▋     | 318000/671595 [01:40<02:03, 2855.87 examples/s]
Running tokenizer on dataset (num_proc=20):  48%|████▊     | 320000/671595 [01:41<01:57, 2989.66 examples/s]
Running tokenizer on dataset (num_proc=20):  48%|████▊     | 321000/671595 [01:41<01:57, 2989.67 examples/s]
Running tokenizer on dataset (num_proc=20):  48%|████▊     | 323000/671595 [01:42<01:27, 4004.13 examples/s]
Running tokenizer on dataset (num_proc=20):  48%|████▊     | 324000/671595 [01:42<01:59, 2914.87 examples/s]
Running tokenizer on dataset (num_proc=20):  49%|████▊     | 326000/671595 [01:42<01:22, 4179.82 examples/s]
Running tokenizer on dataset (num_proc=20):  49%|████▊     | 327000/671595 [01:43<01:16, 4499.75 examples/s]
Running tokenizer on dataset (num_proc=20):  49%|████▉     | 328000/671595 [01:43<01:50, 3121.04 examples/s]
Running tokenizer on dataset (num_proc=20):  49%|████▉     | 329000/671595 [01:44<01:49, 3114.84 examples/s]
Running tokenizer on dataset (num_proc=20):  49%|████▉     | 330000/671595 [01:44<02:19, 2455.71 examples/s]
Running tokenizer on dataset (num_proc=20):  49%|████▉     | 331000/671595 [01:45<02:18, 2461.18 examples/s]
Running tokenizer on dataset (num_proc=20):  49%|████▉     | 332000/671595 [01:45<01:59, 2840.97 examples/s]
Running tokenizer on dataset (num_proc=20):  50%|████▉     | 333000/671595 [01:45<01:52, 3015.21 examples/s]
Running tokenizer on dataset (num_proc=20):  50%|████▉     | 335000/671595 [01:46<01:59, 2809.95 examples/s]
Running tokenizer on dataset (num_proc=20):  50%|█████     | 336000/671595 [01:47<02:18, 2416.89 examples/s]
Running tokenizer on dataset (num_proc=20):  50%|█████     | 337000/671595 [01:47<02:13, 2509.84 examples/s]
Running tokenizer on dataset (num_proc=20):  51%|█████     | 340000/671595 [01:47<01:14, 4442.02 examples/s]
Running tokenizer on dataset (num_proc=20):  51%|█████     | 343000/671595 [01:48<01:18, 4170.28 examples/s]
Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 345000/671595 [01:48<01:00, 5358.50 examples/s]
Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 348000/671595 [01:49<01:26, 3733.67 examples/s]
Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 349000/671595 [01:51<02:21, 2272.46 examples/s]
Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 350000/671595 [01:51<02:14, 2398.24 examples/s]
Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 352000/671595 [01:51<01:56, 2746.52 examples/s]
Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 353000/671595 [01:52<02:06, 2522.84 examples/s]
Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 354000/671595 [01:52<01:56, 2726.02 examples/s]
Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 356000/671595 [01:52<01:25, 3671.19 examples/s]
Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 357000/671595 [01:53<01:22, 3790.33 examples/s]
Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 358000/671595 [01:53<01:30, 3473.20 examples/s]
Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 359000/671595 [01:53<01:27, 3555.06 examples/s]
Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 361000/671595 [01:53<01:02, 4995.53 examples/s]
Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 363000/671595 [01:54<01:04, 4799.72 examples/s]
Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 364000/671595 [01:54<01:05, 4715.56 examples/s]
Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 365000/671595 [01:54<01:14, 4098.35 examples/s]
Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 366000/671595 [01:55<01:14, 4079.23 examples/s]
Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 367000/671595 [01:55<01:27, 3476.16 examples/s]
Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 368000/671595 [01:56<02:16, 2230.23 examples/s]
Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 369000/671595 [01:56<01:56, 2588.37 examples/s]
Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 370000/671595 [01:57<01:56, 2594.34 examples/s]
Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 371000/671595 [01:57<01:57, 2560.01 examples/s]
Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 372000/671595 [01:57<01:56, 2562.71 examples/s]
Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 373000/671595 [01:58<01:35, 3140.84 examples/s]
Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 375000/671595 [01:58<01:10, 4198.11 examples/s]
Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 376000/671595 [01:58<01:21, 3642.96 examples/s]
Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 378000/671595 [01:59<01:23, 3496.63 examples/s]
Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 379000/671595 [01:59<01:11, 4098.44 examples/s]
Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 380000/671595 [02:00<02:11, 2222.35 examples/s]
Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 381000/671595 [02:00<01:56, 2498.84 examples/s]
Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 382000/671595 [02:01<01:47, 2694.45 examples/s]
Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 383000/671595 [02:01<01:33, 3075.74 examples/s]
Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 384000/671595 [02:01<01:48, 2652.97 examples/s]
Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 386000/671595 [02:02<01:23, 3425.15 examples/s]
Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 387000/671595 [02:02<01:54, 2486.61 examples/s]
Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 388000/671595 [02:03<02:06, 2242.70 examples/s]
Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 389000/671595 [02:03<01:41, 2785.43 examples/s]
Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 391000/671595 [02:03<01:07, 4186.64 examples/s]
Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 392000/671595 [02:03<00:58, 4791.06 examples/s]
Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 393000/671595 [02:04<00:58, 4722.90 examples/s]
Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 395000/671595 [02:04<00:47, 5795.48 examples/s]
Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 396000/671595 [02:04<01:02, 4384.44 examples/s]
Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 398000/671595 [02:05<00:52, 5171.57 examples/s]
Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 399000/671595 [02:06<01:40, 2722.15 examples/s]
Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 400000/671595 [02:06<01:34, 2867.16 examples/s]
Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 401000/671595 [02:06<01:35, 2833.45 examples/s]
Running tokenizer on dataset (num_proc=20):  60%|██████    | 403000/671595 [02:06<01:09, 3865.60 examples/s]
Running tokenizer on dataset (num_proc=20):  60%|██████    | 405000/671595 [02:07<01:28, 3015.92 examples/s]
Running tokenizer on dataset (num_proc=20):  60%|██████    | 406000/671595 [02:08<01:50, 2401.07 examples/s]
Running tokenizer on dataset (num_proc=20):  61%|██████    | 407000/671595 [02:08<01:50, 2387.54 examples/s]
Running tokenizer on dataset (num_proc=20):  61%|██████    | 409000/671595 [02:09<01:23, 3131.59 examples/s]
Running tokenizer on dataset (num_proc=20):  61%|██████    | 410000/671595 [02:09<01:11, 3674.95 examples/s]
Running tokenizer on dataset (num_proc=20):  61%|██████    | 411000/671595 [02:09<01:11, 3644.71 examples/s]
Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 412000/671595 [02:10<01:29, 2898.26 examples/s]
Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 413000/671595 [02:10<01:51, 2329.40 examples/s]
Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 414000/671595 [02:11<01:35, 2710.38 examples/s]
Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 416000/671595 [02:11<01:38, 2582.74 examples/s]
Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 417000/671595 [02:12<01:26, 2942.05 examples/s]
Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 419000/671595 [02:12<01:02, 4036.40 examples/s]
Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 420000/671595 [02:12<00:56, 4431.83 examples/s]
Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 421000/671595 [02:13<01:17, 3248.72 examples/s]
Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 423000/671595 [02:13<00:58, 4270.58 examples/s]
Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 424000/671595 [02:13<00:52, 4706.09 examples/s]
Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 425000/671595 [02:14<01:14, 3325.65 examples/s]
Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 426000/671595 [02:14<01:21, 2997.15 examples/s]
Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 427000/671595 [02:14<01:15, 3233.91 examples/s]
Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 428000/671595 [02:14<01:01, 3971.86 examples/s]
Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 429000/671595 [02:15<00:57, 4212.13 examples/s]
Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 430000/671595 [02:15<01:06, 3653.82 examples/s]
Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 431000/671595 [02:15<01:05, 3654.35 examples/s]
Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 432000/671595 [02:16<01:34, 2532.52 examples/s]
Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 433000/671595 [02:16<01:14, 3198.92 examples/s]
Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 434000/671595 [02:17<01:53, 2099.62 examples/s]
Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 435000/671595 [02:17<01:52, 2094.22 examples/s]
Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 437000/671595 [02:17<01:07, 3460.55 examples/s]
Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 438000/671595 [02:18<00:59, 3928.76 examples/s]
Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 440000/671595 [02:18<01:02, 3718.78 examples/s]
Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 442000/671595 [02:19<01:04, 3576.98 examples/s]
Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 443000/671595 [02:20<01:35, 2400.34 examples/s]
Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 444000/671595 [02:20<01:22, 2758.43 examples/s]
Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 445000/671595 [02:20<01:08, 3299.74 examples/s]
Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 447000/671595 [02:20<00:47, 4758.99 examples/s]
Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 448000/671595 [02:20<00:42, 5237.05 examples/s]
Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 449000/671595 [02:21<01:25, 2608.98 examples/s]
Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 450000/671595 [02:21<01:12, 3063.09 examples/s]
Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 451000/671595 [02:22<01:10, 3149.28 examples/s]
Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 452000/671595 [02:23<01:40, 2188.39 examples/s]
Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 454000/671595 [02:23<01:15, 2896.21 examples/s]
Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 455000/671595 [02:23<01:04, 3367.82 examples/s]
Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 456000/671595 [02:23<00:58, 3712.81 examples/s]
Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 457000/671595 [02:24<01:02, 3436.24 examples/s]
Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 458000/671595 [02:24<00:52, 4083.63 examples/s]
Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 459000/671595 [02:25<01:31, 2327.46 examples/s]
Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 460000/671595 [02:25<01:35, 2212.75 examples/s]
Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 461000/671595 [02:25<01:17, 2703.17 examples/s]
Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 462000/671595 [02:25<01:01, 3409.05 examples/s]
Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 464000/671595 [02:26<00:45, 4566.31 examples/s]
Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 465000/671595 [02:26<00:43, 4725.12 examples/s]
Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 466000/671595 [02:26<01:04, 3183.80 examples/s]
Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 467000/671595 [02:27<01:14, 2745.10 examples/s]
Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 468000/671595 [02:27<01:04, 3169.16 examples/s]
Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 469000/671595 [02:28<01:26, 2351.64 examples/s]
Running tokenizer on dataset (num_proc=20):  70%|███████   | 472000/671595 [02:28<00:46, 4273.76 examples/s]
Running tokenizer on dataset (num_proc=20):  70%|███████   | 473000/671595 [02:28<00:50, 3958.08 examples/s]
Running tokenizer on dataset (num_proc=20):  71%|███████   | 474000/671595 [02:29<00:48, 4043.00 examples/s]
Running tokenizer on dataset (num_proc=20):  71%|███████   | 475000/671595 [02:29<00:58, 3332.55 examples/s]
Running tokenizer on dataset (num_proc=20):  71%|███████   | 477000/671595 [02:30<00:57, 3380.79 examples/s]
Running tokenizer on dataset (num_proc=20):  71%|███████   | 478000/671595 [02:30<01:17, 2494.17 examples/s]
Running tokenizer on dataset (num_proc=20):  71%|███████▏  | 479000/671595 [02:31<01:06, 2884.43 examples/s]
Running tokenizer on dataset (num_proc=20):  71%|███████▏  | 480000/671595 [02:31<01:05, 2945.15 examples/s]
Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 482000/671595 [02:31<00:53, 3560.80 examples/s]
Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 483000/671595 [02:32<00:59, 3157.06 examples/s]
Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 484000/671595 [02:32<01:02, 2984.27 examples/s]
Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 485000/671595 [02:32<00:57, 3264.43 examples/s]
Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 486000/671595 [02:33<01:08, 2714.79 examples/s]
Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 487000/671595 [02:34<01:24, 2175.65 examples/s]
Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 488000/671595 [02:34<01:08, 2673.60 examples/s]
Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 489000/671595 [02:34<00:59, 3080.02 examples/s]
Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 490000/671595 [02:34<00:48, 3715.69 examples/s]
Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 491000/671595 [02:34<00:52, 3430.16 examples/s]
Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 493000/671595 [02:35<00:43, 4085.96 examples/s]
Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 494000/671595 [02:35<00:45, 3894.60 examples/s]
Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 495000/671595 [02:36<01:07, 2613.43 examples/s]
Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 496000/671595 [02:36<00:58, 2984.72 examples/s]
Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 497000/671595 [02:37<01:06, 2640.95 examples/s]
Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 499000/671595 [02:37<00:40, 4290.35 examples/s]
Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 500000/671595 [02:37<00:56, 3051.95 examples/s]
Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 501000/671595 [02:38<01:08, 2485.05 examples/s]
Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 502000/671595 [02:38<01:01, 2776.07 examples/s]
Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 504000/671595 [02:38<00:39, 4273.60 examples/s]
Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 505000/671595 [02:39<00:58, 2825.06 examples/s]
Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 507000/671595 [02:40<00:52, 3157.23 examples/s]
Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 508000/671595 [02:40<00:46, 3527.33 examples/s]
Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 509000/671595 [02:40<00:39, 4158.44 examples/s]
Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 510000/671595 [02:40<00:47, 3407.72 examples/s]
Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 511000/671595 [02:40<00:41, 3845.11 examples/s]
Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 512000/671595 [02:41<01:10, 2249.86 examples/s]
Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 514000/671595 [02:42<00:50, 3114.36 examples/s]
Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 515000/671595 [02:42<00:42, 3719.09 examples/s]
Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 516000/671595 [02:42<00:45, 3387.05 examples/s]
Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 518000/671595 [02:43<00:37, 4073.12 examples/s]
Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 519000/671595 [02:43<00:55, 2726.56 examples/s]
Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 520000/671595 [02:44<01:03, 2396.98 examples/s]
Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 523000/671595 [02:45<00:47, 3143.27 examples/s]
Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 524000/671595 [02:45<00:53, 2743.94 examples/s]
Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 525000/671595 [02:46<00:54, 2670.15 examples/s]
Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 527000/671595 [02:46<00:40, 3582.97 examples/s]
Running tokenizer on dataset (num_proc=20):  79%|███████▊  | 528000/671595 [02:47<01:09, 2079.61 examples/s]
Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 531000/671595 [02:47<00:44, 3157.03 examples/s]
Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 532000/671595 [02:48<00:41, 3373.19 examples/s]
Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 534000/671595 [02:48<00:33, 4075.25 examples/s]
Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 535000/671595 [02:48<00:30, 4408.05 examples/s]
Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 536000/671595 [02:48<00:27, 4961.65 examples/s]
Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 537000/671595 [02:48<00:26, 5090.80 examples/s]
Running tokenizer on dataset (num_proc=20):  80%|████████  | 538000/671595 [02:49<00:54, 2438.30 examples/s]
Running tokenizer on dataset (num_proc=20):  80%|████████  | 539000/671595 [02:50<00:59, 2242.79 examples/s]
Running tokenizer on dataset (num_proc=20):  80%|████████  | 540000/671595 [02:50<01:00, 2187.23 examples/s]
Running tokenizer on dataset (num_proc=20):  81%|████████  | 541000/671595 [02:51<00:55, 2356.06 examples/s]
Running tokenizer on dataset (num_proc=20):  81%|████████  | 542000/671595 [02:51<00:43, 2972.19 examples/s]
Running tokenizer on dataset (num_proc=20):  81%|████████  | 544000/671595 [02:51<00:37, 3396.74 examples/s]
Running tokenizer on dataset (num_proc=20):  81%|████████  | 545000/671595 [02:52<00:56, 2225.87 examples/s]
Running tokenizer on dataset (num_proc=20):  81%|████████▏ | 546000/671595 [02:52<00:45, 2731.56 examples/s]
Running tokenizer on dataset (num_proc=20):  81%|████████▏ | 547000/671595 [02:53<00:54, 2302.35 examples/s]
Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 548000/671595 [02:53<00:43, 2833.11 examples/s]
Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 549000/671595 [02:54<00:41, 2959.54 examples/s]
Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 553000/671595 [02:54<00:18, 6529.08 examples/s]
Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 555000/671595 [02:54<00:19, 6054.13 examples/s]
Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 556000/671595 [02:55<00:45, 2554.76 examples/s]
Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 558000/671595 [02:56<00:42, 2680.33 examples/s]
Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 559000/671595 [02:56<00:38, 2954.63 examples/s]
Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 560000/671595 [02:57<00:34, 3262.09 examples/s]
Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 562000/671595 [02:57<00:31, 3460.77 examples/s]
Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 563000/671595 [02:57<00:28, 3813.15 examples/s]
Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 564000/671595 [02:58<00:50, 2134.34 examples/s]
Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 565000/671595 [02:59<00:45, 2338.48 examples/s]
Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 566000/671595 [02:59<00:39, 2689.79 examples/s]
Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 567000/671595 [02:59<00:32, 3234.23 examples/s]
Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 570000/671595 [02:59<00:19, 5091.72 examples/s]
Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 571000/671595 [03:01<00:48, 2090.19 examples/s]
Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 572000/671595 [03:01<00:49, 2023.91 examples/s]
Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 574000/671595 [03:02<00:38, 2505.28 examples/s]
Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 575000/671595 [03:02<00:33, 2916.05 examples/s]
Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 578000/671595 [03:02<00:19, 4716.97 examples/s]
Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 579000/671595 [03:03<00:19, 4834.37 examples/s]
Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 582000/671595 [03:03<00:12, 7083.04 examples/s]
Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 583000/671595 [03:03<00:12, 6820.93 examples/s]
Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 584000/671595 [03:04<00:29, 2988.18 examples/s]
Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 585000/671595 [03:04<00:32, 2633.01 examples/s]
Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 587000/671595 [03:05<00:26, 3226.53 examples/s]
Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 588000/671595 [03:05<00:24, 3352.04 examples/s]
Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 589000/671595 [03:07<00:45, 1809.62 examples/s]
Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 590000/671595 [03:07<00:38, 2120.58 examples/s]
Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 592000/671595 [03:07<00:23, 3381.02 examples/s]
Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 593000/671595 [03:07<00:27, 2886.50 examples/s]
Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 594000/671595 [03:08<00:23, 3247.06 examples/s]
Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 595000/671595 [03:08<00:22, 3347.87 examples/s]
Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 598000/671595 [03:08<00:14, 5244.34 examples/s]
Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 600000/671595 [03:09<00:16, 4311.03 examples/s]
Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 601000/671595 [03:10<00:23, 2942.88 examples/s]
Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 602000/671595 [03:10<00:22, 3093.14 examples/s]
Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 603580/671595 [03:10<00:17, 3877.24 examples/s]
Running tokenizer on dataset (num_proc=20):  90%|█████████ | 604580/671595 [03:10<00:20, 3305.25 examples/s]
Running tokenizer on dataset (num_proc=20):  90%|█████████ | 605580/671595 [03:11<00:21, 3079.59 examples/s]
Running tokenizer on dataset (num_proc=20):  90%|█████████ | 606159/671595 [03:11<00:20, 3204.06 examples/s]
Running tokenizer on dataset (num_proc=20):  90%|█████████ | 607159/671595 [03:11<00:19, 3335.62 examples/s]
Running tokenizer on dataset (num_proc=20):  91%|█████████ | 608739/671595 [03:11<00:14, 4448.14 examples/s]
Running tokenizer on dataset (num_proc=20):  91%|█████████ | 609319/671595 [03:12<00:14, 4185.43 examples/s]
Running tokenizer on dataset (num_proc=20):  91%|█████████ | 610319/671595 [03:12<00:12, 5014.74 examples/s]
Running tokenizer on dataset (num_proc=20):  91%|█████████ | 611319/671595 [03:13<00:26, 2317.51 examples/s]
Running tokenizer on dataset (num_proc=20):  91%|█████████▏| 614319/671595 [03:13<00:14, 3933.20 examples/s]
Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 615319/671595 [03:13<00:15, 3609.19 examples/s]
Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 617319/671595 [03:14<00:12, 4229.85 examples/s]
Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 618319/671595 [03:15<00:22, 2317.99 examples/s]
Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 620319/671595 [03:16<00:21, 2352.55 examples/s]
Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 621899/671595 [03:16<00:19, 2569.07 examples/s]
Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 623479/671595 [03:16<00:14, 3311.82 examples/s]
Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 624479/671595 [03:17<00:13, 3621.66 examples/s]
Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 625479/671595 [03:17<00:13, 3424.48 examples/s]
Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 626479/671595 [03:17<00:15, 2860.68 examples/s]
Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 627059/671595 [03:18<00:18, 2420.82 examples/s]
Running tokenizer on dataset (num_proc=20):  94%|█████████▎| 629059/671595 [03:18<00:11, 3697.17 examples/s]
Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 630059/671595 [03:19<00:14, 2895.89 examples/s]
Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 631059/671595 [03:19<00:11, 3534.33 examples/s]
Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 632059/671595 [03:19<00:10, 3722.93 examples/s]
Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 634059/671595 [03:21<00:18, 2003.31 examples/s]
Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 635059/671595 [03:21<00:16, 2283.28 examples/s]
Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 635639/671595 [03:21<00:14, 2530.80 examples/s]
Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 636639/671595 [03:21<00:15, 2235.45 examples/s]
Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 637639/671595 [03:22<00:12, 2737.96 examples/s]
Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 638218/671595 [03:22<00:11, 2791.73 examples/s]
Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 639218/671595 [03:22<00:09, 3308.65 examples/s]
Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 640218/671595 [03:22<00:08, 3771.09 examples/s]
Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 641218/671595 [03:23<00:10, 2982.68 examples/s]
Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 642218/671595 [03:23<00:12, 2390.96 examples/s]
Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 642798/671595 [03:24<00:12, 2364.79 examples/s]
Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 643377/671595 [03:24<00:10, 2702.47 examples/s]
Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 644377/671595 [03:24<00:08, 3052.36 examples/s]
Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 645957/671595 [03:25<00:12, 2066.61 examples/s]
Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 646957/671595 [03:26<00:18, 1314.89 examples/s]
Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 649116/671595 [03:27<00:10, 2151.15 examples/s]
Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 650116/671595 [03:27<00:09, 2305.46 examples/s]
Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 651116/671595 [03:28<00:10, 1910.42 examples/s]
Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 652116/671595 [03:29<00:12, 1537.53 examples/s]
Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 652696/671595 [03:29<00:13, 1369.25 examples/s]
Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 653696/671595 [03:30<00:11, 1625.74 examples/s]
Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 654696/671595 [03:31<00:15, 1062.85 examples/s]
Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 655696/671595 [03:32<00:13, 1208.29 examples/s]
Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 656696/671595 [03:33<00:11, 1354.17 examples/s]
Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 657696/671595 [03:34<00:12, 1109.41 examples/s]
Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 658276/671595 [03:35<00:16, 798.94 examples/s] 
Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 659276/671595 [03:37<00:15, 811.03 examples/s]
Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 660276/671595 [03:37<00:10, 1106.49 examples/s]
Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 661276/671595 [03:39<00:12, 798.23 examples/s] 
Running tokenizer on dataset (num_proc=20):  99%|█████████▊| 662276/671595 [03:39<00:08, 1108.26 examples/s]
Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 663276/671595 [03:42<00:11, 701.78 examples/s] 
Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 665276/671595 [03:44<00:08, 782.43 examples/s]
Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 665856/671595 [03:44<00:06, 842.06 examples/s]
Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 666856/671595 [03:46<00:06, 680.69 examples/s]
Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 667436/671595 [03:47<00:05, 773.18 examples/s]
Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 668436/671595 [03:47<00:02, 1081.78 examples/s]
Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 669436/671595 [03:51<00:04, 522.79 examples/s] 
Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 670015/671595 [03:51<00:02, 586.48 examples/s]
Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 671015/671595 [03:55<00:01, 417.14 examples/s]
Running tokenizer on dataset (num_proc=20): 100%|██████████| 671595/671595 [03:58<00:00, 353.35 examples/s]
Running tokenizer on dataset (num_proc=20): 100%|██████████| 671595/671595 [03:58<00:00, 2813.51 examples/s]
Concatenating 20 shards
input_ids:
[29871, 30287, 31450, 235, 192, 169, 30888, 30214, 31530, 232, 179, 191, 233, 140, 155, 31885, 31600, 30346, 30533, 235, 192, 169, 30214, 29896, 29889, 29947, 232, 144, 138, 29902, 29899, 29946, 30910, 30846, 31429, 30214, 232, 187, 169, 31462, 31859, 234, 177, 180, 30210, 29896, 31859, 15633, 29911, 30214, 231, 192, 145, 231, 190, 186, 30544, 232, 151, 177, 30584, 29871, 30888, 30698, 31141, 232, 193, 132, 30383, 29899, 30666, 234, 134, 176, 30607, 30658, 233, 145, 149, 31780, 233, 167, 136, 29899, 31352, 236, 149, 168, 232, 143, 156, 31174, 30752, 29899, 235, 150, 160, 234, 140, 156, 31903, 31092, 29899, 31676, 30815, 30872, 232, 167, 138, 30893, 30494, 29899, 235, 194, 159, 31101, 232, 147, 178, 30846, 29899, 31679, 30846, 30408, 234, 173, 154, 29899, 232, 171, 180, 31616, 30689, 31021, 31542, 30858, 29899, 30486, 31613, 31382, 30607, 317, 5098, 4330, 29979, 847, 27827, 29899, 1164, 29903, 30383, 29899, 232, 131, 149, 235, 192, 169, 233, 148, 135, 31551, 31584, 29899, 232, 139, 188, 235, 192, 169, 235, 193, 136, 31931, 29899, 31679, 30846, 31631, 31649, 29899, 232, 135, 194, 234, 174, 168, 30670, 30753, 236, 151, 132, 13, 30744, 30417, 235, 192, 169, 235, 193, 137, 232, 160, 138, 30768, 31138, 21576, 30419, 30768, 30406, 233, 180, 192, 235, 192, 169, 30409, 29896, 29945, 29945, 31888, 29871, 31439, 235, 178, 132, 233, 166, 131, 31851, 30214, 31666, 231, 187, 151, 232, 136, 144, 235, 183, 188, 31302, 231, 193, 158, 29945, 29900, 29900, 29900, 8848, 31391, 29929, 29900, 30408, 30753, 235, 192, 169, 30982, 31273, 30584, 232, 136, 144, 235, 183, 188, 31545, 30287, 234, 177, 180, 233, 181, 188, 29974, 29906, 30936, 31640, 31429, 233, 181, 188, 29974, 30287, 30936, 232, 136, 144, 235, 183, 188, 30928, 235, 192, 177, 30495, 30956, 31520, 31358, 30584, 13, 236, 157, 185, 31360, 30909, 12300, 6028, 1114, 30893, 232, 158, 165, 30419, 30666, 233, 142, 194, 30257, 30878, 30257, 233, 180, 192, 235, 192, 169, 30893, 232, 158, 165, 30409, 30888, 235, 147, 168, 21576, 30419, 30768, 30406, 233, 180, 192, 235, 192, 169, 30409, 233, 154, 154, 30557, 31399, 234, 140, 143, 30214, 233, 142, 168, 30417, 30753, 232, 168, 154, 233, 180, 192, 235, 192, 169, 31520, 31358, 30584, 233, 175, 165, 235, 194, 145, 30805, 234, 145, 172, 30267, 13, 2, 306, 29950, 30705, 31041, 234, 169, 190, 30869, 233, 182, 184, 31573, 31545, 31633, 235, 183, 171, 233, 187, 172, 30898, 30573, 29899, 29906, 29900, 229, 135, 134, 30739, 29896, 29900, 29945, 229, 135, 134, 30214, 31383, 30698, 30594, 236, 138, 138, 30406, 232, 137, 186, 232, 144, 183, 233, 145, 173, 233, 153, 192, 30682, 31573, 31545, 31100, 30528, 233, 187, 172, 30898, 30210, 31633, 235, 183, 171, 30214, 236, 131, 133, 30406, 30909, 30705, 31041, 30330, 30814, 233, 181, 188, 30330, 232, 137, 185, 30659, 30330, 31679, 31074, 30330, 31420, 234, 189, 187, 30330, 31855, 31399, 30330, 31072, 235, 144, 178, 30330, 234, 145, 178, 30982, 30330, 232, 189, 162, 30716, 31548, 30687, 30503, 30733, 30494, 234, 189, 167, 234, 190, 183, 31184, 30448, 31729, 30406, 30909, 31573, 31545, 232, 147, 135, 31893, 235, 136, 147, 235, 157, 131, 30210, 31391, 30413, 232, 136, 132, 235, 177, 187, 233, 180, 164, 233, 162, 150, 30210, 30832, 231, 191, 191, 30909, 30716, 30210, 31633, 235, 183, 171, 30267, 13, 30287, 30330, 29902, 29950, 30883, 232, 144, 170, 30607, 30705, 31041, 234, 169, 190, 30869, 233, 182, 184, 231, 189, 170, 31399, 233, 169, 133, 235, 194, 179, 30383, 13, 29902, 29950, 30883, 30705, 31041, 233, 182, 184, 30392, 31166, 234, 189, 170, 31166, 232, 147, 187, 30419, 235, 192, 183, 31331, 232, 147, 187, 30752, 30409, 233, 133, 175, 235, 138, 133, 30607, 234, 169, 190, 30869, 233, 182, 184, 30214, 231, 193, 158, 31573, 31545, 30413, 232, 147, 174, 232, 158, 189, 30988, 236, 165, 154, 234, 181, 149, 232, 136, 186, 30417, 235, 136, 147, 235, 157, 131, 30952, 30330, 234, 181, 155, 30898, 30832, 231, 191, 191, 30716, 30210, 233, 185, 181, 30988, 30267, 31149, 31062, 31410, 30330, 236, 165, 160, 30495, 30952, 30815, 30503, 232, 179, 189, 232, 178, 187, 31184, 31944, 236, 138, 138, 30406, 31062, 232, 138, 137, 29096, 29906, 29947, 29945, 29947, 30214, 232, 136, 186, 30417, 30952, 30815, 235, 143, 134, 232, 158, 183, 31566, 30330, 31944, 234, 145, 138, 30528, 30330, 30015, 30457, 30705, 30024, 30716, 30606, 30528, 30503, 234, 190, 183, 31273, 30525, 231, 193, 194, 31184, 31141, 30940, 30214, 31149, 31944, 234, 145, 138, 31419, 29943, 30883, 233, 182, 184, 30606, 232, 160, 138, 31302, 30528, 29945, 242, 191, 136, 30214, 30392, 30356, 30613, 233, 145, 171, 31566, 30210, 31669, 30815, 231, 189, 170, 31399, 30267, 13, 29902, 29950, 30883, 30705, 31041, 233, 182, 184, 31573, 31545, 31633, 235, 183, 171, 233, 187, 172, 30898, 30573, 29899, 29906, 29900, 229, 135, 134, 30739, 29896, 29900, 29945, 229, 135, 134, 30214, 31383, 30698, 30594, 236, 138, 138, 30406, 232, 137, 186, 232, 144, 183, 233, 145, 173, 233, 153, 192, 30682, 31573, 31545, 31100, 30528, 233, 187, 172, 30898, 30210, 31633, 235, 183, 171, 30214, 236, 131, 133, 30406, 30909, 30705, 31041, 30330, 30814, 233, 181, 188, 30330, 232, 137, 185, 30659, 30330, 31679, 31074, 30330, 31420, 234, 189, 187, 30330, 31855, 31399, 30330, 31072, 235, 144, 178, 30330, 234, 145, 178, 30982, 30330, 232, 189, 162, 30716, 31548, 30687, 30503, 30733, 30494, 234, 189, 167, 234, 190, 183, 31184, 30448, 31729, 30406, 30909, 31573, 31545, 232, 147, 135, 31893, 235, 136, 147, 235, 157, 131, 30210, 31391, 30413, 232, 136, 132, 235, 177, 187, 233, 180, 164, 233, 162, 150, 30210, 30832, 231, 191, 191, 30909, 30716, 30210, 31633, 235, 183, 171, 30267, 13, 30654, 30606, 31534, 233, 182, 184, 31729, 30893, 232, 158, 165, 30417, 31175, 30539, 30931, 31756, 31729, 31302, 231, 193, 158, 29902, 29950, 30705, 31041, 234, 169, 190, 30869, 233, 182, 184, 31184, 231, 189, 170, 31399, 30689, 31021, 30214, 233, 175, 165, 235, 194, 145, 30805, 31679, 232, 149, 171, 235, 178, 165, 30584, 2, 29871, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 30698, 31522, 31032, 234, 153, 154, 234, 153, 193, 234, 154, 136, 30214, 31333, 233, 142, 172, 30724, 31835, 30210, 232, 143, 190, 30963, 30392, 31838, 31190, 30908, 30698, 30210, 30214, 30868, 234, 156, 159, 236, 166, 145, 30392, 30287, 31893, 31838, 31190, 236, 164, 192, 232, 158, 189, 30210, 234, 153, 193, 234, 154, 136, 30214, 31032, 234, 153, 154, 31558, 30805, 30953, 30392, 31419, 235, 193, 134, 232, 158, 179, 236, 157, 193, 30210, 30214, 30744, 30651, 233, 133, 166, 30767, 30847, 30801, 31423, 30417, 31333, 233, 142, 172, 31076, 30724, 235, 170, 135, 30210, 232, 143, 190, 30963, 30210, 31852, 30214, 234, 153, 193, 234, 154, 136, 30953, 30392, 232, 193, 139, 236, 157, 193, 31032, 234, 153, 154, 30210, 30267, 31356, 31882, 30214, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 30557, 30806, 31238, 235, 177, 172, 232, 136, 179, 30765, 30275, 31367, 30868, 234, 156, 159, 236, 166, 145, 232, 143, 190, 30963, 30210, 30805, 31999, 30257, 30613, 31633, 234, 190, 144, 30287, 30557, 30267, 13, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 13, 29896, 30330, 30868, 234, 156, 159, 236, 166, 145, 30287, 235, 139, 175, 30910, 30486, 30505, 235, 166, 187, 236, 159, 181, 30210, 234, 157, 177, 235, 133, 167, 30429, 30806, 30214, 30910, 234, 154, 136, 31120, 31117, 30868, 233, 153, 148, 30806, 234, 170, 178, 30446, 30214, 30354, 31180, 31022, 30214, 30768, 31190, 31557, 30417, 232, 138, 163, 232, 160, 154, 31391, 30767, 30392, 232, 138, 163, 30940, 30267, 30847, 30413, 234, 152, 156, 31474, 30210, 31852, 30214, 30868, 233, 153, 148, 234, 154, 136, 30910, 31599, 31238, 30437, 31305, 30494, 31022, 30354, 30210, 233, 153, 148, 232, 160, 154, 30910, 31599, 30780, 30287, 31558, 30214, 31943, 235, 138, 183, 30257, 30806, 234, 170, 178, 30210, 30868, 233, 153, 148, 234, 154, 136, 30267, 13, 29906, 30330, 30868, 234, 156, 159, 236, 166, 145, 30658, 31117, 30868, 233, 153, 148, 234, 154, 136, 30910, 234, 154, 136, 31352, 30688, 235, 170, 140, 30952, 30214, 233, 133, 166, 31548, 30636, 30956, 234, 154, 149, 233, 135, 162, 30214, 31325, 231, 187, 151, 233, 133, 166, 31548, 30210, 235, 133, 143, 235, 133, 167, 31066, 30746, 233, 185, 169, 233, 190, 148, 30214, 31352, 236, 182, 161, 232, 180, 148, 31423, 30417, 235, 147, 145, 234, 191, 172, 30210, 30746, 31133, 30544, 31424, 30267, 30810, 31238, 235, 138, 183, 30785, 232, 193, 139, 30923, 234, 154, 136, 30313, 31352, 30545, 31436, 30594, 30210, 30910, 31424, 30868, 233, 153, 148, 234, 154, 136, 30214, 31979, 31420, 30494, 30743, 30868, 233, 153, 148, 234, 154, 136, 30210, 31174, 30287, 233, 176, 168, 30748, 233, 152, 166, 30267, 13, 29941, 30330, 30868, 233, 153, 148, 30437, 236, 157, 146, 234, 160, 131, 234, 153, 193, 234, 154, 136, 30210, 30666, 30908, 236, 165, 159, 31085, 236, 131, 147, 233, 187, 147, 31462, 31947, 30214, 31267, 31149, 31221, 234, 157, 177, 235, 133, 167, 30210, 31993, 30967, 30953, 30437, 31844, 30805, 30267, 30868, 234, 156, 159, 236, 166, 145, 31120, 31117, 30392, 30417, 30287, 31959, 31141, 30210, 31141, 232, 193, 132, 30210, 30214, 30868, 234, 156, 159, 236, 166, 145, 234, 157, 177, 233, 144, 162, 30467, 30505, 31120, 31117, 30287, 235, 139, 175, 30392, 233, 184, 136, 30868, 31085, 30210, 30214, 30868, 234, 156, 159, 236, 166, 145, 30868, 233, 153, 148, 30210, 31109, 232, 158, 183, 30437, 30544, 31424, 30287, 31217, 31935, 31935, 232, 138, 187, 31558, 30210, 234, 133, 145, 234, 154, 138, 30952, 233, 157, 154, 31869, 31085, 31217, 234, 189, 188, 30214, 30287, 235, 139, 175, 30769, 30437, 31695, 234, 190, 176, 30544, 31424, 232, 138, 163, 30502, 30900, 31117, 30267, 13, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 30768, 31138, 30429, 30806, 232, 136, 179, 30765, 30275, 31367, 30868, 234, 156, 159, 236, 166, 145, 232, 143, 190, 30963, 30210, 30210, 31633, 234, 190, 144, 30214, 30257, 30613, 30783, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30417, 30743, 232, 193, 139, 30257, 30210, 30743, 31201, 30267, 31424, 30505, 30672, 31381, 31290, 31412, 30743, 31201, 31050, 30743, 30868, 234, 156, 159, 236, 166, 145, 233, 131, 145, 31882, 232, 141, 161, 30214, 31356, 31882, 30672, 31381, 30953, 31370, 31751, 31043, 30397, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 233, 131, 145, 31882, 31475, 236, 165, 135, 236, 155, 181, 30214, 30672, 31381, 31370, 31751, 30505, 30486, 31704, 30275, 234, 170, 178, 233, 161, 132, 236, 165, 135, 236, 155, 181, 234, 153, 193, 234, 154, 136, 30214, 30810, 31819, 30672, 31381, 31979, 30815, 30210, 31617, 31072, 234, 153, 193, 234, 154, 136, 30214, 232, 138, 146, 31022, 234, 153, 193, 234, 154, 136, 232, 187, 169, 30805, 30210, 232, 144, 180, 232, 177, 182, 30267, 13, 2, 29871, 30287, 30470, 30287, 30898, 233, 157, 148, 232, 132, 138, 232, 144, 182, 30998, 31026, 31020, 30214, 30573, 30446, 233, 159, 142, 31373, 232, 138, 137, 232, 167, 138, 30670, 233, 145, 149, 31704, 30846, 30210, 30613, 31143, 30682, 31424, 30505, 233, 141, 168, 235, 178, 190, 233, 157, 148, 31117, 30395, 232, 193, 135, 234, 146, 176, 30214, 30573, 30446, 233, 159, 142, 31373, 31138, 30287, 30502, 232, 136, 136, 233, 190, 164, 31704, 31074, 30210, 232, 132, 138, 31117, 29871, 30429, 31338, 30533, 30940, 30383, 31502, 30333, 30395, 31885, 31730, 30813, 30330, 31947, 30716, 232, 162, 154, 30330, 31476, 30395, 29871, 30986, 235, 164, 166, 30330, 232, 180, 178, 31649, 30330, 236, 150, 159, 236, 151, 166, 233, 188, 193, 30330, 30998, 31867, 233, 193, 182, 29871, 30257, 232, 162, 151, 31169, 235, 147, 134, 30415, 31071, 30330, 31869, 234, 166, 164, 31169, 235, 147, 134, 232, 188, 191, 234, 171, 157, 232, 158, 176, 29871, 30325, 31117, 30383, 29955, 30534, 29896, 29953, 30325, 235, 138, 182, 29947, 30534, 29906, 29955, 30325, 31117, 31016, 29871, 30783]
inputs:
一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁
所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！
隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。
</s> IH化工离心泵输送介质温度为-20℃～105℃，需要时采用冷却措施可输送更高温度的介质，适用于化工、石油、冶金、电力、造纸、食品、制药、环保、废水处理和合成纤维等行业用于输送各种腐蚀的或不允许污染的类似于水的介质。
一、IH型卧式化工离心泵产品概述：
IH型化工泵是单级单吸（轴向吸入）悬臂式离心泵，供输送不含固体颗粒具有腐蚀性、粘度类似水的液体。其标记、额定性能和尺寸等效采用标准ISO2858，具有性能范围广、效率高、“三化”水平高和维修方便等特点，其效率比F型泵平均提高5％，是国家推广的节能产品。
IH型化工泵输送介质温度为-20℃～105℃，需要时采用冷却措施可输送更高温度的介质，适用于化工、石油、冶金、电力、造纸、食品、制药、环保、废水处理和合成纤维等行业用于输送各种腐蚀的或不允许污染的类似于水的介质。
太平洋泵业集团有限公司专业提供IH化工离心泵等产品信息，欢迎来电咨询！</s> 处在发展期的白癜风应该如何治疗?要想治疗疾病，选择正确的医院是非常重要的，白癜风是一种非常顽固的疾病，治疗起来也是比较困难的，所以患者如果没有选择好正规的医院的话，疾病也是很难治疗的。那么，处在发展期的白癜风应该如何治疗?下面就让兰州中研白癜风医院的来给大家介绍一下。
处在发展期的白癜风应该如何治疗?
1、白癜风一般发生在裸露的皮肤上面，发病初期白斑面积小，数量少，通常只有几块或者是几点。如不留意的话，白斑病发展就会形成少数的斑块发展到一起，导致大面积的白斑病。
2、白癜风前期白斑病发病无自觉性，患处部位痒感，而且患处的肌肤外表润滑，无鳞屑没有萎缩的表象出现。这就致使很多病人无法及时的发现白斑病，才造成了白斑病的进一步分散。
3、白斑会随着疾病的加重颜色逐渐变深，与其他皮肤的边界也会越来。白癜风初期是有一些特的特征的，白癜风皮损区在初期一般是浅白色的，白癜风白斑的周围会出现一条微微凸起的炎症性暗红色条纹，一般都会持续出现几个星期。
处在发展期的白癜风应该如何治疗?通过上面兰州中研白癜风医院的的介绍，大家对发展期的白癜风应该有了很大的了解。现在我们已经了解得了白癜风怎么办，那么我们也应该知道白癜风应该怎么去预防，我们应该在生活中积极预防疾病，这样我们才能的控制疾病，减少疾病带来的危害。
</s> 一年一度暑假即将开始，为小朋友准备安排活动的家长可现在报读暑期田径班，为小朋友过一个充满活力的假期 上堂地点：何文田巴富街、深水埗、沙田 青衣、屯门、铜锣湾、将军澳 大埔德萃学校、红磡德萃幼稚园 日期：7月16日至8月27日期间 对
Caching indices mapping at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-36dc585512f99a66.arrow
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 493432
})
[INFO|trainer.py:586] 2024-03-06 17:39:06,630 >> Using auto half precision backend
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 493432
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 493432
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 493432
})
[INFO|deepspeed.py:325] 2024-03-06 17:39:06,886 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Creating extension directory /home/wangzj/.cache/torch_extensions/py310_cu117/cpu_adam...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/wangzj/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/4] /home/nfs02/cuda_tools/cuda-11.7/bin/nvcc  -ccbin /home/nfs02/anaconda3/envs/wzjsz/bin/x86_64-conda_cos6-linux-gnu-cc -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/home/nfs02/cuda_tools/cuda-11.7/include -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include/TH -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include/THC -isystem /home/nfs02/cuda_tools/cuda-11.7/include -isystem /home/nfs02/anaconda3/envs/wzjsz/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o 
[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/home/nfs02/cuda_tools/cuda-11.7/include -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include/TH -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include/THC -isystem /home/nfs02/cuda_tools/cuda-11.7/include -isystem /home/nfs02/anaconda3/envs/wzjsz/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -L/home/nfs02/cuda_tools/cuda-11.7/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/home/nfs02/cuda_tools/cuda-11.7/include -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include/TH -isystem /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/include/THC -isystem /home/nfs02/cuda_tools/cuda-11.7/include -isystem /home/nfs02/anaconda3/envs/wzjsz/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -L/home/nfs02/cuda_tools/cuda-11.7/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/home/nfs02/cuda_tools/cuda-11.7/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 71.71744632720947 seconds
Loading extension module cpu_adam...
Loading extension module cpu_adam...Loading extension module cpu_adam...

Time to load cpu_adam op: 71.78529739379883 seconds
Time to load cpu_adam op: 71.78629183769226 seconds
Time to load cpu_adam op: 71.78402972221375 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-03-06 17:40:32,313] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
[2024-03-06 17:40:52,962] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-03-06 17:40:52,965] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-03-06 17:40:52,965] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-03-06 17:40:52,973] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-03-06 17:40:52,974] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-03-06 17:40:52,974] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-03-06 17:40:52,974] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-03-06 17:40:52,974] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-03-06 17:40:52,974] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: True
[2024-03-06 17:40:52,974] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-03-06 17:41:12,426] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-03-06 17:41:12,427] [INFO] [utils.py:803:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-03-06 17:41:12,427] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 54.97 GB, percent = 14.6%
[2024-03-06 17:41:19,079] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-03-06 17:41:19,080] [INFO] [utils.py:803:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-03-06 17:41:19,080] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 96.72 GB, percent = 25.7%
[2024-03-06 17:41:19,080] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-03-06 17:41:19,250] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-03-06 17:41:19,251] [INFO] [utils.py:803:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-03-06 17:41:19,251] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 97.12 GB, percent = 25.8%
[2024-03-06 17:41:19,253] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-03-06 17:41:19,253] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-03-06 17:41:19,253] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-03-06 17:41:19,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2024-03-06 17:41:19,255] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-03-06 17:41:19,255] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-03-06 17:41:19,255] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-03-06 17:41:19,255] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-03-06 17:41:19,255] [INFO] [config.py:978:print]   amp_params ................... False
[2024-03-06 17:41:19,255] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-03-06 17:41:19,255] [INFO] [config.py:978:print]   bfloat16_enabled ............. True
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1e5ec296c0>
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   dump_state ................... False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... None
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   fp16_auto_cast ............... None
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   fp16_enabled ................. False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 8
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 1
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   loss_scale ................... 1.0
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-03-06 17:41:19,256] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   pld_params ................... False
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   train_batch_size ............. 256
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  8
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   world_size ................... 4
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-03-06 17:41:19,257] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-03-06 17:41:19,257] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1747] 2024-03-06 17:41:19,257 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-03-06 17:41:19,257 >>   Num examples = 493,432
[INFO|trainer.py:1749] 2024-03-06 17:41:19,257 >>   Num Epochs = 1
[INFO|trainer.py:1750] 2024-03-06 17:41:19,257 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-03-06 17:41:19,257 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:1754] 2024-03-06 17:41:19,257 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1755] 2024-03-06 17:41:19,258 >>   Total optimization steps = 1,927
[INFO|trainer.py:1756] 2024-03-06 17:41:19,259 >>   Number of trainable parameters = 4,328,783,872

  0%|          | 0/1927 [00:00<?, ?it/s]
  0%|          | 1/1927 [01:29<48:03:20, 89.82s/it]
  0%|          | 2/1927 [02:54<46:16:12, 86.53s/it]
  0%|          | 3/1927 [04:18<45:43:49, 85.57s/it]
  0%|          | 4/1927 [05:42<45:20:29, 84.88s/it]
  0%|          | 5/1927 [07:06<45:06:36, 84.49s/it]
  0%|          | 6/1927 [08:29<44:54:23, 84.16s/it]
  0%|          | 7/1927 [09:53<44:45:38, 83.93s/it]
  0%|          | 8/1927 [11:16<44:36:14, 83.68s/it]
  0%|          | 9/1927 [12:39<44:29:25, 83.51s/it]
  1%|          | 10/1927 [14:02<44:20:42, 83.28s/it]
                                                    
{'loss': 1.77, 'learning_rate': 4.999667771634271e-05, 'epoch': 0.01}

  1%|          | 10/1927 [14:02<44:20:42, 83.28s/it]
  1%|          | 11/1927 [15:25<44:15:39, 83.16s/it]
  1%|          | 12/1927 [16:47<44:07:21, 82.95s/it]
  1%|          | 13/1927 [18:09<44:01:18, 82.80s/it]
  1%|          | 14/1927 [19:31<43:51:59, 82.55s/it]
  1%|          | 15/1927 [20:53<43:39:19, 82.20s/it]
  1%|          | 16/1927 [22:14<43:32:09, 82.01s/it]
  1%|          | 17/1927 [23:36<43:26:45, 81.89s/it]
  1%|          | 18/1927 [24:57<43:19:50, 81.71s/it]
  1%|          | 19/1927 [26:18<43:12:58, 81.54s/it]
  1%|          | 20/1927 [27:40<43:14:25, 81.63s/it]
                                                    
{'loss': 1.6439, 'learning_rate': 4.998671174837633e-05, 'epoch': 0.01}

  1%|          | 20/1927 [27:40<43:14:25, 81.63s/it]
  1%|          | 21/1927 [29:01<43:07:14, 81.45s/it]
  1%|          | 22/1927 [30:22<43:02:32, 81.34s/it]
  1%|          | 23/1927 [31:43<42:56:52, 81.20s/it]
  1%|          | 24/1927 [33:04<42:54:40, 81.18s/it]
  1%|▏         | 25/1927 [34:25<42:48:32, 81.03s/it]
  1%|▏         | 26/1927 [35:46<42:45:09, 80.96s/it]
  1%|▏         | 27/1927 [37:07<42:46:25, 81.04s/it]
  1%|▏         | 28/1927 [38:28<42:40:47, 80.91s/it]
  2%|▏         | 29/1927 [39:48<42:37:57, 80.86s/it]
  2%|▏         | 30/1927 [41:09<42:35:09, 80.82s/it]
                                                    
{'loss': 1.6111, 'learning_rate': 4.997010474488265e-05, 'epoch': 0.02}

  2%|▏         | 30/1927 [41:09<42:35:09, 80.82s/it]
  2%|▏         | 31/1927 [42:30<42:31:51, 80.75s/it]
  2%|▏         | 32/1927 [43:51<42:33:44, 80.86s/it]
  2%|▏         | 33/1927 [45:12<42:37:00, 81.00s/it]
  2%|▏         | 34/1927 [46:33<42:34:36, 80.97s/it]
  2%|▏         | 35/1927 [47:54<42:34:09, 81.00s/it]
  2%|▏         | 36/1927 [49:15<42:30:36, 80.93s/it]
  2%|▏         | 37/1927 [50:36<42:27:00, 80.86s/it]
  2%|▏         | 38/1927 [51:57<42:26:18, 80.88s/it]
  2%|▏         | 39/1927 [53:17<42:23:03, 80.82s/it]
  2%|▏         | 40/1927 [54:38<42:19:29, 80.75s/it]
                                                    
{'loss': 1.5963, 'learning_rate': 4.9946861119715796e-05, 'epoch': 0.02}

  2%|▏         | 40/1927 [54:38<42:19:29, 80.75s/it]
  2%|▏         | 41/1927 [55:58<42:17:10, 80.72s/it]
  2%|▏         | 42/1927 [57:19<42:13:58, 80.66s/it]
  2%|▏         | 43/1927 [58:40<42:15:02, 80.73s/it]
  2%|▏         | 44/1927 [1:00:01<42:13:32, 80.73s/it]
  2%|▏         | 45/1927 [1:01:21<42:11:13, 80.70s/it]
  2%|▏         | 46/1927 [1:02:42<42:07:48, 80.63s/it]
  2%|▏         | 47/1927 [1:04:02<42:06:44, 80.64s/it]
  2%|▏         | 48/1927 [1:05:23<42:02:41, 80.55s/it]
  3%|▎         | 49/1927 [1:06:43<42:00:36, 80.53s/it]
  3%|▎         | 50/1927 [1:08:04<42:01:01, 80.59s/it]
                                                      
{'loss': 1.5866, 'learning_rate': 4.991698705062904e-05, 'epoch': 0.03}

  3%|▎         | 50/1927 [1:08:04<42:01:01, 80.59s/it]
  3%|▎         | 51/1927 [1:09:24<41:59:17, 80.57s/it]
  3%|▎         | 52/1927 [1:10:45<42:00:06, 80.64s/it]
  3%|▎         | 53/1927 [1:12:06<41:57:05, 80.59s/it]
  3%|▎         | 54/1927 [1:13:26<41:56:12, 80.60s/it]
  3%|▎         | 55/1927 [1:14:47<41:53:10, 80.55s/it]
  3%|▎         | 56/1927 [1:16:07<41:52:24, 80.57s/it]
  3%|▎         | 57/1927 [1:17:28<41:52:03, 80.60s/it]
  3%|▎         | 58/1927 [1:18:49<41:50:33, 80.60s/it]
  3%|▎         | 59/1927 [1:20:09<41:51:23, 80.67s/it]
  3%|▎         | 60/1927 [1:21:30<41:50:47, 80.69s/it]
                                                      
{'loss': 1.571, 'learning_rate': 4.988049047763289e-05, 'epoch': 0.03}

  3%|▎         | 60/1927 [1:21:30<41:50:47, 80.69s/it]
  3%|▎         | 61/1927 [1:22:51<41:50:06, 80.71s/it]
  3%|▎         | 62/1927 [1:24:12<41:49:08, 80.72s/it]
  3%|▎         | 63/1927 [1:25:33<41:49:50, 80.79s/it]
  3%|▎         | 64/1927 [1:26:53<41:46:01, 80.71s/it]
  3%|▎         | 65/1927 [1:28:14<41:44:07, 80.69s/it]
  3%|▎         | 66/1927 [1:29:34<41:40:47, 80.63s/it]
  3%|▎         | 67/1927 [1:30:55<41:38:12, 80.59s/it]
  4%|▎         | 68/1927 [1:32:15<41:34:57, 80.53s/it]
  4%|▎         | 69/1927 [1:33:36<41:34:20, 80.55s/it]
  4%|▎         | 70/1927 [1:34:56<41:32:50, 80.54s/it]
                                                      
{'loss': 1.5629, 'learning_rate': 4.983738110088481e-05, 'epoch': 0.04}

  4%|▎         | 70/1927 [1:34:56<41:32:50, 80.54s/it]
  4%|▎         | 71/1927 [1:36:17<41:32:59, 80.59s/it]
  4%|▎         | 72/1927 [1:37:37<41:29:42, 80.53s/it]
  4%|▍         | 73/1927 [1:38:58<41:30:30, 80.60s/it]
  4%|▍         | 74/1927 [1:40:19<41:28:03, 80.56s/it]
  4%|▍         | 75/1927 [1:41:39<41:28:36, 80.62s/it]
  4%|▍         | 76/1927 [1:43:00<41:25:40, 80.57s/it]
  4%|▍         | 77/1927 [1:44:21<41:25:30, 80.61s/it]
  4%|▍         | 78/1927 [1:45:41<41:24:56, 80.64s/it]
  4%|▍         | 79/1927 [1:47:02<41:25:28, 80.70s/it]
  4%|▍         | 80/1927 [1:48:23<41:22:29, 80.64s/it]
                                                      
{'loss': 1.5571, 'learning_rate': 4.9787670378111004e-05, 'epoch': 0.04}

  4%|▍         | 80/1927 [1:48:23<41:22:29, 80.64s/it]
  4%|▍         | 81/1927 [1:49:44<41:23:33, 80.72s/it]
  4%|▍         | 82/1927 [1:51:04<41:19:51, 80.65s/it]
  4%|▍         | 83/1927 [1:52:25<41:20:08, 80.70s/it]
  4%|▍         | 84/1927 [1:53:46<41:21:40, 80.79s/it]
  4%|▍         | 85/1927 [1:55:07<41:19:20, 80.76s/it]
  4%|▍         | 86/1927 [1:56:27<41:14:05, 80.63s/it]
  5%|▍         | 87/1927 [1:57:47<41:12:18, 80.62s/it]
  5%|▍         | 88/1927 [1:59:08<41:10:37, 80.61s/it]
  5%|▍         | 89/1927 [2:00:28<41:07:58, 80.57s/it]
  5%|▍         | 90/1927 [2:01:49<41:07:13, 80.58s/it]
                                                      
{'loss': 1.5468, 'learning_rate': 4.9731371521561246e-05, 'epoch': 0.05}

  5%|▍         | 90/1927 [2:01:49<41:07:13, 80.58s/it]
  5%|▍         | 91/1927 [2:03:10<41:06:42, 80.61s/it]
  5%|▍         | 92/1927 [2:04:30<41:05:07, 80.60s/it]
  5%|▍         | 93/1927 [2:05:51<41:04:20, 80.62s/it]
  5%|▍         | 94/1927 [2:07:12<41:02:01, 80.59s/it]
  5%|▍         | 95/1927 [2:08:33<41:05:25, 80.75s/it]
  5%|▍         | 96/1927 [2:09:54<41:07:06, 80.84s/it]
  5%|▌         | 97/1927 [2:11:14<41:02:50, 80.75s/it]
  5%|▌         | 98/1927 [2:12:35<41:02:26, 80.78s/it]
  5%|▌         | 99/1927 [2:13:56<40:59:32, 80.73s/it]
  5%|▌         | 100/1927 [2:15:16<40:57:57, 80.72s/it]
                                                       
{'loss': 1.5341, 'learning_rate': 4.9668499494497195e-05, 'epoch': 0.05}

  5%|▌         | 100/1927 [2:15:17<40:57:57, 80.72s/it]
  5%|▌         | 101/1927 [2:16:37<40:58:19, 80.78s/it]
  5%|▌         | 102/1927 [2:17:58<40:54:10, 80.69s/it]
  5%|▌         | 103/1927 [2:19:18<40:52:44, 80.68s/it]
  5%|▌         | 104/1927 [2:20:39<40:48:11, 80.58s/it]
  5%|▌         | 105/1927 [2:21:59<40:45:46, 80.54s/it]
  6%|▌         | 106/1927 [2:23:20<40:45:05, 80.56s/it]
  6%|▌         | 107/1927 [2:24:40<40:43:37, 80.56s/it]
  6%|▌         | 108/1927 [2:26:01<40:43:09, 80.59s/it]
  6%|▌         | 109/1927 [2:27:22<40:40:33, 80.55s/it]
  6%|▌         | 110/1927 [2:28:42<40:39:30, 80.56s/it]
                                                       
{'loss': 1.5389, 'learning_rate': 4.9599071007215506e-05, 'epoch': 0.06}

  6%|▌         | 110/1927 [2:28:42<40:39:30, 80.56s/it]
  6%|▌         | 111/1927 [2:30:03<40:37:19, 80.53s/it]
  6%|▌         | 112/1927 [2:31:23<40:38:21, 80.61s/it]
  6%|▌         | 113/1927 [2:32:44<40:36:39, 80.60s/it]
  6%|▌         | 114/1927 [2:34:05<40:37:46, 80.68s/it]
  6%|▌         | 115/1927 [2:35:26<40:37:08, 80.70s/it]
  6%|▌         | 116/1927 [2:36:46<40:36:54, 80.74s/it]
  6%|▌         | 117/1927 [2:38:07<40:34:06, 80.69s/it]
  6%|▌         | 118/1927 [2:39:28<40:34:06, 80.73s/it]
  6%|▌         | 119/1927 [2:40:48<40:30:37, 80.66s/it]
  6%|▌         | 120/1927 [2:42:09<40:27:33, 80.61s/it]
                                                       
{'loss': 1.5275, 'learning_rate': 4.9523104512606475e-05, 'epoch': 0.06}

  6%|▌         | 120/1927 [2:42:09<40:27:33, 80.61s/it]
  6%|▋         | 121/1927 [2:43:30<40:27:54, 80.66s/it]
  6%|▋         | 122/1927 [2:44:50<40:25:07, 80.61s/it]
  6%|▋         | 123/1927 [2:46:11<40:23:07, 80.59s/it]
  6%|▋         | 124/1927 [2:47:31<40:20:32, 80.55s/it]
  6%|▋         | 125/1927 [2:48:51<40:17:34, 80.50s/it]
  7%|▋         | 126/1927 [2:50:12<40:16:28, 80.50s/it]
  7%|▋         | 127/1927 [2:51:32<40:15:19, 80.51s/it]
  7%|▋         | 128/1927 [2:52:53<40:14:17, 80.52s/it]
  7%|▋         | 129/1927 [2:54:14<40:17:19, 80.67s/it]
  7%|▋         | 130/1927 [2:55:35<40:14:25, 80.62s/it]
                                                       
{'loss': 1.522, 'learning_rate': 4.9440620201249564e-05, 'epoch': 0.07}

  7%|▋         | 130/1927 [2:55:35<40:14:25, 80.62s/it]
  7%|▋         | 131/1927 [2:56:56<40:17:23, 80.76s/it]
  7%|▋         | 132/1927 [2:58:16<40:13:59, 80.69s/it]
  7%|▋         | 133/1927 [2:59:37<40:13:28, 80.72s/it]
  7%|▋         | 134/1927 [3:00:57<40:10:11, 80.65s/it]
  7%|▋         | 135/1927 [3:02:18<40:07:36, 80.61s/it]
  7%|▋         | 136/1927 [3:03:39<40:06:22, 80.62s/it]
  7%|▋         | 137/1927 [3:04:59<40:04:34, 80.60s/it]
  7%|▋         | 138/1927 [3:06:20<40:06:36, 80.71s/it]
  7%|▋         | 139/1927 [3:07:41<40:05:58, 80.74s/it]
  7%|▋         | 140/1927 [3:09:02<40:05:16, 80.76s/it]
                                                       
{'loss': 1.5227, 'learning_rate': 4.9351639996047175e-05, 'epoch': 0.07}

  7%|▋         | 140/1927 [3:09:02<40:05:16, 80.76s/it]
  7%|▋         | 141/1927 [3:10:22<40:01:09, 80.67s/it]
  7%|▋         | 142/1927 [3:11:43<39:58:03, 80.61s/it]
  7%|▋         | 143/1927 [3:13:04<40:01:53, 80.78s/it]
  7%|▋         | 144/1927 [3:14:24<39:59:17, 80.74s/it]
  8%|▊         | 145/1927 [3:15:46<40:00:40, 80.83s/it]
  8%|▊         | 146/1927 [3:17:06<39:57:30, 80.77s/it]
  8%|▊         | 147/1927 [3:18:27<39:56:33, 80.78s/it]
  8%|▊         | 148/1927 [3:19:48<39:55:14, 80.78s/it]
  8%|▊         | 149/1927 [3:21:09<39:53:47, 80.78s/it]
  8%|▊         | 150/1927 [3:22:29<39:50:42, 80.72s/it]
                                                       
{'loss': 1.5147, 'learning_rate': 4.92561875463978e-05, 'epoch': 0.08}

  8%|▊         | 150/1927 [3:22:29<39:50:42, 80.72s/it]
  8%|▊         | 151/1927 [3:23:50<39:50:00, 80.74s/it]
  8%|▊         | 152/1927 [3:25:11<39:49:01, 80.76s/it]
  8%|▊         | 153/1927 [3:26:31<39:47:14, 80.74s/it]
  8%|▊         | 154/1927 [3:27:52<39:45:52, 80.74s/it]
  8%|▊         | 155/1927 [3:29:13<39:44:58, 80.76s/it]
  8%|▊         | 156/1927 [3:30:34<39:44:50, 80.80s/it]
  8%|▊         | 157/1927 [3:31:54<39:41:49, 80.74s/it]
  8%|▊         | 158/1927 [3:33:15<39:40:10, 80.73s/it]
  8%|▊         | 159/1927 [3:34:36<39:41:42, 80.83s/it]
  8%|▊         | 160/1927 [3:35:57<39:37:18, 80.72s/it]
                                                       
{'loss': 1.5027, 'learning_rate': 4.9154288221910535e-05, 'epoch': 0.08}

  8%|▊         | 160/1927 [3:35:57<39:37:18, 80.72s/it]
  8%|▊         | 161/1927 [3:37:17<39:36:21, 80.74s/it]
  8%|▊         | 162/1927 [3:38:38<39:35:41, 80.76s/it]
  8%|▊         | 163/1927 [3:39:59<39:33:28, 80.73s/it]
  9%|▊         | 164/1927 [3:41:19<39:29:16, 80.63s/it]
  9%|▊         | 165/1927 [3:42:40<39:28:32, 80.65s/it]
  9%|▊         | 166/1927 [3:44:00<39:25:09, 80.58s/it]
  9%|▊         | 167/1927 [3:45:21<39:25:25, 80.64s/it]
  9%|▊         | 168/1927 [3:46:42<39:23:02, 80.60s/it]
  9%|▉         | 169/1927 [3:48:02<39:20:28, 80.56s/it]
  9%|▉         | 170/1927 [3:49:23<39:19:33, 80.58s/it]
                                                       
{'loss': 1.5084, 'learning_rate': 4.904596910566222e-05, 'epoch': 0.09}

  9%|▉         | 170/1927 [3:49:23<39:19:33, 80.58s/it]
  9%|▉         | 171/1927 [3:50:43<39:17:45, 80.56s/it]
  9%|▉         | 172/1927 [3:52:04<39:16:29, 80.56s/it]
  9%|▉         | 173/1927 [3:53:24<39:14:49, 80.55s/it]
  9%|▉         | 174/1927 [3:54:45<39:14:35, 80.59s/it]
  9%|▉         | 175/1927 [3:56:06<39:12:08, 80.55s/it]
  9%|▉         | 176/1927 [3:57:26<39:10:40, 80.55s/it]
  9%|▉         | 177/1927 [3:58:47<39:09:30, 80.55s/it]
  9%|▉         | 178/1927 [4:00:07<39:09:13, 80.59s/it]
  9%|▉         | 179/1927 [4:01:28<39:07:42, 80.59s/it]
  9%|▉         | 180/1927 [4:02:48<39:04:36, 80.52s/it]
                                                       
{'loss': 1.5035, 'learning_rate': 4.893125898699922e-05, 'epoch': 0.09}

  9%|▉         | 180/1927 [4:02:48<39:04:36, 80.52s/it]
  9%|▉         | 181/1927 [4:04:09<39:03:27, 80.53s/it]
  9%|▉         | 182/1927 [4:05:30<39:03:41, 80.59s/it]
  9%|▉         | 183/1927 [4:06:50<39:02:40, 80.60s/it]
 10%|▉         | 184/1927 [4:08:11<39:01:38, 80.61s/it]
 10%|▉         | 185/1927 [4:09:31<38:59:37, 80.58s/it]
 10%|▉         | 186/1927 [4:10:52<38:58:20, 80.59s/it]
 10%|▉         | 187/1927 [4:12:12<38:55:04, 80.52s/it]
 10%|▉         | 188/1927 [4:13:33<38:54:38, 80.55s/it]
 10%|▉         | 189/1927 [4:14:53<38:52:22, 80.52s/it]
 10%|▉         | 190/1927 [4:16:14<38:50:47, 80.51s/it]
                                                       
{'loss': 1.4939, 'learning_rate': 4.881018835388575e-05, 'epoch': 0.1}

 10%|▉         | 190/1927 [4:16:14<38:50:47, 80.51s/it]
 10%|▉         | 191/1927 [4:17:35<38:54:40, 80.69s/it]
 10%|▉         | 192/1927 [4:18:56<38:54:54, 80.75s/it]
 10%|█         | 193/1927 [4:20:16<38:52:01, 80.69s/it]
 10%|█         | 194/1927 [4:21:37<38:49:36, 80.66s/it]
 10%|█         | 195/1927 [4:22:58<38:49:23, 80.69s/it]
 10%|█         | 196/1927 [4:24:18<38:46:32, 80.64s/it]
 10%|█         | 197/1927 [4:25:39<38:43:56, 80.60s/it]
 10%|█         | 198/1927 [4:26:59<38:41:45, 80.57s/it]
 10%|█         | 199/1927 [4:28:20<38:39:45, 80.55s/it]
 10%|█         | 200/1927 [4:29:40<38:38:02, 80.53s/it]
                                                       
{'loss': 1.4903, 'learning_rate': 4.868278938480066e-05, 'epoch': 0.1}

 10%|█         | 200/1927 [4:29:40<38:38:02, 80.53s/it]
 10%|█         | 201/1927 [4:31:01<38:35:28, 80.49s/it]
 10%|█         | 202/1927 [4:32:21<38:33:34, 80.47s/it]
 11%|█         | 203/1927 [4:33:42<38:32:29, 80.48s/it]
 11%|█         | 204/1927 [4:35:02<38:32:56, 80.54s/it]
 11%|█         | 205/1927 [4:36:23<38:30:52, 80.52s/it]
 11%|█         | 206/1927 [4:37:44<38:31:58, 80.60s/it]
 11%|█         | 207/1927 [4:39:04<38:29:45, 80.57s/it]
 11%|█         | 208/1927 [4:40:25<38:29:35, 80.61s/it]
 11%|█         | 209/1927 [4:41:45<38:28:15, 80.61s/it]
 11%|█         | 210/1927 [4:43:06<38:25:03, 80.55s/it]
                                                       
{'loss': 1.4906, 'learning_rate': 4.8549095940184995e-05, 'epoch': 0.11}

 11%|█         | 210/1927 [4:43:06<38:25:03, 80.55s/it]
 11%|█         | 211/1927 [4:44:27<38:25:15, 80.60s/it]
 11%|█         | 212/1927 [4:45:47<38:24:11, 80.61s/it]
 11%|█         | 213/1927 [4:47:08<38:22:19, 80.59s/it]
 11%|█         | 214/1927 [4:48:28<38:19:20, 80.54s/it]
 11%|█         | 215/1927 [4:49:48<38:16:49, 80.50s/it]
 11%|█         | 216/1927 [4:51:09<38:15:42, 80.50s/it]
 11%|█▏        | 217/1927 [4:52:29<38:12:01, 80.42s/it]
 11%|█▏        | 218/1927 [4:53:50<38:14:37, 80.56s/it]
 11%|█▏        | 219/1927 [4:55:10<38:11:29, 80.50s/it]
 11%|█▏        | 220/1927 [4:56:31<38:11:59, 80.56s/it]
                                                       
{'loss': 1.4884, 'learning_rate': 4.840914355344244e-05, 'epoch': 0.11}

 11%|█▏        | 220/1927 [4:56:31<38:11:59, 80.56s/it]
 11%|█▏        | 221/1927 [4:57:52<38:09:58, 80.54s/it]
 12%|█▏        | 222/1927 [4:59:12<38:06:16, 80.46s/it]
 12%|█▏        | 223/1927 [5:00:32<38:05:32, 80.48s/it]
 12%|█▏        | 224/1927 [5:01:53<38:04:06, 80.47s/it]
 12%|█▏        | 225/1927 [5:03:14<38:05:20, 80.56s/it]
 12%|█▏        | 226/1927 [5:04:34<38:05:01, 80.60s/it]
 12%|█▏        | 227/1927 [5:05:55<38:06:34, 80.70s/it]
 12%|█▏        | 228/1927 [5:07:16<38:02:37, 80.61s/it]
 12%|█▏        | 229/1927 [5:08:36<38:00:45, 80.59s/it]
 12%|█▏        | 230/1927 [5:09:57<37:59:16, 80.59s/it]
                                                       
{'loss': 1.4807, 'learning_rate': 4.826296942149517e-05, 'epoch': 0.12}

 12%|█▏        | 230/1927 [5:09:57<37:59:16, 80.59s/it]
 12%|█▏        | 231/1927 [5:11:17<37:55:55, 80.52s/it]
 12%|█▏        | 232/1927 [5:12:38<37:57:10, 80.61s/it]
 12%|█▏        | 233/1927 [5:13:58<37:53:40, 80.53s/it]
 12%|█▏        | 234/1927 [5:15:19<37:52:56, 80.55s/it]
 12%|█▏        | 235/1927 [5:16:40<37:51:26, 80.55s/it]
 12%|█▏        | 236/1927 [5:18:00<37:53:25, 80.67s/it]
 12%|█▏        | 237/1927 [5:19:21<37:51:27, 80.64s/it]
 12%|█▏        | 238/1927 [5:20:42<37:49:12, 80.61s/it]
 12%|█▏        | 239/1927 [5:22:02<37:46:49, 80.57s/it]
 12%|█▏        | 240/1927 [5:23:23<37:45:31, 80.58s/it]
                                                       
{'loss': 1.4852, 'learning_rate': 4.811061239489758e-05, 'epoch': 0.12}

 12%|█▏        | 240/1927 [5:23:23<37:45:31, 80.58s/it]
 13%|█▎        | 241/1927 [5:24:43<37:43:25, 80.55s/it]
 13%|█▎        | 242/1927 [5:26:04<37:42:34, 80.57s/it]
 13%|█▎        | 243/1927 [5:27:24<37:41:55, 80.59s/it]
 13%|█▎        | 244/1927 [5:28:45<37:43:06, 80.68s/it]
 13%|█▎        | 245/1927 [5:30:06<37:39:26, 80.60s/it]
 13%|█▎        | 246/1927 [5:31:26<37:37:58, 80.59s/it]
 13%|█▎        | 247/1927 [5:32:47<37:34:45, 80.53s/it]
 13%|█▎        | 248/1927 [5:34:07<37:34:06, 80.55s/it]
 13%|█▎        | 249/1927 [5:35:28<37:31:18, 80.50s/it]
 13%|█▎        | 250/1927 [5:36:48<37:29:02, 80.47s/it]
                                                       
{'loss': 1.4728, 'learning_rate': 4.795211296751042e-05, 'epoch': 0.13}

 13%|█▎        | 250/1927 [5:36:48<37:29:02, 80.47s/it]
 13%|█▎        | 251/1927 [5:38:09<37:28:46, 80.50s/it]
 13%|█▎        | 252/1927 [5:39:29<37:28:27, 80.54s/it]
 13%|█▎        | 253/1927 [5:40:50<37:28:17, 80.58s/it]
 13%|█▎        | 254/1927 [5:42:10<37:25:46, 80.54s/it]
 13%|█▎        | 255/1927 [5:43:31<37:24:01, 80.53s/it]
 13%|█▎        | 256/1927 [5:44:51<37:21:33, 80.49s/it]
 13%|█▎        | 257/1927 [5:46:12<37:20:05, 80.48s/it]
 13%|█▎        | 258/1927 [5:47:32<37:20:23, 80.54s/it]
 13%|█▎        | 259/1927 [5:48:53<37:19:07, 80.54s/it]
 13%|█▎        | 260/1927 [5:50:14<37:19:18, 80.60s/it]
                                                       
{'loss': 1.4764, 'learning_rate': 4.7787513265738276e-05, 'epoch': 0.13}

 13%|█▎        | 260/1927 [5:50:14<37:19:18, 80.60s/it]
 14%|█▎        | 261/1927 [5:51:34<37:17:09, 80.57s/it]
 14%|█▎        | 262/1927 [5:52:55<37:14:59, 80.54s/it]
 14%|█▎        | 263/1927 [5:54:15<37:14:45, 80.58s/it]
 14%|█▎        | 264/1927 [5:55:36<37:13:01, 80.57s/it]
 14%|█▍        | 265/1927 [5:56:56<37:11:11, 80.55s/it]
 14%|█▍        | 266/1927 [5:58:17<37:09:30, 80.54s/it]
 14%|█▍        | 267/1927 [5:59:37<37:08:28, 80.55s/it]
 14%|█▍        | 268/1927 [6:00:58<37:05:52, 80.50s/it]
 14%|█▍        | 269/1927 [6:02:18<37:03:54, 80.48s/it]
 14%|█▍        | 270/1927 [6:03:39<37:01:09, 80.43s/it]
                                                       
{'loss': 1.4813, 'learning_rate': 4.761685703733308e-05, 'epoch': 0.14}

 14%|█▍        | 270/1927 [6:03:39<37:01:09, 80.43s/it]
 14%|█▍        | 271/1927 [6:04:59<37:00:28, 80.45s/it]
 14%|█▍        | 272/1927 [6:06:19<36:58:05, 80.41s/it]
 14%|█▍        | 273/1927 [6:07:40<36:57:19, 80.43s/it]
 14%|█▍        | 274/1927 [6:09:01<36:58:16, 80.52s/it]
 14%|█▍        | 275/1927 [6:10:21<36:56:26, 80.50s/it]
 14%|█▍        | 276/1927 [6:11:41<36:54:26, 80.48s/it]
 14%|█▍        | 277/1927 [6:13:02<36:53:22, 80.49s/it]
 14%|█▍        | 278/1927 [6:14:22<36:52:01, 80.49s/it]
 14%|█▍        | 279/1927 [6:15:43<36:50:41, 80.49s/it]
 15%|█▍        | 280/1927 [6:17:03<36:48:41, 80.46s/it]
                                                       
{'loss': 1.477, 'learning_rate': 4.744018963976673e-05, 'epoch': 0.15}

 15%|█▍        | 280/1927 [6:17:03<36:48:41, 80.46s/it]
 15%|█▍        | 281/1927 [6:18:24<36:46:28, 80.43s/it]
 15%|█▍        | 282/1927 [6:19:44<36:45:20, 80.44s/it]
 15%|█▍        | 283/1927 [6:21:05<36:45:09, 80.48s/it]
 15%|█▍        | 284/1927 [6:22:25<36:42:48, 80.44s/it]
 15%|█▍        | 285/1927 [6:23:46<36:43:20, 80.51s/it]
 15%|█▍        | 286/1927 [6:25:06<36:41:20, 80.49s/it]
 15%|█▍        | 287/1927 [6:26:27<36:39:47, 80.48s/it]
 15%|█▍        | 288/1927 [6:27:47<36:40:40, 80.56s/it]
 15%|█▍        | 289/1927 [6:29:08<36:39:47, 80.58s/it]
 15%|█▌        | 290/1927 [6:30:29<36:39:39, 80.62s/it]
                                                       
{'loss': 1.4706, 'learning_rate': 4.7257558028175844e-05, 'epoch': 0.15}

 15%|█▌        | 290/1927 [6:30:29<36:39:39, 80.62s/it]
 15%|█▌        | 291/1927 [6:31:49<36:37:47, 80.60s/it]
 15%|█▌        | 292/1927 [6:33:10<36:36:18, 80.60s/it]
 15%|█▌        | 293/1927 [6:34:30<36:33:59, 80.56s/it]
 15%|█▌        | 294/1927 [6:35:51<36:32:50, 80.57s/it]
 15%|█▌        | 295/1927 [6:37:12<36:33:59, 80.66s/it]
 15%|█▌        | 296/1927 [6:38:32<36:31:42, 80.63s/it]
 15%|█▌        | 297/1927 [6:39:53<36:29:05, 80.58s/it]
 15%|█▌        | 298/1927 [6:41:13<36:27:27, 80.57s/it]
 16%|█▌        | 299/1927 [6:42:34<36:25:12, 80.54s/it]
 16%|█▌        | 300/1927 [6:43:54<36:24:07, 80.55s/it]
                                                       
{'loss': 1.4586, 'learning_rate': 4.706901074288189e-05, 'epoch': 0.16}

 16%|█▌        | 300/1927 [6:43:55<36:24:07, 80.55s/it]
 16%|█▌        | 301/1927 [6:45:15<36:23:15, 80.56s/it]
 16%|█▌        | 302/1927 [6:46:36<36:21:44, 80.56s/it]
 16%|█▌        | 303/1927 [6:47:56<36:20:12, 80.55s/it]
 16%|█▌        | 304/1927 [6:49:16<36:16:55, 80.48s/it]
 16%|█▌        | 305/1927 [6:50:37<36:16:09, 80.50s/it]
 16%|█▌        | 306/1927 [6:51:58<36:16:29, 80.56s/it]
 16%|█▌        | 307/1927 [6:53:19<36:20:07, 80.75s/it]
 16%|█▌        | 308/1927 [6:54:39<36:16:24, 80.66s/it]
 16%|█▌        | 309/1927 [6:56:00<36:17:01, 80.73s/it]
 16%|█▌        | 310/1927 [6:57:21<36:13:45, 80.66s/it]
                                                       
{'loss': 1.4688, 'learning_rate': 4.687459789649005e-05, 'epoch': 0.16}

 16%|█▌        | 310/1927 [6:57:21<36:13:45, 80.66s/it]
 16%|█▌        | 311/1927 [6:58:41<36:12:50, 80.68s/it]
 16%|█▌        | 312/1927 [7:00:02<36:11:23, 80.67s/it]
 16%|█▌        | 313/1927 [7:01:23<36:09:48, 80.66s/it]
 16%|█▋        | 314/1927 [7:02:43<36:07:52, 80.64s/it]
 16%|█▋        | 315/1927 [7:04:04<36:06:45, 80.65s/it]
 16%|█▋        | 316/1927 [7:05:24<36:03:17, 80.57s/it]
 16%|█▋        | 317/1927 [7:06:45<36:00:12, 80.50s/it]
 17%|█▋        | 318/1927 [7:08:05<35:57:58, 80.47s/it]
 17%|█▋        | 319/1927 [7:09:25<35:54:46, 80.40s/it]
 17%|█▋        | 320/1927 [7:10:46<35:52:21, 80.36s/it]
                                                       
{'loss': 1.4539, 'learning_rate': 4.6674371160570087e-05, 'epoch': 0.17}

 17%|█▋        | 320/1927 [7:10:46<35:52:21, 80.36s/it]
 17%|█▋        | 321/1927 [7:12:06<35:52:11, 80.41s/it]
 17%|█▋        | 322/1927 [7:13:27<35:53:05, 80.49s/it]
 17%|█▋        | 323/1927 [7:14:47<35:50:59, 80.46s/it]
 17%|█▋        | 324/1927 [7:16:08<35:51:11, 80.52s/it]
 17%|█▋        | 325/1927 [7:17:28<35:47:27, 80.43s/it]
 17%|█▋        | 326/1927 [7:18:49<35:47:49, 80.49s/it]
 17%|█▋        | 327/1927 [7:20:09<35:46:54, 80.51s/it]
 17%|█▋        | 328/1927 [7:21:30<35:44:29, 80.47s/it]
 17%|█▋        | 329/1927 [7:22:50<35:44:10, 80.51s/it]
 17%|█▋        | 330/1927 [7:24:11<35:41:52, 80.47s/it]
                                                       
{'loss': 1.4553, 'learning_rate': 4.646838375192302e-05, 'epoch': 0.17}

 17%|█▋        | 330/1927 [7:24:11<35:41:52, 80.47s/it]
 17%|█▋        | 331/1927 [7:25:31<35:41:11, 80.50s/it]
 17%|█▋        | 332/1927 [7:26:52<35:41:01, 80.54s/it]
 17%|█▋        | 333/1927 [7:28:13<35:41:05, 80.59s/it]
 17%|█▋        | 334/1927 [7:29:33<35:38:17, 80.54s/it]
 17%|█▋        | 335/1927 [7:30:54<35:37:55, 80.57s/it]
 17%|█▋        | 336/1927 [7:32:14<35:36:39, 80.58s/it]
 17%|█▋        | 337/1927 [7:33:35<35:36:40, 80.63s/it]
 18%|█▊        | 338/1927 [7:34:55<35:32:55, 80.54s/it]
 18%|█▊        | 339/1927 [7:36:16<35:30:50, 80.51s/it]
 18%|█▊        | 340/1927 [7:37:37<35:32:05, 80.61s/it]
                                                       
{'loss': 1.4535, 'learning_rate': 4.625669041843695e-05, 'epoch': 0.18}

 18%|█▊        | 340/1927 [7:37:37<35:32:05, 80.61s/it]
 18%|█▊        | 341/1927 [7:38:57<35:31:15, 80.63s/it]
 18%|█▊        | 342/1927 [7:40:18<35:27:49, 80.55s/it]
 18%|█▊        | 343/1927 [7:41:38<35:26:31, 80.55s/it]
 18%|█▊        | 344/1927 [7:42:59<35:27:19, 80.63s/it]
 18%|█▊        | 345/1927 [7:44:20<35:25:10, 80.60s/it]
 18%|█▊        | 346/1927 [7:45:40<35:23:50, 80.60s/it]
 18%|█▊        | 347/1927 [7:47:01<35:24:12, 80.67s/it]
 18%|█▊        | 348/1927 [7:48:21<35:21:27, 80.61s/it]
 18%|█▊        | 349/1927 [7:49:42<35:18:38, 80.56s/it]
 18%|█▊        | 350/1927 [7:51:03<35:18:57, 80.62s/it]
                                                       
{'loss': 1.4576, 'learning_rate': 4.6039347424536064e-05, 'epoch': 0.18}

 18%|█▊        | 350/1927 [7:51:03<35:18:57, 80.62s/it]
 18%|█▊        | 351/1927 [7:52:23<35:18:41, 80.66s/it]
 18%|█▊        | 352/1927 [7:53:44<35:15:05, 80.57s/it]
 18%|█▊        | 353/1927 [7:55:04<35:14:32, 80.60s/it]
 18%|█▊        | 354/1927 [7:56:25<35:12:05, 80.56s/it]
 18%|█▊        | 355/1927 [7:57:45<35:10:11, 80.54s/it]
 18%|█▊        | 356/1927 [7:59:06<35:09:03, 80.55s/it]
 19%|█▊        | 357/1927 [8:00:26<35:06:46, 80.51s/it]
 19%|█▊        | 358/1927 [8:01:47<35:05:08, 80.50s/it]
 19%|█▊        | 359/1927 [8:03:07<35:02:51, 80.47s/it]
 19%|█▊        | 360/1927 [8:04:28<35:01:37, 80.47s/it]
                                                       
{'loss': 1.4429, 'learning_rate': 4.581641253622648e-05, 'epoch': 0.19}

 19%|█▊        | 360/1927 [8:04:28<35:01:37, 80.47s/it]
 19%|█▊        | 361/1927 [8:05:48<35:00:12, 80.47s/it]
 19%|█▉        | 362/1927 [8:07:09<34:58:38, 80.46s/it]
 19%|█▉        | 363/1927 [8:08:29<34:57:31, 80.47s/it]
 19%|█▉        | 364/1927 [8:09:50<34:55:58, 80.46s/it]
 19%|█▉        | 365/1927 [8:11:10<34:56:05, 80.52s/it]
 19%|█▉        | 366/1927 [8:12:31<34:54:59, 80.52s/it]
 19%|█▉        | 367/1927 [8:13:51<34:52:00, 80.46s/it]
 19%|█▉        | 368/1927 [8:15:11<34:49:32, 80.42s/it]
 19%|█▉        | 369/1927 [8:16:32<34:48:57, 80.45s/it]
 19%|█▉        | 370/1927 [8:17:52<34:48:20, 80.48s/it]
                                                       
{'loss': 1.4422, 'learning_rate': 4.5587945005743094e-05, 'epoch': 0.19}

 19%|█▉        | 370/1927 [8:17:52<34:48:20, 80.48s/it]
 19%|█▉        | 371/1927 [8:19:13<34:47:20, 80.49s/it]
 19%|█▉        | 372/1927 [8:20:33<34:45:10, 80.46s/it]
 19%|█▉        | 373/1927 [8:21:54<34:43:00, 80.43s/it]
 19%|█▉        | 374/1927 [8:23:14<34:41:35, 80.42s/it]
 19%|█▉        | 375/1927 [8:24:34<34:39:00, 80.37s/it]
 20%|█▉        | 376/1927 [8:25:55<34:37:33, 80.37s/it]
 20%|█▉        | 377/1927 [8:27:15<34:37:04, 80.40s/it]
 20%|█▉        | 378/1927 [8:28:36<34:36:41, 80.44s/it]
 20%|█▉        | 379/1927 [8:29:56<34:37:28, 80.52s/it]
 20%|█▉        | 380/1927 [8:31:17<34:35:07, 80.48s/it]
                                                       
{'loss': 1.4492, 'learning_rate': 4.535400555580132e-05, 'epoch': 0.2}

 20%|█▉        | 380/1927 [8:31:17<34:35:07, 80.48s/it]
 20%|█▉        | 381/1927 [8:32:37<34:33:47, 80.48s/it]
 20%|█▉        | 382/1927 [8:33:58<34:34:05, 80.55s/it]
 20%|█▉        | 383/1927 [8:35:19<34:35:13, 80.64s/it]
 20%|█▉        | 384/1927 [8:36:40<34:34:04, 80.65s/it]
 20%|█▉        | 385/1927 [8:38:00<34:32:20, 80.64s/it]
 20%|██        | 386/1927 [8:39:21<34:29:31, 80.58s/it]
 20%|██        | 387/1927 [8:40:41<34:29:04, 80.61s/it]
 20%|██        | 388/1927 [8:42:02<34:26:01, 80.55s/it]
 20%|██        | 389/1927 [8:43:22<34:24:52, 80.55s/it]
 20%|██        | 390/1927 [8:44:43<34:24:38, 80.60s/it]
                                                       
{'loss': 1.4391, 'learning_rate': 4.5114656363458067e-05, 'epoch': 0.2}

 20%|██        | 390/1927 [8:44:43<34:24:38, 80.60s/it]
 20%|██        | 391/1927 [8:46:04<34:24:02, 80.63s/it]
 20%|██        | 392/1927 [8:47:24<34:24:15, 80.69s/it]
 20%|██        | 393/1927 [8:48:45<34:24:03, 80.73s/it]
 20%|██        | 394/1927 [8:50:06<34:24:53, 80.82s/it]
 20%|██        | 395/1927 [8:51:27<34:21:26, 80.74s/it]
 21%|██        | 396/1927 [8:52:47<34:18:14, 80.66s/it]
 21%|██        | 397/1927 [8:54:08<34:16:20, 80.64s/it]
 21%|██        | 398/1927 [8:55:29<34:14:39, 80.63s/it]
 21%|██        | 399/1927 [8:56:49<34:11:19, 80.55s/it]
 21%|██        | 400/1927 [8:58:09<34:09:54, 80.55s/it]
                                                       
{'loss': 1.4382, 'learning_rate': 4.486996104358614e-05, 'epoch': 0.21}

 21%|██        | 400/1927 [8:58:10<34:09:54, 80.55s/it]
 21%|██        | 401/1927 [8:59:30<34:07:45, 80.51s/it]
 21%|██        | 402/1927 [9:00:51<34:09:02, 80.62s/it]
 21%|██        | 403/1927 [9:02:12<34:10:53, 80.74s/it]
 21%|██        | 404/1927 [9:03:32<34:08:01, 80.68s/it]
 21%|██        | 405/1927 [9:04:53<34:04:49, 80.61s/it]
 21%|██        | 406/1927 [9:06:13<34:02:05, 80.56s/it]
 21%|██        | 407/1927 [9:07:34<34:02:38, 80.63s/it]
 21%|██        | 408/1927 [9:08:55<34:00:32, 80.60s/it]
 21%|██        | 409/1927 [9:10:15<33:58:39, 80.58s/it]
 21%|██▏       | 410/1927 [9:11:36<33:59:53, 80.68s/it]
                                                       
{'loss': 1.4418, 'learning_rate': 4.4619984631966524e-05, 'epoch': 0.21}

 21%|██▏       | 410/1927 [9:11:36<33:59:53, 80.68s/it]
 21%|██▏       | 411/1927 [9:12:57<33:57:59, 80.66s/it]
 21%|██▏       | 412/1927 [9:14:17<33:56:56, 80.67s/it]
 21%|██▏       | 413/1927 [9:15:38<33:53:45, 80.60s/it]
 21%|██▏       | 414/1927 [9:16:58<33:53:00, 80.62s/it]
 22%|██▏       | 415/1927 [9:18:19<33:50:32, 80.58s/it]
 22%|██▏       | 416/1927 [9:19:39<33:47:59, 80.53s/it]
 22%|██▏       | 417/1927 [9:21:00<33:49:43, 80.65s/it]
 22%|██▏       | 418/1927 [9:22:21<33:45:47, 80.55s/it]
 22%|██▏       | 419/1927 [9:23:41<33:45:58, 80.61s/it]
 22%|██▏       | 420/1927 [9:25:02<33:43:50, 80.58s/it]
                                                       
{'loss': 1.443, 'learning_rate': 4.436479356800298e-05, 'epoch': 0.22}

 22%|██▏       | 420/1927 [9:25:02<33:43:50, 80.58s/it]
 22%|██▏       | 421/1927 [9:26:22<33:43:20, 80.61s/it]
 22%|██▏       | 422/1927 [9:27:43<33:43:12, 80.66s/it]
 22%|██▏       | 423/1927 [9:29:04<33:42:57, 80.70s/it]
 22%|██▏       | 424/1927 [9:30:25<33:41:12, 80.69s/it]
 22%|██▏       | 425/1927 [9:31:45<33:39:26, 80.67s/it]
 22%|██▏       | 426/1927 [9:33:06<33:37:30, 80.65s/it]
 22%|██▏       | 427/1927 [9:34:27<33:37:25, 80.70s/it]
 22%|██▏       | 428/1927 [9:35:47<33:34:44, 80.64s/it]
 22%|██▏       | 429/1927 [9:37:08<33:33:21, 80.64s/it]
 22%|██▏       | 430/1927 [9:38:29<33:32:01, 80.64s/it]
                                                       
{'loss': 1.4342, 'learning_rate': 4.4104455677063604e-05, 'epoch': 0.22}

 22%|██▏       | 430/1927 [9:38:29<33:32:01, 80.64s/it]
 22%|██▏       | 431/1927 [9:39:49<33:30:24, 80.63s/it]
 22%|██▏       | 432/1927 [9:41:10<33:28:02, 80.59s/it]
 22%|██▏       | 433/1927 [9:42:30<33:26:52, 80.60s/it]
 23%|██▎       | 434/1927 [9:43:51<33:24:38, 80.56s/it]
 23%|██▎       | 435/1927 [9:45:11<33:22:25, 80.53s/it]
 23%|██▎       | 436/1927 [9:46:32<33:22:52, 80.60s/it]
 23%|██▎       | 437/1927 [9:47:53<33:21:23, 80.59s/it]
 23%|██▎       | 438/1927 [9:49:13<33:17:34, 80.49s/it]
 23%|██▎       | 439/1927 [9:50:33<33:15:06, 80.45s/it]
 23%|██▎       | 440/1927 [9:51:53<33:12:37, 80.40s/it]
                                                       
{'loss': 1.4338, 'learning_rate': 4.3839040152454046e-05, 'epoch': 0.23}

 23%|██▎       | 440/1927 [9:51:53<33:12:37, 80.40s/it]
 23%|██▎       | 441/1927 [9:53:14<33:11:54, 80.43s/it]
 23%|██▎       | 442/1927 [9:54:34<33:09:49, 80.40s/it]
 23%|██▎       | 443/1927 [9:55:55<33:10:02, 80.46s/it]
 23%|██▎       | 444/1927 [9:57:15<33:07:46, 80.42s/it]
 23%|██▎       | 445/1927 [9:58:36<33:06:16, 80.42s/it]
 23%|██▎       | 446/1927 [9:59:56<33:06:33, 80.48s/it]
 23%|██▎       | 447/1927 [10:01:17<33:05:20, 80.49s/it]
 23%|██▎       | 448/1927 [10:02:37<33:03:05, 80.45s/it]
 23%|██▎       | 449/1927 [10:03:57<33:00:39, 80.41s/it]
 23%|██▎       | 450/1927 [10:05:18<33:00:00, 80.43s/it]
                                                        
{'loss': 1.4375, 'learning_rate': 4.3568617537027076e-05, 'epoch': 0.23}

 23%|██▎       | 450/1927 [10:05:18<33:00:00, 80.43s/it]
 23%|██▎       | 451/1927 [10:06:38<32:58:06, 80.41s/it]
 23%|██▎       | 452/1927 [10:07:59<32:57:28, 80.44s/it]
 24%|██▎       | 453/1927 [10:09:19<32:57:06, 80.48s/it]
 24%|██▎       | 454/1927 [10:10:40<32:56:53, 80.52s/it]
 24%|██▎       | 455/1927 [10:12:00<32:54:26, 80.48s/it]
 24%|██▎       | 456/1927 [10:13:21<32:55:24, 80.57s/it]
 24%|██▎       | 457/1927 [10:14:42<32:53:05, 80.53s/it]
 24%|██▍       | 458/1927 [10:16:02<32:51:13, 80.51s/it]
 24%|██▍       | 459/1927 [10:17:22<32:49:31, 80.50s/it]
 24%|██▍       | 460/1927 [10:18:43<32:47:01, 80.45s/it]
                                                        
{'loss': 1.4414, 'learning_rate': 4.3293259704433564e-05, 'epoch': 0.24}

 24%|██▍       | 460/1927 [10:18:43<32:47:01, 80.45s/it]
 24%|██▍       | 461/1927 [10:20:04<32:52:54, 80.75s/it]
 24%|██▍       | 462/1927 [10:21:25<32:48:13, 80.61s/it]
 24%|██▍       | 463/1927 [10:22:45<32:47:51, 80.65s/it]
 24%|██▍       | 464/1927 [10:24:06<32:44:53, 80.58s/it]
 24%|██▍       | 465/1927 [10:25:26<32:43:26, 80.58s/it]
 24%|██▍       | 466/1927 [10:26:47<32:43:18, 80.63s/it]
 24%|██▍       | 467/1927 [10:28:07<32:39:52, 80.54s/it]
 24%|██▍       | 468/1927 [10:29:28<32:37:14, 80.49s/it]
 24%|██▍       | 469/1927 [10:30:48<32:35:01, 80.45s/it]
 24%|██▍       | 470/1927 [10:32:09<32:36:12, 80.56s/it]
                                                        
{'loss': 1.4295, 'learning_rate': 4.301303984001967e-05, 'epoch': 0.24}

 24%|██▍       | 470/1927 [10:32:09<32:36:12, 80.56s/it]
 24%|██▍       | 471/1927 [10:33:30<32:35:03, 80.57s/it]
 24%|██▍       | 472/1927 [10:34:50<32:33:08, 80.54s/it]
 25%|██▍       | 473/1927 [10:36:10<32:29:55, 80.46s/it]
 25%|██▍       | 474/1927 [10:37:31<32:28:00, 80.44s/it]
 25%|██▍       | 475/1927 [10:38:51<32:27:06, 80.46s/it]
 25%|██▍       | 476/1927 [10:40:12<32:26:10, 80.48s/it]
 25%|██▍       | 477/1927 [10:41:32<32:23:47, 80.43s/it]
 25%|██▍       | 478/1927 [10:42:52<32:21:28, 80.39s/it]
 25%|██▍       | 479/1927 [10:44:13<32:24:05, 80.56s/it]
 25%|██▍       | 480/1927 [10:45:34<32:20:42, 80.47s/it]
                                                        
{'loss': 1.4305, 'learning_rate': 4.2728032421375495e-05, 'epoch': 0.25}

 25%|██▍       | 480/1927 [10:45:34<32:20:42, 80.47s/it]
 25%|██▍       | 481/1927 [10:46:54<32:19:42, 80.49s/it]
 25%|██▌       | 482/1927 [10:48:15<32:18:29, 80.49s/it]
 25%|██▌       | 483/1927 [10:49:35<32:17:44, 80.52s/it]
 25%|██▌       | 484/1927 [10:50:55<32:15:05, 80.46s/it]
 25%|██▌       | 485/1927 [10:52:16<32:14:19, 80.48s/it]
 25%|██▌       | 486/1927 [10:53:37<32:13:27, 80.51s/it]
 25%|██▌       | 487/1927 [10:54:57<32:12:12, 80.51s/it]
 25%|██▌       | 488/1927 [10:56:17<32:10:09, 80.48s/it]
 25%|██▌       | 489/1927 [10:57:38<32:08:30, 80.47s/it]
 25%|██▌       | 490/1927 [10:58:59<32:08:54, 80.54s/it]
                                                        
{'loss': 1.4291, 'learning_rate': 4.243831319854016e-05, 'epoch': 0.25}

 25%|██▌       | 490/1927 [10:58:59<32:08:54, 80.54s/it]
 25%|██▌       | 491/1927 [11:00:19<32:08:19, 80.57s/it]
 26%|██▌       | 492/1927 [11:01:40<32:06:30, 80.55s/it]
 26%|██▌       | 493/1927 [11:03:00<32:04:38, 80.53s/it]
 26%|██▌       | 494/1927 [11:04:21<32:05:42, 80.63s/it]
 26%|██▌       | 495/1927 [11:05:42<32:02:44, 80.56s/it]
 26%|██▌       | 496/1927 [11:07:02<31:59:22, 80.48s/it]
 26%|██▌       | 497/1927 [11:08:22<31:56:30, 80.41s/it]
 26%|██▌       | 498/1927 [11:09:43<31:55:44, 80.44s/it]
 26%|██▌       | 499/1927 [11:11:03<31:56:34, 80.53s/it]
 26%|██▌       | 500/1927 [11:12:24<31:53:27, 80.45s/it]
                                                        
{'loss': 1.4332, 'learning_rate': 4.21439591738688e-05, 'epoch': 0.26}

 26%|██▌       | 500/1927 [11:12:24<31:53:27, 80.45s/it]
 26%|██▌       | 501/1927 [11:13:44<31:52:03, 80.45s/it]
 26%|██▌       | 502/1927 [11:15:04<31:49:39, 80.41s/it]
 26%|██▌       | 503/1927 [11:16:25<31:49:08, 80.44s/it]
 26%|██▌       | 504/1927 [11:17:45<31:46:54, 80.40s/it]
 26%|██▌       | 505/1927 [11:19:06<31:45:32, 80.40s/it]
 26%|██▋       | 506/1927 [11:20:26<31:46:57, 80.52s/it]
 26%|██▋       | 507/1927 [11:21:47<31:45:27, 80.51s/it]
 26%|██▋       | 508/1927 [11:23:07<31:43:16, 80.48s/it]
 26%|██▋       | 509/1927 [11:24:28<31:44:28, 80.58s/it]
 26%|██▋       | 510/1927 [11:25:49<31:42:27, 80.56s/it]
                                                        
{'loss': 1.4274, 'learning_rate': 4.184504858156667e-05, 'epoch': 0.26}

 26%|██▋       | 510/1927 [11:25:49<31:42:27, 80.56s/it]
 27%|██▋       | 511/1927 [11:27:09<31:40:55, 80.55s/it]
 27%|██▋       | 512/1927 [11:28:30<31:40:33, 80.59s/it]
 27%|██▋       | 513/1927 [11:29:51<31:40:57, 80.66s/it]
 27%|██▋       | 514/1927 [11:31:12<31:42:20, 80.78s/it]
 27%|██▋       | 515/1927 [11:32:32<31:38:13, 80.66s/it]
 27%|██▋       | 516/1927 [11:33:53<31:37:32, 80.69s/it]
 27%|██▋       | 517/1927 [11:35:13<31:35:11, 80.65s/it]
 27%|██▋       | 518/1927 [11:36:34<31:32:19, 80.58s/it]
 27%|██▋       | 519/1927 [11:37:54<31:29:50, 80.53s/it]
 27%|██▋       | 520/1927 [11:39:15<31:28:24, 80.53s/it]
                                                        
{'loss': 1.4177, 'learning_rate': 4.154166086689583e-05, 'epoch': 0.27}

 27%|██▋       | 520/1927 [11:39:15<31:28:24, 80.53s/it]
 27%|██▋       | 521/1927 [11:40:35<31:27:12, 80.54s/it]
 27%|██▋       | 522/1927 [11:41:56<31:25:02, 80.50s/it]
 27%|██▋       | 523/1927 [11:43:16<31:24:50, 80.55s/it]
 27%|██▋       | 524/1927 [11:44:37<31:21:49, 80.48s/it]
 27%|██▋       | 525/1927 [11:45:57<31:20:44, 80.49s/it]
 27%|██▋       | 526/1927 [11:47:18<31:21:19, 80.57s/it]
 27%|██▋       | 527/1927 [11:48:39<31:21:37, 80.64s/it]
 27%|██▋       | 528/1927 [11:49:59<31:21:05, 80.68s/it]
 27%|██▋       | 529/1927 [11:51:20<31:18:54, 80.64s/it]
 28%|██▊       | 530/1927 [11:52:40<31:15:44, 80.56s/it]
                                                        
{'loss': 1.4161, 'learning_rate': 4.1233876665059986e-05, 'epoch': 0.27}

 28%|██▊       | 530/1927 [11:52:40<31:15:44, 80.56s/it]
 28%|██▊       | 531/1927 [11:54:01<31:14:50, 80.58s/it]
 28%|██▊       | 532/1927 [11:55:22<31:14:46, 80.64s/it]
 28%|██▊       | 533/1927 [11:56:42<31:12:57, 80.62s/it]
 28%|██▊       | 534/1927 [11:58:03<31:11:17, 80.60s/it]
 28%|██▊       | 535/1927 [11:59:23<31:07:45, 80.51s/it]
 28%|██▊       | 536/1927 [12:00:44<31:07:49, 80.57s/it]
 28%|██▊       | 537/1927 [12:02:04<31:04:37, 80.49s/it]
 28%|██▊       | 538/1927 [12:03:25<31:04:11, 80.53s/it]
 28%|██▊       | 539/1927 [12:04:45<31:02:44, 80.52s/it]
 28%|██▊       | 540/1927 [12:06:06<31:01:43, 80.54s/it]
                                                        
{'loss': 1.4222, 'learning_rate': 4.092177777977304e-05, 'epoch': 0.28}

 28%|██▊       | 540/1927 [12:06:06<31:01:43, 80.54s/it]
 28%|██▊       | 541/1927 [12:07:27<31:01:41, 80.59s/it]
 28%|██▊       | 542/1927 [12:08:48<31:02:28, 80.68s/it]
 28%|██▊       | 543/1927 [12:10:08<30:59:12, 80.60s/it]
 28%|██▊       | 544/1927 [12:11:29<30:57:43, 80.60s/it]
 28%|██▊       | 545/1927 [12:12:49<30:55:15, 80.55s/it]
 28%|██▊       | 546/1927 [12:14:09<30:52:59, 80.51s/it]
 28%|██▊       | 547/1927 [12:15:30<30:51:56, 80.52s/it]
 28%|██▊       | 548/1927 [12:16:50<30:50:15, 80.50s/it]
 28%|██▊       | 549/1927 [12:18:11<30:48:21, 80.48s/it]
 29%|██▊       | 550/1927 [12:19:31<30:46:18, 80.45s/it]
                                                        
{'loss': 1.4072, 'learning_rate': 4.0605447161517065e-05, 'epoch': 0.29}

 29%|██▊       | 550/1927 [12:19:31<30:46:18, 80.45s/it]
 29%|██▊       | 551/1927 [12:20:52<30:45:09, 80.46s/it]
 29%|██▊       | 552/1927 [12:22:12<30:43:21, 80.44s/it]
 29%|██▊       | 553/1927 [12:23:32<30:41:45, 80.43s/it]
 29%|██▊       | 554/1927 [12:24:53<30:41:07, 80.46s/it]
 29%|██▉       | 555/1927 [12:26:14<30:40:09, 80.47s/it]
 29%|██▉       | 556/1927 [12:27:34<30:40:05, 80.53s/it]
 29%|██▉       | 557/1927 [12:28:55<30:38:42, 80.53s/it]
 29%|██▉       | 558/1927 [12:30:16<30:39:33, 80.62s/it]
 29%|██▉       | 559/1927 [12:31:36<30:36:43, 80.56s/it]
 29%|██▉       | 560/1927 [12:32:56<30:34:54, 80.54s/it]
                                                        
{'loss': 1.4105, 'learning_rate': 4.028496888549553e-05, 'epoch': 0.29}

 29%|██▉       | 560/1927 [12:32:57<30:34:54, 80.54s/it]
 29%|██▉       | 561/1927 [12:34:17<30:34:06, 80.56s/it]
 29%|██▉       | 562/1927 [12:35:37<30:31:34, 80.51s/it]
 29%|██▉       | 563/1927 [12:36:58<30:30:03, 80.50s/it]
 29%|██▉       | 564/1927 [12:38:18<30:27:54, 80.47s/it]
 29%|██▉       | 565/1927 [12:39:39<30:28:00, 80.53s/it]
 29%|██▉       | 566/1927 [12:40:59<30:25:01, 80.46s/it]
 29%|██▉       | 567/1927 [12:42:20<30:24:47, 80.51s/it]
 29%|██▉       | 568/1927 [12:43:40<30:22:12, 80.45s/it]
 30%|██▉       | 569/1927 [12:45:01<30:20:47, 80.45s/it]
 30%|██▉       | 570/1927 [12:46:21<30:19:22, 80.44s/it]
                                                        
{'loss': 1.4249, 'learning_rate': 3.9960428129287566e-05, 'epoch': 0.3}

 30%|██▉       | 570/1927 [12:46:21<30:19:22, 80.44s/it]
 30%|██▉       | 571/1927 [12:47:42<30:19:14, 80.50s/it]
 30%|██▉       | 572/1927 [12:49:02<30:18:30, 80.52s/it]
 30%|██▉       | 573/1927 [12:50:23<30:21:04, 80.70s/it]
 30%|██▉       | 574/1927 [12:51:44<30:18:16, 80.63s/it]
 30%|██▉       | 575/1927 [12:53:05<30:18:05, 80.68s/it]
 30%|██▉       | 576/1927 [12:54:25<30:15:16, 80.62s/it]
 30%|██▉       | 577/1927 [12:55:45<30:10:57, 80.49s/it]
 30%|██▉       | 578/1927 [12:57:06<30:10:03, 80.51s/it]
 30%|███       | 579/1927 [12:58:26<30:08:08, 80.48s/it]
 30%|███       | 580/1927 [12:59:47<30:06:29, 80.47s/it]
                                                        
{'loss': 1.4142, 'learning_rate': 3.963191115020919e-05, 'epoch': 0.3}

 30%|███       | 580/1927 [12:59:47<30:06:29, 80.47s/it]
 30%|███       | 581/1927 [13:01:07<30:04:43, 80.45s/it]
 30%|███       | 582/1927 [13:02:28<30:02:58, 80.43s/it]
 30%|███       | 583/1927 [13:03:48<30:02:19, 80.46s/it]
 30%|███       | 584/1927 [13:05:09<30:00:58, 80.46s/it]
 30%|███       | 585/1927 [13:06:29<30:00:47, 80.51s/it]
 30%|███       | 586/1927 [13:07:50<30:02:01, 80.63s/it]
 30%|███       | 587/1927 [13:09:11<30:00:19, 80.61s/it]
 31%|███       | 588/1927 [13:10:31<29:58:12, 80.58s/it]
 31%|███       | 589/1927 [13:11:52<29:57:09, 80.59s/it]
 31%|███       | 590/1927 [13:13:12<29:55:27, 80.57s/it]
                                                        
{'loss': 1.4192, 'learning_rate': 3.929950526238767e-05, 'epoch': 0.31}

 31%|███       | 590/1927 [13:13:12<29:55:27, 80.57s/it]
 31%|███       | 591/1927 [13:14:33<29:56:22, 80.68s/it]
 31%|███       | 592/1927 [13:15:54<29:54:22, 80.65s/it]
 31%|███       | 593/1927 [13:17:14<29:51:44, 80.59s/it]
 31%|███       | 594/1927 [13:18:35<29:51:23, 80.63s/it]
 31%|███       | 595/1927 [13:19:56<29:52:06, 80.73s/it]
 31%|███       | 596/1927 [13:21:16<29:48:14, 80.61s/it]
 31%|███       | 597/1927 [13:22:37<29:45:24, 80.54s/it]
 31%|███       | 598/1927 [13:23:57<29:44:14, 80.55s/it]
 31%|███       | 599/1927 [13:25:18<29:41:38, 80.50s/it]
 31%|███       | 600/1927 [13:26:38<29:39:43, 80.47s/it]
                                                        
{'loss': 1.4125, 'learning_rate': 3.8963298813554906e-05, 'epoch': 0.31}

 31%|███       | 600/1927 [13:26:38<29:39:43, 80.47s/it]
 31%|███       | 601/1927 [13:27:59<29:38:40, 80.48s/it]
 31%|███       | 602/1927 [13:29:19<29:36:55, 80.46s/it]
 31%|███▏      | 603/1927 [13:30:40<29:36:18, 80.50s/it]
 31%|███▏      | 604/1927 [13:32:00<29:34:35, 80.48s/it]
 31%|███▏      | 605/1927 [13:33:20<29:32:15, 80.44s/it]
 31%|███▏      | 606/1927 [13:34:41<29:30:37, 80.42s/it]
 31%|███▏      | 607/1927 [13:36:01<29:29:32, 80.43s/it]
 32%|███▏      | 608/1927 [13:37:21<29:27:36, 80.41s/it]
 32%|███▏      | 609/1927 [13:38:42<29:29:54, 80.57s/it]
 32%|███▏      | 610/1927 [13:40:03<29:31:07, 80.69s/it]
                                                        
{'loss': 1.4037, 'learning_rate': 3.862338116156612e-05, 'epoch': 0.32}

 32%|███▏      | 610/1927 [13:40:03<29:31:07, 80.69s/it]
 32%|███▏      | 611/1927 [13:41:24<29:29:43, 80.69s/it]
 32%|███▏      | 612/1927 [13:42:45<29:26:39, 80.61s/it]
 32%|███▏      | 613/1927 [13:44:05<29:25:00, 80.59s/it]
 32%|███▏      | 614/1927 [13:45:25<29:22:16, 80.53s/it]
 32%|███▏      | 615/1927 [13:46:46<29:20:17, 80.50s/it]
 32%|███▏      | 616/1927 [13:48:06<29:19:37, 80.53s/it]
 32%|███▏      | 617/1927 [13:49:27<29:19:17, 80.58s/it]
 32%|███▏      | 618/1927 [13:50:48<29:17:42, 80.57s/it]
 32%|███▏      | 619/1927 [13:52:08<29:17:44, 80.63s/it]
 32%|███▏      | 620/1927 [13:53:29<29:15:33, 80.59s/it]
                                                        
{'loss': 1.4111, 'learning_rate': 3.827984265065013e-05, 'epoch': 0.32}

 32%|███▏      | 620/1927 [13:53:29<29:15:33, 80.59s/it]
 32%|███▏      | 621/1927 [13:54:50<29:15:27, 80.65s/it]
 32%|███▏      | 622/1927 [13:56:10<29:11:50, 80.54s/it]
 32%|███▏      | 623/1927 [13:57:31<29:10:24, 80.54s/it]
 32%|███▏      | 624/1927 [13:58:51<29:07:59, 80.49s/it]
 32%|███▏      | 625/1927 [14:00:11<29:05:37, 80.44s/it]
 32%|███▏      | 626/1927 [14:01:32<29:03:58, 80.43s/it]
 33%|███▎      | 627/1927 [14:02:52<29:01:55, 80.40s/it]
 33%|███▎      | 628/1927 [14:04:13<29:01:38, 80.45s/it]
 33%|███▎      | 629/1927 [14:05:33<28:59:30, 80.41s/it]
 33%|███▎      | 630/1927 [14:06:53<28:57:51, 80.39s/it]
                                                        
{'loss': 1.4143, 'learning_rate': 3.7932774587397365e-05, 'epoch': 0.33}

 33%|███▎      | 630/1927 [14:06:53<28:57:51, 80.39s/it]
 33%|███▎      | 631/1927 [14:08:14<28:56:34, 80.40s/it]
 33%|███▎      | 632/1927 [14:09:34<28:56:52, 80.47s/it]
 33%|███▎      | 633/1927 [14:10:55<28:54:52, 80.44s/it]
 33%|███▎      | 634/1927 [14:12:15<28:53:20, 80.43s/it]
 33%|███▎      | 635/1927 [14:13:36<28:53:25, 80.50s/it]
 33%|███▎      | 636/1927 [14:14:56<28:50:27, 80.42s/it]
 33%|███▎      | 637/1927 [14:16:17<28:49:45, 80.45s/it]
 33%|███▎      | 638/1927 [14:17:37<28:48:29, 80.46s/it]
 33%|███▎      | 639/1927 [14:18:58<28:48:34, 80.52s/it]
 33%|███▎      | 640/1927 [14:20:18<28:47:51, 80.55s/it]
                                                        
{'loss': 1.4073, 'learning_rate': 3.7582269216492185e-05, 'epoch': 0.33}

 33%|███▎      | 640/1927 [14:20:18<28:47:51, 80.55s/it]
 33%|███▎      | 641/1927 [14:21:39<28:45:43, 80.52s/it]
 33%|███▎      | 642/1927 [14:22:59<28:44:55, 80.54s/it]
 33%|███▎      | 643/1927 [14:24:20<28:42:52, 80.51s/it]
 33%|███▎      | 644/1927 [14:25:40<28:40:31, 80.46s/it]
 33%|███▎      | 645/1927 [14:27:01<28:39:23, 80.47s/it]
 34%|███▎      | 646/1927 [14:28:21<28:36:45, 80.41s/it]
 34%|███▎      | 647/1927 [14:29:41<28:36:38, 80.47s/it]
 34%|███▎      | 648/1927 [14:31:02<28:34:13, 80.42s/it]
 34%|███▎      | 649/1927 [14:32:23<28:35:17, 80.53s/it]
 34%|███▎      | 650/1927 [14:33:43<28:35:30, 80.60s/it]
                                                        
{'loss': 1.4133, 'learning_rate': 3.722841969619583e-05, 'epoch': 0.34}

 34%|███▎      | 650/1927 [14:33:43<28:35:30, 80.60s/it]
 34%|███▍      | 651/1927 [14:35:04<28:34:17, 80.61s/it]
 34%|███▍      | 652/1927 [14:36:25<28:33:13, 80.62s/it]
 34%|███▍      | 653/1927 [14:37:45<28:31:20, 80.60s/it]
 34%|███▍      | 654/1927 [14:39:06<28:30:09, 80.60s/it]
 34%|███▍      | 655/1927 [14:40:27<28:29:48, 80.65s/it]
 34%|███▍      | 656/1927 [14:41:47<28:27:19, 80.60s/it]
 34%|███▍      | 657/1927 [14:43:08<28:27:00, 80.65s/it]
 34%|███▍      | 658/1927 [14:44:28<28:25:14, 80.63s/it]
 34%|███▍      | 659/1927 [14:45:49<28:22:40, 80.57s/it]
 34%|███▍      | 660/1927 [14:47:09<28:19:50, 80.50s/it]
                                                        
{'loss': 1.409, 'learning_rate': 3.687132007358659e-05, 'epoch': 0.34}

 34%|███▍      | 660/1927 [14:47:09<28:19:50, 80.50s/it]
 34%|███▍      | 661/1927 [14:48:30<28:19:34, 80.55s/it]
 34%|███▍      | 662/1927 [14:49:50<28:17:17, 80.50s/it]
 34%|███▍      | 663/1927 [14:51:10<28:14:30, 80.44s/it]
 34%|███▍      | 664/1927 [14:52:31<28:11:50, 80.37s/it]
 35%|███▍      | 665/1927 [14:53:51<28:10:39, 80.38s/it]
 35%|███▍      | 666/1927 [14:55:11<28:08:47, 80.35s/it]
 35%|███▍      | 667/1927 [14:56:32<28:09:24, 80.45s/it]
 35%|███▍      | 668/1927 [14:57:52<28:07:41, 80.43s/it]
 35%|███▍      | 669/1927 [14:59:13<28:07:08, 80.47s/it]
 35%|███▍      | 670/1927 [15:00:33<28:04:48, 80.42s/it]
                                                        
{'loss': 1.4007, 'learning_rate': 3.6511065259563666e-05, 'epoch': 0.35}

 35%|███▍      | 670/1927 [15:00:33<28:04:48, 80.42s/it]
 35%|███▍      | 671/1927 [15:01:54<28:06:36, 80.57s/it]
 35%|███▍      | 672/1927 [15:03:15<28:03:38, 80.49s/it]
 35%|███▍      | 673/1927 [15:04:35<28:02:47, 80.52s/it]
 35%|███▍      | 674/1927 [15:05:55<28:00:29, 80.47s/it]
 35%|███▌      | 675/1927 [15:07:16<27:59:45, 80.50s/it]
 35%|███▌      | 676/1927 [15:08:36<27:57:13, 80.44s/it]
 35%|███▌      | 677/1927 [15:09:57<27:56:07, 80.45s/it]
 35%|███▌      | 678/1927 [15:11:17<27:55:55, 80.51s/it]
 35%|███▌      | 679/1927 [15:12:38<27:55:16, 80.54s/it]
 35%|███▌      | 680/1927 [15:13:59<27:54:23, 80.56s/it]
                                                        
{'loss': 1.403, 'learning_rate': 3.614775100362155e-05, 'epoch': 0.35}

 35%|███▌      | 680/1927 [15:13:59<27:54:23, 80.56s/it]
 35%|███▌      | 681/1927 [15:15:19<27:53:11, 80.57s/it]
 35%|███▌      | 682/1927 [15:16:40<27:50:56, 80.53s/it]
 35%|███▌      | 683/1927 [15:18:00<27:49:23, 80.52s/it]
 35%|███▌      | 684/1927 [15:19:21<27:46:44, 80.45s/it]
 36%|███▌      | 685/1927 [15:20:41<27:48:23, 80.60s/it]
 36%|███▌      | 686/1927 [15:22:02<27:47:33, 80.62s/it]
 36%|███▌      | 687/1927 [15:23:23<27:45:04, 80.57s/it]
 36%|███▌      | 688/1927 [15:24:43<27:42:11, 80.49s/it]
 36%|███▌      | 689/1927 [15:26:03<27:39:48, 80.44s/it]
 36%|███▌      | 690/1927 [15:27:24<27:38:48, 80.46s/it]
                                                        
{'loss': 1.4059, 'learning_rate': 3.578147386840145e-05, 'epoch': 0.36}

 36%|███▌      | 690/1927 [15:27:24<27:38:48, 80.46s/it]
 36%|███▌      | 691/1927 [15:28:44<27:38:31, 80.51s/it]
 36%|███▌      | 692/1927 [15:30:05<27:36:16, 80.47s/it]
 36%|███▌      | 693/1927 [15:31:25<27:34:59, 80.47s/it]
 36%|███▌      | 694/1927 [15:32:46<27:33:46, 80.48s/it]
 36%|███▌      | 695/1927 [15:34:06<27:32:09, 80.46s/it]
 36%|███▌      | 696/1927 [15:35:27<27:31:55, 80.52s/it]
 36%|███▌      | 697/1927 [15:36:47<27:31:26, 80.56s/it]
 36%|███▌      | 698/1927 [15:38:08<27:30:50, 80.59s/it]
 36%|███▋      | 699/1927 [15:39:29<27:28:48, 80.56s/it]
 36%|███▋      | 700/1927 [15:40:49<27:27:47, 80.58s/it]
                                                        
{'loss': 1.4006, 'learning_rate': 3.5412331204026586e-05, 'epoch': 0.36}

 36%|███▋      | 700/1927 [15:40:49<27:27:47, 80.58s/it]
 36%|███▋      | 701/1927 [15:42:10<27:24:58, 80.50s/it]
 36%|███▋      | 702/1927 [15:43:30<27:23:27, 80.50s/it]
 36%|███▋      | 703/1927 [15:44:50<27:20:51, 80.43s/it]
 37%|███▋      | 704/1927 [15:46:11<27:19:24, 80.43s/it]
 37%|███▋      | 705/1927 [15:47:31<27:17:40, 80.41s/it]
 37%|███▋      | 706/1927 [15:48:51<27:15:43, 80.38s/it]
 37%|███▋      | 707/1927 [15:50:12<27:14:55, 80.41s/it]
 37%|███▋      | 708/1927 [15:51:32<27:13:44, 80.41s/it]
 37%|███▋      | 709/1927 [15:52:53<27:13:39, 80.48s/it]
 37%|███▋      | 710/1927 [15:54:13<27:13:05, 80.51s/it]
                                                        
{'loss': 1.3996, 'learning_rate': 3.504042112222824e-05, 'epoch': 0.37}

 37%|███▋      | 710/1927 [15:54:14<27:13:05, 80.51s/it]
 37%|███▋      | 711/1927 [15:55:34<27:13:07, 80.58s/it]
 37%|███▋      | 712/1927 [15:56:55<27:12:01, 80.59s/it]
 37%|███▋      | 713/1927 [15:58:15<27:09:43, 80.55s/it]
 37%|███▋      | 714/1927 [15:59:36<27:09:33, 80.61s/it]
 37%|███▋      | 715/1927 [16:00:57<27:10:07, 80.70s/it]
 37%|███▋      | 716/1927 [16:02:18<27:07:53, 80.66s/it]
 37%|███▋      | 717/1927 [16:03:38<27:05:52, 80.62s/it]
 37%|███▋      | 718/1927 [16:04:59<27:08:31, 80.82s/it]
 37%|███▋      | 719/1927 [16:06:20<27:03:37, 80.64s/it]
 37%|███▋      | 720/1927 [16:07:40<27:01:55, 80.63s/it]
                                                        
{'loss': 1.3983, 'learning_rate': 3.466584247026936e-05, 'epoch': 0.37}

 37%|███▋      | 720/1927 [16:07:40<27:01:55, 80.63s/it]
 37%|███▋      | 721/1927 [16:09:01<27:00:10, 80.61s/it]
 37%|███▋      | 722/1927 [16:10:22<27:00:10, 80.67s/it]
 38%|███▊      | 723/1927 [16:11:42<26:59:38, 80.71s/it]
 38%|███▊      | 724/1927 [16:13:03<26:58:55, 80.74s/it]
 38%|███▊      | 725/1927 [16:14:24<26:55:53, 80.66s/it]
 38%|███▊      | 726/1927 [16:15:44<26:54:18, 80.65s/it]
 38%|███▊      | 727/1927 [16:17:05<26:52:44, 80.64s/it]
 38%|███▊      | 728/1927 [16:18:26<26:56:17, 80.88s/it]
 38%|███▊      | 729/1927 [16:19:47<26:52:04, 80.74s/it]
 38%|███▊      | 730/1927 [16:21:07<26:49:47, 80.69s/it]
                                                        
{'loss': 1.3995, 'learning_rate': 3.4288694804672646e-05, 'epoch': 0.38}

 38%|███▊      | 730/1927 [16:21:07<26:49:47, 80.69s/it]
 38%|███▊      | 731/1927 [16:22:28<26:49:27, 80.74s/it]
 38%|███▊      | 732/1927 [16:23:49<26:51:11, 80.90s/it]
 38%|███▊      | 733/1927 [16:25:10<26:49:33, 80.88s/it]
 38%|███▊      | 734/1927 [16:26:31<26:45:59, 80.77s/it]
 38%|███▊      | 735/1927 [16:27:51<26:42:55, 80.68s/it]
 38%|███▊      | 736/1927 [16:29:12<26:41:10, 80.66s/it]
 38%|███▊      | 737/1927 [16:30:32<26:38:40, 80.61s/it]
 38%|███▊      | 738/1927 [16:31:53<26:38:00, 80.64s/it]
 38%|███▊      | 739/1927 [16:33:14<26:35:37, 80.59s/it]
 38%|███▊      | 740/1927 [16:34:34<26:34:56, 80.62s/it]
                                                        
{'loss': 1.4016, 'learning_rate': 3.390907836476016e-05, 'epoch': 0.38}

 38%|███▊      | 740/1927 [16:34:34<26:34:56, 80.62s/it]
 38%|███▊      | 741/1927 [16:35:55<26:32:54, 80.59s/it]
 39%|███▊      | 742/1927 [16:37:16<26:32:58, 80.66s/it]
 39%|███▊      | 743/1927 [16:38:36<26:32:04, 80.68s/it]
 39%|███▊      | 744/1927 [16:39:57<26:30:18, 80.66s/it]
 39%|███▊      | 745/1927 [16:41:17<26:28:34, 80.64s/it]
 39%|███▊      | 746/1927 [16:42:38<26:25:58, 80.57s/it]
 39%|███▉      | 747/1927 [16:43:58<26:24:15, 80.56s/it]
 39%|███▉      | 748/1927 [16:45:19<26:21:35, 80.49s/it]
 39%|███▉      | 749/1927 [16:46:39<26:21:27, 80.55s/it]
 39%|███▉      | 750/1927 [16:48:00<26:18:52, 80.49s/it]
                                                        
{'loss': 1.3945, 'learning_rate': 3.352709404601145e-05, 'epoch': 0.39}

 39%|███▉      | 750/1927 [16:48:00<26:18:52, 80.49s/it]
 39%|███▉      | 751/1927 [16:49:20<26:17:53, 80.50s/it]
 39%|███▉      | 752/1927 [16:50:41<26:16:01, 80.48s/it]
 39%|███▉      | 753/1927 [16:52:01<26:15:58, 80.54s/it]
 39%|███▉      | 754/1927 [16:53:22<26:13:49, 80.50s/it]
 39%|███▉      | 755/1927 [16:54:42<26:11:36, 80.46s/it]
 39%|███▉      | 756/1927 [16:56:03<26:10:26, 80.47s/it]
 39%|███▉      | 757/1927 [16:57:23<26:07:58, 80.41s/it]
 39%|███▉      | 758/1927 [16:58:43<26:06:03, 80.38s/it]
 39%|███▉      | 759/1927 [17:00:04<26:05:03, 80.40s/it]
 39%|███▉      | 760/1927 [17:01:24<26:03:13, 80.37s/it]
                                                        
{'loss': 1.3963, 'learning_rate': 3.314284337324728e-05, 'epoch': 0.39}

 39%|███▉      | 760/1927 [17:01:24<26:03:13, 80.37s/it]
 39%|███▉      | 761/1927 [17:02:44<26:01:53, 80.37s/it]
 40%|███▉      | 762/1927 [17:04:05<26:00:50, 80.39s/it]
 40%|███▉      | 763/1927 [17:05:25<26:00:18, 80.43s/it]
 40%|███▉      | 764/1927 [17:06:46<25:59:21, 80.45s/it]
 40%|███▉      | 765/1927 [17:08:06<25:58:59, 80.50s/it]
 40%|███▉      | 766/1927 [17:09:27<25:56:43, 80.45s/it]
 40%|███▉      | 767/1927 [17:10:48<26:01:09, 80.75s/it]
 40%|███▉      | 768/1927 [17:12:09<25:58:34, 80.69s/it]
 40%|███▉      | 769/1927 [17:13:30<25:57:46, 80.71s/it]
 40%|███▉      | 770/1927 [17:14:50<25:55:28, 80.66s/it]
                                                        
{'loss': 1.3855, 'learning_rate': 3.2756428473646086e-05, 'epoch': 0.4}

 40%|███▉      | 770/1927 [17:14:50<25:55:28, 80.66s/it]
 40%|████      | 771/1927 [17:16:11<25:53:45, 80.64s/it]
 40%|████      | 772/1927 [17:17:31<25:51:38, 80.60s/it]
 40%|████      | 773/1927 [17:18:51<25:48:09, 80.49s/it]
 40%|████      | 774/1927 [17:20:12<25:45:55, 80.45s/it]
 40%|████      | 775/1927 [17:21:32<25:44:31, 80.44s/it]
 40%|████      | 776/1927 [17:22:53<25:44:16, 80.50s/it]
 40%|████      | 777/1927 [17:24:14<25:43:47, 80.55s/it]
 40%|████      | 778/1927 [17:25:34<25:42:24, 80.54s/it]
 40%|████      | 779/1927 [17:26:54<25:40:26, 80.51s/it]
 40%|████      | 780/1927 [17:28:15<25:38:41, 80.49s/it]
                                                        
{'loss': 1.3891, 'learning_rate': 3.2367952049600326e-05, 'epoch': 0.4}

 40%|████      | 780/1927 [17:28:15<25:38:41, 80.49s/it]
 41%|████      | 781/1927 [17:29:35<25:36:33, 80.45s/it]
 41%|████      | 782/1927 [17:30:56<25:34:19, 80.40s/it]
 41%|████      | 783/1927 [17:32:16<25:32:31, 80.38s/it]
 41%|████      | 784/1927 [17:33:36<25:30:43, 80.35s/it]
 41%|████      | 785/1927 [17:34:57<25:29:39, 80.37s/it]
 41%|████      | 786/1927 [17:36:17<25:29:59, 80.46s/it]
 41%|████      | 787/1927 [17:37:38<25:29:02, 80.48s/it]
 41%|████      | 788/1927 [17:38:58<25:27:34, 80.47s/it]
 41%|████      | 789/1927 [17:40:19<25:28:39, 80.60s/it]
 41%|████      | 790/1927 [17:41:40<25:28:15, 80.65s/it]
                                                        
{'loss': 1.392, 'learning_rate': 3.197751735142e-05, 'epoch': 0.41}

 41%|████      | 790/1927 [17:41:40<25:28:15, 80.65s/it]
 41%|████      | 791/1927 [17:43:00<25:26:13, 80.61s/it]
 41%|████      | 792/1927 [17:44:21<25:24:28, 80.59s/it]
 41%|████      | 793/1927 [17:45:42<25:24:24, 80.66s/it]
 41%|████      | 794/1927 [17:47:02<25:20:11, 80.50s/it]
 41%|████▏     | 795/1927 [17:48:22<25:18:14, 80.47s/it]
 41%|████▏     | 796/1927 [17:49:43<25:16:52, 80.47s/it]
 41%|████▏     | 797/1927 [17:51:03<25:14:53, 80.44s/it]
 41%|████▏     | 798/1927 [17:52:24<25:13:12, 80.42s/it]
 41%|████▏     | 799/1927 [17:53:44<25:11:40, 80.41s/it]
 42%|████▏     | 800/1927 [17:55:05<25:12:45, 80.54s/it]
                                                        
{'loss': 1.3904, 'learning_rate': 3.158522814989047e-05, 'epoch': 0.42}

 42%|████▏     | 800/1927 [17:55:05<25:12:45, 80.54s/it]
 42%|████▏     | 801/1927 [17:56:25<25:11:01, 80.52s/it]
 42%|████▏     | 802/1927 [17:57:46<25:08:38, 80.46s/it]
 42%|████▏     | 803/1927 [17:59:06<25:06:42, 80.43s/it]
 42%|████▏     | 804/1927 [18:00:26<25:05:28, 80.44s/it]
 42%|████▏     | 805/1927 [18:01:47<25:03:48, 80.42s/it]
 42%|████▏     | 806/1927 [18:03:07<25:02:09, 80.40s/it]
 42%|████▏     | 807/1927 [18:04:28<25:01:23, 80.43s/it]
 42%|████▏     | 808/1927 [18:05:48<24:59:43, 80.41s/it]
 42%|████▏     | 809/1927 [18:07:09<24:59:16, 80.46s/it]
 42%|████▏     | 810/1927 [18:08:29<24:59:31, 80.55s/it]
                                                        
{'loss': 1.3972, 'learning_rate': 3.119118870869198e-05, 'epoch': 0.42}

 42%|████▏     | 810/1927 [18:08:29<24:59:31, 80.55s/it]
 42%|████▏     | 811/1927 [18:09:50<24:59:13, 80.60s/it]
 42%|████▏     | 812/1927 [18:11:11<25:00:51, 80.76s/it]
 42%|████▏     | 813/1927 [18:12:32<24:59:05, 80.74s/it]
 42%|████▏     | 814/1927 [18:13:52<24:57:19, 80.72s/it]
 42%|████▏     | 815/1927 [18:15:13<24:55:28, 80.69s/it]
 42%|████▏     | 816/1927 [18:16:34<24:52:24, 80.60s/it]
 42%|████▏     | 817/1927 [18:17:54<24:50:03, 80.54s/it]
 42%|████▏     | 818/1927 [18:19:15<24:48:59, 80.56s/it]
 43%|████▎     | 819/1927 [18:20:35<24:48:49, 80.62s/it]
 43%|████▎     | 820/1927 [18:21:56<24:46:55, 80.59s/it]
                                                        
{'loss': 1.3919, 'learning_rate': 3.079550375668821e-05, 'epoch': 0.43}

 43%|████▎     | 820/1927 [18:21:56<24:46:55, 80.59s/it]
 43%|████▎     | 821/1927 [18:23:16<24:44:51, 80.55s/it]
 43%|████▎     | 822/1927 [18:24:37<24:44:00, 80.58s/it]
 43%|████▎     | 823/1927 [18:25:58<24:43:31, 80.63s/it]
 43%|████▎     | 824/1927 [18:27:18<24:40:54, 80.56s/it]
 43%|████▎     | 825/1927 [18:28:39<24:40:23, 80.60s/it]
 43%|████▎     | 826/1927 [18:29:59<24:37:54, 80.54s/it]
 43%|████▎     | 827/1927 [18:31:20<24:37:36, 80.60s/it]
 43%|████▎     | 828/1927 [18:32:40<24:34:26, 80.50s/it]
 43%|████▎     | 829/1927 [18:34:01<24:35:02, 80.60s/it]
 43%|████▎     | 830/1927 [18:35:22<24:36:25, 80.75s/it]
                                                        
{'loss': 1.391, 'learning_rate': 3.0398278460091105e-05, 'epoch': 0.43}

 43%|████▎     | 830/1927 [18:35:22<24:36:25, 80.75s/it]
 43%|████▎     | 831/1927 [18:36:43<24:34:01, 80.69s/it]
 43%|████▎     | 832/1927 [18:38:03<24:32:12, 80.67s/it]
 43%|████▎     | 833/1927 [18:39:24<24:30:58, 80.68s/it]
 43%|████▎     | 834/1927 [18:40:45<24:29:50, 80.69s/it]
 43%|████▎     | 835/1927 [18:42:05<24:26:48, 80.59s/it]
 43%|████▎     | 836/1927 [18:43:26<24:25:00, 80.57s/it]
 43%|████▎     | 837/1927 [18:44:46<24:22:53, 80.53s/it]
 43%|████▎     | 838/1927 [18:46:06<24:20:19, 80.46s/it]
 44%|████▎     | 839/1927 [18:47:26<24:17:16, 80.36s/it]
 44%|████▎     | 840/1927 [18:48:47<24:18:49, 80.52s/it]
                                                        
{'loss': 1.3892, 'learning_rate': 2.999961839450956e-05, 'epoch': 0.44}

 44%|████▎     | 840/1927 [18:48:47<24:18:49, 80.52s/it]
 44%|████▎     | 841/1927 [18:50:08<24:16:14, 80.46s/it]
 44%|████▎     | 842/1927 [18:51:28<24:14:29, 80.43s/it]
 44%|████▎     | 843/1927 [18:52:48<24:12:37, 80.40s/it]
 44%|████▍     | 844/1927 [18:54:08<24:10:00, 80.33s/it]
 44%|████▍     | 845/1927 [18:55:29<24:09:55, 80.40s/it]
 44%|████▍     | 846/1927 [18:56:50<24:09:09, 80.43s/it]
 44%|████▍     | 847/1927 [18:58:10<24:07:40, 80.43s/it]
 44%|████▍     | 848/1927 [18:59:31<24:08:04, 80.52s/it]
 44%|████▍     | 849/1927 [19:00:52<24:08:07, 80.60s/it]
 44%|████▍     | 850/1927 [19:02:12<24:06:17, 80.57s/it]
                                                        
{'loss': 1.3921, 'learning_rate': 2.9599629516889238e-05, 'epoch': 0.44}

 44%|████▍     | 850/1927 [19:02:12<24:06:17, 80.57s/it]
 44%|████▍     | 851/1927 [19:03:33<24:05:54, 80.63s/it]
 44%|████▍     | 852/1927 [19:04:53<24:02:50, 80.53s/it]
 44%|████▍     | 853/1927 [19:06:14<24:02:30, 80.59s/it]
 44%|████▍     | 854/1927 [19:07:34<24:01:18, 80.60s/it]
 44%|████▍     | 855/1927 [19:08:55<23:59:05, 80.55s/it]
 44%|████▍     | 856/1927 [19:10:15<23:57:15, 80.52s/it]
 44%|████▍     | 857/1927 [19:11:36<23:54:31, 80.44s/it]
 45%|████▍     | 858/1927 [19:12:56<23:53:37, 80.47s/it]
 45%|████▍     | 859/1927 [19:14:17<23:54:22, 80.58s/it]
 45%|████▍     | 860/1927 [19:15:37<23:51:26, 80.49s/it]
                                                        
{'loss': 1.3867, 'learning_rate': 2.919841813735103e-05, 'epoch': 0.45}

 45%|████▍     | 860/1927 [19:15:37<23:51:26, 80.49s/it]
 45%|████▍     | 861/1927 [19:16:58<23:50:23, 80.51s/it]
 45%|████▍     | 862/1927 [19:18:18<23:48:57, 80.51s/it]
 45%|████▍     | 863/1927 [19:19:39<23:47:51, 80.52s/it]
 45%|████▍     | 864/1927 [19:20:59<23:47:14, 80.56s/it]
 45%|████▍     | 865/1927 [19:22:20<23:45:08, 80.52s/it]
 45%|████▍     | 866/1927 [19:23:40<23:44:02, 80.53s/it]
 45%|████▍     | 867/1927 [19:25:01<23:44:08, 80.61s/it]
 45%|████▌     | 868/1927 [19:26:22<23:42:10, 80.58s/it]
 45%|████▌     | 869/1927 [19:27:42<23:40:44, 80.57s/it]
 45%|████▌     | 870/1927 [19:29:03<23:38:34, 80.52s/it]
                                                        
{'loss': 1.3821, 'learning_rate': 2.8796090890935678e-05, 'epoch': 0.45}

 45%|████▌     | 870/1927 [19:29:03<23:38:34, 80.52s/it]
 45%|████▌     | 871/1927 [19:30:23<23:37:14, 80.52s/it]
 45%|████▌     | 872/1927 [19:31:44<23:38:41, 80.68s/it]
 45%|████▌     | 873/1927 [19:33:05<23:37:14, 80.68s/it]
 45%|████▌     | 874/1927 [19:34:25<23:35:06, 80.63s/it]
 45%|████▌     | 875/1927 [19:35:46<23:33:16, 80.60s/it]
 45%|████▌     | 876/1927 [19:37:07<23:34:31, 80.75s/it]
 46%|████▌     | 877/1927 [19:38:28<23:32:29, 80.71s/it]
 46%|████▌     | 878/1927 [19:39:48<23:29:20, 80.61s/it]
 46%|████▌     | 879/1927 [19:41:09<23:27:16, 80.57s/it]
 46%|████▌     | 880/1927 [19:42:29<23:25:33, 80.55s/it]
                                                        
{'loss': 1.3769, 'learning_rate': 2.8392754709262054e-05, 'epoch': 0.46}

 46%|████▌     | 880/1927 [19:42:29<23:25:33, 80.55s/it]
 46%|████▌     | 881/1927 [19:43:50<23:28:19, 80.78s/it]
 46%|████▌     | 882/1927 [19:45:11<23:27:33, 80.82s/it]
 46%|████▌     | 883/1927 [19:46:32<23:25:48, 80.79s/it]
 46%|████▌     | 884/1927 [19:47:53<23:22:54, 80.70s/it]
 46%|████▌     | 885/1927 [19:49:14<23:25:55, 80.96s/it]
 46%|████▌     | 886/1927 [19:50:34<23:21:40, 80.79s/it]
 46%|████▌     | 887/1927 [19:51:55<23:18:29, 80.68s/it]
 46%|████▌     | 888/1927 [19:53:15<23:15:52, 80.61s/it]
 46%|████▌     | 889/1927 [19:54:36<23:15:46, 80.68s/it]
 46%|████▌     | 890/1927 [19:55:57<23:14:01, 80.66s/it]
                                                        
{'loss': 1.381, 'learning_rate': 2.798851679210652e-05, 'epoch': 0.46}

 46%|████▌     | 890/1927 [19:55:57<23:14:01, 80.66s/it]
 46%|████▌     | 891/1927 [19:57:17<23:11:31, 80.59s/it]
 46%|████▋     | 892/1927 [19:58:37<23:08:06, 80.47s/it]
 46%|████▋     | 893/1927 [19:59:58<23:09:02, 80.60s/it]
 46%|████▋     | 894/1927 [20:01:19<23:07:45, 80.61s/it]
 46%|████▋     | 895/1927 [20:02:40<23:06:10, 80.59s/it]
 46%|████▋     | 896/1927 [20:04:00<23:06:12, 80.67s/it]
 47%|████▋     | 897/1927 [20:05:21<23:06:03, 80.74s/it]
 47%|████▋     | 898/1927 [20:06:42<23:04:05, 80.71s/it]
 47%|████▋     | 899/1927 [20:08:05<23:14:07, 81.37s/it]
 47%|████▋     | 900/1927 [20:09:26<23:09:49, 81.20s/it]
                                                        
{'loss': 1.3774, 'learning_rate': 2.758348457891115e-05, 'epoch': 0.47}

 47%|████▋     | 900/1927 [20:09:26<23:09:49, 81.20s/it]
 47%|████▋     | 901/1927 [20:10:46<23:06:47, 81.10s/it]
 47%|████▋     | 902/1927 [20:12:07<23:02:31, 80.93s/it]
 47%|████▋     | 903/1927 [20:13:27<22:58:32, 80.77s/it]
 47%|████▋     | 904/1927 [20:14:48<22:55:49, 80.69s/it]
 47%|████▋     | 905/1927 [20:16:08<22:52:57, 80.60s/it]
 47%|████▋     | 906/1927 [20:17:29<22:52:26, 80.65s/it]
 47%|████▋     | 907/1927 [20:18:50<22:50:48, 80.64s/it]
 47%|████▋     | 908/1927 [20:20:10<22:49:31, 80.64s/it]
 47%|████▋     | 909/1927 [20:21:31<22:48:39, 80.67s/it]
 47%|████▋     | 910/1927 [20:22:52<22:46:21, 80.61s/it]
                                                        
{'loss': 1.3804, 'learning_rate': 2.7177765720228154e-05, 'epoch': 0.47}

 47%|████▋     | 910/1927 [20:22:52<22:46:21, 80.61s/it]
 47%|████▋     | 911/1927 [20:24:12<22:45:22, 80.63s/it]
 47%|████▋     | 912/1927 [20:25:33<22:43:48, 80.62s/it]
 47%|████▋     | 913/1927 [20:26:53<22:42:39, 80.63s/it]
 47%|████▋     | 914/1927 [20:28:14<22:40:46, 80.60s/it]
 47%|████▋     | 915/1927 [20:29:35<22:39:50, 80.62s/it]
 48%|████▊     | 916/1927 [20:30:55<22:37:51, 80.58s/it]
 48%|████▊     | 917/1927 [20:32:16<22:36:44, 80.60s/it]
 48%|████▊     | 918/1927 [20:33:36<22:33:44, 80.50s/it]
 48%|████▊     | 919/1927 [20:34:57<22:32:37, 80.51s/it]
 48%|████▊     | 920/1927 [20:36:17<22:32:26, 80.58s/it]
                                                        
{'loss': 1.3866, 'learning_rate': 2.6771468049108218e-05, 'epoch': 0.48}

 48%|████▊     | 920/1927 [20:36:17<22:32:26, 80.58s/it]
 48%|████▊     | 921/1927 [20:37:38<22:30:00, 80.52s/it]
 48%|████▊     | 922/1927 [20:38:58<22:27:22, 80.44s/it]
 48%|████▊     | 923/1927 [20:40:19<22:28:24, 80.58s/it]
 48%|████▊     | 924/1927 [20:41:39<22:26:29, 80.55s/it]
 48%|████▊     | 925/1927 [20:43:00<22:25:46, 80.59s/it]
 48%|████▊     | 926/1927 [20:44:21<22:24:04, 80.56s/it]
 48%|████▊     | 927/1927 [20:45:41<22:22:31, 80.55s/it]
 48%|████▊     | 928/1927 [20:47:02<22:20:34, 80.51s/it]
 48%|████▊     | 929/1927 [20:48:22<22:19:30, 80.53s/it]
 48%|████▊     | 930/1927 [20:49:43<22:18:34, 80.56s/it]
                                                        
{'loss': 1.3893, 'learning_rate': 2.636469955244036e-05, 'epoch': 0.48}

 48%|████▊     | 930/1927 [20:49:43<22:18:34, 80.56s/it]
 48%|████▊     | 931/1927 [20:51:04<22:18:48, 80.65s/it]
 48%|████▊     | 932/1927 [20:52:24<22:16:42, 80.61s/it]
 48%|████▊     | 933/1927 [20:53:44<22:14:06, 80.53s/it]
 48%|████▊     | 934/1927 [20:55:05<22:12:28, 80.51s/it]
 49%|████▊     | 935/1927 [20:56:26<22:11:41, 80.55s/it]
 49%|████▊     | 936/1927 [20:57:46<22:09:03, 80.47s/it]
 49%|████▊     | 937/1927 [20:59:06<22:06:57, 80.42s/it]
 49%|████▊     | 938/1927 [21:00:27<22:06:48, 80.49s/it]
 49%|████▊     | 939/1927 [21:01:47<22:05:21, 80.49s/it]
 49%|████▉     | 940/1927 [21:03:08<22:02:54, 80.42s/it]
                                                        
{'loss': 1.3821, 'learning_rate': 2.595756834225089e-05, 'epoch': 0.49}

 49%|████▉     | 940/1927 [21:03:08<22:02:54, 80.42s/it]
 49%|████▉     | 941/1927 [21:04:28<22:04:12, 80.58s/it]
 49%|████▉     | 942/1927 [21:05:49<22:02:02, 80.53s/it]
 49%|████▉     | 943/1927 [21:07:09<22:00:02, 80.49s/it]
 49%|████▉     | 944/1927 [21:08:30<21:58:49, 80.50s/it]
 49%|████▉     | 945/1927 [21:09:50<21:57:06, 80.47s/it]
 49%|████▉     | 946/1927 [21:11:11<21:55:54, 80.48s/it]
 49%|████▉     | 947/1927 [21:12:31<21:55:05, 80.52s/it]
 49%|████▉     | 948/1927 [21:13:52<21:53:34, 80.51s/it]
 49%|████▉     | 949/1927 [21:15:13<21:53:46, 80.60s/it]
 49%|████▉     | 950/1927 [21:16:33<21:51:30, 80.54s/it]
                                                        
{'loss': 1.3769, 'learning_rate': 2.5550182626969077e-05, 'epoch': 0.49}

 49%|████▉     | 950/1927 [21:16:33<21:51:30, 80.54s/it]
 49%|████▉     | 951/1927 [21:17:54<21:50:02, 80.54s/it]
 49%|████▉     | 952/1927 [21:19:14<21:48:03, 80.50s/it]
 49%|████▉     | 953/1927 [21:20:34<21:46:35, 80.49s/it]
 50%|████▉     | 954/1927 [21:21:55<21:45:58, 80.53s/it]
 50%|████▉     | 955/1927 [21:23:17<21:49:28, 80.83s/it]
 50%|████▉     | 956/1927 [21:24:37<21:46:24, 80.73s/it]
 50%|████▉     | 957/1927 [21:25:58<21:45:03, 80.72s/it]
 50%|████▉     | 958/1927 [21:27:19<21:44:08, 80.75s/it]
 50%|████▉     | 959/1927 [21:28:39<21:42:44, 80.75s/it]
 50%|████▉     | 960/1927 [21:30:00<21:40:42, 80.71s/it]
                                                        
{'loss': 1.3829, 'learning_rate': 2.5142650682667267e-05, 'epoch': 0.5}

 50%|████▉     | 960/1927 [21:30:00<21:40:42, 80.71s/it]
 50%|████▉     | 961/1927 [21:31:20<21:38:29, 80.65s/it]
 50%|████▉     | 962/1927 [21:32:41<21:37:41, 80.69s/it]
 50%|████▉     | 963/1927 [21:34:02<21:35:06, 80.61s/it]
 50%|█████     | 964/1927 [21:35:23<21:34:52, 80.68s/it]
 50%|█████     | 965/1927 [21:36:43<21:33:39, 80.69s/it]
 50%|█████     | 966/1927 [21:38:04<21:33:28, 80.76s/it]
 50%|█████     | 967/1927 [21:39:25<21:32:17, 80.77s/it]
 50%|█████     | 968/1927 [21:40:46<21:31:56, 80.83s/it]
 50%|█████     | 969/1927 [21:42:07<21:30:32, 80.83s/it]
 50%|█████     | 970/1927 [21:43:27<21:28:25, 80.78s/it]
                                                        
{'loss': 1.3893, 'learning_rate': 2.4735080824282904e-05, 'epoch': 0.5}

 50%|█████     | 970/1927 [21:43:27<21:28:25, 80.78s/it]
 50%|█████     | 971/1927 [21:44:48<21:26:05, 80.72s/it]
 50%|█████     | 972/1927 [21:46:08<21:23:21, 80.63s/it]
 50%|█████     | 973/1927 [21:47:29<21:21:53, 80.62s/it]
 51%|█████     | 974/1927 [21:48:49<21:19:13, 80.54s/it]
 51%|█████     | 975/1927 [21:50:10<21:17:25, 80.51s/it]
 51%|█████     | 976/1927 [21:51:30<21:15:34, 80.48s/it]
 51%|█████     | 977/1927 [21:52:52<21:18:19, 80.74s/it]
 51%|█████     | 978/1927 [21:54:12<21:15:59, 80.67s/it]
 51%|█████     | 979/1927 [21:55:33<21:13:51, 80.62s/it]
 51%|█████     | 980/1927 [21:56:53<21:11:47, 80.58s/it]
                                                        
{'loss': 1.3811, 'learning_rate': 2.4327581376830395e-05, 'epoch': 0.51}

 51%|█████     | 980/1927 [21:56:53<21:11:47, 80.58s/it]
 51%|█████     | 981/1927 [21:58:14<21:10:39, 80.59s/it]
 51%|█████     | 982/1927 [21:59:34<21:09:25, 80.60s/it]
 51%|█████     | 983/1927 [22:00:55<21:09:23, 80.68s/it]
 51%|█████     | 984/1927 [22:02:16<21:07:12, 80.63s/it]
 51%|█████     | 985/1927 [22:03:36<21:06:17, 80.66s/it]
 51%|█████     | 986/1927 [22:04:57<21:03:43, 80.58s/it]
 51%|█████     | 987/1927 [22:06:17<21:02:24, 80.58s/it]
 51%|█████▏    | 988/1927 [22:07:38<21:00:09, 80.52s/it]
 51%|█████▏    | 989/1927 [22:08:58<20:58:36, 80.51s/it]
 51%|█████▏    | 990/1927 [22:10:19<20:57:33, 80.53s/it]
                                                        
{'loss': 1.3749, 'learning_rate': 2.392026064661009e-05, 'epoch': 0.51}

 51%|█████▏    | 990/1927 [22:10:19<20:57:33, 80.53s/it]
 51%|█████▏    | 991/1927 [22:11:39<20:56:39, 80.56s/it]
 51%|█████▏    | 992/1927 [22:13:00<20:56:45, 80.65s/it]
 52%|█████▏    | 993/1927 [22:14:21<20:54:00, 80.56s/it]
 52%|█████▏    | 994/1927 [22:15:41<20:52:45, 80.56s/it]
 52%|█████▏    | 995/1927 [22:17:02<20:50:51, 80.53s/it]
 52%|█████▏    | 996/1927 [22:18:22<20:48:36, 80.47s/it]
 52%|█████▏    | 997/1927 [22:19:42<20:47:11, 80.46s/it]
 52%|█████▏    | 998/1927 [22:21:03<20:46:14, 80.49s/it]
 52%|█████▏    | 999/1927 [22:22:24<20:46:25, 80.59s/it]
 52%|█████▏    | 1000/1927 [22:23:45<20:45:54, 80.64s/it]
                                                         
{'loss': 1.3753, 'learning_rate': 2.3513226892422416e-05, 'epoch': 0.52}

 52%|█████▏    | 1000/1927 [22:23:45<20:45:54, 80.64s/it][INFO|trainer.py:2979] 2024-03-07 16:05:19,717 >> Saving model checkpoint to /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1/tmp-checkpoint-1000
/home/nfs02/wangzj/public_code/hitsz/peft/src/peft/utils/save_and_load.py:151: UserWarning: Could not find a config file in /home/nfs02/model/llama2/hf/Llama-2-7b-hf - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2435] 2024-03-07 16:07:01,247 >> tokenizer config file saved in /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-03-07 16:07:01,289 >> Special tokens file saved in /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1/tmp-checkpoint-1000/special_tokens_map.json

 52%|█████▏    | 1001/1927 [22:27:01<29:43:02, 115.53s/it]
 52%|█████▏    | 1002/1927 [22:28:22<26:58:40, 104.99s/it]
 52%|█████▏    | 1003/1927 [22:29:42<25:03:35, 97.64s/it] 
 52%|█████▏    | 1004/1927 [22:31:03<23:44:09, 92.58s/it]
 52%|█████▏    | 1005/1927 [22:32:24<22:47:46, 89.01s/it]
 52%|█████▏    | 1006/1927 [22:33:44<22:07:52, 86.51s/it]
 52%|█████▏    | 1007/1927 [22:35:05<21:38:47, 84.70s/it]
 52%|█████▏    | 1008/1927 [22:36:26<21:19:34, 83.54s/it]
 52%|█████▏    | 1009/1927 [22:37:46<21:03:32, 82.58s/it]
 52%|█████▏    | 1010/1927 [22:39:07<20:54:53, 82.11s/it]
                                                         
{'loss': 1.3813, 'learning_rate': 2.3106588296794538e-05, 'epoch': 0.52}

 52%|█████▏    | 1010/1927 [22:39:07<20:54:53, 82.11s/it]
 52%|█████▏    | 1011/1927 [22:40:28<20:46:48, 81.67s/it]
 53%|█████▎    | 1012/1927 [22:41:49<20:41:29, 81.41s/it]
 53%|█████▎    | 1013/1927 [22:43:09<20:35:50, 81.13s/it]
 53%|█████▎    | 1014/1927 [22:44:30<20:31:23, 80.92s/it]
 53%|█████▎    | 1015/1927 [22:45:50<20:26:59, 80.72s/it]
 53%|█████▎    | 1016/1927 [22:47:10<20:25:29, 80.71s/it]
 53%|█████▎    | 1017/1927 [22:48:31<20:23:21, 80.66s/it]
 53%|█████▎    | 1018/1927 [22:49:52<20:21:54, 80.65s/it]
 53%|█████▎    | 1019/1927 [22:51:12<20:19:25, 80.58s/it]
 53%|█████▎    | 1020/1927 [22:52:33<20:17:40, 80.55s/it]
                                                         
{'loss': 1.3691, 'learning_rate': 2.2700452937227302e-05, 'epoch': 0.53}

 53%|█████▎    | 1020/1927 [22:52:33<20:17:40, 80.55s/it]
 53%|█████▎    | 1021/1927 [22:53:53<20:16:05, 80.54s/it]
 53%|█████▎    | 1022/1927 [22:55:14<20:16:10, 80.63s/it]
 53%|█████▎    | 1023/1927 [22:56:34<20:13:37, 80.55s/it]
 53%|█████▎    | 1024/1927 [22:57:55<20:11:52, 80.52s/it]
 53%|█████▎    | 1025/1927 [22:59:15<20:10:38, 80.53s/it]
 53%|█████▎    | 1026/1927 [23:00:36<20:08:27, 80.47s/it]
 53%|█████▎    | 1027/1927 [23:01:56<20:07:15, 80.48s/it]
 53%|█████▎    | 1028/1927 [23:03:17<20:05:47, 80.48s/it]
 53%|█████▎    | 1029/1927 [23:04:37<20:03:38, 80.42s/it]
 53%|█████▎    | 1030/1927 [23:05:57<20:01:29, 80.37s/it]
                                                         
{'loss': 1.3802, 'learning_rate': 2.229492875747014e-05, 'epoch': 0.53}

 53%|█████▎    | 1030/1927 [23:05:57<20:01:29, 80.37s/it]
 54%|█████▎    | 1031/1927 [23:07:18<20:01:33, 80.46s/it]
 54%|█████▎    | 1032/1927 [23:08:38<19:59:16, 80.40s/it]
 54%|█████▎    | 1033/1927 [23:09:58<19:57:11, 80.35s/it]
 54%|█████▎    | 1034/1927 [23:11:19<19:55:50, 80.35s/it]
 54%|█████▎    | 1035/1927 [23:12:39<19:53:54, 80.31s/it]
 54%|█████▍    | 1036/1927 [23:13:59<19:53:01, 80.34s/it]
 54%|█████▍    | 1037/1927 [23:15:20<19:51:59, 80.36s/it]
 54%|█████▍    | 1038/1927 [23:16:40<19:51:31, 80.42s/it]
 54%|█████▍    | 1039/1927 [23:18:00<19:49:18, 80.36s/it]
 54%|█████▍    | 1040/1927 [23:19:21<19:48:07, 80.37s/it]
                                                         
{'loss': 1.365, 'learning_rate': 2.189012353883143e-05, 'epoch': 0.54}

 54%|█████▍    | 1040/1927 [23:19:21<19:48:07, 80.37s/it]
 54%|█████▍    | 1041/1927 [23:20:42<19:48:13, 80.47s/it]
 54%|█████▍    | 1042/1927 [23:22:02<19:45:55, 80.40s/it]
 54%|█████▍    | 1043/1927 [23:23:22<19:44:47, 80.42s/it]
 54%|█████▍    | 1044/1927 [23:24:43<19:43:03, 80.39s/it]
 54%|█████▍    | 1045/1927 [23:26:03<19:42:23, 80.43s/it]
 54%|█████▍    | 1046/1927 [23:27:24<19:41:07, 80.44s/it]
 54%|█████▍    | 1047/1927 [23:28:44<19:40:03, 80.46s/it]
 54%|█████▍    | 1048/1927 [23:30:05<19:39:16, 80.50s/it]
 54%|█████▍    | 1049/1927 [23:31:25<19:37:54, 80.49s/it]
 54%|█████▍    | 1050/1927 [23:32:45<19:35:50, 80.45s/it]
                                                         
{'loss': 1.3721, 'learning_rate': 2.1486144871532194e-05, 'epoch': 0.54}

 54%|█████▍    | 1050/1927 [23:32:45<19:35:50, 80.45s/it]
 55%|█████▍    | 1051/1927 [23:34:06<19:34:17, 80.43s/it]
 55%|█████▍    | 1052/1927 [23:35:26<19:32:55, 80.43s/it]
 55%|█████▍    | 1053/1927 [23:36:47<19:31:31, 80.42s/it]
 55%|█████▍    | 1054/1927 [23:38:07<19:30:50, 80.47s/it]
 55%|█████▍    | 1055/1927 [23:39:28<19:30:08, 80.51s/it]
 55%|█████▍    | 1056/1927 [23:40:48<19:28:15, 80.48s/it]
 55%|█████▍    | 1057/1927 [23:42:09<19:26:29, 80.45s/it]
 55%|█████▍    | 1058/1927 [23:43:29<19:24:55, 80.43s/it]
 55%|█████▍    | 1059/1927 [23:44:49<19:23:01, 80.39s/it]
 55%|█████▌    | 1060/1927 [23:46:10<19:23:41, 80.53s/it]
                                                         
{'loss': 1.373, 'learning_rate': 2.1083100126110332e-05, 'epoch': 0.55}

 55%|█████▌    | 1060/1927 [23:46:10<19:23:41, 80.53s/it]
 55%|█████▌    | 1061/1927 [23:47:31<19:21:48, 80.50s/it]
 55%|█████▌    | 1062/1927 [23:48:51<19:20:02, 80.47s/it]
 55%|█████▌    | 1063/1927 [23:50:11<19:18:23, 80.44s/it]
 55%|█████▌    | 1064/1927 [23:51:32<19:16:36, 80.41s/it]
 55%|█████▌    | 1065/1927 [23:52:52<19:15:02, 80.40s/it]
 55%|█████▌    | 1066/1927 [23:54:13<19:14:41, 80.47s/it]
 55%|█████▌    | 1067/1927 [23:55:33<19:13:51, 80.50s/it]
 55%|█████▌    | 1068/1927 [23:56:54<19:13:43, 80.59s/it]
 55%|█████▌    | 1069/1927 [23:58:15<19:11:45, 80.54s/it]
 56%|█████▌    | 1070/1927 [23:59:35<19:09:41, 80.49s/it]
                                                         
{'loss': 1.3806, 'learning_rate': 2.068109642488354e-05, 'epoch': 0.56}

 56%|█████▌    | 1070/1927 [23:59:35<19:09:41, 80.49s/it]
 56%|█████▌    | 1071/1927 [24:00:55<19:07:22, 80.42s/it]
 56%|█████▌    | 1072/1927 [24:02:16<19:06:10, 80.43s/it]
 56%|█████▌    | 1073/1927 [24:03:36<19:04:13, 80.39s/it]
 56%|█████▌    | 1074/1927 [24:04:56<19:03:37, 80.44s/it]
 56%|█████▌    | 1075/1927 [24:06:17<19:02:34, 80.46s/it]
 56%|█████▌    | 1076/1927 [24:07:38<19:02:18, 80.54s/it]
 56%|█████▌    | 1077/1927 [24:08:58<19:01:33, 80.58s/it]
 56%|█████▌    | 1078/1927 [24:10:19<19:01:11, 80.65s/it]
 56%|█████▌    | 1079/1927 [24:11:40<18:59:08, 80.60s/it]
 56%|█████▌    | 1080/1927 [24:13:00<18:57:38, 80.59s/it]
                                                         
{'loss': 1.3782, 'learning_rate': 2.0280240613477953e-05, 'epoch': 0.56}

 56%|█████▌    | 1080/1927 [24:13:00<18:57:38, 80.59s/it]
 56%|█████▌    | 1081/1927 [24:14:21<18:56:02, 80.57s/it]
 56%|█████▌    | 1082/1927 [24:15:41<18:55:02, 80.59s/it]
 56%|█████▌    | 1083/1927 [24:17:02<18:53:08, 80.55s/it]
 56%|█████▋    | 1084/1927 [24:18:22<18:51:27, 80.53s/it]
 56%|█████▋    | 1085/1927 [24:19:43<18:50:46, 80.58s/it]
 56%|█████▋    | 1086/1927 [24:21:03<18:48:50, 80.54s/it]
 56%|█████▋    | 1087/1927 [24:22:24<18:47:32, 80.54s/it]
 56%|█████▋    | 1088/1927 [24:23:45<18:46:01, 80.53s/it]
 57%|█████▋    | 1089/1927 [24:25:06<18:46:40, 80.67s/it]
 57%|█████▋    | 1090/1927 [24:26:26<18:44:53, 80.64s/it]
                                                         
{'loss': 1.3781, 'learning_rate': 1.988063923243046e-05, 'epoch': 0.57}

 57%|█████▋    | 1090/1927 [24:26:26<18:44:53, 80.64s/it]
 57%|█████▋    | 1091/1927 [24:27:47<18:43:13, 80.61s/it]
 57%|█████▋    | 1092/1927 [24:29:07<18:42:27, 80.66s/it]
 57%|█████▋    | 1093/1927 [24:30:28<18:40:14, 80.59s/it]
 57%|█████▋    | 1094/1927 [24:31:48<18:38:23, 80.56s/it]
 57%|█████▋    | 1095/1927 [24:33:09<18:36:51, 80.54s/it]
 57%|█████▋    | 1096/1927 [24:34:30<18:36:10, 80.59s/it]
 57%|█████▋    | 1097/1927 [24:35:50<18:34:22, 80.56s/it]
 57%|█████▋    | 1098/1927 [24:37:11<18:35:32, 80.74s/it]
 57%|█████▋    | 1099/1927 [24:38:32<18:33:05, 80.66s/it]
 57%|█████▋    | 1100/1927 [24:39:52<18:30:35, 80.58s/it]
                                                         
{'loss': 1.3756, 'learning_rate': 1.9482398488872075e-05, 'epoch': 0.57}

 57%|█████▋    | 1100/1927 [24:39:52<18:30:35, 80.58s/it]
 57%|█████▋    | 1101/1927 [24:41:13<18:31:00, 80.70s/it]
 57%|█████▋    | 1102/1927 [24:42:33<18:28:39, 80.63s/it]
 57%|█████▋    | 1103/1927 [24:43:54<18:27:27, 80.64s/it]
 57%|█████▋    | 1104/1927 [24:45:15<18:27:59, 80.78s/it]
 57%|█████▋    | 1105/1927 [24:46:36<18:24:32, 80.62s/it]
 57%|█████▋    | 1106/1927 [24:47:56<18:22:30, 80.57s/it]
 57%|█████▋    | 1107/1927 [24:49:16<18:20:45, 80.54s/it]
 57%|█████▋    | 1108/1927 [24:50:37<18:19:46, 80.57s/it]
 58%|█████▊    | 1109/1927 [24:51:58<18:18:37, 80.58s/it]
 58%|█████▊    | 1110/1927 [24:53:18<18:17:17, 80.58s/it]
                                                         
{'loss': 1.368, 'learning_rate': 1.9085624228299924e-05, 'epoch': 0.58}

 58%|█████▊    | 1110/1927 [24:53:18<18:17:17, 80.58s/it]
 58%|█████▊    | 1111/1927 [24:54:39<18:15:45, 80.57s/it]
 58%|█████▊    | 1112/1927 [24:56:00<18:15:08, 80.62s/it]
 58%|█████▊    | 1113/1927 [24:57:20<18:13:02, 80.57s/it]
 58%|█████▊    | 1114/1927 [24:58:40<18:11:05, 80.52s/it]
 58%|█████▊    | 1115/1927 [25:00:01<18:09:44, 80.52s/it]
 58%|█████▊    | 1116/1927 [25:01:22<18:08:43, 80.55s/it]
 58%|█████▊    | 1117/1927 [25:02:42<18:07:28, 80.55s/it]
 58%|█████▊    | 1118/1927 [25:04:03<18:05:55, 80.54s/it]
 58%|█████▊    | 1119/1927 [25:05:23<18:04:45, 80.55s/it]
 58%|█████▊    | 1120/1927 [25:06:44<18:03:00, 80.52s/it]
                                                         
{'loss': 1.3703, 'learning_rate': 1.8690421906445316e-05, 'epoch': 0.58}

 58%|█████▊    | 1120/1927 [25:06:44<18:03:00, 80.52s/it]
 58%|█████▊    | 1121/1927 [25:08:04<18:01:58, 80.54s/it]
 58%|█████▊    | 1122/1927 [25:09:25<18:02:21, 80.67s/it]
 58%|█████▊    | 1123/1927 [25:10:46<18:01:16, 80.69s/it]
 58%|█████▊    | 1124/1927 [25:12:06<17:59:21, 80.65s/it]
 58%|█████▊    | 1125/1927 [25:13:27<17:58:08, 80.66s/it]
 58%|█████▊    | 1126/1927 [25:14:48<17:56:44, 80.65s/it]
 58%|█████▊    | 1127/1927 [25:16:08<17:55:18, 80.65s/it]
 59%|█████▊    | 1128/1927 [25:17:29<17:53:20, 80.60s/it]
 59%|█████▊    | 1129/1927 [25:18:49<17:51:37, 80.57s/it]
 59%|█████▊    | 1130/1927 [25:20:10<17:50:52, 80.62s/it]
                                                         
{'loss': 1.3651, 'learning_rate': 1.8296896561245495e-05, 'epoch': 0.59}

 59%|█████▊    | 1130/1927 [25:20:10<17:50:52, 80.62s/it]
 59%|█████▊    | 1131/1927 [25:21:31<17:48:36, 80.55s/it]
 59%|█████▊    | 1132/1927 [25:22:51<17:46:21, 80.48s/it]
 59%|█████▉    | 1133/1927 [25:24:11<17:45:24, 80.51s/it]
 59%|█████▉    | 1134/1927 [25:25:32<17:43:35, 80.47s/it]
 59%|█████▉    | 1135/1927 [25:26:52<17:42:52, 80.52s/it]
 59%|█████▉    | 1136/1927 [25:28:13<17:41:22, 80.51s/it]
 59%|█████▉    | 1137/1927 [25:29:34<17:40:35, 80.55s/it]
 59%|█████▉    | 1138/1927 [25:30:54<17:39:15, 80.55s/it]
 59%|█████▉    | 1139/1927 [25:32:15<17:37:11, 80.50s/it]
 59%|█████▉    | 1140/1927 [25:33:35<17:35:36, 80.48s/it]
                                                         
{'loss': 1.3653, 'learning_rate': 1.7905152784926284e-05, 'epoch': 0.59}

 59%|█████▉    | 1140/1927 [25:33:35<17:35:36, 80.48s/it]
 59%|█████▉    | 1141/1927 [25:34:55<17:33:55, 80.45s/it]
 59%|█████▉    | 1142/1927 [25:36:16<17:32:58, 80.48s/it]
 59%|█████▉    | 1143/1927 [25:37:36<17:31:11, 80.45s/it]
 59%|█████▉    | 1144/1927 [25:38:57<17:30:51, 80.52s/it]
 59%|█████▉    | 1145/1927 [25:40:17<17:29:13, 80.50s/it]
 59%|█████▉    | 1146/1927 [25:41:38<17:26:57, 80.43s/it]
 60%|█████▉    | 1147/1927 [25:42:58<17:25:53, 80.45s/it]
 60%|█████▉    | 1148/1927 [25:44:19<17:24:56, 80.48s/it]
 60%|█████▉    | 1149/1927 [25:45:39<17:24:27, 80.55s/it]
 60%|█████▉    | 1150/1927 [25:47:00<17:22:33, 80.51s/it]
                                                         
{'loss': 1.381, 'learning_rate': 1.7515294696203354e-05, 'epoch': 0.6}

 60%|█████▉    | 1150/1927 [25:47:00<17:22:33, 80.51s/it]
 60%|█████▉    | 1151/1927 [25:48:21<17:21:48, 80.55s/it]
 60%|█████▉    | 1152/1927 [25:49:41<17:19:13, 80.46s/it]
 60%|█████▉    | 1153/1927 [25:51:02<17:18:59, 80.54s/it]
 60%|█████▉    | 1154/1927 [25:52:22<17:18:00, 80.57s/it]
 60%|█████▉    | 1155/1927 [25:53:43<17:15:57, 80.51s/it]
 60%|█████▉    | 1156/1927 [25:55:03<17:15:34, 80.59s/it]
 60%|██████    | 1157/1927 [25:56:24<17:13:06, 80.50s/it]
 60%|██████    | 1158/1927 [25:57:44<17:12:14, 80.54s/it]
 60%|██████    | 1159/1927 [25:59:04<17:09:53, 80.46s/it]
 60%|██████    | 1160/1927 [26:00:25<17:08:22, 80.45s/it]
                                                         
{'loss': 1.3652, 'learning_rate': 1.712742591260928e-05, 'epoch': 0.6}

 60%|██████    | 1160/1927 [26:00:25<17:08:22, 80.45s/it]
 60%|██████    | 1161/1927 [26:01:45<17:07:06, 80.45s/it]
 60%|██████    | 1162/1927 [26:03:06<17:05:54, 80.46s/it]
 60%|██████    | 1163/1927 [26:04:26<17:04:58, 80.50s/it]
 60%|██████    | 1164/1927 [26:05:47<17:04:45, 80.58s/it]
 60%|██████    | 1165/1927 [26:07:08<17:03:59, 80.63s/it]
 61%|██████    | 1166/1927 [26:08:28<17:01:22, 80.53s/it]
 61%|██████    | 1167/1927 [26:09:49<17:00:09, 80.54s/it]
 61%|██████    | 1168/1927 [26:11:09<16:58:24, 80.51s/it]
 61%|██████    | 1169/1927 [26:12:29<16:55:59, 80.42s/it]
 61%|██████    | 1170/1927 [26:13:50<16:55:55, 80.52s/it]
                                                         
{'loss': 1.3701, 'learning_rate': 1.6741649522953696e-05, 'epoch': 0.61}

 61%|██████    | 1170/1927 [26:13:50<16:55:55, 80.52s/it]
 61%|██████    | 1171/1927 [26:15:11<16:55:16, 80.58s/it]
 61%|██████    | 1172/1927 [26:16:31<16:53:30, 80.54s/it]
 61%|██████    | 1173/1927 [26:17:52<16:51:37, 80.50s/it]
 61%|██████    | 1174/1927 [26:19:12<16:49:53, 80.47s/it]
 61%|██████    | 1175/1927 [26:20:32<16:47:50, 80.41s/it]
 61%|██████    | 1176/1927 [26:21:53<16:46:12, 80.39s/it]
 61%|██████    | 1177/1927 [26:23:14<16:46:25, 80.51s/it]
 61%|██████    | 1178/1927 [26:24:34<16:45:42, 80.56s/it]
 61%|██████    | 1179/1927 [26:25:55<16:44:38, 80.59s/it]
 61%|██████    | 1180/1927 [26:27:15<16:42:36, 80.53s/it]
                                                         
{'loss': 1.3626, 'learning_rate': 1.6358068059924207e-05, 'epoch': 0.61}

 61%|██████    | 1180/1927 [26:27:15<16:42:36, 80.53s/it]
 61%|██████▏   | 1181/1927 [26:28:36<16:41:19, 80.53s/it]
 61%|██████▏   | 1182/1927 [26:29:57<16:40:30, 80.58s/it]
 61%|██████▏   | 1183/1927 [26:31:17<16:38:25, 80.52s/it]
 61%|██████▏   | 1184/1927 [26:32:38<16:39:01, 80.68s/it]
 61%|██████▏   | 1185/1927 [26:33:58<16:36:42, 80.60s/it]
 62%|██████▏   | 1186/1927 [26:35:19<16:34:38, 80.54s/it]
 62%|██████▏   | 1187/1927 [26:36:39<16:33:24, 80.55s/it]
 62%|██████▏   | 1188/1927 [26:38:00<16:33:10, 80.64s/it]
 62%|██████▏   | 1189/1927 [26:39:21<16:31:46, 80.63s/it]
 62%|██████▏   | 1190/1927 [26:40:41<16:29:54, 80.59s/it]
                                                         
{'loss': 1.3719, 'learning_rate': 1.597678347283487e-05, 'epoch': 0.62}

 62%|██████▏   | 1190/1927 [26:40:41<16:29:54, 80.59s/it]
 62%|██████▏   | 1191/1927 [26:42:02<16:28:06, 80.55s/it]
 62%|██████▏   | 1192/1927 [26:43:22<16:26:29, 80.53s/it]
 62%|██████▏   | 1193/1927 [26:44:43<16:24:30, 80.48s/it]
 62%|██████▏   | 1194/1927 [26:46:03<16:23:18, 80.49s/it]
 62%|██████▏   | 1195/1927 [26:47:24<16:22:35, 80.54s/it]
 62%|██████▏   | 1196/1927 [26:48:44<16:20:52, 80.51s/it]
 62%|██████▏   | 1197/1927 [26:50:05<16:20:49, 80.62s/it]
 62%|██████▏   | 1198/1927 [26:51:25<16:18:25, 80.53s/it]
 62%|██████▏   | 1199/1927 [26:52:46<16:17:56, 80.60s/it]
 62%|██████▏   | 1200/1927 [26:54:07<16:16:35, 80.60s/it]
                                                         
{'loss': 1.3605, 'learning_rate': 1.5597897100529896e-05, 'epoch': 0.62}

 62%|██████▏   | 1200/1927 [26:54:07<16:16:35, 80.60s/it]
 62%|██████▏   | 1201/1927 [26:55:27<16:15:25, 80.61s/it]
 62%|██████▏   | 1202/1927 [26:56:48<16:14:00, 80.61s/it]
 62%|██████▏   | 1203/1927 [26:58:08<16:12:01, 80.55s/it]
 62%|██████▏   | 1204/1927 [26:59:29<16:11:05, 80.59s/it]
 63%|██████▎   | 1205/1927 [27:00:50<16:09:50, 80.60s/it]
 63%|██████▎   | 1206/1927 [27:02:10<16:08:06, 80.56s/it]
 63%|██████▎   | 1207/1927 [27:03:31<16:06:38, 80.55s/it]
 63%|██████▎   | 1208/1927 [27:04:51<16:05:12, 80.55s/it]
 63%|██████▎   | 1209/1927 [27:06:12<16:04:00, 80.56s/it]
 63%|██████▎   | 1210/1927 [27:07:32<16:02:37, 80.55s/it]
                                                         
{'loss': 1.3693, 'learning_rate': 1.5221509644449481e-05, 'epoch': 0.63}

 63%|██████▎   | 1210/1927 [27:07:32<16:02:37, 80.55s/it]
 63%|██████▎   | 1211/1927 [27:08:53<16:01:23, 80.56s/it]
 63%|██████▎   | 1212/1927 [27:10:14<16:02:25, 80.76s/it]
 63%|██████▎   | 1213/1927 [27:11:35<16:00:54, 80.75s/it]
 63%|██████▎   | 1214/1927 [27:12:56<15:59:46, 80.77s/it]
 63%|██████▎   | 1215/1927 [27:14:16<15:57:44, 80.71s/it]
 63%|██████▎   | 1216/1927 [27:15:37<15:56:51, 80.75s/it]
 63%|██████▎   | 1217/1927 [27:16:58<15:54:37, 80.67s/it]
 63%|██████▎   | 1218/1927 [27:18:18<15:52:19, 80.59s/it]
 63%|██████▎   | 1219/1927 [27:19:39<15:50:59, 80.59s/it]
 63%|██████▎   | 1220/1927 [27:20:59<15:49:10, 80.55s/it]
                                                         
{'loss': 1.3658, 'learning_rate': 1.4847721141865174e-05, 'epoch': 0.63}

 63%|██████▎   | 1220/1927 [27:20:59<15:49:10, 80.55s/it]
 63%|██████▎   | 1221/1927 [27:22:19<15:47:04, 80.49s/it]
 63%|██████▎   | 1222/1927 [27:23:40<15:45:21, 80.46s/it]
 63%|██████▎   | 1223/1927 [27:25:00<15:44:15, 80.48s/it]
 64%|██████▎   | 1224/1927 [27:26:21<15:42:32, 80.44s/it]
 64%|██████▎   | 1225/1927 [27:27:41<15:41:58, 80.51s/it]
 64%|██████▎   | 1226/1927 [27:29:02<15:40:07, 80.47s/it]
 64%|██████▎   | 1227/1927 [27:30:22<15:39:04, 80.49s/it]
 64%|██████▎   | 1228/1927 [27:31:43<15:37:44, 80.49s/it]
 64%|██████▍   | 1229/1927 [27:33:03<15:36:19, 80.49s/it]
 64%|██████▍   | 1230/1927 [27:34:24<15:34:53, 80.48s/it]
                                                         
{'loss': 1.3632, 'learning_rate': 1.447663093929163e-05, 'epoch': 0.64}

 64%|██████▍   | 1230/1927 [27:34:24<15:34:53, 80.48s/it]
 64%|██████▍   | 1231/1927 [27:35:44<15:33:23, 80.46s/it]
 64%|██████▍   | 1232/1927 [27:37:05<15:32:43, 80.52s/it]
 64%|██████▍   | 1233/1927 [27:38:25<15:31:39, 80.55s/it]
 64%|██████▍   | 1234/1927 [27:39:46<15:30:34, 80.57s/it]
 64%|██████▍   | 1235/1927 [27:41:06<15:28:07, 80.47s/it]
 64%|██████▍   | 1236/1927 [27:42:27<15:27:00, 80.49s/it]
 64%|██████▍   | 1237/1927 [27:43:47<15:25:23, 80.47s/it]
 64%|██████▍   | 1238/1927 [27:45:08<15:24:06, 80.47s/it]
 64%|██████▍   | 1239/1927 [27:46:28<15:22:21, 80.44s/it]
 64%|██████▍   | 1240/1927 [27:47:49<15:21:41, 80.50s/it]
                                                         
{'loss': 1.372, 'learning_rate': 1.4108337666082091e-05, 'epoch': 0.64}

 64%|██████▍   | 1240/1927 [27:47:49<15:21:41, 80.50s/it]
 64%|██████▍   | 1241/1927 [27:49:09<15:19:34, 80.43s/it]
 64%|██████▍   | 1242/1927 [27:50:30<15:18:36, 80.46s/it]
 65%|██████▍   | 1243/1927 [27:51:50<15:16:56, 80.43s/it]
 65%|██████▍   | 1244/1927 [27:53:11<15:16:11, 80.48s/it]
 65%|██████▍   | 1245/1927 [27:54:31<15:14:16, 80.43s/it]
 65%|██████▍   | 1246/1927 [27:55:51<15:13:04, 80.45s/it]
 65%|██████▍   | 1247/1927 [27:57:12<15:11:36, 80.44s/it]
 65%|██████▍   | 1248/1927 [27:58:32<15:09:49, 80.40s/it]
 65%|██████▍   | 1249/1927 [27:59:53<15:09:01, 80.45s/it]
 65%|██████▍   | 1250/1927 [28:01:13<15:07:26, 80.42s/it]
                                                         
{'loss': 1.3636, 'learning_rate': 1.3742939208214373e-05, 'epoch': 0.65}

 65%|██████▍   | 1250/1927 [28:01:13<15:07:26, 80.42s/it]
 65%|██████▍   | 1251/1927 [28:02:33<15:06:26, 80.45s/it]
 65%|██████▍   | 1252/1927 [28:03:54<15:05:54, 80.53s/it]
 65%|██████▌   | 1253/1927 [28:05:15<15:04:38, 80.53s/it]
 65%|██████▌   | 1254/1927 [28:06:35<15:03:02, 80.51s/it]
 65%|██████▌   | 1255/1927 [28:07:56<15:01:33, 80.50s/it]
 65%|██████▌   | 1256/1927 [28:09:16<15:00:20, 80.51s/it]
 65%|██████▌   | 1257/1927 [28:10:37<14:59:28, 80.55s/it]
 65%|██████▌   | 1258/1927 [28:11:57<14:57:42, 80.51s/it]
 65%|██████▌   | 1259/1927 [28:13:18<14:56:23, 80.51s/it]
 65%|██████▌   | 1260/1927 [28:14:39<14:55:55, 80.59s/it]
                                                         
{'loss': 1.3691, 'learning_rate': 1.3380532682274477e-05, 'epoch': 0.65}

 65%|██████▌   | 1260/1927 [28:14:39<14:55:55, 80.59s/it]
 65%|██████▌   | 1261/1927 [28:15:59<14:55:17, 80.66s/it]
 65%|██████▌   | 1262/1927 [28:17:20<14:53:53, 80.65s/it]
 66%|██████▌   | 1263/1927 [28:18:40<14:51:25, 80.55s/it]
 66%|██████▌   | 1264/1927 [28:20:01<14:50:05, 80.55s/it]
 66%|██████▌   | 1265/1927 [28:21:21<14:48:55, 80.57s/it]
 66%|██████▌   | 1266/1927 [28:22:42<14:47:38, 80.57s/it]
 66%|██████▌   | 1267/1927 [28:24:02<14:45:40, 80.52s/it]
 66%|██████▌   | 1268/1927 [28:25:23<14:44:00, 80.49s/it]
 66%|██████▌   | 1269/1927 [28:26:43<14:42:56, 80.51s/it]
 66%|██████▌   | 1270/1927 [28:28:04<14:41:40, 80.52s/it]
                                                         
{'loss': 1.3579, 'learning_rate': 1.3021214409644656e-05, 'epoch': 0.66}

 66%|██████▌   | 1270/1927 [28:28:04<14:41:40, 80.52s/it]
 66%|██████▌   | 1271/1927 [28:29:24<14:40:09, 80.50s/it]
 66%|██████▌   | 1272/1927 [28:30:45<14:38:57, 80.52s/it]
 66%|██████▌   | 1273/1927 [28:32:06<14:37:59, 80.55s/it]
 66%|██████▌   | 1274/1927 [28:33:26<14:36:17, 80.52s/it]
 66%|██████▌   | 1275/1927 [28:34:46<14:34:37, 80.49s/it]
 66%|██████▌   | 1276/1927 [28:36:07<14:32:46, 80.44s/it]
 66%|██████▋   | 1277/1927 [28:37:27<14:31:23, 80.44s/it]
 66%|██████▋   | 1278/1927 [28:38:48<14:30:01, 80.43s/it]
 66%|██████▋   | 1279/1927 [28:40:08<14:29:03, 80.47s/it]
 66%|██████▋   | 1280/1927 [28:41:29<14:27:54, 80.49s/it]
                                                         
{'loss': 1.3619, 'learning_rate': 1.2665079890902936e-05, 'epoch': 0.66}

 66%|██████▋   | 1280/1927 [28:41:29<14:27:54, 80.49s/it]
 66%|██████▋   | 1281/1927 [28:42:49<14:26:57, 80.52s/it]
 67%|██████▋   | 1282/1927 [28:44:10<14:26:18, 80.59s/it]
 67%|██████▋   | 1283/1927 [28:45:31<14:24:58, 80.59s/it]
 67%|██████▋   | 1284/1927 [28:46:52<14:24:43, 80.69s/it]
 67%|██████▋   | 1285/1927 [28:48:12<14:23:24, 80.69s/it]
 67%|██████▋   | 1286/1927 [28:49:33<14:21:32, 80.64s/it]
 67%|██████▋   | 1287/1927 [28:50:54<14:22:16, 80.84s/it]
 67%|██████▋   | 1288/1927 [28:52:15<14:19:39, 80.72s/it]
 67%|██████▋   | 1289/1927 [28:53:35<14:18:47, 80.76s/it]
 67%|██████▋   | 1290/1927 [28:54:56<14:16:28, 80.67s/it]
                                                         
{'loss': 1.3649, 'learning_rate': 1.2312223780440608e-05, 'epoch': 0.67}

 67%|██████▋   | 1290/1927 [28:54:56<14:16:28, 80.67s/it]
 67%|██████▋   | 1291/1927 [28:56:16<14:14:26, 80.61s/it]
 67%|██████▋   | 1292/1927 [28:57:37<14:13:23, 80.64s/it]
 67%|██████▋   | 1293/1927 [28:58:57<14:10:57, 80.53s/it]
 67%|██████▋   | 1294/1927 [29:00:18<14:09:29, 80.52s/it]
 67%|██████▋   | 1295/1927 [29:01:38<14:08:28, 80.55s/it]
 67%|██████▋   | 1296/1927 [29:02:59<14:06:58, 80.54s/it]
 67%|██████▋   | 1297/1927 [29:04:19<14:05:37, 80.54s/it]
 67%|██████▋   | 1298/1927 [29:05:40<14:03:32, 80.47s/it]
 67%|██████▋   | 1299/1927 [29:07:00<14:02:20, 80.48s/it]
 67%|██████▋   | 1300/1927 [29:08:21<14:01:27, 80.52s/it]
                                                         
{'loss': 1.3616, 'learning_rate': 1.1962739861304805e-05, 'epoch': 0.67}

 67%|██████▋   | 1300/1927 [29:08:21<14:01:27, 80.52s/it]
 68%|██████▊   | 1301/1927 [29:09:41<14:00:01, 80.51s/it]
 68%|██████▊   | 1302/1927 [29:11:02<13:58:19, 80.48s/it]
 68%|██████▊   | 1303/1927 [29:12:22<13:57:26, 80.52s/it]
 68%|██████▊   | 1304/1927 [29:13:43<13:55:37, 80.48s/it]
 68%|██████▊   | 1305/1927 [29:15:03<13:54:35, 80.51s/it]
 68%|██████▊   | 1306/1927 [29:16:24<13:54:09, 80.60s/it]
 68%|██████▊   | 1307/1927 [29:17:45<13:53:24, 80.65s/it]
 68%|██████▊   | 1308/1927 [29:19:06<13:52:23, 80.68s/it]
 68%|██████▊   | 1309/1927 [29:20:26<13:51:21, 80.71s/it]
 68%|██████▊   | 1310/1927 [29:21:47<13:49:51, 80.70s/it]
                                                         
{'loss': 1.3632, 'learning_rate': 1.161672102027259e-05, 'epoch': 0.68}

 68%|██████▊   | 1310/1927 [29:21:47<13:49:51, 80.70s/it]
 68%|██████▊   | 1311/1927 [29:23:08<13:47:54, 80.64s/it]
 68%|██████▊   | 1312/1927 [29:24:28<13:46:29, 80.63s/it]
 68%|██████▊   | 1313/1927 [29:25:49<13:44:05, 80.53s/it]
 68%|██████▊   | 1314/1927 [29:27:09<13:42:40, 80.52s/it]
 68%|██████▊   | 1315/1927 [29:28:30<13:42:50, 80.67s/it]
 68%|██████▊   | 1316/1927 [29:29:50<13:40:11, 80.54s/it]
 68%|██████▊   | 1317/1927 [29:31:11<13:38:22, 80.50s/it]
 68%|██████▊   | 1318/1927 [29:32:32<13:39:58, 80.79s/it]
 68%|██████▊   | 1319/1927 [29:33:53<13:37:34, 80.68s/it]
 69%|██████▊   | 1320/1927 [29:35:13<13:35:12, 80.58s/it]
                                                         
{'loss': 1.3657, 'learning_rate': 1.1274259223163189e-05, 'epoch': 0.68}

 69%|██████▊   | 1320/1927 [29:35:13<13:35:12, 80.58s/it]
 69%|██████▊   | 1321/1927 [29:36:33<13:33:37, 80.56s/it]
 69%|██████▊   | 1322/1927 [29:37:54<13:31:51, 80.51s/it]
 69%|██████▊   | 1323/1927 [29:39:14<13:30:17, 80.49s/it]
 69%|██████▊   | 1324/1927 [29:40:35<13:28:45, 80.47s/it]
 69%|██████▉   | 1325/1927 [29:41:55<13:27:48, 80.51s/it]
 69%|██████▉   | 1326/1927 [29:43:16<13:26:23, 80.50s/it]
 69%|██████▉   | 1327/1927 [29:44:36<13:24:53, 80.49s/it]
 69%|██████▉   | 1328/1927 [29:45:57<13:23:55, 80.53s/it]
 69%|██████▉   | 1329/1927 [29:47:18<13:23:06, 80.58s/it]
 69%|██████▉   | 1330/1927 [29:48:38<13:21:54, 80.59s/it]
                                                         
{'loss': 1.3604, 'learning_rate': 1.0935445490395158e-05, 'epoch': 0.69}

 69%|██████▉   | 1330/1927 [29:48:38<13:21:54, 80.59s/it]
 69%|██████▉   | 1331/1927 [29:49:59<13:20:58, 80.64s/it]
 69%|██████▉   | 1332/1927 [29:51:19<13:19:15, 80.60s/it]
 69%|██████▉   | 1333/1927 [29:52:40<13:17:59, 80.61s/it]
 69%|██████▉   | 1334/1927 [29:54:01<13:16:30, 80.59s/it]
 69%|██████▉   | 1335/1927 [29:55:22<13:16:17, 80.71s/it]
 69%|██████▉   | 1336/1927 [29:56:42<13:14:59, 80.71s/it]
 69%|██████▉   | 1337/1927 [29:58:03<13:13:14, 80.67s/it]
 69%|██████▉   | 1338/1927 [29:59:23<13:11:34, 80.64s/it]
 69%|██████▉   | 1339/1927 [30:00:44<13:11:04, 80.72s/it]
 70%|██████▉   | 1340/1927 [30:02:05<13:08:59, 80.65s/it]
                                                         
{'loss': 1.3639, 'learning_rate': 1.0600369872794676e-05, 'epoch': 0.7}

 70%|██████▉   | 1340/1927 [30:02:05<13:08:59, 80.65s/it]
 70%|██████▉   | 1341/1927 [30:03:26<13:07:49, 80.66s/it]
 70%|██████▉   | 1342/1927 [30:04:46<13:05:59, 80.61s/it]
 70%|██████▉   | 1343/1927 [30:06:06<13:04:02, 80.55s/it]
 70%|██████▉   | 1344/1927 [30:07:27<13:03:13, 80.61s/it]
 70%|██████▉   | 1345/1927 [30:08:48<13:01:47, 80.60s/it]
 70%|██████▉   | 1346/1927 [30:10:08<13:00:24, 80.59s/it]
 70%|██████▉   | 1347/1927 [30:11:29<12:59:31, 80.64s/it]
 70%|██████▉   | 1348/1927 [30:12:50<12:58:42, 80.70s/it]
 70%|███████   | 1349/1927 [30:14:10<12:56:53, 80.65s/it]
 70%|███████   | 1350/1927 [30:15:31<12:55:37, 80.65s/it]
                                                         
{'loss': 1.3676, 'learning_rate': 1.0269121427661621e-05, 'epoch': 0.7}

 70%|███████   | 1350/1927 [30:15:31<12:55:37, 80.65s/it]
 70%|███████   | 1351/1927 [30:16:52<12:53:33, 80.58s/it]
 70%|███████   | 1352/1927 [30:18:12<12:52:16, 80.59s/it]
 70%|███████   | 1353/1927 [30:19:33<12:50:16, 80.52s/it]
 70%|███████   | 1354/1927 [30:20:53<12:49:18, 80.56s/it]
 70%|███████   | 1355/1927 [30:22:14<12:48:55, 80.66s/it]
 70%|███████   | 1356/1927 [30:23:35<12:47:03, 80.60s/it]
 70%|███████   | 1357/1927 [30:24:55<12:45:04, 80.53s/it]
 70%|███████   | 1358/1927 [30:26:15<12:43:48, 80.54s/it]
 71%|███████   | 1359/1927 [30:27:36<12:41:59, 80.49s/it]
 71%|███████   | 1360/1927 [30:28:56<12:41:06, 80.54s/it]
                                                         
{'loss': 1.3656, 'learning_rate': 9.941788195099622e-06, 'epoch': 0.71}

 71%|███████   | 1360/1927 [30:28:57<12:41:06, 80.54s/it]
 71%|███████   | 1361/1927 [30:30:17<12:39:54, 80.56s/it]
 71%|███████   | 1362/1927 [30:31:37<12:38:00, 80.50s/it]
 71%|███████   | 1363/1927 [30:32:58<12:37:13, 80.56s/it]
 71%|███████   | 1364/1927 [30:34:18<12:35:09, 80.48s/it]
 71%|███████   | 1365/1927 [30:35:39<12:33:20, 80.43s/it]
 71%|███████   | 1366/1927 [30:36:59<12:31:39, 80.39s/it]
 71%|███████   | 1367/1927 [30:38:20<12:31:00, 80.47s/it]
 71%|███████   | 1368/1927 [30:39:40<12:29:32, 80.45s/it]
 71%|███████   | 1369/1927 [30:41:01<12:28:38, 80.50s/it]
 71%|███████   | 1370/1927 [30:42:21<12:27:19, 80.50s/it]
                                                         
{'loss': 1.3587, 'learning_rate': 9.618457174616638e-06, 'epoch': 0.71}

 71%|███████   | 1370/1927 [30:42:21<12:27:19, 80.50s/it]
 71%|███████   | 1371/1927 [30:43:42<12:25:26, 80.44s/it]
 71%|███████   | 1372/1927 [30:45:02<12:24:30, 80.49s/it]
 71%|███████▏  | 1373/1927 [30:46:22<12:22:40, 80.43s/it]
 71%|███████▏  | 1374/1927 [30:47:43<12:21:50, 80.49s/it]
 71%|███████▏  | 1375/1927 [30:49:04<12:20:35, 80.50s/it]
 71%|███████▏  | 1376/1927 [30:50:24<12:19:34, 80.53s/it]
 71%|███████▏  | 1377/1927 [30:51:45<12:19:15, 80.65s/it]
 72%|███████▏  | 1378/1927 [30:53:05<12:17:14, 80.57s/it]
 72%|███████▏  | 1379/1927 [30:54:26<12:15:34, 80.54s/it]
 72%|███████▏  | 1380/1927 [30:55:46<12:14:12, 80.53s/it]
                                                         
{'loss': 1.3723, 'learning_rate': 9.299214302001863e-06, 'epoch': 0.72}

 72%|███████▏  | 1380/1927 [30:55:47<12:14:12, 80.53s/it]
 72%|███████▏  | 1381/1927 [30:57:07<12:12:23, 80.48s/it]
 72%|███████▏  | 1382/1927 [30:58:27<12:10:48, 80.46s/it]
 72%|███████▏  | 1383/1927 [30:59:48<12:09:24, 80.45s/it]
 72%|███████▏  | 1384/1927 [31:01:08<12:08:17, 80.47s/it]
 72%|███████▏  | 1385/1927 [31:02:29<12:06:50, 80.46s/it]
 72%|███████▏  | 1386/1927 [31:03:49<12:05:09, 80.42s/it]
 72%|███████▏  | 1387/1927 [31:05:09<12:03:39, 80.41s/it]
 72%|███████▏  | 1388/1927 [31:06:30<12:02:46, 80.46s/it]
 72%|███████▏  | 1389/1927 [31:07:50<12:01:11, 80.43s/it]
 72%|███████▏  | 1390/1927 [31:09:11<11:59:46, 80.42s/it]
                                                         
{'loss': 1.3561, 'learning_rate': 8.984144426485578e-06, 'epoch': 0.72}

 72%|███████▏  | 1390/1927 [31:09:11<11:59:46, 80.42s/it]
 72%|███████▏  | 1391/1927 [31:10:31<11:59:02, 80.49s/it]
 72%|███████▏  | 1392/1927 [31:11:52<11:57:39, 80.48s/it]
 72%|███████▏  | 1393/1927 [31:13:13<11:56:58, 80.56s/it]
 72%|███████▏  | 1394/1927 [31:14:33<11:55:47, 80.58s/it]
 72%|███████▏  | 1395/1927 [31:15:54<11:56:00, 80.75s/it]
 72%|███████▏  | 1396/1927 [31:17:15<11:54:14, 80.70s/it]
 72%|███████▏  | 1397/1927 [31:18:36<11:53:18, 80.75s/it]
 73%|███████▎  | 1398/1927 [31:19:56<11:51:51, 80.74s/it]
 73%|███████▎  | 1399/1927 [31:21:17<11:49:32, 80.63s/it]
 73%|███████▎  | 1400/1927 [31:22:37<11:48:09, 80.63s/it]
                                                         
{'loss': 1.3553, 'learning_rate': 8.673331288187648e-06, 'epoch': 0.73}

 73%|███████▎  | 1400/1927 [31:22:38<11:48:09, 80.63s/it]
 73%|███████▎  | 1401/1927 [31:23:58<11:46:52, 80.63s/it]
 73%|███████▎  | 1402/1927 [31:25:19<11:45:21, 80.61s/it]
 73%|███████▎  | 1403/1927 [31:26:39<11:43:49, 80.59s/it]
 73%|███████▎  | 1404/1927 [31:28:00<11:41:50, 80.52s/it]
 73%|███████▎  | 1405/1927 [31:29:20<11:40:34, 80.53s/it]
 73%|███████▎  | 1406/1927 [31:30:41<11:38:56, 80.49s/it]
 73%|███████▎  | 1407/1927 [31:32:01<11:37:42, 80.50s/it]
 73%|███████▎  | 1408/1927 [31:33:22<11:36:56, 80.57s/it]
 73%|███████▎  | 1409/1927 [31:34:42<11:35:10, 80.52s/it]
 73%|███████▎  | 1410/1927 [31:36:03<11:33:33, 80.49s/it]
                                                         
{'loss': 1.3606, 'learning_rate': 8.36685749586087e-06, 'epoch': 0.73}

 73%|███████▎  | 1410/1927 [31:36:03<11:33:33, 80.49s/it]
 73%|███████▎  | 1411/1927 [31:37:23<11:32:48, 80.56s/it]
 73%|███████▎  | 1412/1927 [31:38:44<11:31:07, 80.52s/it]
 73%|███████▎  | 1413/1927 [31:40:04<11:29:35, 80.50s/it]
 73%|███████▎  | 1414/1927 [31:41:25<11:28:02, 80.47s/it]
 73%|███████▎  | 1415/1927 [31:42:45<11:26:15, 80.42s/it]
 73%|███████▎  | 1416/1927 [31:44:06<11:25:53, 80.54s/it]
 74%|███████▎  | 1417/1927 [31:45:26<11:24:23, 80.52s/it]
 74%|███████▎  | 1418/1927 [31:46:47<11:22:34, 80.46s/it]
 74%|███████▎  | 1419/1927 [31:48:07<11:21:52, 80.54s/it]
 74%|███████▎  | 1420/1927 [31:49:28<11:20:03, 80.48s/it]
                                                         
{'loss': 1.358, 'learning_rate': 8.064804504934958e-06, 'epoch': 0.74}

 74%|███████▎  | 1420/1927 [31:49:28<11:20:03, 80.48s/it]
 74%|███████▎  | 1421/1927 [31:50:48<11:19:38, 80.59s/it]
 74%|███████▍  | 1422/1927 [31:52:09<11:18:16, 80.59s/it]
 74%|███████▍  | 1423/1927 [31:53:29<11:16:39, 80.55s/it]
 74%|███████▍  | 1424/1927 [31:54:50<11:14:54, 80.51s/it]
 74%|███████▍  | 1425/1927 [31:56:10<11:13:25, 80.49s/it]
 74%|███████▍  | 1426/1927 [31:57:31<11:11:55, 80.47s/it]
 74%|███████▍  | 1427/1927 [31:58:51<11:10:25, 80.45s/it]
 74%|███████▍  | 1428/1927 [32:00:12<11:09:22, 80.49s/it]
 74%|███████▍  | 1429/1927 [32:01:32<11:07:44, 80.45s/it]
 74%|███████▍  | 1430/1927 [32:02:52<11:06:17, 80.44s/it]
                                                         
{'loss': 1.3545, 'learning_rate': 7.767252595867153e-06, 'epoch': 0.74}

 74%|███████▍  | 1430/1927 [32:02:53<11:06:17, 80.44s/it]
 74%|███████▍  | 1431/1927 [32:04:13<11:05:23, 80.49s/it]
 74%|███████▍  | 1432/1927 [32:05:34<11:04:06, 80.50s/it]
 74%|███████▍  | 1433/1927 [32:06:54<11:02:34, 80.48s/it]
 74%|███████▍  | 1434/1927 [32:08:15<11:03:17, 80.73s/it]
 74%|███████▍  | 1435/1927 [32:09:36<11:01:40, 80.69s/it]
 75%|███████▍  | 1436/1927 [32:10:57<11:01:17, 80.81s/it]
 75%|███████▍  | 1437/1927 [32:12:18<10:59:21, 80.74s/it]
 75%|███████▍  | 1438/1927 [32:13:38<10:57:06, 80.63s/it]
 75%|███████▍  | 1439/1927 [32:14:58<10:55:12, 80.56s/it]
 75%|███████▍  | 1440/1927 [32:16:19<10:53:34, 80.52s/it]
                                                         
{'loss': 1.3605, 'learning_rate': 7.474280852805027e-06, 'epoch': 0.75}

 75%|███████▍  | 1440/1927 [32:16:19<10:53:34, 80.52s/it]
 75%|███████▍  | 1441/1927 [32:17:39<10:51:57, 80.49s/it]
 75%|███████▍  | 1442/1927 [32:19:00<10:50:08, 80.43s/it]
 75%|███████▍  | 1443/1927 [32:20:20<10:48:29, 80.39s/it]
 75%|███████▍  | 1444/1927 [32:21:40<10:47:02, 80.38s/it]
 75%|███████▍  | 1445/1927 [32:23:01<10:45:47, 80.39s/it]
 75%|███████▌  | 1446/1927 [32:24:21<10:44:29, 80.39s/it]
 75%|███████▌  | 1447/1927 [32:25:41<10:42:44, 80.34s/it]
 75%|███████▌  | 1448/1927 [32:27:02<10:41:33, 80.36s/it]
 75%|███████▌  | 1449/1927 [32:28:22<10:40:02, 80.34s/it]
 75%|███████▌  | 1450/1927 [32:29:42<10:39:07, 80.39s/it]
                                                         
{'loss': 1.3518, 'learning_rate': 7.1859671425673096e-06, 'epoch': 0.75}

 75%|███████▌  | 1450/1927 [32:29:42<10:39:07, 80.39s/it]
 75%|███████▌  | 1451/1927 [32:31:03<10:38:26, 80.47s/it]
 75%|███████▌  | 1452/1927 [32:32:24<10:37:29, 80.53s/it]
 75%|███████▌  | 1453/1927 [32:33:44<10:36:16, 80.54s/it]
 75%|███████▌  | 1454/1927 [32:35:05<10:34:35, 80.50s/it]
 76%|███████▌  | 1455/1927 [32:36:26<10:34:52, 80.71s/it]
 76%|███████▌  | 1456/1927 [32:37:46<10:32:52, 80.62s/it]
 76%|███████▌  | 1457/1927 [32:39:07<10:31:09, 80.57s/it]
 76%|███████▌  | 1458/1927 [32:40:28<10:30:46, 80.70s/it]
 76%|███████▌  | 1459/1927 [32:41:48<10:28:48, 80.62s/it]
 76%|███████▌  | 1460/1927 [32:43:09<10:27:28, 80.62s/it]
                                                         
{'loss': 1.3561, 'learning_rate': 6.902388093948206e-06, 'epoch': 0.76}

 76%|███████▌  | 1460/1927 [32:43:09<10:27:28, 80.62s/it]
 76%|███████▌  | 1461/1927 [32:44:29<10:25:53, 80.59s/it]
 76%|███████▌  | 1462/1927 [32:45:50<10:25:38, 80.73s/it]
 76%|███████▌  | 1463/1927 [32:47:11<10:23:33, 80.63s/it]
 76%|███████▌  | 1464/1927 [32:48:31<10:21:48, 80.58s/it]
 76%|███████▌  | 1465/1927 [32:49:52<10:20:43, 80.61s/it]
 76%|███████▌  | 1466/1927 [32:51:12<10:19:10, 80.59s/it]
 76%|███████▌  | 1467/1927 [32:52:33<10:17:38, 80.56s/it]
 76%|███████▌  | 1468/1927 [32:53:53<10:16:01, 80.53s/it]
 76%|███████▌  | 1469/1927 [32:55:14<10:14:49, 80.54s/it]
 76%|███████▋  | 1470/1927 [32:56:34<10:13:09, 80.50s/it]
                                                         
{'loss': 1.3587, 'learning_rate': 6.623619077350821e-06, 'epoch': 0.76}

 76%|███████▋  | 1470/1927 [32:56:34<10:13:09, 80.50s/it]
 76%|███████▋  | 1471/1927 [32:57:55<10:11:19, 80.44s/it]
 76%|███████▋  | 1472/1927 [32:59:15<10:09:52, 80.42s/it]
 76%|███████▋  | 1473/1927 [33:00:36<10:09:12, 80.51s/it]
 76%|███████▋  | 1474/1927 [33:01:56<10:07:23, 80.45s/it]
 77%|███████▋  | 1475/1927 [33:03:17<10:07:41, 80.67s/it]
 77%|███████▋  | 1476/1927 [33:04:38<10:05:42, 80.58s/it]
 77%|███████▋  | 1477/1927 [33:05:58<10:04:09, 80.55s/it]
 77%|███████▋  | 1478/1927 [33:07:19<10:02:39, 80.53s/it]
 77%|███████▋  | 1479/1927 [33:08:39<10:00:59, 80.49s/it]
 77%|███████▋  | 1480/1927 [33:10:00<9:59:37, 80.49s/it] 
                                                        
{'loss': 1.347, 'learning_rate': 6.349734184755004e-06, 'epoch': 0.77}

 77%|███████▋  | 1480/1927 [33:10:00<9:59:37, 80.49s/it]
 77%|███████▋  | 1481/1927 [33:11:20<9:58:05, 80.46s/it]
 77%|███████▋  | 1482/1927 [33:12:40<9:56:34, 80.44s/it]
 77%|███████▋  | 1483/1927 [33:14:01<9:55:45, 80.51s/it]
 77%|███████▋  | 1484/1927 [33:15:22<9:54:51, 80.57s/it]
 77%|███████▋  | 1485/1927 [33:16:42<9:54:02, 80.64s/it]
 77%|███████▋  | 1486/1927 [33:18:03<9:52:29, 80.61s/it]
 77%|███████▋  | 1487/1927 [33:19:24<9:51:48, 80.70s/it]
 77%|███████▋  | 1488/1927 [33:20:45<9:50:08, 80.66s/it]
 77%|███████▋  | 1489/1927 [33:22:05<9:48:26, 80.61s/it]
 77%|███████▋  | 1490/1927 [33:23:25<9:46:48, 80.57s/it]
                                                        
{'loss': 1.3526, 'learning_rate': 6.080806210024947e-06, 'epoch': 0.77}

 77%|███████▋  | 1490/1927 [33:23:26<9:46:48, 80.57s/it]
 77%|███████▋  | 1491/1927 [33:24:46<9:45:59, 80.64s/it]
 77%|███████▋  | 1492/1927 [33:26:07<9:43:59, 80.55s/it]
 77%|███████▋  | 1493/1927 [33:27:27<9:42:36, 80.55s/it]
 78%|███████▊  | 1494/1927 [33:28:48<9:41:05, 80.52s/it]
 78%|███████▊  | 1495/1927 [33:30:09<9:40:32, 80.63s/it]
 78%|███████▊  | 1496/1927 [33:31:29<9:38:26, 80.53s/it]
 78%|███████▊  | 1497/1927 [33:32:49<9:37:28, 80.58s/it]
 78%|███████▊  | 1498/1927 [33:34:10<9:35:44, 80.52s/it]
 78%|███████▊  | 1499/1927 [33:35:30<9:34:00, 80.47s/it]
 78%|███████▊  | 1500/1927 [33:36:51<9:33:12, 80.54s/it]
                                                        
{'loss': 1.3557, 'learning_rate': 5.816906629561911e-06, 'epoch': 0.78}

 78%|███████▊  | 1500/1927 [33:36:51<9:33:12, 80.54s/it]
 78%|███████▊  | 1501/1927 [33:38:11<9:31:32, 80.50s/it]
 78%|███████▊  | 1502/1927 [33:39:32<9:30:08, 80.49s/it]
 78%|███████▊  | 1503/1927 [33:40:52<9:29:00, 80.52s/it]
 78%|███████▊  | 1504/1927 [33:42:13<9:27:16, 80.46s/it]
 78%|███████▊  | 1505/1927 [33:43:33<9:26:00, 80.47s/it]
 78%|███████▊  | 1506/1927 [33:44:54<9:24:39, 80.47s/it]
 78%|███████▊  | 1507/1927 [33:46:14<9:23:49, 80.55s/it]
 78%|███████▊  | 1508/1927 [33:47:35<9:22:02, 80.48s/it]
 78%|███████▊  | 1509/1927 [33:48:55<9:20:44, 80.49s/it]
 78%|███████▊  | 1510/1927 [33:50:16<9:18:56, 80.42s/it]
                                                        
{'loss': 1.3574, 'learning_rate': 5.558105583306941e-06, 'epoch': 0.78}

 78%|███████▊  | 1510/1927 [33:50:16<9:18:56, 80.42s/it]
 78%|███████▊  | 1511/1927 [33:51:36<9:18:33, 80.56s/it]
 78%|███████▊  | 1512/1927 [33:52:57<9:17:05, 80.54s/it]
 79%|███████▊  | 1513/1927 [33:54:17<9:15:48, 80.55s/it]
 79%|███████▊  | 1514/1927 [33:55:38<9:14:16, 80.53s/it]
 79%|███████▊  | 1515/1927 [33:56:58<9:12:49, 80.51s/it]
 79%|███████▊  | 1516/1927 [33:58:19<9:11:35, 80.52s/it]
 79%|███████▊  | 1517/1927 [33:59:39<9:10:06, 80.50s/it]
 79%|███████▉  | 1518/1927 [34:01:00<9:08:44, 80.50s/it]
 79%|███████▉  | 1519/1927 [34:02:20<9:07:28, 80.51s/it]
 79%|███████▉  | 1520/1927 [34:03:41<9:06:02, 80.50s/it]
                                                        
{'loss': 1.3533, 'learning_rate': 5.304471856098963e-06, 'epoch': 0.79}

 79%|███████▉  | 1520/1927 [34:03:41<9:06:02, 80.50s/it]
 79%|███████▉  | 1521/1927 [34:05:02<9:04:53, 80.52s/it]
 79%|███████▉  | 1522/1927 [34:06:22<9:03:33, 80.53s/it]
 79%|███████▉  | 1523/1927 [34:07:42<9:02:00, 80.50s/it]
 79%|███████▉  | 1524/1927 [34:09:03<9:00:33, 80.48s/it]
 79%|███████▉  | 1525/1927 [34:10:23<8:59:23, 80.51s/it]
 79%|███████▉  | 1526/1927 [34:11:44<8:57:48, 80.47s/it]
 79%|███████▉  | 1527/1927 [34:13:05<8:57:32, 80.63s/it]
 79%|███████▉  | 1528/1927 [34:14:25<8:55:25, 80.52s/it]
 79%|███████▉  | 1529/1927 [34:15:46<8:53:57, 80.50s/it]
 79%|███████▉  | 1530/1927 [34:17:06<8:52:33, 80.49s/it]
                                                        
{'loss': 1.3598, 'learning_rate': 5.056072859392924e-06, 'epoch': 0.79}

 79%|███████▉  | 1530/1927 [34:17:06<8:52:33, 80.49s/it]
 79%|███████▉  | 1531/1927 [34:18:27<8:51:09, 80.48s/it]
 80%|███████▉  | 1532/1927 [34:19:47<8:49:55, 80.49s/it]
 80%|███████▉  | 1533/1927 [34:21:07<8:48:11, 80.44s/it]
 80%|███████▉  | 1534/1927 [34:22:28<8:46:37, 80.40s/it]
 80%|███████▉  | 1535/1927 [34:23:48<8:45:23, 80.42s/it]
 80%|███████▉  | 1536/1927 [34:25:09<8:44:29, 80.49s/it]
 80%|███████▉  | 1537/1927 [34:26:29<8:43:07, 80.48s/it]
 80%|███████▉  | 1538/1927 [34:27:50<8:42:05, 80.53s/it]
 80%|███████▉  | 1539/1927 [34:29:11<8:41:04, 80.58s/it]
 80%|███████▉  | 1540/1927 [34:30:31<8:40:25, 80.68s/it]
                                                        
{'loss': 1.3508, 'learning_rate': 4.812974613343011e-06, 'epoch': 0.8}

 80%|███████▉  | 1540/1927 [34:30:32<8:40:25, 80.68s/it]
 80%|███████▉  | 1541/1927 [34:31:52<8:38:44, 80.63s/it]
 80%|████████  | 1542/1927 [34:33:13<8:38:38, 80.83s/it]
 80%|████████  | 1543/1927 [34:34:34<8:36:54, 80.77s/it]
 80%|████████  | 1544/1927 [34:35:55<8:35:13, 80.71s/it]
 80%|████████  | 1545/1927 [34:37:15<8:33:20, 80.63s/it]
 80%|████████  | 1546/1927 [34:38:36<8:32:08, 80.65s/it]
 80%|████████  | 1547/1927 [34:39:56<8:31:03, 80.69s/it]
 80%|████████  | 1548/1927 [34:41:17<8:29:33, 80.67s/it]
 80%|████████  | 1549/1927 [34:42:38<8:27:57, 80.63s/it]
 80%|████████  | 1550/1927 [34:43:58<8:26:35, 80.62s/it]
                                                        
{'loss': 1.3475, 'learning_rate': 4.5752417292556e-06, 'epoch': 0.8}

 80%|████████  | 1550/1927 [34:43:58<8:26:35, 80.62s/it]
 80%|████████  | 1551/1927 [34:45:19<8:25:02, 80.59s/it]
 81%|████████  | 1552/1927 [34:46:39<8:23:35, 80.57s/it]
 81%|████████  | 1553/1927 [34:48:00<8:22:11, 80.57s/it]
 81%|████████  | 1554/1927 [34:49:20<8:20:40, 80.54s/it]
 81%|████████  | 1555/1927 [34:50:41<8:19:19, 80.54s/it]
 81%|████████  | 1556/1927 [34:52:01<8:17:58, 80.53s/it]
 81%|████████  | 1557/1927 [34:53:22<8:16:35, 80.53s/it]
 81%|████████  | 1558/1927 [34:54:42<8:15:27, 80.56s/it]
 81%|████████  | 1559/1927 [34:56:03<8:13:56, 80.53s/it]
 81%|████████  | 1560/1927 [34:57:23<8:12:17, 80.48s/it]
                                                        
{'loss': 1.3517, 'learning_rate': 4.342937392416768e-06, 'epoch': 0.81}

 81%|████████  | 1560/1927 [34:57:23<8:12:17, 80.48s/it]
 81%|████████  | 1561/1927 [34:58:44<8:11:40, 80.60s/it]
 81%|████████  | 1562/1927 [35:00:05<8:09:50, 80.52s/it]
 81%|████████  | 1563/1927 [35:01:25<8:08:35, 80.54s/it]
 81%|████████  | 1564/1927 [35:02:45<8:06:54, 80.48s/it]
 81%|████████  | 1565/1927 [35:04:06<8:05:15, 80.43s/it]
 81%|████████▏ | 1566/1927 [35:05:26<8:03:42, 80.40s/it]
 81%|████████▏ | 1567/1927 [35:06:46<8:02:16, 80.38s/it]
 81%|████████▏ | 1568/1927 [35:08:07<8:00:52, 80.37s/it]
 81%|████████▏ | 1569/1927 [35:09:27<7:59:45, 80.41s/it]
 81%|████████▏ | 1570/1927 [35:10:50<8:01:42, 80.96s/it]
                                                        
{'loss': 1.3505, 'learning_rate': 4.116123345298634e-06, 'epoch': 0.81}

 81%|████████▏ | 1570/1927 [35:10:50<8:01:42, 80.96s/it]
 82%|████████▏ | 1571/1927 [35:12:10<7:59:42, 80.85s/it]
 82%|████████▏ | 1572/1927 [35:13:31<7:58:04, 80.80s/it]
 82%|████████▏ | 1573/1927 [35:14:51<7:56:19, 80.73s/it]
 82%|████████▏ | 1574/1927 [35:16:13<7:56:40, 81.02s/it]
 82%|████████▏ | 1575/1927 [35:17:33<7:53:53, 80.78s/it]
 82%|████████▏ | 1576/1927 [35:18:54<7:52:01, 80.69s/it]
 82%|████████▏ | 1577/1927 [35:20:14<7:49:57, 80.56s/it]
 82%|████████▏ | 1578/1927 [35:21:34<7:47:57, 80.45s/it]
 82%|████████▏ | 1579/1927 [35:22:55<7:46:33, 80.44s/it]
 82%|████████▏ | 1580/1927 [35:24:15<7:45:22, 80.47s/it]
                                                        
{'loss': 1.3529, 'learning_rate': 3.89485987114937e-06, 'epoch': 0.82}

 82%|████████▏ | 1580/1927 [35:24:15<7:45:22, 80.47s/it]
 82%|████████▏ | 1581/1927 [35:25:35<7:43:48, 80.43s/it]
 82%|████████▏ | 1582/1927 [35:26:56<7:42:01, 80.35s/it]
 82%|████████▏ | 1583/1927 [35:28:16<7:41:11, 80.44s/it]
 82%|████████▏ | 1584/1927 [35:29:37<7:40:07, 80.49s/it]
 82%|████████▏ | 1585/1927 [35:30:57<7:38:33, 80.45s/it]
 82%|████████▏ | 1586/1927 [35:32:18<7:37:27, 80.49s/it]
 82%|████████▏ | 1587/1927 [35:33:39<7:36:44, 80.60s/it]
 82%|████████▏ | 1588/1927 [35:34:59<7:35:39, 80.65s/it]
 82%|████████▏ | 1589/1927 [35:36:20<7:34:27, 80.67s/it]
 83%|████████▎ | 1590/1927 [35:37:41<7:32:33, 80.58s/it]
                                                        
{'loss': 1.3492, 'learning_rate': 3.679205777970901e-06, 'epoch': 0.82}

 83%|████████▎ | 1590/1927 [35:37:41<7:32:33, 80.58s/it]
 83%|████████▎ | 1591/1927 [35:39:01<7:31:26, 80.61s/it]
 83%|████████▎ | 1592/1927 [35:40:22<7:29:33, 80.52s/it]
 83%|████████▎ | 1593/1927 [35:41:42<7:27:47, 80.44s/it]
 83%|████████▎ | 1594/1927 [35:43:02<7:26:09, 80.39s/it]
 83%|████████▎ | 1595/1927 [35:44:23<7:25:03, 80.43s/it]
 83%|████████▎ | 1596/1927 [35:45:43<7:23:43, 80.43s/it]
 83%|████████▎ | 1597/1927 [35:47:04<7:22:38, 80.48s/it]
 83%|████████▎ | 1598/1927 [35:48:24<7:21:37, 80.54s/it]
 83%|████████▎ | 1599/1927 [35:49:45<7:19:52, 80.47s/it]
 83%|████████▎ | 1600/1927 [35:51:05<7:18:20, 80.43s/it]
                                                        
{'loss': 1.3537, 'learning_rate': 3.4692183828887863e-06, 'epoch': 0.83}

 83%|████████▎ | 1600/1927 [35:51:05<7:18:20, 80.43s/it]
 83%|████████▎ | 1601/1927 [35:52:25<7:17:00, 80.43s/it]
 83%|████████▎ | 1602/1927 [35:53:46<7:15:28, 80.40s/it]
 83%|████████▎ | 1603/1927 [35:55:06<7:14:25, 80.45s/it]
 83%|████████▎ | 1604/1927 [35:56:27<7:13:04, 80.45s/it]
 83%|████████▎ | 1605/1927 [35:57:47<7:11:38, 80.43s/it]
 83%|████████▎ | 1606/1927 [35:59:07<7:10:11, 80.41s/it]
 83%|████████▎ | 1607/1927 [36:00:28<7:09:27, 80.52s/it]
 83%|████████▎ | 1608/1927 [36:01:48<7:07:35, 80.42s/it]
 83%|████████▎ | 1609/1927 [36:03:09<7:06:43, 80.51s/it]
 84%|████████▎ | 1610/1927 [36:04:30<7:05:22, 80.51s/it]
                                                        
{'loss': 1.3507, 'learning_rate': 3.2649534969182883e-06, 'epoch': 0.84}

 84%|████████▎ | 1610/1927 [36:04:30<7:05:22, 80.51s/it]
 84%|████████▎ | 1611/1927 [36:05:50<7:04:12, 80.55s/it]
 84%|████████▎ | 1612/1927 [36:07:11<7:03:01, 80.58s/it]
 84%|████████▎ | 1613/1927 [36:08:32<7:01:47, 80.60s/it]
 84%|████████▍ | 1614/1927 [36:09:53<7:01:19, 80.77s/it]
 84%|████████▍ | 1615/1927 [36:11:13<6:59:48, 80.73s/it]
 84%|████████▍ | 1616/1927 [36:12:34<6:58:07, 80.67s/it]
 84%|████████▍ | 1617/1927 [36:13:55<6:56:56, 80.70s/it]
 84%|████████▍ | 1618/1927 [36:15:15<6:55:39, 80.71s/it]
 84%|████████▍ | 1619/1927 [36:16:36<6:53:55, 80.63s/it]
 84%|████████▍ | 1620/1927 [36:17:56<6:52:13, 80.57s/it]
                                                        
{'loss': 1.3539, 'learning_rate': 3.066465410130806e-06, 'epoch': 0.84}

 84%|████████▍ | 1620/1927 [36:17:56<6:52:13, 80.57s/it]
 84%|████████▍ | 1621/1927 [36:19:17<6:50:48, 80.55s/it]
 84%|████████▍ | 1622/1927 [36:20:37<6:49:29, 80.56s/it]
 84%|████████▍ | 1623/1927 [36:21:58<6:48:35, 80.64s/it]
 84%|████████▍ | 1624/1927 [36:23:19<6:47:00, 80.60s/it]
 84%|████████▍ | 1625/1927 [36:24:39<6:45:32, 80.57s/it]
 84%|████████▍ | 1626/1927 [36:26:00<6:44:06, 80.55s/it]
 84%|████████▍ | 1627/1927 [36:27:20<6:42:52, 80.57s/it]
 84%|████████▍ | 1628/1927 [36:28:41<6:41:25, 80.55s/it]
 85%|████████▍ | 1629/1927 [36:30:01<6:40:05, 80.56s/it]
 85%|████████▍ | 1630/1927 [36:31:22<6:38:57, 80.60s/it]
                                                        
{'loss': 1.365, 'learning_rate': 2.873806877224497e-06, 'epoch': 0.85}

 85%|████████▍ | 1630/1927 [36:31:22<6:38:57, 80.60s/it]
 85%|████████▍ | 1631/1927 [36:32:43<6:37:27, 80.57s/it]
 85%|████████▍ | 1632/1927 [36:34:03<6:35:58, 80.54s/it]
 85%|████████▍ | 1633/1927 [36:35:24<6:34:27, 80.50s/it]
 85%|████████▍ | 1634/1927 [36:36:44<6:32:50, 80.45s/it]
 85%|████████▍ | 1635/1927 [36:38:04<6:31:20, 80.41s/it]
 85%|████████▍ | 1636/1927 [36:39:25<6:30:12, 80.45s/it]
 85%|████████▍ | 1637/1927 [36:40:45<6:28:59, 80.48s/it]
 85%|████████▌ | 1638/1927 [36:42:06<6:27:26, 80.44s/it]
 85%|████████▌ | 1639/1927 [36:43:26<6:26:27, 80.51s/it]
 85%|████████▌ | 1640/1927 [36:44:47<6:25:04, 80.51s/it]
                                                        
{'loss': 1.349, 'learning_rate': 2.687029103502972e-06, 'epoch': 0.85}

 85%|████████▌ | 1640/1927 [36:44:47<6:25:04, 80.51s/it]
 85%|████████▌ | 1641/1927 [36:46:07<6:23:52, 80.53s/it]
 85%|████████▌ | 1642/1927 [36:47:28<6:22:35, 80.54s/it]
 85%|████████▌ | 1643/1927 [36:48:49<6:21:42, 80.64s/it]
 85%|████████▌ | 1644/1927 [36:50:09<6:20:12, 80.61s/it]
 85%|████████▌ | 1645/1927 [36:51:30<6:18:57, 80.63s/it]
 85%|████████▌ | 1646/1927 [36:52:51<6:17:26, 80.59s/it]
 85%|████████▌ | 1647/1927 [36:54:11<6:15:53, 80.55s/it]
 86%|████████▌ | 1648/1927 [36:55:32<6:14:32, 80.55s/it]
 86%|████████▌ | 1649/1927 [36:56:52<6:13:06, 80.53s/it]
 86%|████████▌ | 1650/1927 [36:58:13<6:11:46, 80.53s/it]
                                                        
{'loss': 1.361, 'learning_rate': 2.5061817312658554e-06, 'epoch': 0.86}

 86%|████████▌ | 1650/1927 [36:58:13<6:11:46, 80.53s/it]
 86%|████████▌ | 1651/1927 [36:59:33<6:10:05, 80.46s/it]
 86%|████████▌ | 1652/1927 [37:00:53<6:08:53, 80.49s/it]
 86%|████████▌ | 1653/1927 [37:02:14<6:07:46, 80.53s/it]
 86%|████████▌ | 1654/1927 [37:03:35<6:06:57, 80.65s/it]
 86%|████████▌ | 1655/1927 [37:04:55<6:05:24, 80.61s/it]
 86%|████████▌ | 1656/1927 [37:06:16<6:04:04, 80.61s/it]
 86%|████████▌ | 1657/1927 [37:07:37<6:02:33, 80.57s/it]
 86%|████████▌ | 1658/1927 [37:08:57<6:01:02, 80.53s/it]
 86%|████████▌ | 1659/1927 [37:10:18<6:00:04, 80.62s/it]
 86%|████████▌ | 1660/1927 [37:11:38<5:58:38, 80.60s/it]
                                                        
{'loss': 1.3699, 'learning_rate': 2.3313128266146873e-06, 'epoch': 0.86}

 86%|████████▌ | 1660/1927 [37:11:38<5:58:38, 80.60s/it]
 86%|████████▌ | 1661/1927 [37:12:59<5:57:07, 80.56s/it]
 86%|████████▌ | 1662/1927 [37:14:19<5:55:30, 80.49s/it]
 86%|████████▋ | 1663/1927 [37:15:40<5:54:03, 80.47s/it]
 86%|████████▋ | 1664/1927 [37:17:00<5:52:42, 80.47s/it]
 86%|████████▋ | 1665/1927 [37:18:20<5:51:17, 80.45s/it]
 86%|████████▋ | 1666/1927 [37:19:41<5:50:02, 80.47s/it]
 87%|████████▋ | 1667/1927 [37:21:02<5:49:11, 80.58s/it]
 87%|████████▋ | 1668/1927 [37:22:22<5:47:58, 80.61s/it]
 87%|████████▋ | 1669/1927 [37:23:43<5:46:19, 80.54s/it]
 87%|████████▋ | 1670/1927 [37:25:03<5:44:52, 80.51s/it]
                                                        
{'loss': 1.3485, 'learning_rate': 2.1624688666777915e-06, 'epoch': 0.87}

 87%|████████▋ | 1670/1927 [37:25:03<5:44:52, 80.51s/it]
 87%|████████▋ | 1671/1927 [37:26:24<5:43:26, 80.50s/it]
 87%|████████▋ | 1672/1927 [37:27:44<5:42:14, 80.53s/it]
 87%|████████▋ | 1673/1927 [37:29:05<5:41:09, 80.59s/it]
 87%|████████▋ | 1674/1927 [37:30:25<5:39:33, 80.53s/it]
 87%|████████▋ | 1675/1927 [37:31:46<5:38:43, 80.65s/it]
 87%|████████▋ | 1676/1927 [37:33:07<5:37:00, 80.56s/it]
 87%|████████▋ | 1677/1927 [37:34:27<5:35:41, 80.57s/it]
 87%|████████▋ | 1678/1927 [37:35:48<5:34:11, 80.53s/it]
 87%|████████▋ | 1679/1927 [37:37:09<5:33:10, 80.61s/it]
 87%|████████▋ | 1680/1927 [37:38:29<5:31:39, 80.56s/it]
                                                        
{'loss': 1.3446, 'learning_rate': 1.9996947272574672e-06, 'epoch': 0.87}

 87%|████████▋ | 1680/1927 [37:38:29<5:31:39, 80.56s/it]
 87%|████████▋ | 1681/1927 [37:39:49<5:30:05, 80.51s/it]
 87%|████████▋ | 1682/1927 [37:41:10<5:28:50, 80.53s/it]
 87%|████████▋ | 1683/1927 [37:42:31<5:27:53, 80.63s/it]
 87%|████████▋ | 1684/1927 [37:43:51<5:26:14, 80.55s/it]
 87%|████████▋ | 1685/1927 [37:45:12<5:24:38, 80.49s/it]
 87%|████████▋ | 1686/1927 [37:46:32<5:23:22, 80.51s/it]
 88%|████████▊ | 1687/1927 [37:47:53<5:21:56, 80.48s/it]
 88%|████████▊ | 1688/1927 [37:49:13<5:20:28, 80.45s/it]
 88%|████████▊ | 1689/1927 [37:50:34<5:19:44, 80.61s/it]
 88%|████████▊ | 1690/1927 [37:51:54<5:18:21, 80.60s/it]
                                                        
{'loss': 1.3615, 'learning_rate': 1.8430336709027801e-06, 'epoch': 0.88}

 88%|████████▊ | 1690/1927 [37:51:55<5:18:21, 80.60s/it]
 88%|████████▊ | 1691/1927 [37:53:15<5:16:59, 80.59s/it]
 88%|████████▊ | 1692/1927 [37:54:35<5:15:28, 80.55s/it]
 88%|████████▊ | 1693/1927 [37:55:56<5:13:57, 80.50s/it]
 88%|████████▊ | 1694/1927 [37:57:16<5:12:25, 80.45s/it]
 88%|████████▊ | 1695/1927 [37:58:37<5:11:05, 80.46s/it]
 88%|████████▊ | 1696/1927 [37:59:57<5:09:54, 80.50s/it]
 88%|████████▊ | 1697/1927 [38:01:18<5:09:16, 80.68s/it]
 88%|████████▊ | 1698/1927 [38:02:39<5:07:35, 80.59s/it]
 88%|████████▊ | 1699/1927 [38:04:00<5:06:32, 80.67s/it]
 88%|████████▊ | 1700/1927 [38:05:20<5:05:11, 80.67s/it]
                                                        
{'loss': 1.3566, 'learning_rate': 1.6925273354110982e-06, 'epoch': 0.88}

 88%|████████▊ | 1700/1927 [38:05:20<5:05:11, 80.67s/it]
 88%|████████▊ | 1701/1927 [38:06:41<5:03:40, 80.62s/it]
 88%|████████▊ | 1702/1927 [38:08:01<5:02:07, 80.57s/it]
 88%|████████▊ | 1703/1927 [38:09:22<5:00:27, 80.48s/it]
 88%|████████▊ | 1704/1927 [38:10:42<4:59:02, 80.46s/it]
 88%|████████▊ | 1705/1927 [38:12:02<4:57:41, 80.46s/it]
 89%|████████▊ | 1706/1927 [38:13:23<4:56:22, 80.46s/it]
 89%|████████▊ | 1707/1927 [38:14:43<4:55:01, 80.46s/it]
 89%|████████▊ | 1708/1927 [38:16:04<4:53:41, 80.46s/it]
 89%|████████▊ | 1709/1927 [38:17:24<4:52:30, 80.51s/it]
 89%|████████▊ | 1710/1927 [38:18:45<4:51:17, 80.54s/it]
                                                        
{'loss': 1.3486, 'learning_rate': 1.5482157227615285e-06, 'epoch': 0.89}

 89%|████████▊ | 1710/1927 [38:18:45<4:51:17, 80.54s/it]
 89%|████████▉ | 1711/1927 [38:20:05<4:49:41, 80.47s/it]
 89%|████████▉ | 1712/1927 [38:21:26<4:48:30, 80.51s/it]
 89%|████████▉ | 1713/1927 [38:22:47<4:47:19, 80.56s/it]
 89%|████████▉ | 1714/1927 [38:24:07<4:46:14, 80.63s/it]
 89%|████████▉ | 1715/1927 [38:25:28<4:44:57, 80.65s/it]
 89%|████████▉ | 1716/1927 [38:26:49<4:43:28, 80.61s/it]
 89%|████████▉ | 1717/1927 [38:28:09<4:41:57, 80.56s/it]
 89%|████████▉ | 1718/1927 [38:29:29<4:40:27, 80.51s/it]
 89%|████████▉ | 1719/1927 [38:30:50<4:39:19, 80.58s/it]
 89%|████████▉ | 1720/1927 [38:32:11<4:37:47, 80.52s/it]
                                                        
{'loss': 1.3426, 'learning_rate': 1.410137188483046e-06, 'epoch': 0.89}

 89%|████████▉ | 1720/1927 [38:32:11<4:37:47, 80.52s/it]
 89%|████████▉ | 1721/1927 [38:33:31<4:36:19, 80.48s/it]
 89%|████████▉ | 1722/1927 [38:34:52<4:35:03, 80.50s/it]
 89%|████████▉ | 1723/1927 [38:36:12<4:33:49, 80.54s/it]
 89%|████████▉ | 1724/1927 [38:37:33<4:32:35, 80.57s/it]
 90%|████████▉ | 1725/1927 [38:38:53<4:31:08, 80.54s/it]
 90%|████████▉ | 1726/1927 [38:40:14<4:29:45, 80.53s/it]
 90%|████████▉ | 1727/1927 [38:41:35<4:28:49, 80.65s/it]
 90%|████████▉ | 1728/1927 [38:42:55<4:27:38, 80.70s/it]
 90%|████████▉ | 1729/1927 [38:44:16<4:26:20, 80.71s/it]
 90%|████████▉ | 1730/1927 [38:45:37<4:24:42, 80.62s/it]
                                                        
{'loss': 1.3642, 'learning_rate': 1.2783284314602879e-06, 'epoch': 0.9}

 90%|████████▉ | 1730/1927 [38:45:37<4:24:42, 80.62s/it]
 90%|████████▉ | 1731/1927 [38:46:57<4:23:09, 80.56s/it]
 90%|████████▉ | 1732/1927 [38:48:18<4:21:44, 80.54s/it]
 90%|████████▉ | 1733/1927 [38:49:38<4:20:28, 80.56s/it]
 90%|████████▉ | 1734/1927 [38:50:59<4:19:02, 80.53s/it]
 90%|█████████ | 1735/1927 [38:52:19<4:17:33, 80.49s/it]
 90%|█████████ | 1736/1927 [38:53:39<4:16:11, 80.48s/it]
 90%|█████████ | 1737/1927 [38:55:00<4:14:53, 80.49s/it]
 90%|█████████ | 1738/1927 [38:56:21<4:13:53, 80.60s/it]
 90%|█████████ | 1739/1927 [38:57:41<4:12:35, 80.62s/it]
 90%|█████████ | 1740/1927 [38:59:02<4:11:08, 80.58s/it]
                                                        
{'loss': 1.3552, 'learning_rate': 1.1528244841795876e-06, 'epoch': 0.9}

 90%|█████████ | 1740/1927 [38:59:02<4:11:08, 80.58s/it]
 90%|█████████ | 1741/1927 [39:00:22<4:09:30, 80.49s/it]
 90%|█████████ | 1742/1927 [39:01:43<4:08:17, 80.53s/it]
 90%|█████████ | 1743/1927 [39:03:04<4:07:05, 80.58s/it]
 91%|█████████ | 1744/1927 [39:04:24<4:05:42, 80.56s/it]
 91%|█████████ | 1745/1927 [39:05:45<4:04:19, 80.55s/it]
 91%|█████████ | 1746/1927 [39:07:05<4:02:55, 80.53s/it]
 91%|█████████ | 1747/1927 [39:08:26<4:01:44, 80.58s/it]
 91%|█████████ | 1748/1927 [39:09:46<4:00:16, 80.54s/it]
 91%|█████████ | 1749/1927 [39:11:07<3:58:57, 80.55s/it]
 91%|█████████ | 1750/1927 [39:12:27<3:57:32, 80.52s/it]
                                                        
{'loss': 1.3519, 'learning_rate': 1.0336587034180001e-06, 'epoch': 0.91}

 91%|█████████ | 1750/1927 [39:12:27<3:57:32, 80.52s/it]
 91%|█████████ | 1751/1927 [39:13:48<3:56:10, 80.52s/it]
 91%|█████████ | 1752/1927 [39:15:09<3:55:13, 80.65s/it]
 91%|█████████ | 1753/1927 [39:16:29<3:53:44, 80.60s/it]
 91%|█████████ | 1754/1927 [39:17:50<3:52:30, 80.64s/it]
 91%|█████████ | 1755/1927 [39:19:10<3:51:05, 80.61s/it]
 91%|█████████ | 1756/1927 [39:20:31<3:49:40, 80.59s/it]
 91%|█████████ | 1757/1927 [39:21:52<3:48:25, 80.62s/it]
 91%|█████████ | 1758/1927 [39:23:13<3:47:13, 80.67s/it]
 91%|█████████▏| 1759/1927 [39:24:33<3:45:47, 80.64s/it]
 91%|█████████▏| 1760/1927 [39:25:54<3:44:23, 80.62s/it]
                                                        
{'loss': 1.3474, 'learning_rate': 9.208627613775861e-07, 'epoch': 0.91}

 91%|█████████▏| 1760/1927 [39:25:54<3:44:23, 80.62s/it]
 91%|█████████▏| 1761/1927 [39:27:14<3:42:46, 80.52s/it]
 91%|█████████▏| 1762/1927 [39:28:34<3:41:23, 80.51s/it]
 91%|█████████▏| 1763/1927 [39:29:55<3:40:07, 80.53s/it]
 92%|█████████▏| 1764/1927 [39:31:16<3:39:03, 80.64s/it]
 92%|█████████▏| 1765/1927 [39:32:36<3:37:27, 80.54s/it]
 92%|█████████▏| 1766/1927 [39:33:57<3:36:13, 80.58s/it]
 92%|█████████▏| 1767/1927 [39:35:17<3:34:42, 80.51s/it]
 92%|█████████▏| 1768/1927 [39:36:38<3:33:20, 80.51s/it]
 92%|█████████▏| 1769/1927 [39:37:58<3:31:53, 80.46s/it]
 92%|█████████▏| 1770/1927 [39:39:19<3:30:38, 80.50s/it]
                                                        
{'loss': 1.358, 'learning_rate': 8.144666372675414e-07, 'epoch': 0.92}

 92%|█████████▏| 1770/1927 [39:39:19<3:30:38, 80.50s/it]
 92%|█████████▏| 1771/1927 [39:40:39<3:29:21, 80.53s/it]
 92%|█████████▏| 1772/1927 [39:42:00<3:28:04, 80.55s/it]
 92%|█████████▏| 1773/1927 [39:43:21<3:26:50, 80.59s/it]
 92%|█████████▏| 1774/1927 [39:44:41<3:25:27, 80.57s/it]
 92%|█████████▏| 1775/1927 [39:46:02<3:24:16, 80.63s/it]
 92%|█████████▏| 1776/1927 [39:47:22<3:22:38, 80.52s/it]
 92%|█████████▏| 1777/1927 [39:48:43<3:21:20, 80.54s/it]
 92%|█████████▏| 1778/1927 [39:50:03<3:19:55, 80.51s/it]
 92%|█████████▏| 1779/1927 [39:51:24<3:18:32, 80.49s/it]
 92%|█████████▏| 1780/1927 [39:52:44<3:17:08, 80.47s/it]
                                                        
{'loss': 1.358, 'learning_rate': 7.144986093362116e-07, 'epoch': 0.92}

 92%|█████████▏| 1780/1927 [39:52:44<3:17:08, 80.47s/it]
 92%|█████████▏| 1781/1927 [39:54:04<3:15:48, 80.47s/it]
 92%|█████████▏| 1782/1927 [39:55:25<3:14:28, 80.47s/it]
 93%|█████████▎| 1783/1927 [39:56:45<3:13:08, 80.48s/it]
 93%|█████████▎| 1784/1927 [39:58:06<3:11:44, 80.45s/it]
 93%|█████████▎| 1785/1927 [39:59:27<3:10:44, 80.60s/it]
 93%|█████████▎| 1786/1927 [40:00:47<3:09:19, 80.57s/it]
 93%|█████████▎| 1787/1927 [40:02:08<3:07:59, 80.57s/it]
 93%|█████████▎| 1788/1927 [40:03:28<3:06:30, 80.51s/it]
 93%|█████████▎| 1789/1927 [40:04:49<3:05:07, 80.49s/it]
 93%|█████████▎| 1790/1927 [40:06:09<3:03:46, 80.48s/it]
                                                        
{'loss': 1.3594, 'learning_rate': 6.209852473552324e-07, 'epoch': 0.93}

 93%|█████████▎| 1790/1927 [40:06:09<3:03:46, 80.48s/it]
 93%|█████████▎| 1791/1927 [40:07:30<3:02:26, 80.49s/it]
 93%|█████████▎| 1792/1927 [40:08:50<3:01:05, 80.49s/it]
 93%|█████████▎| 1793/1927 [40:10:11<2:59:46, 80.50s/it]
 93%|█████████▎| 1794/1927 [40:11:31<2:58:24, 80.48s/it]
 93%|█████████▎| 1795/1927 [40:12:52<2:57:10, 80.54s/it]
 93%|█████████▎| 1796/1927 [40:14:12<2:55:55, 80.58s/it]
 93%|█████████▎| 1797/1927 [40:15:33<2:54:27, 80.52s/it]
 93%|█████████▎| 1798/1927 [40:16:53<2:52:57, 80.45s/it]
 93%|█████████▎| 1799/1927 [40:18:14<2:51:38, 80.46s/it]
 93%|█████████▎| 1800/1927 [40:19:34<2:50:16, 80.45s/it]
                                                        
{'loss': 1.3582, 'learning_rate': 5.339514055577393e-07, 'epoch': 0.93}

 93%|█████████▎| 1800/1927 [40:19:34<2:50:16, 80.45s/it]
 93%|█████████▎| 1801/1927 [40:20:54<2:48:47, 80.37s/it]
 94%|█████████▎| 1802/1927 [40:22:14<2:47:25, 80.36s/it]
 94%|█████████▎| 1803/1927 [40:23:35<2:46:07, 80.38s/it]
 94%|█████████▎| 1804/1927 [40:24:55<2:44:48, 80.39s/it]
 94%|█████████▎| 1805/1927 [40:26:16<2:43:28, 80.40s/it]
 94%|█████████▎| 1806/1927 [40:27:36<2:42:10, 80.41s/it]
 94%|█████████▍| 1807/1927 [40:28:57<2:40:56, 80.47s/it]
 94%|█████████▍| 1808/1927 [40:30:18<2:39:50, 80.59s/it]
 94%|█████████▍| 1809/1927 [40:31:38<2:38:24, 80.55s/it]
 94%|█████████▍| 1810/1927 [40:32:59<2:37:06, 80.57s/it]
                                                        
{'loss': 1.3439, 'learning_rate': 4.534202160325546e-07, 'epoch': 0.94}

 94%|█████████▍| 1810/1927 [40:32:59<2:37:06, 80.57s/it]
 94%|█████████▍| 1811/1927 [40:34:19<2:35:47, 80.58s/it]
 94%|█████████▍| 1812/1927 [40:35:40<2:34:33, 80.64s/it]
 94%|█████████▍| 1813/1927 [40:37:01<2:33:06, 80.58s/it]
 94%|█████████▍| 1814/1927 [40:38:21<2:31:40, 80.53s/it]
 94%|█████████▍| 1815/1927 [40:39:41<2:30:11, 80.46s/it]
 94%|█████████▍| 1816/1927 [40:41:02<2:28:46, 80.42s/it]
 94%|█████████▍| 1817/1927 [40:42:22<2:27:27, 80.43s/it]
 94%|█████████▍| 1818/1927 [40:43:42<2:26:00, 80.37s/it]
 94%|█████████▍| 1819/1927 [40:45:03<2:24:54, 80.50s/it]
 94%|█████████▍| 1820/1927 [40:46:23<2:23:29, 80.46s/it]
                                                        
{'loss': 1.3562, 'learning_rate': 3.794130825760689e-07, 'epoch': 0.94}

 94%|█████████▍| 1820/1927 [40:46:23<2:23:29, 80.46s/it]
 94%|█████████▍| 1821/1927 [40:47:44<2:22:08, 80.46s/it]
 95%|█████████▍| 1822/1927 [40:49:04<2:20:46, 80.45s/it]
 95%|█████████▍| 1823/1927 [40:50:25<2:19:29, 80.48s/it]
 95%|█████████▍| 1824/1927 [40:51:45<2:18:03, 80.42s/it]
 95%|█████████▍| 1825/1927 [40:53:05<2:16:30, 80.30s/it]
 95%|█████████▍| 1826/1927 [40:54:26<2:15:17, 80.37s/it]
 95%|█████████▍| 1827/1927 [40:55:46<2:14:01, 80.42s/it]
 95%|█████████▍| 1828/1927 [40:57:07<2:12:37, 80.38s/it]
 95%|█████████▍| 1829/1927 [40:58:27<2:11:17, 80.38s/it]
 95%|█████████▍| 1830/1927 [40:59:48<2:10:04, 80.46s/it]
                                                        
{'loss': 1.3488, 'learning_rate': 3.119496750034756e-07, 'epoch': 0.95}

 95%|█████████▍| 1830/1927 [40:59:48<2:10:04, 80.46s/it]
 95%|█████████▌| 1831/1927 [41:01:08<2:08:44, 80.46s/it]
 95%|█████████▌| 1832/1927 [41:02:29<2:07:30, 80.53s/it]
 95%|█████████▌| 1833/1927 [41:03:49<2:06:13, 80.57s/it]
 95%|█████████▌| 1834/1927 [41:05:10<2:05:03, 80.68s/it]
 95%|█████████▌| 1835/1927 [41:06:31<2:03:42, 80.68s/it]
 95%|█████████▌| 1836/1927 [41:07:52<2:02:16, 80.62s/it]
 95%|█████████▌| 1837/1927 [41:09:12<2:01:01, 80.69s/it]
 95%|█████████▌| 1838/1927 [41:10:33<1:59:32, 80.59s/it]
 95%|█████████▌| 1839/1927 [41:11:53<1:58:13, 80.60s/it]
 95%|█████████▌| 1840/1927 [41:13:14<1:56:59, 80.68s/it]
                                                        
{'loss': 1.3561, 'learning_rate': 2.510479239208996e-07, 'epoch': 0.95}

 95%|█████████▌| 1840/1927 [41:13:14<1:56:59, 80.68s/it]
 96%|█████████▌| 1841/1927 [41:14:35<1:55:31, 80.60s/it]
 96%|█████████▌| 1842/1927 [41:15:55<1:54:12, 80.61s/it]
 96%|█████████▌| 1843/1927 [41:17:16<1:52:47, 80.56s/it]
 96%|█████████▌| 1844/1927 [41:18:36<1:51:22, 80.52s/it]
 96%|█████████▌| 1845/1927 [41:19:57<1:49:59, 80.48s/it]
 96%|█████████▌| 1846/1927 [41:21:17<1:48:36, 80.45s/it]
 96%|█████████▌| 1847/1927 [41:22:37<1:47:14, 80.43s/it]
 96%|█████████▌| 1848/1927 [41:23:58<1:45:58, 80.48s/it]
 96%|█████████▌| 1849/1927 [41:25:18<1:44:37, 80.48s/it]
 96%|█████████▌| 1850/1927 [41:26:39<1:43:17, 80.49s/it]
                                                        
{'loss': 1.3561, 'learning_rate': 1.9672401595972333e-07, 'epoch': 0.96}

 96%|█████████▌| 1850/1927 [41:26:39<1:43:17, 80.49s/it]
 96%|█████████▌| 1851/1927 [41:28:00<1:42:01, 80.55s/it]
 96%|█████████▌| 1852/1927 [41:29:20<1:40:41, 80.56s/it]
 96%|█████████▌| 1853/1927 [41:30:41<1:39:19, 80.53s/it]
 96%|█████████▌| 1854/1927 [41:32:01<1:37:58, 80.52s/it]
 96%|█████████▋| 1855/1927 [41:33:22<1:36:39, 80.55s/it]
 96%|█████████▋| 1856/1927 [41:34:42<1:35:14, 80.49s/it]
 96%|█████████▋| 1857/1927 [41:36:02<1:33:53, 80.47s/it]
 96%|█████████▋| 1858/1927 [41:37:23<1:32:31, 80.46s/it]
 96%|█████████▋| 1859/1927 [41:38:43<1:31:09, 80.44s/it]
 97%|█████████▋| 1860/1927 [41:40:04<1:29:49, 80.45s/it]
                                                        
{'loss': 1.3477, 'learning_rate': 1.489923894744727e-07, 'epoch': 0.96}

 97%|█████████▋| 1860/1927 [41:40:04<1:29:49, 80.45s/it]
 97%|█████████▋| 1861/1927 [41:41:24<1:28:28, 80.43s/it]
 97%|█████████▋| 1862/1927 [41:42:45<1:27:12, 80.49s/it]
 97%|█████████▋| 1863/1927 [41:44:05<1:25:49, 80.46s/it]
 97%|█████████▋| 1864/1927 [41:45:26<1:24:29, 80.47s/it]
 97%|█████████▋| 1865/1927 [41:46:46<1:23:11, 80.51s/it]
 97%|█████████▋| 1866/1927 [41:48:07<1:21:47, 80.46s/it]
 97%|█████████▋| 1867/1927 [41:49:27<1:20:31, 80.53s/it]
 97%|█████████▋| 1868/1927 [41:50:48<1:19:12, 80.55s/it]
 97%|█████████▋| 1869/1927 [41:52:08<1:17:50, 80.53s/it]
 97%|█████████▋| 1870/1927 [41:53:29<1:16:26, 80.46s/it]
                                                        
{'loss': 1.3513, 'learning_rate': 1.0786573070535877e-07, 'epoch': 0.97}

 97%|█████████▋| 1870/1927 [41:53:29<1:16:26, 80.46s/it]
 97%|█████████▋| 1871/1927 [41:54:49<1:15:05, 80.46s/it]
 97%|█████████▋| 1872/1927 [41:56:10<1:13:43, 80.43s/it]
 97%|█████████▋| 1873/1927 [41:57:30<1:12:19, 80.37s/it]
 97%|█████████▋| 1874/1927 [41:58:50<1:11:01, 80.41s/it]
 97%|█████████▋| 1875/1927 [42:00:10<1:09:37, 80.34s/it]
 97%|█████████▋| 1876/1927 [42:01:31<1:08:19, 80.39s/it]
 97%|█████████▋| 1877/1927 [42:02:51<1:06:56, 80.33s/it]
 97%|█████████▋| 1878/1927 [42:04:11<1:05:34, 80.30s/it]
 98%|█████████▊| 1879/1927 [42:05:32<1:04:16, 80.34s/it]
 98%|█████████▊| 1880/1927 [42:06:52<1:02:56, 80.34s/it]
                                                        
{'loss': 1.3484, 'learning_rate': 7.335497040648898e-08, 'epoch': 0.98}

 98%|█████████▊| 1880/1927 [42:06:52<1:02:56, 80.34s/it]
 98%|█████████▊| 1881/1927 [42:08:13<1:01:37, 80.37s/it]
 98%|█████████▊| 1882/1927 [42:09:33<1:00:19, 80.42s/it]
 98%|█████████▊| 1883/1927 [42:10:54<59:06, 80.60s/it]  
 98%|█████████▊| 1884/1927 [42:12:15<57:45, 80.59s/it]
 98%|█████████▊| 1885/1927 [42:13:35<56:22, 80.55s/it]
 98%|█████████▊| 1886/1927 [42:14:56<55:03, 80.58s/it]
 98%|█████████▊| 1887/1927 [42:16:16<53:41, 80.55s/it]
 98%|█████████▊| 1888/1927 [42:17:37<52:20, 80.52s/it]
 98%|█████████▊| 1889/1927 [42:18:57<50:58, 80.49s/it]
 98%|█████████▊| 1890/1927 [42:20:17<49:36, 80.45s/it]
                                                      
{'loss': 1.3486, 'learning_rate': 4.5469280940654835e-08, 'epoch': 0.98}

 98%|█████████▊| 1890/1927 [42:20:18<49:36, 80.45s/it]
 98%|█████████▊| 1891/1927 [42:21:38<48:16, 80.45s/it]
 98%|█████████▊| 1892/1927 [42:22:59<46:58, 80.52s/it]
 98%|█████████▊| 1893/1927 [42:24:19<45:35, 80.44s/it]
 98%|█████████▊| 1894/1927 [42:25:39<44:13, 80.42s/it]
 98%|█████████▊| 1895/1927 [42:26:59<42:51, 80.36s/it]
 98%|█████████▊| 1896/1927 [42:28:20<41:30, 80.34s/it]
 98%|█████████▊| 1897/1927 [42:29:40<40:11, 80.37s/it]
 98%|█████████▊| 1898/1927 [42:31:01<38:53, 80.45s/it]
 99%|█████████▊| 1899/1927 [42:32:21<37:32, 80.44s/it]
 99%|█████████▊| 1900/1927 [42:33:42<36:16, 80.60s/it]
                                                      
{'loss': 1.3541, 'learning_rate': 2.4216073841487608e-08, 'epoch': 0.99}

 99%|█████████▊| 1900/1927 [42:33:42<36:16, 80.60s/it]
 99%|█████████▊| 1901/1927 [42:35:03<34:53, 80.54s/it]
 99%|█████████▊| 1902/1927 [42:36:23<33:33, 80.52s/it]
 99%|█████████▉| 1903/1927 [42:37:44<32:12, 80.52s/it]
 99%|█████████▉| 1904/1927 [42:39:04<30:53, 80.58s/it]
 99%|█████████▉| 1905/1927 [42:40:25<29:32, 80.58s/it]
 99%|█████████▉| 1906/1927 [42:41:46<28:13, 80.65s/it]
 99%|█████████▉| 1907/1927 [42:43:07<26:53, 80.69s/it]
 99%|█████████▉| 1908/1927 [42:44:27<25:33, 80.70s/it]
 99%|█████████▉| 1909/1927 [42:45:48<24:11, 80.66s/it]
 99%|█████████▉| 1910/1927 [42:47:08<22:51, 80.66s/it]
                                                      
{'loss': 1.3629, 'learning_rate': 9.600997843597914e-09, 'epoch': 0.99}

 99%|█████████▉| 1910/1927 [42:47:09<22:51, 80.66s/it]
 99%|█████████▉| 1911/1927 [42:48:29<21:29, 80.60s/it]
 99%|█████████▉| 1912/1927 [42:49:50<20:09, 80.63s/it]
 99%|█████████▉| 1913/1927 [42:51:10<18:47, 80.55s/it]
 99%|█████████▉| 1914/1927 [42:52:31<17:27, 80.56s/it]
 99%|█████████▉| 1915/1927 [42:53:51<16:07, 80.60s/it]
 99%|█████████▉| 1916/1927 [42:55:12<14:46, 80.56s/it]
 99%|█████████▉| 1917/1927 [42:56:32<13:24, 80.50s/it]
100%|█████████▉| 1918/1927 [42:57:53<12:04, 80.52s/it]
100%|█████████▉| 1919/1927 [42:59:13<10:43, 80.44s/it]
100%|█████████▉| 1920/1927 [43:00:33<09:23, 80.46s/it]
                                                      
{'loss': 1.3528, 'learning_rate': 1.6279373812350119e-09, 'epoch': 1.0}

100%|█████████▉| 1920/1927 [43:00:33<09:23, 80.46s/it]
100%|█████████▉| 1921/1927 [43:01:54<08:02, 80.45s/it]
100%|█████████▉| 1922/1927 [43:03:14<06:42, 80.50s/it]
100%|█████████▉| 1923/1927 [43:04:35<05:22, 80.53s/it]
100%|█████████▉| 1924/1927 [43:05:56<04:01, 80.50s/it]
100%|█████████▉| 1925/1927 [43:07:16<02:40, 80.47s/it]
100%|█████████▉| 1926/1927 [43:08:37<01:20, 80.51s/it]
100%|██████████| 1927/1927 [43:09:57<00:00, 80.55s/it][INFO|trainer.py:1988] 2024-03-08 12:51:16,931 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)



                                                      
{'train_runtime': 155397.6719, 'train_samples_per_second': 3.175, 'train_steps_per_second': 0.012, 'train_loss': 1.4047479545383423, 'epoch': 1.0}

100%|██████████| 1927/1927 [43:09:57<00:00, 80.55s/it]
100%|██████████| 1927/1927 [43:09:57<00:00, 80.64s/it]
[INFO|trainer.py:2979] 2024-03-08 12:51:32,341 >> Saving model checkpoint to /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1
[2024-03-08 12:51:40,427] [INFO] [launch.py:347:main] Process 7007 exits successfully.
[2024-03-08 12:51:42,430] [INFO] [launch.py:347:main] Process 7006 exits successfully.
[2024-03-08 12:51:42,431] [INFO] [launch.py:347:main] Process 7005 exits successfully.
[INFO|tokenization_utils_base.py:2435] 2024-03-08 12:52:58,741 >> tokenizer config file saved in /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-03-08 12:52:58,782 >> Special tokens file saved in /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1/special_tokens_map.json
***** train metrics *****
  epoch                    =                1.0
  train_loss               =             1.4047
  train_runtime            = 1 day, 19:09:57.67
  train_samples_per_second =              3.175
  train_steps_per_second   =              0.012
Figure saved: /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-top1/training_loss.png
03/08/2024 12:53:00 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-03-08 12:53:00,425 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-03-08 12:53:05,524] [INFO] [launch.py:347:main] Process 7004 exits successfully.
