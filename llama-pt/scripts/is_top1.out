[2024-02-26 09:31:29,394] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-26 09:31:30,430] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-02-26 09:31:30,431] [INFO] [runner.py:568:main] cmd = /home/yangdezhao/anaconda3/envs/zhouh/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json --stage pt --model_name_or_path /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf --flash_attn --do_train --dataset is_1b --preprocessing_num_workers 20 --cutoff_len 2048 --finetuning_type moe --moe_every_k_layers 1 --moe_router_type top1 --moe_num_experts 2 --output_dir /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1 --overwrite_output_dir --per_device_train_batch_size 12 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 1 --save_only_model --save_steps 100000 --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-02-26 09:31:32,505] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-26 09:31:33,496] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-26 09:31:33,496] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-26 09:31:33,497] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-26 09:31:33,497] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-26 09:31:33,497] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-26 09:31:36,930] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-26 09:31:36,959] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-26 09:31:36,980] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-26 09:31:37,031] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-26 09:31:39,484] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-26 09:31:39,491] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-26 09:31:39,499] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-26 09:31:39,499] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-26 09:31:39,524] [INFO] [comm.py:637:init_distributed] cdb=None
02/26/2024 09:31:39 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/26/2024 09:31:39 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1/runs/Feb26_09-31-39_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=12,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=100000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/26/2024 09:31:39 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
02/26/2024 09:31:40 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/26/2024 09:31:40 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1/runs/Feb26_09-31-39_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=12,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=100000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-02-26 09:31:40,621 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2027] 2024-02-26 09:31:40,621 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-02-26 09:31:40,621 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-02-26 09:31:40,621 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-02-26 09:31:40,621 >> loading file tokenizer.json
02/26/2024 09:31:40 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/26/2024 09:31:40 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1/runs/Feb26_09-31-39_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=12,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=100000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/26/2024 09:31:40 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/26/2024 09:31:40 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1/runs/Feb26_09-31-39_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=12,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=100000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:727] 2024-02-26 09:31:40,726 >> loading configuration file /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:792] 2024-02-26 09:31:40,727 >> Model config LlamaConfig {
  "_name_or_path": "/home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

02/26/2024 09:31:40 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
[INFO|modeling_utils.py:3334] 2024-02-26 09:31:40,746 >> loading weights file /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1459] 2024-02-26 09:31:40,746 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[WARNING|logging.py:329] 2024-02-26 09:31:40,746 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[WARNING|logging.py:329] 2024-02-26 09:31:40,747 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:827] 2024-02-26 09:31:40,749 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

02/26/2024 09:31:40 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
02/26/2024 09:31:40 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.14s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.30s/it]
02/26/2024 09:31:42 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/26/2024 09:31:42 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
02/26/2024 09:31:43 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
02/26/2024 09:31:43 - INFO - llmtuner.data.template - Add pad token: </s>
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.84s/it]
02/26/2024 09:31:44 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/26/2024 09:31:44 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.02s/it]02/26/2024 09:31:45 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
02/26/2024 09:31:45 - INFO - llmtuner.data.template - Add pad token: </s>
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it]
[INFO|modeling_utils.py:4070] 2024-02-26 09:31:45,723 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4078] 2024-02-26 09:31:45,723 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-02-26 09:31:45,726 >> loading configuration file /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:827] 2024-02-26 09:31:45,726 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

02/26/2024 09:31:45 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/26/2024 09:31:45 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
02/26/2024 09:31:46 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
02/26/2024 09:31:46 - INFO - llmtuner.data.template - Add pad token: </s>
Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]
02/26/2024 09:31:46 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/26/2024 09:31:46 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
02/26/2024 09:31:46 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
02/26/2024 09:31:46 - INFO - llmtuner.data.template - Add pad token: </s>
02/26/2024 09:31:52 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
Using custom data configuration default-5a96c05fd21bf34e
Loading Dataset Infos from /home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Downloading and preparing dataset json/default to /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 12087.33it/s]
Downloading took 0.0 min
Checksum Computation took 0.0 min
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1562.71it/s]
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10155 examples [00:00, 66914.72 examples/s]Generating train split: 22586 examples [00:00, 82411.96 examples/s]Generating train split: 34964 examples [00:00, 87822.67 examples/s]Generating train split: 47412 examples [00:00, 91016.27 examples/s]Generating train split: 59830 examples [00:00, 93627.37 examples/s]Generating train split: 71828 examples [00:00, 91912.11 examples/s]Generating train split: 84116 examples [00:00, 92039.56 examples/s]Generating train split: 94198 examples [00:01, 90487.17 examples/s]Generating train split: 106723 examples [00:01, 93529.57 examples/s]Generating train split: 118916 examples [00:01, 93718.45 examples/s]Generating train split: 130936 examples [00:01, 92446.37 examples/s]Generating train split: 143305 examples [00:01, 93100.15 examples/s]Generating train split: 155096 examples [00:01, 92116.32 examples/s]Generating train split: 167048 examples [00:01, 91401.87 examples/s]Generating train split: 179208 examples [00:01, 93994.09 examples/s]Generating train split: 191644 examples [00:02, 94539.94 examples/s]Generating train split: 203670 examples [00:02, 93605.70 examples/s]Generating train split: 216093 examples [00:02, 92903.44 examples/s]Generating train split: 228184 examples [00:02, 93421.67 examples/s]Generating train split: 240193 examples [00:02, 93614.31 examples/s]Generating train split: 252720 examples [00:02, 94030.75 examples/s]Generating train split: 264866 examples [00:02, 94597.97 examples/s]Generating train split: 277104 examples [00:03, 91384.04 examples/s]Generating train split: 291359 examples [00:03, 87098.98 examples/s]Generating train split: 303816 examples [00:03, 89359.26 examples/s]Generating train split: 316089 examples [00:03, 89663.25 examples/s]Generating train split: 328405 examples [00:03, 90248.14 examples/s]Generating train split: 340589 examples [00:03, 90548.69 examples/s]Generating train split: 352683 examples [00:03, 91289.22 examples/s]Generating train split: 364834 examples [00:03, 93118.71 examples/s]Generating train split: 376714 examples [00:04, 91362.60 examples/s]Generating train split: 388555 examples [00:04, 92147.33 examples/s]Generating train split: 400770 examples [00:04, 92666.52 examples/s]Generating train split: 412811 examples [00:04, 90349.66 examples/s]Generating train split: 425037 examples [00:04, 91177.15 examples/s]Generating train split: 437231 examples [00:04, 92440.76 examples/s]Generating train split: 449431 examples [00:04, 94487.22 examples/s]Generating train split: 461627 examples [00:05, 95392.28 examples/s]Generating train split: 473724 examples [00:05, 93807.37 examples/s]Generating train split: 485915 examples [00:05, 95135.43 examples/s]Generating train split: 498410 examples [00:05, 96228.25 examples/s]Generating train split: 510506 examples [00:05, 95145.96 examples/s]Generating train split: 520453 examples [00:05, 91295.88 examples/s]Generating train split: 532682 examples [00:05, 93005.79 examples/s]Generating train split: 544559 examples [00:05, 91747.28 examples/s]Generating train split: 556772 examples [00:06, 92451.98 examples/s]Generating train split: 568949 examples [00:06, 93212.78 examples/s]Generating train split: 581232 examples [00:06, 94168.78 examples/s]Generating train split: 593337 examples [00:06, 94703.08 examples/s]Generating train split: 605484 examples [00:06, 93843.66 examples/s]Generating train split: 617956 examples [00:06, 94210.24 examples/s]Generating train split: 619741 examples [00:06, 92339.02 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
Process #0 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00000_of_00020.arrow
Process #1 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00001_of_00020.arrow
Process #2 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00002_of_00020.arrow
Process #3 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00003_of_00020.arrow
Process #4 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00004_of_00020.arrow
Process #5 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00005_of_00020.arrow
Process #6 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00006_of_00020.arrow
Process #7 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00007_of_00020.arrow
Process #8 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00008_of_00020.arrow
Process #9 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00009_of_00020.arrow
Process #10 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00010_of_00020.arrow
Process #11 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00011_of_00020.arrow
Process #12 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00012_of_00020.arrow
Process #13 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00013_of_00020.arrow
Process #14 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00014_of_00020.arrow
Process #15 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00015_of_00020.arrow
Process #16 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00016_of_00020.arrow
Process #17 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00017_of_00020.arrow
Process #18 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00018_of_00020.arrow
Process #19 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00019_of_00020.arrow
Spawning 20 processes
Converting format of dataset (num_proc=20):   0%|          | 0/619741 [00:00<?, ? examples/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00001_of_00020.arrow
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00000_of_00020.arrow
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00003_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00004_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00006_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00002_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00014_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00018_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00005_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00007_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00015_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00019_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00011_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00010_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00013_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00009_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00016_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00008_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00012_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00017_of_00020.arrow
Converting format of dataset (num_proc=20):   0%|          | 1000/619741 [00:00<04:45, 2167.65 examples/s]Converting format of dataset (num_proc=20):   9%|▊         | 54000/619741 [00:00<00:04, 126891.56 examples/s]Converting format of dataset (num_proc=20):  20%|██        | 126000/619741 [00:00<00:01, 276068.86 examples/s]Converting format of dataset (num_proc=20):  31%|███       | 190000/619741 [00:00<00:01, 371585.40 examples/s]Converting format of dataset (num_proc=20):  42%|████▏     | 258000/619741 [00:00<00:00, 454234.93 examples/s]Converting format of dataset (num_proc=20):  53%|█████▎    | 329000/619741 [00:00<00:00, 523774.86 examples/s]Converting format of dataset (num_proc=20):  66%|██████▌   | 406000/619741 [00:01<00:00, 568709.27 examples/s]Converting format of dataset (num_proc=20):  78%|███████▊  | 483000/619741 [00:01<00:00, 624635.40 examples/s]Converting format of dataset (num_proc=20):  89%|████████▉ | 552987/619741 [00:01<00:00, 643458.81 examples/s]Converting format of dataset (num_proc=20): 100%|██████████| 619741/619741 [00:03<00:00, 203914.90 examples/s]
Concatenating 20 shards
Process #0 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00000_of_00020.arrow
Process #1 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00001_of_00020.arrow
Process #2 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00002_of_00020.arrow
Process #3 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00003_of_00020.arrow
Process #4 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00004_of_00020.arrow
Process #5 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00005_of_00020.arrow
Process #6 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00006_of_00020.arrow
Process #7 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00007_of_00020.arrow
Process #8 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00008_of_00020.arrow
Process #9 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00009_of_00020.arrow
Process #10 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00010_of_00020.arrow
Process #11 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00011_of_00020.arrow
Process #12 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00012_of_00020.arrow
Process #13 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00013_of_00020.arrow
Process #14 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00014_of_00020.arrow
Process #15 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00015_of_00020.arrow
Process #16 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00016_of_00020.arrow
Process #17 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00017_of_00020.arrow
Process #18 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00018_of_00020.arrow
Process #19 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00019_of_00020.arrow
02/26/2024 09:32:16 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
02/26/2024 09:32:16 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
02/26/2024 09:32:16 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
Spawning 20 processes
Running tokenizer on dataset (num_proc=20):   0%|          | 0/619741 [00:00<?, ? examples/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00000_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00005_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00015_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00006_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00003_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00001_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00013_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   0%|          | 1000/619741 [00:07<1:14:23, 138.63 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00004_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00018_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   0%|          | 3000/619741 [00:07<19:36, 524.08 examples/s]  Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00009_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00011_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00007_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00014_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00008_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   1%|          | 7000/619741 [00:07<06:43, 1518.79 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00019_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00016_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   2%|▏         | 10000/619741 [00:07<04:04, 2495.88 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00010_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   2%|▏         | 12000/619741 [00:07<03:02, 3330.11 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00012_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   2%|▏         | 15000/619741 [00:07<02:04, 4872.84 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00002_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   3%|▎         | 17000/619741 [00:08<01:40, 6022.14 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f636e624988216d3_00017_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   3%|▎         | 19000/619741 [00:08<01:43, 5808.96 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 21000/619741 [00:13<08:45, 1139.65 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▎         | 22000/619741 [00:14<08:04, 1234.38 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▎         | 23000/619741 [00:14<06:43, 1479.70 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 25000/619741 [00:14<04:42, 2105.09 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 28000/619741 [00:14<02:55, 3372.57 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 31000/619741 [00:14<01:59, 4934.83 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 34000/619741 [00:15<01:31, 6383.65 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 36000/619741 [00:15<01:28, 6626.59 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 38000/619741 [00:15<01:30, 6438.87 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▋         | 40000/619741 [00:16<01:28, 6560.89 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 41000/619741 [00:20<08:33, 1127.94 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 42000/619741 [00:21<07:49, 1230.14 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 44000/619741 [00:21<05:34, 1720.65 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 47000/619741 [00:21<03:20, 2861.81 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 50000/619741 [00:21<02:10, 4360.09 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 52000/619741 [00:21<01:47, 5290.78 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▊         | 54000/619741 [00:22<01:50, 5110.84 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 56000/619741 [00:22<01:40, 5618.14 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 58000/619741 [00:22<01:21, 6934.31 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 60000/619741 [00:23<01:34, 5928.32 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 62000/619741 [00:27<07:09, 1297.53 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 63000/619741 [00:28<07:32, 1230.58 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 66000/619741 [00:28<04:28, 2065.95 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 69000/619741 [00:28<02:54, 3159.87 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█▏        | 71000/619741 [00:29<02:34, 3562.41 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 73000/619741 [00:29<02:06, 4324.88 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 75000/619741 [00:30<02:18, 3941.55 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 77000/619741 [00:30<01:49, 4958.27 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 79000/619741 [00:30<01:28, 6110.59 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 81000/619741 [00:34<06:13, 1441.85 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 83000/619741 [00:35<05:58, 1495.86 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▎        | 85000/619741 [00:35<04:23, 2026.51 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 86000/619741 [00:35<03:56, 2261.26 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 89000/619741 [00:36<02:28, 3571.38 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 90000/619741 [00:36<02:12, 4002.98 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 91000/619741 [00:36<02:04, 4258.56 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 93000/619741 [00:36<01:40, 5254.03 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 95000/619741 [00:37<01:53, 4622.21 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 96000/619741 [00:37<01:59, 4378.27 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 97000/619741 [00:37<01:50, 4726.45 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 98000/619741 [00:37<02:02, 4242.66 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 100000/619741 [00:38<02:17, 3774.44 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▋        | 101000/619741 [00:41<06:53, 1253.90 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▋        | 102000/619741 [00:41<05:47, 1491.18 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 103000/619741 [00:42<06:40, 1289.21 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 105000/619741 [00:42<04:09, 2065.87 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 106000/619741 [00:42<03:29, 2449.13 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 107000/619741 [00:42<02:52, 2968.23 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 108000/619741 [00:43<02:57, 2885.69 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 110000/619741 [00:43<02:10, 3904.55 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 112000/619741 [00:43<01:38, 5179.57 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 113000/619741 [00:44<01:54, 4437.68 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 115000/619741 [00:44<01:23, 6069.07 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 117000/619741 [00:44<01:04, 7802.05 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 119000/619741 [00:45<02:17, 3651.02 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 120000/619741 [00:45<02:28, 3375.52 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 121000/619741 [00:48<05:59, 1386.39 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 122000/619741 [00:48<05:40, 1460.81 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 123000/619741 [00:48<04:54, 1685.30 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 124000/619741 [00:49<03:50, 2149.82 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 126000/619741 [00:49<03:22, 2442.63 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 128000/619741 [00:50<02:45, 2974.43 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 129000/619741 [00:50<02:23, 3418.02 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 130000/619741 [00:50<02:01, 4038.51 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██▏       | 132000/619741 [00:50<01:29, 5431.27 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██▏       | 133000/619741 [00:50<01:51, 4381.11 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 136000/619741 [00:51<01:10, 6846.44 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 137000/619741 [00:51<01:06, 7216.42 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 138000/619741 [00:52<02:13, 3616.31 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 139000/619741 [00:52<02:56, 2717.60 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 140000/619741 [00:53<03:46, 2113.83 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 141000/619741 [00:54<04:41, 1698.44 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 142000/619741 [00:55<05:54, 1347.83 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 143000/619741 [00:56<05:42, 1391.86 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 144000/619741 [00:56<04:18, 1842.92 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 145000/619741 [00:56<03:56, 2003.28 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▎       | 146000/619741 [00:57<03:54, 2017.64 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 148000/619741 [00:57<02:18, 3402.08 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 149000/619741 [00:57<02:15, 3473.55 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 150000/619741 [00:57<02:00, 3886.91 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 151000/619741 [00:57<01:46, 4404.83 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 153000/619741 [00:58<01:14, 6265.79 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 156000/619741 [00:58<00:56, 8234.49 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 157000/619741 [00:59<01:47, 4293.06 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 158000/619741 [00:59<02:07, 3621.14 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 160000/619741 [01:00<02:35, 2947.90 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 161000/619741 [01:01<04:25, 1728.10 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 162000/619741 [01:02<04:01, 1896.50 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▋       | 163000/619741 [01:03<04:39, 1632.57 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▋       | 164000/619741 [01:04<05:44, 1322.12 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 166000/619741 [01:04<03:35, 2101.14 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 167000/619741 [01:04<02:56, 2571.12 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 170000/619741 [01:04<01:50, 4059.65 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 171000/619741 [01:04<01:43, 4317.43 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 173000/619741 [01:05<01:18, 5656.53 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 176000/619741 [01:05<01:03, 7034.79 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 177000/619741 [01:06<01:52, 3946.04 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 178000/619741 [01:06<02:05, 3526.90 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 179000/619741 [01:07<02:13, 3310.02 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 180000/619741 [01:07<02:28, 2969.95 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 181000/619741 [01:08<04:06, 1777.63 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 183000/619741 [01:10<04:25, 1646.01 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 184000/619741 [01:10<04:59, 1455.31 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 186000/619741 [01:11<03:09, 2289.08 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 187000/619741 [01:11<03:03, 2355.62 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 188000/619741 [01:11<02:36, 2752.36 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 190000/619741 [01:11<01:45, 4079.20 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 193000/619741 [01:12<01:32, 4608.92 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███▏      | 194000/619741 [01:12<01:49, 3877.27 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 196000/619741 [01:13<01:44, 4041.09 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 198000/619741 [01:13<01:57, 3589.58 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 199000/619741 [01:14<01:48, 3895.31 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 201000/619741 [01:15<02:50, 2451.61 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 202000/619741 [01:16<03:34, 1951.17 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 203000/619741 [01:16<03:36, 1927.16 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 204000/619741 [01:17<03:47, 1823.88 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 205000/619741 [01:18<03:35, 1926.10 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 206000/619741 [01:18<03:08, 2198.47 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 209000/619741 [01:18<01:53, 3608.44 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 211000/619741 [01:18<01:34, 4341.35 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 212000/619741 [01:19<01:57, 3483.12 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 213000/619741 [01:19<01:44, 3887.72 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 214000/619741 [01:20<02:17, 2942.62 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 216000/619741 [01:20<01:37, 4141.62 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 217000/619741 [01:20<01:32, 4366.82 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 219000/619741 [01:21<01:41, 3946.82 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 221000/619741 [01:22<02:24, 2749.99 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 222000/619741 [01:23<03:34, 1857.23 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 223000/619741 [01:23<03:23, 1947.32 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 224000/619741 [01:24<03:53, 1696.84 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▋      | 226000/619741 [01:24<02:27, 2662.63 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 227000/619741 [01:25<02:39, 2466.80 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 228000/619741 [01:25<02:19, 2804.97 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 229000/619741 [01:26<02:32, 2567.13 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 231000/619741 [01:26<01:47, 3627.69 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 234000/619741 [01:27<01:47, 3599.75 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 235000/619741 [01:27<01:39, 3867.26 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 236000/619741 [01:27<01:36, 3960.85 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 237000/619741 [01:27<01:28, 4302.30 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▊      | 239000/619741 [01:27<01:10, 5369.42 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▊      | 240000/619741 [01:28<01:26, 4399.68 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 241000/619741 [01:29<02:36, 2423.34 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 242000/619741 [01:29<02:36, 2420.12 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 243000/619741 [01:31<04:09, 1512.09 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 244000/619741 [01:31<03:45, 1664.10 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 245000/619741 [01:31<03:22, 1853.37 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 246000/619741 [01:32<02:49, 2200.10 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 247000/619741 [01:32<02:26, 2541.39 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 249000/619741 [01:32<01:37, 3792.80 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 250000/619741 [01:32<01:27, 4232.35 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 251000/619741 [01:33<01:26, 4271.12 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 253000/619741 [01:33<01:05, 5620.12 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 254000/619741 [01:34<02:26, 2502.79 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 255000/619741 [01:34<02:04, 2936.82 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████▏     | 256000/619741 [01:34<02:03, 2946.35 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████▏     | 257000/619741 [01:35<01:46, 3409.27 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 259000/619741 [01:35<01:48, 3334.89 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 260000/619741 [01:35<01:42, 3515.13 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 261000/619741 [01:36<01:48, 3302.14 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 262000/619741 [01:37<02:49, 2114.54 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 263000/619741 [01:38<04:16, 1391.06 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 265000/619741 [01:39<02:58, 1983.97 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 266000/619741 [01:39<02:45, 2136.33 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 269000/619741 [01:39<01:37, 3585.26 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▎     | 271000/619741 [01:39<01:23, 4156.58 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 272000/619741 [01:40<01:38, 3542.26 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 273000/619741 [01:41<02:04, 2787.56 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 274000/619741 [01:41<01:53, 3038.80 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 275000/619741 [01:41<01:39, 3457.69 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 277000/619741 [01:41<01:19, 4320.83 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 278000/619741 [01:42<01:50, 3101.02 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 279000/619741 [01:42<01:45, 3220.40 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 281000/619741 [01:43<01:37, 3487.12 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 282000/619741 [01:43<02:08, 2623.13 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 283000/619741 [01:45<03:22, 1664.86 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 284000/619741 [01:45<02:43, 2051.00 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 285000/619741 [01:45<02:08, 2603.53 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 286000/619741 [01:46<02:44, 2028.12 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 287000/619741 [01:46<02:11, 2526.27 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 288000/619741 [01:46<01:51, 2979.92 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 289000/619741 [01:46<01:43, 3204.18 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 291000/619741 [01:47<01:18, 4176.51 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 292000/619741 [01:47<01:10, 4661.61 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 293000/619741 [01:47<01:20, 4058.71 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 294000/619741 [01:47<01:25, 3807.38 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 295000/619741 [01:48<02:04, 2606.74 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 296000/619741 [01:48<01:40, 3216.71 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 297000/619741 [01:49<01:44, 3089.35 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 298000/619741 [01:49<01:31, 3528.13 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 299000/619741 [01:49<01:59, 2687.70 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 300000/619741 [01:50<01:43, 3094.81 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▊     | 301000/619741 [01:50<01:45, 3026.74 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▊     | 302000/619741 [01:51<02:31, 2094.83 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 303000/619741 [01:51<02:10, 2429.87 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 304000/619741 [01:52<03:49, 1377.78 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 306000/619741 [01:53<02:14, 2330.98 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 307000/619741 [01:53<02:11, 2382.20 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 308000/619741 [01:53<01:44, 2969.48 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 309000/619741 [01:53<01:46, 2925.05 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 310000/619741 [01:54<01:27, 3523.79 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 312000/619741 [01:54<01:21, 3770.44 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 313000/619741 [01:54<01:20, 3820.00 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 315000/619741 [01:54<00:56, 5432.43 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 316000/619741 [01:55<01:43, 2948.55 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 317000/619741 [01:56<01:55, 2613.15 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 318000/619741 [01:56<02:07, 2370.10 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 320000/619741 [01:56<01:19, 3747.27 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 321000/619741 [01:57<01:32, 3235.07 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 322000/619741 [01:58<02:03, 2414.46 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 323000/619741 [01:58<02:13, 2230.01 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 324000/619741 [01:59<02:29, 1978.68 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 325000/619741 [02:00<02:55, 1675.91 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 328000/619741 [02:00<01:42, 2838.95 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 329000/619741 [02:01<01:45, 2755.96 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 331000/619741 [02:01<01:13, 3914.92 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 332000/619741 [02:02<01:51, 2589.30 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 334000/619741 [02:02<01:27, 3279.50 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 336000/619741 [02:03<01:54, 2478.32 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 338000/619741 [02:03<01:26, 3248.19 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 339000/619741 [02:04<01:27, 3195.86 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 340000/619741 [02:04<01:15, 3688.61 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 342000/619741 [02:05<01:33, 2966.43 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 343000/619741 [02:05<01:23, 3326.07 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 344000/619741 [02:06<02:21, 1948.92 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 345000/619741 [02:07<02:43, 1680.29 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 346000/619741 [02:07<02:17, 1996.40 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 348000/619741 [02:07<01:39, 2725.06 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 349000/619741 [02:08<01:38, 2742.80 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 350000/619741 [02:08<01:29, 3010.46 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 351000/619741 [02:08<01:17, 3472.28 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 352000/619741 [02:08<01:13, 3651.43 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 354000/619741 [02:09<00:58, 4567.00 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 355000/619741 [02:09<01:00, 4401.02 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 356000/619741 [02:09<01:06, 3979.38 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 357000/619741 [02:10<01:41, 2582.83 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 358000/619741 [02:10<01:29, 2928.31 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 360000/619741 [02:11<01:15, 3429.97 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 361000/619741 [02:11<01:04, 3990.98 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 363000/619741 [02:12<01:32, 2773.93 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 364000/619741 [02:14<02:53, 1475.46 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 365000/619741 [02:14<02:23, 1772.69 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 366000/619741 [02:14<01:57, 2165.91 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 367000/619741 [02:14<01:47, 2361.69 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 368000/619741 [02:15<01:34, 2674.16 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 371000/619741 [02:15<01:02, 3961.86 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 373000/619741 [02:15<00:50, 4858.06 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 374000/619741 [02:16<00:54, 4491.49 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 375000/619741 [02:16<00:52, 4669.95 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 376000/619741 [02:17<01:53, 2148.43 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 379000/619741 [02:18<01:13, 3266.97 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 381000/619741 [02:18<01:02, 3824.60 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 382000/619741 [02:18<01:02, 3795.23 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 383000/619741 [02:19<01:32, 2559.43 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 384000/619741 [02:21<02:44, 1437.11 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 385000/619741 [02:21<02:14, 1739.41 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 386000/619741 [02:21<01:48, 2145.53 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 388000/619741 [02:22<01:23, 2772.01 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 389000/619741 [02:22<01:14, 3095.61 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 390000/619741 [02:22<01:12, 3187.58 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 391000/619741 [02:22<01:02, 3679.83 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 392000/619741 [02:22<00:54, 4148.78 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 393000/619741 [02:23<00:49, 4576.20 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 395000/619741 [02:23<00:40, 5502.14 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 396000/619741 [02:23<01:02, 3602.24 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 397000/619741 [02:24<01:08, 3273.58 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 399000/619741 [02:25<01:23, 2644.17 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 401000/619741 [02:25<01:00, 3615.21 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 402000/619741 [02:25<00:59, 3662.65 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 403000/619741 [02:26<01:20, 2700.90 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 404000/619741 [02:28<02:48, 1280.68 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 405000/619741 [02:28<02:11, 1631.81 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 408000/619741 [02:28<01:09, 3049.55 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 409000/619741 [02:29<01:20, 2627.87 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 410000/619741 [02:29<01:13, 2835.54 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 411000/619741 [02:29<01:01, 3402.34 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 413000/619741 [02:29<00:46, 4402.33 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 414000/619741 [02:30<00:54, 3807.04 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 416000/619741 [02:30<00:38, 5286.05 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 417000/619741 [02:31<01:01, 3313.48 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 418000/619741 [02:31<00:53, 3763.45 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 419000/619741 [02:32<01:26, 2307.95 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 420000/619741 [02:32<01:23, 2392.77 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 422000/619741 [02:33<01:02, 3139.12 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 423000/619741 [02:33<01:06, 2959.32 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 424000/619741 [02:34<01:53, 1719.43 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 425000/619741 [02:35<01:41, 1918.92 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 426000/619741 [02:35<01:48, 1781.24 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 427000/619741 [02:36<01:38, 1961.78 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 428000/619741 [02:36<01:17, 2464.12 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 429000/619741 [02:36<01:01, 3096.25 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 430000/619741 [02:36<00:49, 3808.18 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 432000/619741 [02:36<00:32, 5732.86 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 433000/619741 [02:37<00:42, 4344.58 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 434000/619741 [02:37<00:59, 3139.14 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 436000/619741 [02:37<00:43, 4248.01 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 437000/619741 [02:38<00:47, 3858.47 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 438000/619741 [02:38<00:47, 3827.78 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 439000/619741 [02:39<01:17, 2346.42 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 440000/619741 [02:39<01:10, 2567.49 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 441000/619741 [02:39<00:55, 3197.31 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████▏  | 442000/619741 [02:40<01:06, 2674.69 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 444000/619741 [02:41<01:27, 2016.02 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 445000/619741 [02:42<01:38, 1771.44 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 446000/619741 [02:42<01:22, 2113.81 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 447000/619741 [02:43<01:22, 2102.90 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 448000/619741 [02:43<01:07, 2547.41 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 451000/619741 [02:43<00:40, 4137.07 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 453000/619741 [02:45<01:08, 2430.57 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 456000/619741 [02:45<00:42, 3894.35 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 458000/619741 [02:45<00:34, 4722.88 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 459000/619741 [02:46<01:05, 2439.63 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 460000/619741 [02:47<01:03, 2502.48 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 462000/619741 [02:47<00:44, 3511.73 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 463000/619741 [02:47<00:43, 3604.58 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 464000/619741 [02:48<01:10, 2193.84 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 465000/619741 [02:49<01:22, 1864.44 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 466000/619741 [02:49<01:06, 2311.49 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 467000/619741 [02:49<00:57, 2642.07 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 468000/619741 [02:49<00:47, 3220.89 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 469000/619741 [02:50<00:38, 3924.09 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 470000/619741 [02:50<00:45, 3280.32 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 471000/619741 [02:50<00:45, 3289.48 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 472000/619741 [02:50<00:39, 3747.61 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 473000/619741 [02:51<01:01, 2385.49 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 474000/619741 [02:51<00:53, 2712.92 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 476000/619741 [02:52<00:44, 3215.89 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 477000/619741 [02:52<00:51, 2772.86 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 479000/619741 [02:53<00:59, 2350.22 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 481000/619741 [02:54<00:48, 2831.72 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 483000/619741 [02:54<00:36, 3736.11 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 484000/619741 [02:55<00:39, 3412.13 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 485000/619741 [02:56<01:08, 1967.12 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 486000/619741 [02:56<01:07, 1977.66 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▊  | 488000/619741 [02:57<00:45, 2879.31 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 489000/619741 [02:57<00:47, 2768.42 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 490000/619741 [02:57<00:43, 2964.81 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 491000/619741 [02:57<00:35, 3580.24 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 492000/619741 [02:58<00:42, 3008.02 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 493000/619741 [02:58<00:45, 2812.61 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 494000/619741 [02:59<00:45, 2764.29 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 495000/619741 [02:59<00:43, 2869.33 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 496000/619741 [02:59<00:41, 3003.38 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 497000/619741 [03:00<00:40, 3009.48 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 498000/619741 [03:00<00:42, 2865.14 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 499000/619741 [03:01<01:00, 1994.10 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 501000/619741 [03:01<00:45, 2591.75 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 502000/619741 [03:01<00:39, 2976.63 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 503000/619741 [03:02<00:38, 3057.04 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████▏ | 505000/619741 [03:02<00:37, 3038.64 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 506000/619741 [03:03<00:52, 2167.95 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 507000/619741 [03:03<00:42, 2667.60 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 509000/619741 [03:04<00:32, 3434.39 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 512000/619741 [03:05<00:40, 2649.74 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 513000/619741 [03:06<00:39, 2735.38 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 514000/619741 [03:06<00:40, 2624.84 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 515000/619741 [03:06<00:35, 2943.37 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 516000/619741 [03:07<00:35, 2958.07 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 517000/619741 [03:07<00:35, 2898.92 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 518000/619741 [03:07<00:33, 3054.57 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 519000/619741 [03:07<00:31, 3217.11 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 520000/619741 [03:08<00:31, 3172.55 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 521000/619741 [03:08<00:33, 2916.73 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 522000/619741 [03:09<00:38, 2509.26 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 524000/619741 [03:09<00:24, 3981.43 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 525000/619741 [03:10<00:43, 2154.97 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 526000/619741 [03:10<00:45, 2080.59 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 528000/619741 [03:11<00:31, 2905.47 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 530000/619741 [03:11<00:29, 3059.85 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 531000/619741 [03:12<00:29, 3027.74 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 532000/619741 [03:12<00:26, 3316.33 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 533000/619741 [03:13<00:37, 2343.26 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 534000/619741 [03:13<00:33, 2594.43 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▋ | 535000/619741 [03:13<00:26, 3182.62 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▋ | 536000/619741 [03:14<00:38, 2190.23 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 538000/619741 [03:15<00:32, 2484.94 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 539000/619741 [03:15<00:30, 2665.73 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 541000/619741 [03:15<00:21, 3634.19 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 542000/619741 [03:15<00:21, 3697.43 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 543000/619741 [03:16<00:22, 3459.25 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 544000/619741 [03:16<00:26, 2843.86 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 545000/619741 [03:16<00:21, 3490.95 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 546000/619741 [03:17<00:32, 2243.49 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 547000/619741 [03:18<00:28, 2555.86 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 549000/619741 [03:18<00:26, 2711.17 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 551000/619741 [03:19<00:20, 3434.60 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 553000/619741 [03:20<00:27, 2439.48 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 554000/619741 [03:20<00:26, 2512.31 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 555000/619741 [03:20<00:22, 2845.45 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 556000/619741 [03:21<00:26, 2440.14 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 557000/619741 [03:21<00:27, 2268.71 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 558000/619741 [03:22<00:23, 2588.39 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 559000/619741 [03:22<00:21, 2815.25 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 561000/619741 [03:22<00:14, 4070.99 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 562000/619741 [03:22<00:12, 4561.99 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 563000/619741 [03:23<00:13, 4314.68 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 564000/619741 [03:23<00:15, 3576.63 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 565000/619741 [03:23<00:15, 3505.07 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████▏| 566000/619741 [03:24<00:26, 2065.22 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 569000/619741 [03:25<00:17, 2833.37 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 570000/619741 [03:25<00:17, 2803.10 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 572000/619741 [03:26<00:12, 3897.45 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 573000/619741 [03:27<00:21, 2194.93 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 574000/619741 [03:27<00:19, 2337.18 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 575000/619741 [03:28<00:23, 1882.28 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 576000/619741 [03:28<00:18, 2317.79 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 577000/619741 [03:28<00:15, 2746.32 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 578000/619741 [03:28<00:12, 3370.69 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 579000/619741 [03:29<00:13, 2990.44 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▎| 581000/619741 [03:29<00:12, 3182.44 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 582000/619741 [03:30<00:12, 3073.97 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 583000/619741 [03:31<00:15, 2367.13 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 585000/619741 [03:31<00:11, 2991.70 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 586000/619741 [03:31<00:12, 2653.19 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 587000/619741 [03:32<00:11, 2801.07 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 588000/619741 [03:32<00:11, 2816.38 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 589000/619741 [03:32<00:09, 3252.12 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 590000/619741 [03:32<00:07, 3863.25 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 591000/619741 [03:33<00:07, 3721.83 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 592000/619741 [03:33<00:06, 4441.98 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 592987/619741 [03:34<00:10, 2562.14 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 593987/619741 [03:34<00:08, 3157.92 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 594987/619741 [03:35<00:12, 1954.99 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 595974/619741 [03:35<00:12, 1934.10 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 596974/619741 [03:36<00:11, 2063.96 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 598962/619741 [03:36<00:06, 3032.37 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 600962/619741 [03:36<00:04, 4056.53 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 601949/619741 [03:37<00:04, 3720.84 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 602936/619741 [03:38<00:07, 2233.20 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 603936/619741 [03:38<00:06, 2480.08 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 604923/619741 [03:38<00:05, 2720.58 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 606910/619741 [03:39<00:04, 2955.32 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 608884/619741 [03:39<00:02, 4041.35 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 609871/619741 [03:39<00:02, 4202.99 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▊| 611845/619741 [03:40<00:03, 2431.19 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 613819/619741 [03:41<00:02, 2312.63 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 614806/619741 [03:42<00:02, 1974.58 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 615793/619741 [03:42<00:01, 2217.63 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 616780/619741 [03:43<00:01, 2325.67 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 618754/619741 [03:45<00:00, 1625.76 examples/s]Running tokenizer on dataset (num_proc=20): 100%|██████████| 619741/619741 [03:45<00:00, 1721.43 examples/s]Running tokenizer on dataset (num_proc=20): 100%|██████████| 619741/619741 [03:46<00:00, 2740.43 examples/s]
Concatenating 20 shards
input_ids:
[435, 30178, 4375, 29954, 29967, 30094, 29943, 341, 30175, 29940, 438, 29954, 476, 30232, 29934, 30178, 323, 6227, 379, 5348, 29903, 319, 10262, 1718, 29903, 448, 1605, 355, 1212, 13, 29953, 29953, 30073, 29940, 272, 30189, 332, 29928, 2190, 29924, 30094, 29934, 29968, 29924, 1430, 29915, 29903, 6850, 29979, 1307, 28577, 2672, 13171, 3094, 1964, 1254, 29979, 1307, 13, 30232, 29873, 492, 3516, 30189, 7019, 398, 14921, 1984, 289, 2518, 4677, 29876, 381, 3976, 29871, 30340, 2108, 273, 9523, 30189, 29892, 289, 7012, 381, 263, 30189, 1147, 29874, 3514, 273, 14468, 29871, 29945, 28786, 3671, 29871, 30340, 29976, 3602, 398, 3516, 30189, 263, 30189, 2377, 1764, 376, 30232, 15761, 398, 3516, 30189, 14921, 1984, 289, 2518, 263, 30189, 10235, 29874, 298, 369, 3963, 30189, 5848, 19615, 5444, 14468, 432, 29980, 3110, 12382, 29888, 29892, 321, 30189, 29874, 302, 30052, 1764, 24721, 2039, 688, 29884, 29892, 1999, 284, 2883, 29908, 785, 16210, 321, 30189, 29874, 289, 2518, 2215, 29874, 14468, 301, 30030, 29916, 375, 1341, 29983, 29889, 9444, 604, 16210, 394, 29122, 28683, 29983, 808, 29873, 29892, 3671, 27442, 1436, 29876, 303, 321, 6859, 814, 408, 29876, 744, 4141, 29871, 30340, 387, 279, 263, 30189, 12416, 330, 1572, 29871, 30340, 29874, 30189, 29889, 3067, 29887, 658, 5444, 30189, 29875, 289, 2518, 19421, 2464, 29888, 398, 27442, 263, 30189, 7779, 343, 29878, 30189, 29875, 14921, 1984, 28316, 16301, 30189, 275, 29892, 7779, 560, 4621, 263, 30189, 10235, 29874, 29892, 7779, 560, 4621, 263, 30189, 282, 30078, 433, 3671, 3731, 2681, 29892, 29871, 30078, 29871, 30340, 29875, 30189, 325, 4812, 30189, 29892, 599, 29873, 14468, 413, 5393, 398, 330, 1764, 29888, 20860, 30189, 1099, 1922, 432, 29980, 1915, 29892, 427, 298, 1064, 604, 7779, 29889, 3067, 29887, 9814, 30189, 29875, 29871, 30340, 11300, 14468, 28786, 29889, 29871, 30452, 11300, 722, 24883, 299, 279, 298, 29894, 548, 29875, 24721, 2039, 26281, 29892, 321, 30189, 29874, 286, 431, 433, 29892, 4934, 332, 12979, 1984, 29889, 16462, 1984, 3052, 16757, 1764, 4934, 332, 29892, 7779, 604, 343, 28034, 29542, 29871, 1715, 30078, 29887, 30189, 332, 592, 30189, 29871, 30340, 11300, 3671, 7779, 592, 3055, 2377, 1764, 380, 557, 29895, 10282, 29976, 29871, 30340, 404, 29884, 29889, 10630, 30189, 330, 29976, 29888, 398, 298, 369, 3963, 30189, 5848, 323, 513, 332, 3720, 22833, 4347, 1424, 29976, 29871, 29953, 29953, 30073, 29940, 272, 30189, 332, 785, 427, 285, 4316, 303, 722, 1820, 415, 1153, 29884, 30189, 29899, 932, 1379, 3642, 688, 2497, 3671, 29871, 30340, 29874, 30189, 4677, 1652, 29926, 29980, 698, 14468, 301, 29926, 7173, 263, 30189, 29871, 30340, 29874, 30189, 722, 14921, 1984, 269, 1690, 29879, 263, 30189, 3516, 30189, 330, 30078, 29873, 398, 316, 2782, 3720, 22833, 348, 1240, 29889, 10630, 30189, 3764, 398, 2215, 29876, 381, 14468, 1589, 407, 1240, 298, 369, 325, 557, 1056, 30189, 29875, 285, 4316, 29878, 3976, 286, 990, 29876, 1648, 6928, 263, 30189, 16511, 4207, 398, 14468, 325, 2559, 4347, 29892, 427, 3516, 30189, 286, 30078, 29873, 398, 3976, 269, 3304, 260, 29983, 655, 269, 1567, 21416, 29889, 13, 29903, 1365, 3514, 804, 290, 352, 17698, 30189, 722, 263, 30189, 3516, 30189, 286, 870, 398, 316, 4233, 23911, 1056, 30189, 29875, 3976, 302, 30052, 29878, 374, 3720, 29880, 3746, 29892, 321, 30189, 29874, 298, 812, 14468, 1153, 348, 262, 1240, 376, 1989, 415, 29875, 4365, 3720, 29878, 29908, 623, 1379, 3642, 688, 21528, 3671, 7779, 302, 30052, 698, 29875, 29871, 30340, 812, 6584, 292, 10282, 29983, 3731, 1340, 9161, 29889, 317, 1365, 10442, 1147, 30189, 398, 3516, 30189, 298, 11500, 18597, 314, 381, 6928, 29871, 30078, 17191, 17029, 29889, 1174, 3720, 29880, 8357, 604, 592, 3055, 3671, 1375, 1056, 10282, 29879, 2495, 7779, 29892, 427, 29871, 30340, 585, 3976, 698, 29884, 1011, 29874, 3976, 15012, 17930, 371, 29887, 1943, 298, 1064, 14468, 9879, 1785, 29892, 3031, 722, 10263, 1335, 289, 2518, 2071, 309, 370, 29877, 30189, 1424, 29976, 394, 6391, 262, 398, 263, 30189, 7779, 3976, 698, 29875, 263, 30189, 867, 29983, 3274, 298, 1648, 29889, 3067, 29887, 604, 1075, 2559, 29880, 361, 392, 29875, 592, 30189, 298, 1648, 29892, 3671, 12979, 21474, 3127, 2559, 3031, 7779, 604, 29892, 29871, 30340, 29976, 10263, 30189, 332, 27442, 1011, 29879, 3671, 7779, 540, 29888, 16511, 30189, 1011, 29882, 369, 29926, 398, 304, 407, 29889, 3067, 29887, 330, 24900, 13445, 1397, 29874, 560, 4621, 29871, 30340, 9297, 3720, 29880, 3746, 29889, 13, 25639, 913, 1137, 414, 14166, 3976, 269, 5963, 566, 810, 29885, 990, 1240, 785, 9083, 2574, 330, 13533, 263, 30189, 316, 4233, 29871, 30340, 9584, 592, 30189, 343, 6859, 332, 263, 30189, 29871, 30340, 404, 279, 289, 1314, 332, 604, 29884, 302, 30052, 1989, 415, 279, 3671, 29871, 30340, 17930, 3764, 29884, 28316, 29871, 30340, 29878, 29997, 865, 279, 263, 30189, 7779, 22500, 1896, 29871, 30340, 17930, 29871, 30078, 29873, 6092, 30189, 29884, 263, 30189, 7689, 996, 1764, 3976, 27442, 413, 25293, 595, 279, 29871, 30340, 387, 279, 7779, 1820, 415, 29875, 29871, 30340, 17930, 29892, 28316, 330, 29976, 21154, 29871, 30340, 17930, 4365, 29871, 30340, 29874, 30189, 286, 10058, 30189, 263, 30189, 7779, 330, 30078, 2034, 470, 30189, 29875, 30189, 29871, 5471, 29948, 698, 332, 3671, 451, 29874, 30189, 29871, 30340, 17930, 3976, 592, 30189, 29887, 29997, 865, 29884, 29889, 379, 3228, 335, 29871, 30340, 29878, 996, 29875, 7779, 289, 1314, 332, 29973, 317, 29926, 29980, 30189, 29874, 29871, 30340, 17930, 14468, 29871, 29929, 29900, 30073, 29973, 22305, 343, 29915, 497, 29889, 13, 29967, 30178, 4375, 30178, 16033, 1964, 9047, 1177, 29940, 341, 1177, 29940, 29871, 29906, 29900, 29896, 29955, 448, 13, 30178, 6059, 17896, 29923, 29902, 402, 1307, 30633, 29902, 1307, 29954, 2190, 383, 29979, 29934, 1254, 29909, 5012, 1660, 9486, 1001, 6824, 6824, 8982, 365, 30175, 3738, 30633, 382, 29968, 29968, 29902, 360, 30102, 29903, 5194, 1307, 23799, 476, 4717, 29968, 29968, 1718, 6824, 3067, 29887, 604, 289, 30030, 2559, 263, 30189, 413, 585, 3274, 599, 517, 29888, 15276, 279, 432, 29980, 3110, 1764, 28034, 3671, 27442, 1436, 29876, 303, 7779, 20642, 321, 4324, 2989, 29873, 321, 615, 381, 29889, 360, 12382, 8159, 560, 4621, 7779, 29871, 30340, 2108, 273, 260, 29983, 655, 3976, 2288, 29889, 3067, 29887, 604, 289, 15948, 303, 2142, 1397, 29874, 491, 29878, 1764, 30189, 332, 263, 30189, 13547, 1764, 6836, 30189, 332, 9489, 2679, 29895, 329, 29983, 25096, 29892, 3671, 604, 289, 30030, 2559, 263, 30189, 731, 1764, 2989, 29873, 14468, 25364, 1581, 29875, 30189, 22219, 28316, 7779, 30098, 2, 18574, 1049, 448, 14109, 29892, 1424, 29926, 2464, 4977, 394, 1341, 30078, 30189, 381, 4812, 30189, 13, 11746, 262, 2151, 260, 686, 398, 2464, 29901, 14859, 30340, 30052, 4621, 29892, 7319, 29890, 29983, 4621, 13, 29950, 29997, 21154, 30189, 5173, 30189, 332, 29901, 25132, 13, 20754, 9144, 348, 29901, 29871, 29941, 29889, 23882, 29871, 29896, 29929, 29929, 29900, 13, 29943, 5066, 2817, 2464, 29901, 29871, 29896, 29947, 29889, 29946, 29906, 29900, 29892, 29896, 29945, 2383, 30088, 13, 29924, 812, 29888, 12382, 430, 29875, 29901, 29871, 29946, 29892, 29900, 29945, 29900, 286, 14042, 29980, 29889, 313, 29941, 29900, 29889, 18251, 29871, 29906, 29900, 29896, 29946, 29897, 13, 30452, 29948, 698, 280, 10058, 491, 1505, 30189, 279, 29901, 29871, 29906, 29906, 29900, 29914, 8848, 30088, 13, 29963, 1389, 29879, 29983, 30189, 29874, 29901, 269, 14145, 29889, 311, 13, 29943, 943, 30078, 28898, 6135, 30189, 2276, 336, 29901, 5765, 476, 2267, 816, 1050, 313, 6530, 29965, 29897, 13, 29903, 1165, 1049, 313, 30340, 30052, 4621, 29901, 3878, 2079, 271, 23783, 29936, 7319, 29890, 29983, 4621, 29901, 3925, 711, 397, 1460, 1002, 19736, 4621, 29897, 604, 260, 29983, 8917, 380, 17930, 5173, 269, 1117, 4167, 1049, 29871, 30452, 30052, 808, 284, 4167, 29892, 29871, 29896, 29947, 29889, 29946, 29906, 29900, 2383, 11298, 29871, 30452, 29874, 30189, 604, 263, 30189, 269, 3304, 2071, 2754, 29871, 30340, 29874, 30189, 269, 1117, 4167, 1049, 3031, 302, 17930, 28537, 303, 6928, 21996, 1295, 29889, 27614, 18574, 1049, 29875, 289, 30030, 29874, 29871, 29946, 3533, 29926, 888, 381, 767, 1056, 313, 29941, 29900, 29889, 18251, 29871, 29906, 29900, 29896, 29946, 29897, 3671, 604, 29871, 30340, 11300, 29871, 30340, 9584, 19421, 8705, 29874, 285, 29926, 5873, 1527, 29876, 5427, 269, 1117, 4167, 1049, 29871, 30452, 30052, 808, 284, 4167, 29889, 11623, 21154, 30189, 14203, 262, 604, 25132, 29889, 2191, 30189, 284, 2791, 369, 30189, 336, 16511, 698, 30030, 26822, 29891, 374, 6050, 30078, 336, 14468, 18574, 1049, 29875, 10269, 452, 29888, 1056, 1652, 29926, 29980, 2034, 30189, 18574, 761, 29875, 29892, 285, 29926, 497, 5397, 30189, 2559, 17390, 29888, 12382, 645, 313, 2110, 29920, 4310, 381, 479, 29897, 3671, 8188, 698, 25934, 299, 381, 24156, 14468, 18574, 4515, 1335, 15012, 790, 313, 29903, 29437, 4070, 22703, 467, 13, 29903, 1165, 1049, 14172, 29887, 332, 21996, 579, 14468, 29871, 30452, 30052, 808, 11627, 29875, 3671, 3976, 301, 29997, 865, 2982, 314, 30078, 374, 263, 30189, 323, 5859, 29895, 1049, 29875, 29892, 427, 1011, 22234, 263, 30189, 349, 29980, 645, 392, 29875, 29889, 383, 29891, 12416, 3643, 30189, 273, 604, 269, 1117, 4167, 1049, 29875, 30189, 1771, 7257, 14203, 29892, 285, 29891, 12416, 3643, 30189, 10147, 273, 604, 23783, 29899, 2744, 10647, 3671, 285, 29891, 12416, 16779, 273, 604, 29884, 29871, 30452, 30052, 5393, 11627, 313, 1349, 1276, 4289, 29897, 3671, 350, 30078, 4758, 11627, 29889, 13, 29943, 29976, 1240, 18574, 5252, 604, 592, 30189, 260, 345, 326, 332, 301, 3054, 29948, 698, 398, 367, 6859, 29926, 398, 29892, 298, 29894, 2468, 398, 263, 30189, 310, 273, 3671, 867, 30078, 1949, 263, 30189, 452, 30189, 273, 29889, 4971, 29926, 2741, 279, 1050, 1984, 30189, 604, 29871, 30340, 369, 29878, 29997, 299, 29980, 698, 29892, 3731, 442, 3671, 3671, 330, 499, 29892, 592, 30189, 867, 30078, 1949, 9820, 30189, 29874, 3976, 2071, 29976, 343, 28034, 29889, 402, 21528, 3671, 3731, 1340, 9161, 7697, 595, 279, 604, 29884, 10282, 3389, 24156, 1424, 29976, 26579, 273, 29983, 261, 29899, 30078, 698, 262, 1240, 3976, 29871, 29896, 29906, 29889, 3963, 430, 29889, 1632, 30078, 1056, 2071, 29976, 4089, 30189, 273, 398, 722, 289, 30078, 698, 3516, 30189, 29871, 29896, 29906, 29953, 29900, 29889, 29871, 30452, 387, 279, 26579, 273, 29983, 261, 29899, 30078, 698, 262, 270, 29980, 3720, 29873, 260, 15948, 2791, 7979, 361, 2559, 1424, 29976, 2191, 13119, 269, 1064, 29871, 30340, 11300, 260, 29976, 3959, 3671, 540, 22613, 29871, 30340, 29874, 30189, 298, 2741, 391, 16296, 30189, 273, 29889, 13, 2816, 30189, 29875, 30189, 23783, 337, 23843, 364, 15827, 16296, 1056, 6928, 22593, 19041, 29871, 30078, 698, 29890, 2464, 2039, 1144, 18574, 29874, 29889, 1394, 30189, 29875, 30189, 872, 5389, 604, 270, 1727, 29875, 30189, 2511, 24988, 433, 22593, 19041, 470, 30189, 8675, 26024, 29879, 29892, 3031, 2778, 14166, 269, 369, 30189, 321, 30189, 29874, 6361, 332, 298, 2213, 22613, 313, 29879, 1182, 29889, 872, 29916, 3976, 14468, 2536, 575, 2120, 467, 518, 29896, 29962, 13, 29903, 1165, 1049, 16511, 30189, 29875, 3976, 30189, 332, 285, 4316, 29878, 343, 28034, 15207, 30189, 29976, 698, 398, 10058, 30189, 3731, 30078, 30189, 29875, 14468, 3643, 30189, 332, 4415, 6637, 10442, 369, 392, 29875, 29871, 30452, 30052, 808, 284, 4167, 29889, 3467, 5288, 4211, 7381, 298, 814, 15948, 22243, 273, 298, 29880, 6637, 29871, 30340, 404, 3731, 30078, 30189, 275, 16296, 30189, 433, 3976, 29871, 29947, 29889, 3963, 430, 3671, 14468, 10282, 29882, 2142, 29875, 29871, 30340, 29872, 381, 13678, 302, 29983, 870, 29884, 29889, 15012, 30078, 30189, 29875, 30189, 3031, 590, 299, 279, 10442, 369, 392, 29875, 18574, 1049, 722, 29871, 30340, 29980, 14921, 1984, 263, 30189, 2989, 29884, 954, 29875, 30189, 2511, 22593, 29997, 1949, 285, 4316, 29878, 427, 592, 30189, 2982, 13980, 1195, 29884, 286, 638, 433, 3976, 29871, 29896, 29906, 29889, 3963, 430, 29889, 1174, 29876, 14468, 12136, 24964, 29878, 269, 4112, 4515, 23843, 1375, 1240, 4415, 11321, 14468, 298, 6947, 30189, 8675, 29889, 2379, 342, 289, 30078, 1764, 29899, 3671, 289, 990, 29874, 354, 4812, 29871, 30340, 279, 604, 29884, 269, 4112, 4515, 29895, 263, 30189, 10282, 3389, 29874, 29889, 6162, 5173, 659, 1388, 30078, 698, 262, 14468, 298, 6947, 30189, 8675, 722, 26579, 273, 29983, 261, 29899, 30078, 698, 262, 29889, 7509, 303, 2559, 14468, 18574, 1049, 29875, 722, 413, 24900, 22613, 303, 29875, 14468, 29871, 30340, 30052, 4621, 364, 23576, 8675, 29889, 7260, 374, 30189, 29871, 29896, 29946, 29947, 29945, 413, 417, 29888, 3433, 30189, 29884, 22500, 582, 30189, 262, 18574, 1049, 3671, 29871, 30452, 30052, 5393, 11627, 14468, 9631, 29997, 22500, 582, 30189, 29889, 27614, 29871, 29941, 29900, 3976]
inputs:
JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet
66°NorðurDANMÖRKMEN'S STYLENEW INPERSONALSTYLE
Ætli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.
Svo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.
Drulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y'all.
JÓLAÓSKALISTINN MINN 2017 -
ÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…</s> Saxland - Wikipedia, frjálsa alfræðiritið
Opinbert tungumál: háþýska, sorbíska
Höfuðstaður: Dresden
Stofnun: 3. október 1990
Flatarmál: 18.420,15 km²
Mannfjöldi: 4,050 mljó. (30. september 2014)
Þéttleiki byggðar: 220/km²
Vefsíða: sachsen.de
Forsætisráðherra: Michael Kretschmer (CDU)
Saxland (þýska: Freistaat Sachsen; sorbíska: Swobodny stat Sakska) er tíunda stærsta sambandsland Þýskalands, 18.420 km². Það er að sama skapi það sambandsland sem nær lengst til austurs. Í Saxlandi búa 4 milljónir manna (30. september 2014) og er þetta því sjötta fjölmennasta sambandsland Þýskalands. Höfuðborgin er Dresden. Meðal markverðra náttúrufyrirbæra í Saxlandi má nefna fljótið Saxelfi, fjallgarðinn Erzfjöll (Erzgebirge) og klettamyndirnar í Saxneska Sviss (Sächsiche Schweiz).
Saxland liggur austast í Þýskalandi og á löng landamæri að Tékklandi, en einnig að Póllandi. Fyrir norðan er sambandslandið Brandenborg, fyrir norðvestan er Sachsen-Anhalt og fyrir vestan eru Þýringaland (Thüringen) og Bæjaraland.
Fáni Saxlands er með tveimur láréttum bekkjum, hvítum að ofan og grænum að neðan. Skjaldarmerkið er þverröndótt, svart og og gult, með grænum borða á ská yfir. Gulu og svörtu rendurnar eru upprunnar frá Askaníer-ættinni á 12. öld. Græna skáborðanum var bætt við 1260. Þegar Askaníer-ættin dó út tók markgreifinn frá Meissen sér þetta tákn og hefur það haldist síðan.
Orðið Sachsen rekur rót sína til germanska ættbálksins Saxa. Orðið saxi er dregið af gamla germanska orðinu sahs, sem merkir sverð eða langur hnífur (sbr. sax á íslensku). [1]
Saxland náði áður fyrr yfir víðáttumikið svæði í norðurhluta núverandi Þýskalands. Karlamagnús hertók mestan hluta þess svæðis síðla á 8. öld og í upphafi þeirrar níundu. Svæðið sem myndar núverandi Saxland var þó ekki að fullu numið af germönum fyrr en með landnáminu mikla á 12. öld. Enn í dag býr slavneskur minnihluti í héraðinu. Flest bæja- og borgaheiti þar eru slavnesk að uppruna. Helsta valdaættin í héraðinu var Askaníer-ættin. Furstinn í Saxlandi var kjörfursti í þýska ríkinu. Árið 1485 klofnuðu héruðin Saxland og Þýringaland í tvö héruð. Í 30 á
Caching indices mapping at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ef3597bd689d146b.arrow
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 487956
})
[INFO|trainer.py:586] 2024-02-26 09:36:05,989 >> Using auto half precision backend
[INFO|deepspeed.py:325] 2024-02-26 09:36:06,317 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yangdezhao/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4821512699127197 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-02-26 09:36:10,667] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
Running tokenizer on dataset (num_proc=20):   0%|          | 0/619741 [00:00<?, ? examples/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
Running tokenizer on dataset (num_proc=20):   0%|          | 0/619741 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 0/619741 [00:00<?, ? examples/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.
  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)
Running tokenizer on dataset (num_proc=20):   0%|          | 1000/619741 [00:11<1:58:19, 87.16 examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 1000/619741 [00:11<1:54:52, 89.77 examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 2000/619741 [00:12<54:04, 190.37 examples/s] Running tokenizer on dataset (num_proc=20):   0%|          | 2000/619741 [00:11<51:05, 201.53 examples/s] Running tokenizer on dataset (num_proc=20):   1%|          | 4000/619741 [00:12<22:01, 465.86 examples/s]Running tokenizer on dataset (num_proc=20):   1%|          | 6000/619741 [00:13<12:21, 827.62 examples/s]Running tokenizer on dataset (num_proc=20):   1%|▏         | 8000/619741 [00:13<07:39, 1330.70 examples/s]Running tokenizer on dataset (num_proc=20):   1%|▏         | 9000/619741 [00:13<06:24, 1587.90 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 11000/619741 [00:13<04:10, 2434.35 examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 3000/619741 [00:13<33:32, 306.42 examples/s]Running tokenizer on dataset (num_proc=20):   1%|          | 4000/619741 [00:13<20:47, 493.74 examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 1000/619741 [00:13<2:16:22, 75.61 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 14000/619741 [00:14<02:50, 3550.77 examples/s]Running tokenizer on dataset (num_proc=20):   1%|          | 5000/619741 [00:13<14:07, 725.58 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 15000/619741 [00:14<02:45, 3664.22 examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 2000/619741 [00:13<58:45, 175.23 examples/s] Running tokenizer on dataset (num_proc=20):   1%|          | 6000/619741 [00:13<10:26, 979.05 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 17000/619741 [00:14<02:16, 4417.48 examples/s]Running tokenizer on dataset (num_proc=20):   1%|          | 5000/619741 [00:13<17:05, 599.73 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 18000/619741 [00:14<02:26, 4114.01 examples/s]Running tokenizer on dataset (num_proc=20):   1%|          | 7000/619741 [00:14<08:57, 1140.32 examples/s]Running tokenizer on dataset (num_proc=20):   1%|          | 6000/619741 [00:14<13:51, 738.35 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 19000/619741 [00:15<02:15, 4424.98 examples/s]Running tokenizer on dataset (num_proc=20):   1%|          | 7000/619741 [00:14<10:25, 980.17 examples/s]Running tokenizer on dataset (num_proc=20):   1%|▏         | 9000/619741 [00:14<04:59, 2039.14 examples/s]Running tokenizer on dataset (num_proc=20):   1%|▏         | 9000/619741 [00:14<06:07, 1661.63 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 11000/619741 [00:14<03:09, 3208.57 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 12000/619741 [00:14<03:25, 2959.01 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 20000/619741 [00:15<03:08, 3186.84 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 14000/619741 [00:15<02:54, 3477.02 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 14000/619741 [00:15<03:37, 2789.07 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 15000/619741 [00:15<03:09, 3195.17 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 16000/619741 [00:15<02:59, 3356.22 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 16000/619741 [00:15<02:56, 3419.65 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 18000/619741 [00:16<02:04, 4833.39 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 17000/619741 [00:16<03:18, 3036.82 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 19000/619741 [00:16<02:03, 4879.00 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 20000/619741 [00:16<02:54, 3430.70 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 21000/619741 [00:22<13:40, 730.08 examples/s] Running tokenizer on dataset (num_proc=20):   3%|▎         | 21000/619741 [00:24<14:55, 668.62 examples/s] Running tokenizer on dataset (num_proc=20):   3%|▎         | 21000/619741 [00:25<26:39, 374.30 examples/s] Running tokenizer on dataset (num_proc=20):   4%|▎         | 22000/619741 [00:24<14:17, 697.06 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▎         | 23000/619741 [00:24<11:39, 852.81 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▎         | 22000/619741 [00:25<20:50, 478.11 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▎         | 23000/619741 [00:26<16:16, 611.20 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 24000/619741 [00:25<10:24, 953.45 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▎         | 22000/619741 [00:26<15:43, 633.70 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 25000/619741 [00:27<10:51, 913.10 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 26000/619741 [00:27<08:37, 1146.60 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 29000/619741 [00:27<04:42, 2092.95 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▎         | 23000/619741 [00:26<13:49, 719.80 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 25000/619741 [00:27<11:23, 870.49 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 24000/619741 [00:27<10:54, 910.22 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 32000/619741 [00:27<03:18, 2965.24 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 27000/619741 [00:27<07:12, 1369.41 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 34000/619741 [00:28<02:42, 3609.10 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 28000/619741 [00:27<05:53, 1672.54 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 36000/619741 [00:28<02:10, 4458.32 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 25000/619741 [00:27<09:35, 1033.99 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 29000/619741 [00:27<04:56, 1994.43 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 29000/619741 [00:27<04:10, 2353.79 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 37000/619741 [00:28<02:25, 4009.50 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 31000/619741 [00:28<03:37, 2711.98 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 31000/619741 [00:28<03:25, 2859.26 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 33000/619741 [00:28<02:44, 3561.88 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 32000/619741 [00:28<03:12, 3054.53 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 38000/619741 [00:29<03:56, 2456.04 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 33000/619741 [00:29<03:44, 2609.02 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 34000/619741 [00:29<03:36, 2705.75 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▋         | 39000/619741 [00:30<03:24, 2843.87 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 35000/619741 [00:29<03:52, 2519.43 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 34000/619741 [00:29<04:21, 2236.27 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 35000/619741 [00:29<03:40, 2657.16 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 36000/619741 [00:30<03:15, 2981.53 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▋         | 40000/619741 [00:31<04:56, 1956.86 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 38000/619741 [00:30<02:29, 3885.89 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 37000/619741 [00:30<04:30, 2151.19 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▋         | 39000/619741 [00:30<02:53, 3339.54 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▋         | 39000/619741 [00:30<03:15, 2971.81 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▋         | 40000/619741 [00:30<02:28, 3891.35 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▋         | 40000/619741 [00:31<03:25, 2814.30 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 41000/619741 [00:33<08:55, 1081.25 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 41000/619741 [00:36<13:56, 692.13 examples/s] Running tokenizer on dataset (num_proc=20):   7%|▋         | 42000/619741 [00:37<13:32, 710.74 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 43000/619741 [00:38<11:18, 849.78 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 42000/619741 [00:38<19:02, 505.76 examples/s] Running tokenizer on dataset (num_proc=20):   7%|▋         | 44000/619741 [00:39<10:05, 950.63 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 41000/619741 [00:39<25:20, 380.68 examples/s] Running tokenizer on dataset (num_proc=20):   7%|▋         | 44000/619741 [00:39<12:01, 798.17 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 43000/619741 [00:40<15:13, 631.56 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 44000/619741 [00:40<12:12, 785.75 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 46000/619741 [00:40<08:04, 1184.20 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 47000/619741 [00:40<06:43, 1418.28 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 45000/619741 [00:40<11:33, 828.48 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 46000/619741 [00:40<08:25, 1134.81 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 49000/619741 [00:41<04:57, 1918.23 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 47000/619741 [00:40<07:16, 1311.12 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 51000/619741 [00:41<03:21, 2824.68 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 46000/619741 [00:40<09:57, 959.92 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 48000/619741 [00:40<05:49, 1633.93 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 52000/619741 [00:41<02:56, 3208.29 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 49000/619741 [00:40<04:33, 2087.32 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 48000/619741 [00:41<06:14, 1526.01 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 50000/619741 [00:41<03:34, 2657.91 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 50000/619741 [00:41<04:10, 2270.81 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▊         | 53000/619741 [00:42<03:04, 3067.42 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▊         | 54000/619741 [00:42<02:46, 3397.00 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 51000/619741 [00:41<04:25, 2142.17 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 57000/619741 [00:42<01:57, 4783.03 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 51000/619741 [00:41<04:44, 1998.46 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 52000/619741 [00:42<04:01, 2347.38 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 58000/619741 [00:43<02:19, 4018.78 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▊         | 53000/619741 [00:42<03:53, 2432.28 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 52000/619741 [00:42<04:52, 1944.13 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▊         | 54000/619741 [00:42<03:05, 3047.00 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▊         | 53000/619741 [00:42<03:57, 2383.19 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 55000/619741 [00:42<02:34, 3655.80 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 55000/619741 [00:42<02:37, 3582.09 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 57000/619741 [00:42<01:52, 5010.02 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 56000/619741 [00:43<02:55, 3220.65 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 59000/619741 [00:44<03:54, 2388.14 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 57000/619741 [00:43<04:17, 2184.92 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 58000/619741 [00:44<03:56, 2370.60 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 58000/619741 [00:44<04:30, 2074.73 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 59000/619741 [00:44<04:08, 2258.61 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 60000/619741 [00:45<03:49, 2443.99 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 60000/619741 [00:45<04:14, 2199.34 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 61000/619741 [00:45<03:34, 2608.83 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 61000/619741 [00:49<12:51, 724.11 examples/s] Running tokenizer on dataset (num_proc=20):  10%|█         | 62000/619741 [00:50<15:27, 601.27 examples/s] Running tokenizer on dataset (num_proc=20):  10%|█         | 62000/619741 [00:50<12:17, 755.81 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 63000/619741 [00:51<11:50, 783.82 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 61000/619741 [00:52<17:51, 521.67 examples/s] Running tokenizer on dataset (num_proc=20):  10%|█         | 64000/619741 [00:52<09:30, 974.52 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 65000/619741 [00:52<07:04, 1306.49 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 62000/619741 [00:53<14:36, 636.41 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 63000/619741 [00:52<17:22, 534.28 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 66000/619741 [00:53<08:30, 1084.25 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 68000/619741 [00:53<04:56, 1863.51 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 64000/619741 [00:53<14:25, 641.91 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 63000/619741 [00:54<15:03, 615.90 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 66000/619741 [00:54<08:42, 1059.05 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 64000/619741 [00:55<11:41, 792.44 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 66000/619741 [00:55<07:04, 1304.57 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 69000/619741 [00:54<05:54, 1552.67 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 68000/619741 [00:54<06:39, 1381.36 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 69000/619741 [00:55<04:31, 2027.06 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█▏        | 70000/619741 [00:56<04:11, 2188.81 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 69000/619741 [00:55<06:09, 1490.44 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█▏        | 70000/619741 [00:55<06:07, 1495.44 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█▏        | 70000/619741 [00:55<05:12, 1756.74 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 73000/619741 [00:55<02:44, 3324.75 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 72000/619741 [00:56<03:35, 2541.13 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█▏        | 71000/619741 [00:55<05:31, 1653.84 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 74000/619741 [00:56<02:38, 3440.89 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 75000/619741 [00:56<02:20, 3873.64 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 72000/619741 [00:56<04:46, 1908.91 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 73000/619741 [00:56<03:49, 2384.70 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 76000/619741 [00:57<02:28, 3659.74 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 77000/619741 [00:57<02:18, 3923.56 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 75000/619741 [00:56<03:18, 2745.34 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 74000/619741 [00:56<04:18, 2113.67 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 78000/619741 [00:57<02:58, 3029.25 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 75000/619741 [00:57<04:03, 2240.87 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 79000/619741 [00:58<02:40, 3361.03 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 77000/619741 [00:57<03:27, 2611.60 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 80000/619741 [00:58<03:11, 2825.06 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 78000/619741 [00:58<03:39, 2468.70 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 76000/619741 [00:58<05:30, 1646.41 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 79000/619741 [00:58<03:49, 2353.50 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 77000/619741 [00:58<04:54, 1840.34 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 80000/619741 [00:59<04:08, 2168.65 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 78000/619741 [00:59<05:32, 1628.66 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 79000/619741 [00:59<04:34, 1971.55 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 80000/619741 [00:59<03:37, 2484.94 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 81000/619741 [01:01<07:18, 1227.55 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 81000/619741 [01:01<07:51, 1143.38 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 82000/619741 [01:03<10:24, 860.76 examples/s] Running tokenizer on dataset (num_proc=20):  13%|█▎        | 82000/619741 [01:03<11:02, 812.12 examples/s] Running tokenizer on dataset (num_proc=20):  13%|█▎        | 83000/619741 [01:04<09:06, 982.22 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▎        | 84000/619741 [01:04<07:40, 1164.38 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 83000/619741 [01:05<12:10, 734.48 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 81000/619741 [01:06<21:29, 417.77 examples/s] Running tokenizer on dataset (num_proc=20):  14%|█▎        | 85000/619741 [01:05<06:49, 1307.38 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 82000/619741 [01:06<16:42, 536.56 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 86000/619741 [01:06<07:43, 1150.77 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▎        | 84000/619741 [01:06<13:00, 685.97 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▎        | 85000/619741 [01:06<09:38, 924.81 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 88000/619741 [01:07<05:17, 1676.79 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 83000/619741 [01:08<15:34, 574.56 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 87000/619741 [01:07<06:51, 1294.15 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▎        | 84000/619741 [01:08<11:53, 751.32 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 88000/619741 [01:08<06:07, 1448.71 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 89000/619741 [01:08<06:50, 1294.28 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 89000/619741 [01:08<05:20, 1654.43 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 90000/619741 [01:08<05:19, 1659.19 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▎        | 85000/619741 [01:09<10:34, 843.10 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 86000/619741 [01:09<07:50, 1134.16 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 90000/619741 [01:08<04:42, 1876.75 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 91000/619741 [01:09<03:54, 2250.60 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 88000/619741 [01:09<04:47, 1851.55 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 89000/619741 [01:10<03:55, 2254.50 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 91000/619741 [01:10<02:35, 3390.98 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 92000/619741 [01:10<02:14, 3927.60 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 92000/619741 [01:09<04:17, 2052.80 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 93000/619741 [01:10<02:20, 3748.32 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 93000/619741 [01:09<03:41, 2373.41 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 91000/619741 [01:09<07:11, 1226.59 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 92000/619741 [01:10<05:28, 1607.22 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 94000/619741 [01:11<02:50, 3079.15 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 93000/619741 [01:10<04:27, 1969.72 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 95000/619741 [01:11<02:29, 3507.19 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 94000/619741 [01:10<05:03, 1734.07 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 96000/619741 [01:10<02:57, 2952.79 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 96000/619741 [01:11<03:03, 2852.90 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 97000/619741 [01:12<02:51, 3055.80 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 94000/619741 [01:11<05:46, 1518.18 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 98000/619741 [01:12<02:17, 3807.24 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 98000/619741 [01:11<02:35, 3352.38 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 95000/619741 [01:11<04:33, 1921.68 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 99000/619741 [01:11<02:48, 3082.27 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 99000/619741 [01:12<03:03, 2831.52 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 100000/619741 [01:12<02:30, 3448.50 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 97000/619741 [01:12<03:46, 2309.10 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▋        | 101000/619741 [01:12<02:53, 2988.62 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 100000/619741 [01:13<03:49, 2265.17 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 98000/619741 [01:13<06:28, 1341.48 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 99000/619741 [01:14<05:43, 1517.97 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 100000/619741 [01:15<06:32, 1325.26 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▋        | 101000/619741 [01:15<06:00, 1440.58 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▋        | 102000/619741 [01:16<11:46, 732.75 examples/s] Running tokenizer on dataset (num_proc=20):  16%|█▋        | 102000/619741 [01:17<08:28, 1017.69 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 103000/619741 [01:18<07:10, 1200.61 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 104000/619741 [01:18<07:10, 1197.94 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 105000/619741 [01:19<05:29, 1560.82 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 103000/619741 [01:19<14:05, 611.37 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▋        | 101000/619741 [01:20<19:42, 438.86 examples/s] Running tokenizer on dataset (num_proc=20):  17%|█▋        | 106000/619741 [01:20<06:23, 1338.46 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 104000/619741 [01:20<12:31, 686.62 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▋        | 102000/619741 [01:20<16:03, 537.23 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 103000/619741 [01:21<12:47, 672.96 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 105000/619741 [01:20<10:40, 804.23 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 106000/619741 [01:21<08:23, 1020.95 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 107000/619741 [01:21<06:19, 1349.53 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 108000/619741 [01:21<04:56, 1727.74 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 107000/619741 [01:21<08:15, 1034.77 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 108000/619741 [01:21<06:18, 1352.80 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 109000/619741 [01:21<04:41, 1813.86 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 104000/619741 [01:23<12:59, 661.58 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 110000/619741 [01:22<04:25, 1920.56 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 105000/619741 [01:23<09:27, 907.58 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 106000/619741 [01:23<07:11, 1189.62 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 109000/619741 [01:22<06:49, 1248.57 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 112000/619741 [01:22<03:30, 2406.76 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 110000/619741 [01:23<05:44, 1478.77 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 107000/619741 [01:23<06:09, 1387.16 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 113000/619741 [01:23<03:09, 2679.85 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 112000/619741 [01:23<03:48, 2221.02 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 109000/619741 [01:24<04:06, 2068.75 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 114000/619741 [01:23<03:28, 2426.19 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 111000/619741 [01:24<02:57, 2861.49 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 113000/619741 [01:23<04:02, 2086.78 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 112000/619741 [01:24<02:46, 3053.66 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 115000/619741 [01:24<03:30, 2397.59 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 114000/619741 [01:25<02:00, 4180.53 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 116000/619741 [01:24<02:46, 3031.50 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 117000/619741 [01:24<02:17, 3663.84 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 114000/619741 [01:24<04:15, 1980.84 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 116000/619741 [01:25<02:04, 4040.91 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 117000/619741 [01:25<01:49, 4591.12 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 118000/619741 [01:24<02:55, 2865.82 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 115000/619741 [01:25<04:17, 1963.73 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 116000/619741 [01:25<03:56, 2131.14 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 119000/619741 [01:25<03:40, 2272.61 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 118000/619741 [01:26<02:48, 2981.68 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 120000/619741 [01:25<02:56, 2826.98 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 119000/619741 [01:27<04:05, 2036.31 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 117000/619741 [01:26<05:44, 1458.01 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 120000/619741 [01:27<04:15, 1956.01 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 118000/619741 [01:28<08:50, 946.19 examples/s] Running tokenizer on dataset (num_proc=20):  19%|█▉        | 119000/619741 [01:29<07:11, 1161.54 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 120000/619741 [01:29<06:06, 1363.95 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 121000/619741 [01:30<06:32, 1269.92 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 123000/619741 [01:30<03:59, 2075.04 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 124000/619741 [01:30<03:38, 2272.99 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 125000/619741 [01:31<04:29, 1833.48 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 122000/619741 [01:32<13:34, 611.07 examples/s] Running tokenizer on dataset (num_proc=20):  20%|█▉        | 123000/619741 [01:32<11:31, 717.84 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 121000/619741 [01:33<16:28, 504.55 examples/s] Running tokenizer on dataset (num_proc=20):  20%|█▉        | 122000/619741 [01:34<12:29, 663.86 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 125000/619741 [01:33<08:12, 1003.94 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 126000/619741 [01:33<07:03, 1166.99 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 126000/619741 [01:34<08:19, 988.43 examples/s] Running tokenizer on dataset (num_proc=20):  20%|█▉        | 123000/619741 [01:34<10:44, 770.52 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 127000/619741 [01:34<06:20, 1293.34 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 124000/619741 [01:35<08:35, 960.98 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 128000/619741 [01:34<05:06, 1604.41 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 127000/619741 [01:34<08:04, 1017.49 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 125000/619741 [01:36<07:46, 1060.86 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 128000/619741 [01:35<07:00, 1169.02 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 126000/619741 [01:36<06:06, 1346.31 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 129000/619741 [01:35<06:07, 1335.69 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 130000/619741 [01:35<05:11, 1574.28 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 130000/619741 [01:36<04:34, 1785.68 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 131000/619741 [01:36<04:25, 1838.74 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██▏       | 132000/619741 [01:36<03:44, 2169.66 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 127000/619741 [01:37<06:28, 1268.41 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██▏       | 133000/619741 [01:36<03:02, 2663.96 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 134000/619741 [01:36<02:39, 3053.52 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 131000/619741 [01:36<05:02, 1615.38 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 128000/619741 [01:37<05:47, 1414.34 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██▏       | 132000/619741 [01:37<04:15, 1911.42 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 129000/619741 [01:38<04:46, 1712.45 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██▏       | 133000/619741 [01:37<03:18, 2452.34 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 130000/619741 [01:38<04:01, 2024.99 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 135000/619741 [01:37<03:55, 2056.16 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██▏       | 132000/619741 [01:38<02:24, 3385.84 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 134000/619741 [01:37<03:29, 2318.81 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 136000/619741 [01:38<04:20, 1854.48 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 137000/619741 [01:38<03:19, 2422.64 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 138000/619741 [01:38<02:35, 3103.28 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 136000/619741 [01:39<02:13, 3618.83 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 140000/619741 [01:38<01:58, 4035.27 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 137000/619741 [01:40<02:33, 3135.95 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 141000/619741 [01:39<02:16, 3503.28 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 138000/619741 [01:40<02:30, 3210.62 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 135000/619741 [01:39<06:58, 1158.39 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 136000/619741 [01:39<05:32, 1457.01 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 139000/619741 [01:40<03:00, 2658.27 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 137000/619741 [01:40<05:01, 1598.77 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 138000/619741 [01:41<06:02, 1328.97 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 140000/619741 [01:43<06:17, 1272.50 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 139000/619741 [01:42<06:16, 1276.78 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 141000/619741 [01:43<05:19, 1498.90 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 140000/619741 [01:43<07:47, 1027.09 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 141000/619741 [01:44<07:06, 1121.48 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 142000/619741 [01:45<06:53, 1154.15 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 143000/619741 [01:45<05:05, 1562.31 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 145000/619741 [01:45<03:16, 2412.51 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 142000/619741 [01:46<16:20, 487.27 examples/s] Running tokenizer on dataset (num_proc=20):  23%|██▎       | 142000/619741 [01:47<11:56, 666.90 examples/s] Running tokenizer on dataset (num_proc=20):  23%|██▎       | 143000/619741 [01:46<12:36, 630.01 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 144000/619741 [01:47<11:00, 719.83 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 145000/619741 [01:47<08:35, 921.59 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▎       | 146000/619741 [01:48<06:39, 1186.70 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 143000/619741 [01:49<12:46, 621.66 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▎       | 147000/619741 [01:48<05:05, 1546.53 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 148000/619741 [01:48<04:02, 1942.73 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▎       | 146000/619741 [01:49<09:16, 850.57 examples/s] Running tokenizer on dataset (num_proc=20):  23%|██▎       | 144000/619741 [01:49<11:12, 707.93 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 149000/619741 [01:49<04:51, 1614.37 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 151000/619741 [01:49<03:05, 2525.05 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 148000/619741 [01:49<06:25, 1225.28 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 145000/619741 [01:50<09:44, 811.65 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 152000/619741 [01:50<03:02, 2563.00 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 149000/619741 [01:50<05:26, 1443.14 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 153000/619741 [01:50<02:35, 3002.50 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 150000/619741 [01:50<04:32, 1722.96 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▎       | 146000/619741 [01:51<07:56, 993.52 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 152000/619741 [01:50<03:06, 2510.93 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 154000/619741 [01:50<02:55, 2652.73 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▎       | 147000/619741 [01:51<06:11, 1271.99 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 155000/619741 [01:50<02:35, 2992.02 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 148000/619741 [01:52<05:52, 1340.14 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 153000/619741 [01:51<03:55, 1981.01 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 156000/619741 [01:51<02:59, 2581.69 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 149000/619741 [01:52<04:37, 1694.66 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 154000/619741 [01:51<03:21, 2310.08 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 158000/619741 [01:51<01:59, 3859.83 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 150000/619741 [01:52<03:31, 2216.01 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 151000/619741 [01:52<02:59, 2613.82 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 154000/619741 [01:52<01:42, 4557.13 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 160000/619741 [01:52<02:27, 3118.34 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 157000/619741 [01:53<02:01, 3813.04 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 158000/619741 [01:54<02:37, 2934.22 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 159000/619741 [01:54<02:16, 3382.91 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 161000/619741 [01:54<04:59, 1529.28 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 155000/619741 [01:54<07:59, 968.25 examples/s] Running tokenizer on dataset (num_proc=20):  25%|██▌       | 156000/619741 [01:54<06:24, 1206.23 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 160000/619741 [01:55<03:11, 2402.19 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 161000/619741 [01:56<04:04, 1875.41 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 158000/619741 [01:55<05:25, 1419.85 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 159000/619741 [01:56<05:00, 1533.50 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 162000/619741 [01:57<09:05, 839.12 examples/s] Running tokenizer on dataset (num_proc=20):  26%|██▌       | 160000/619741 [01:58<07:21, 1042.18 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 161000/619741 [01:58<05:42, 1339.05 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▋       | 164000/619741 [01:59<03:45, 2025.40 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 165000/619741 [01:59<03:10, 2387.62 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 162000/619741 [02:01<11:57, 638.20 examples/s] Running tokenizer on dataset (num_proc=20):  26%|██▋       | 163000/619741 [02:00<12:43, 598.59 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▋       | 163000/619741 [02:02<11:06, 685.71 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▋       | 164000/619741 [02:01<11:55, 637.36 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 165000/619741 [02:01<09:10, 826.41 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 166000/619741 [02:02<07:54, 956.58 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 166000/619741 [02:02<08:43, 867.39 examples/s] Running tokenizer on dataset (num_proc=20):  27%|██▋       | 167000/619741 [02:03<07:07, 1059.92 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 165000/619741 [02:04<09:19, 812.43 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 168000/619741 [02:03<05:35, 1344.79 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 169000/619741 [02:03<04:49, 1559.62 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 170000/619741 [02:03<03:23, 2209.70 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 166000/619741 [02:04<08:02, 939.56 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 170000/619741 [02:03<04:41, 1595.38 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 171000/619741 [02:04<03:40, 2038.69 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 171000/619741 [02:04<03:51, 1940.96 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 173000/619741 [02:04<02:36, 2859.44 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 174000/619741 [02:04<02:15, 3283.70 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 175000/619741 [02:04<01:55, 3856.63 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 167000/619741 [02:05<07:37, 989.62 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 172000/619741 [02:04<04:17, 1736.68 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 176000/619741 [02:04<02:03, 3600.84 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 173000/619741 [02:05<03:41, 2012.98 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 177000/619741 [02:05<02:10, 3393.72 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 168000/619741 [02:06<07:05, 1062.20 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 178000/619741 [02:05<02:09, 3417.51 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 170000/619741 [02:06<04:15, 1758.25 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 171000/619741 [02:06<04:06, 1822.52 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 174000/619741 [02:07<02:18, 3222.31 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 180000/619741 [02:06<02:24, 3038.81 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 174000/619741 [02:07<06:43, 1103.42 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 177000/619741 [02:08<02:20, 3158.45 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 175000/619741 [02:08<07:06, 1042.25 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 181000/619741 [02:08<05:25, 1345.96 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 176000/619741 [02:08<06:31, 1133.95 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 179000/619741 [02:09<03:30, 2093.56 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 180000/619741 [02:09<03:05, 2373.69 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 177000/619741 [02:09<05:12, 1414.60 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 178000/619741 [02:09<04:04, 1808.60 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 181000/619741 [02:10<03:13, 2267.38 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 179000/619741 [02:09<03:58, 1844.53 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 182000/619741 [02:09<07:07, 1023.92 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 180000/619741 [02:10<03:27, 2121.34 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 182000/619741 [02:11<04:05, 1784.86 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 181000/619741 [02:10<04:02, 1812.85 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 182000/619741 [02:11<03:39, 1997.54 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 183000/619741 [02:11<03:48, 1915.28 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 184000/619741 [02:13<05:16, 1377.50 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 186000/619741 [02:13<03:48, 1896.12 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 183000/619741 [02:13<13:00, 559.70 examples/s] Running tokenizer on dataset (num_proc=20):  30%|██▉       | 183000/619741 [02:15<09:45, 746.05 examples/s] Running tokenizer on dataset (num_proc=20):  30%|██▉       | 184000/619741 [02:15<11:55, 609.23 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 184000/619741 [02:16<09:30, 763.86 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 185000/619741 [02:15<09:31, 760.21 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 187000/619741 [02:16<07:10, 1005.88 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 185000/619741 [02:16<07:51, 922.87 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 186000/619741 [02:16<09:01, 800.86 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 187000/619741 [02:16<06:40, 1080.62 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 188000/619741 [02:16<06:55, 1038.49 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 189000/619741 [02:17<03:50, 1866.84 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 189000/619741 [02:17<05:26, 1320.63 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 190000/619741 [02:17<03:18, 2168.19 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 191000/619741 [02:17<03:04, 2327.57 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 193000/619741 [02:17<02:00, 3540.69 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 190000/619741 [02:17<05:14, 1366.46 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 186000/619741 [02:18<09:01, 800.57 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███▏      | 194000/619741 [02:18<01:52, 3796.33 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 191000/619741 [02:18<04:20, 1643.49 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███▏      | 195000/619741 [02:18<02:09, 3279.19 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 187000/619741 [02:19<07:50, 919.46 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 192000/619741 [02:18<04:03, 1760.05 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 188000/619741 [02:19<05:57, 1208.82 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 193000/619741 [02:18<03:13, 2208.45 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 189000/619741 [02:19<04:57, 1446.25 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 197000/619741 [02:19<02:19, 3040.45 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 190000/619741 [02:20<03:46, 1894.02 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 192000/619741 [02:20<02:22, 3004.23 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 193000/619741 [02:20<02:27, 2901.28 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 198000/619741 [02:19<02:51, 2459.11 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 199000/619741 [02:20<03:08, 2234.53 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███▏      | 194000/619741 [02:21<03:12, 2213.61 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███▏      | 195000/619741 [02:21<02:49, 2499.32 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 201000/619741 [02:21<02:59, 2338.41 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 196000/619741 [02:22<03:03, 2307.78 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███▏      | 194000/619741 [02:21<07:59, 888.48 examples/s] Running tokenizer on dataset (num_proc=20):  32%|███▏      | 198000/619741 [02:22<02:07, 3313.92 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 202000/619741 [02:22<03:31, 1973.18 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███▏      | 195000/619741 [02:22<06:36, 1071.58 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 199000/619741 [02:23<03:07, 2249.68 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 197000/619741 [02:22<04:27, 1582.15 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 200000/619741 [02:23<03:13, 2165.93 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 198000/619741 [02:23<04:24, 1595.94 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 199000/619741 [02:23<03:42, 1889.95 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 201000/619741 [02:24<03:47, 1840.43 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 201000/619741 [02:24<03:11, 2187.43 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 202000/619741 [02:25<03:33, 1955.88 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 202000/619741 [02:25<04:29, 1547.91 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 203000/619741 [02:26<04:40, 1485.03 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 204000/619741 [02:26<04:05, 1693.08 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 206000/619741 [02:27<03:04, 2239.27 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 203000/619741 [02:27<12:39, 548.47 examples/s] Running tokenizer on dataset (num_proc=20):  33%|███▎      | 203000/619741 [02:28<10:03, 690.14 examples/s] Running tokenizer on dataset (num_proc=20):  33%|███▎      | 204000/619741 [02:28<09:53, 700.59 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 206000/619741 [02:28<05:56, 1160.64 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 207000/619741 [02:28<05:30, 1248.57 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 207000/619741 [02:29<05:54, 1164.72 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 208000/619741 [02:30<06:03, 1132.61 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 204000/619741 [02:30<11:15, 615.27 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 205000/619741 [02:31<08:40, 796.76 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 209000/619741 [02:30<05:09, 1328.85 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 208000/619741 [02:30<06:41, 1025.71 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 209000/619741 [02:30<05:23, 1268.16 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 206000/619741 [02:31<07:07, 967.95 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 211000/619741 [02:31<03:58, 1711.16 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 213000/619741 [02:31<02:43, 2493.20 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 210000/619741 [02:31<05:08, 1327.67 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 214000/619741 [02:31<02:25, 2788.98 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 207000/619741 [02:32<06:37, 1038.67 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 211000/619741 [02:31<04:35, 1485.29 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 208000/619741 [02:33<05:50, 1173.15 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 209000/619741 [02:33<04:32, 1505.92 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 216000/619741 [02:32<02:51, 2350.64 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 210000/619741 [02:33<03:33, 1915.76 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 211000/619741 [02:33<02:43, 2501.45 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 213000/619741 [02:33<04:22, 1549.30 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 212000/619741 [02:34<03:09, 2148.28 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 213000/619741 [02:34<02:52, 2352.64 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 217000/619741 [02:33<04:02, 1663.46 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 214000/619741 [02:34<02:31, 2674.11 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 215000/619741 [02:35<02:25, 2790.87 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 218000/619741 [02:34<03:51, 1733.76 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 216000/619741 [02:35<02:26, 2752.00 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 219000/619741 [02:34<03:33, 1877.58 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 221000/619741 [02:35<02:30, 2657.02 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 217000/619741 [02:36<03:36, 1858.38 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 218000/619741 [02:36<02:44, 2443.95 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 222000/619741 [02:35<02:59, 2214.58 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 214000/619741 [02:35<07:40, 880.25 examples/s] Running tokenizer on dataset (num_proc=20):  35%|███▌      | 219000/619741 [02:37<02:53, 2306.92 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 216000/619741 [02:36<05:31, 1218.68 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 218000/619741 [02:36<03:36, 1855.20 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 220000/619741 [02:37<02:48, 2365.66 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 219000/619741 [02:36<03:15, 2054.38 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 221000/619741 [02:37<02:34, 2585.98 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 220000/619741 [02:37<03:38, 1828.98 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 221000/619741 [02:37<02:57, 2246.45 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 222000/619741 [02:38<02:41, 2469.43 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 222000/619741 [02:39<05:06, 1297.36 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 223000/619741 [02:38<03:23, 1949.18 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 224000/619741 [02:40<05:48, 1135.83 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 223000/619741 [02:40<10:33, 626.46 examples/s] Running tokenizer on dataset (num_proc=20):  36%|███▋      | 225000/619741 [02:41<05:03, 1301.92 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▋      | 226000/619741 [02:41<04:08, 1583.45 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 223000/619741 [02:42<09:25, 701.97 examples/s] Running tokenizer on dataset (num_proc=20):  36%|███▌      | 224000/619741 [02:41<09:06, 723.83 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▋      | 226000/619741 [02:41<05:32, 1185.73 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 227000/619741 [02:42<04:34, 1431.01 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 224000/619741 [02:43<08:04, 816.79 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 227000/619741 [02:42<05:30, 1188.82 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 228000/619741 [02:43<05:34, 1170.34 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▋      | 225000/619741 [02:44<08:08, 807.36 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 230000/619741 [02:43<03:28, 1873.11 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 231000/619741 [02:43<02:56, 2203.01 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 232000/619741 [02:44<02:26, 2650.65 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 233000/619741 [02:44<02:05, 3079.92 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 228000/619741 [02:44<06:46, 964.24 examples/s] Running tokenizer on dataset (num_proc=20):  36%|███▋      | 226000/619741 [02:45<07:39, 857.63 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 229000/619741 [02:44<05:32, 1175.35 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 227000/619741 [02:46<06:28, 1009.94 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 228000/619741 [02:46<04:46, 1366.91 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 234000/619741 [02:45<04:25, 1454.15 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 235000/619741 [02:45<03:24, 1885.22 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 230000/619741 [02:46<06:14, 1040.33 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 231000/619741 [02:46<04:47, 1354.18 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 236000/619741 [02:46<03:22, 1895.10 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 229000/619741 [02:47<05:41, 1144.60 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 233000/619741 [02:46<03:11, 2018.71 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 230000/619741 [02:47<04:22, 1481.92 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 231000/619741 [02:47<03:46, 1716.87 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 233000/619741 [02:48<02:42, 2380.51 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 234000/619741 [02:48<02:36, 2465.06 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 237000/619741 [02:48<05:20, 1195.73 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 238000/619741 [02:48<04:12, 1514.75 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▊      | 239000/619741 [02:48<03:25, 1856.80 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 236000/619741 [02:49<02:25, 2630.40 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 237000/619741 [02:49<02:22, 2680.70 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▊      | 240000/619741 [02:49<03:18, 1913.27 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 234000/619741 [02:49<06:36, 972.78 examples/s] Running tokenizer on dataset (num_proc=20):  38%|███▊      | 235000/619741 [02:49<05:27, 1176.40 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 241000/619741 [02:49<03:37, 1739.15 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 238000/619741 [02:50<02:59, 2120.95 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 236000/619741 [02:50<04:39, 1373.88 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▊      | 239000/619741 [02:51<02:55, 2166.86 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 238000/619741 [02:50<02:52, 2218.89 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▊      | 240000/619741 [02:50<01:58, 3208.14 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▊      | 240000/619741 [02:51<02:40, 2361.57 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 242000/619741 [02:50<04:06, 1532.27 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 241000/619741 [02:50<01:48, 3483.25 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 243000/619741 [02:50<03:25, 1832.29 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 242000/619741 [02:51<02:18, 2722.47 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 241000/619741 [02:52<04:10, 1514.86 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 243000/619741 [02:52<02:59, 2096.17 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 242000/619741 [02:54<05:47, 1087.98 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 244000/619741 [02:53<04:34, 1367.92 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 244000/619741 [02:53<07:51, 797.74 examples/s] Running tokenizer on dataset (num_proc=20):  40%|███▉      | 245000/619741 [02:53<03:54, 1599.09 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 245000/619741 [02:54<07:41, 811.41 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 246000/619741 [02:55<04:55, 1264.52 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 246000/619741 [02:55<05:52, 1060.81 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 247000/619741 [02:55<04:30, 1377.15 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 243000/619741 [02:56<08:15, 760.05 examples/s] Running tokenizer on dataset (num_proc=20):  40%|████      | 248000/619741 [02:55<03:43, 1661.95 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 249000/619741 [02:55<02:55, 2106.81 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 247000/619741 [02:56<05:36, 1107.03 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 244000/619741 [02:57<07:29, 835.83 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 250000/619741 [02:57<04:02, 1527.25 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 251000/619741 [02:57<03:45, 1638.74 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 252000/619741 [02:57<02:59, 2045.08 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 247000/619741 [02:58<04:58, 1246.98 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 248000/619741 [02:58<07:38, 810.38 examples/s] Running tokenizer on dataset (num_proc=20):  40%|████      | 249000/619741 [02:58<06:25, 961.65 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 248000/619741 [02:59<05:12, 1189.10 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 253000/619741 [02:59<04:41, 1303.18 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 249000/619741 [03:00<04:39, 1327.11 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 250000/619741 [02:59<05:42, 1079.10 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 254000/619741 [02:59<04:16, 1424.06 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 255000/619741 [03:00<04:08, 1470.61 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 251000/619741 [03:00<05:31, 1111.02 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 252000/619741 [03:00<04:11, 1461.52 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 250000/619741 [03:01<05:08, 1199.72 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 253000/619741 [03:00<03:08, 1940.99 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 251000/619741 [03:01<04:08, 1485.46 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████▏     | 257000/619741 [03:00<02:54, 2075.76 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 254000/619741 [03:00<02:45, 2208.72 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 258000/619741 [03:01<02:29, 2413.33 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 252000/619741 [03:01<03:33, 1721.40 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 259000/619741 [03:01<02:53, 2083.78 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 254000/619741 [03:03<03:32, 1718.56 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 260000/619741 [03:02<03:07, 1922.94 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████▏     | 256000/619741 [03:03<02:27, 2469.63 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 261000/619741 [03:02<03:15, 1831.47 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 255000/619741 [03:03<06:34, 923.66 examples/s] Running tokenizer on dataset (num_proc=20):  41%|████▏     | 257000/619741 [03:03<03:41, 1636.83 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 258000/619741 [03:03<03:00, 2000.25 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 259000/619741 [03:03<02:24, 2504.03 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████▏     | 257000/619741 [03:04<03:44, 1613.14 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 258000/619741 [03:04<03:06, 1937.40 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 262000/619741 [03:04<05:21, 1111.36 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 260000/619741 [03:04<03:18, 1811.70 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 261000/619741 [03:05<02:37, 2278.02 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 263000/619741 [03:05<04:53, 1216.56 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 260000/619741 [03:06<03:35, 1671.67 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 262000/619741 [03:05<02:48, 2126.52 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 261000/619741 [03:06<03:24, 1750.83 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 264000/619741 [03:07<06:25, 923.71 examples/s] Running tokenizer on dataset (num_proc=20):  42%|████▏     | 263000/619741 [03:07<05:01, 1183.55 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 264000/619741 [03:07<04:22, 1357.38 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 265000/619741 [03:08<04:05, 1445.04 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 262000/619741 [03:09<06:09, 968.58 examples/s] Running tokenizer on dataset (num_proc=20):  43%|████▎     | 265000/619741 [03:08<07:43, 765.35 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 266000/619741 [03:09<04:29, 1314.31 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 266000/619741 [03:09<06:09, 956.38 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 263000/619741 [03:10<05:52, 1011.28 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 264000/619741 [03:10<04:35, 1290.62 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 265000/619741 [03:10<03:29, 1694.65 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 268000/619741 [03:09<04:02, 1452.42 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 269000/619741 [03:10<03:21, 1744.51 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▎     | 270000/619741 [03:10<02:56, 1982.94 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 266000/619741 [03:11<03:47, 1557.28 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 267000/619741 [03:12<04:44, 1239.45 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 273000/619741 [03:11<02:40, 2163.18 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 267000/619741 [03:11<07:46, 756.70 examples/s] Running tokenizer on dataset (num_proc=20):  43%|████▎     | 268000/619741 [03:12<05:56, 986.32 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 274000/619741 [03:12<03:06, 1855.19 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 269000/619741 [03:12<04:43, 1236.95 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 275000/619741 [03:12<02:43, 2102.34 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▎     | 270000/619741 [03:12<03:43, 1566.67 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 268000/619741 [03:13<05:34, 1050.05 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▎     | 271000/619741 [03:13<03:12, 1810.81 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 272000/619741 [03:13<02:44, 2114.27 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 273000/619741 [03:13<02:19, 2493.76 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 276000/619741 [03:13<03:26, 1660.87 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 269000/619741 [03:15<06:10, 945.51 examples/s] Running tokenizer on dataset (num_proc=20):  45%|████▍     | 277000/619741 [03:14<03:33, 1602.21 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▎     | 270000/619741 [03:15<04:51, 1201.72 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 274000/619741 [03:14<03:21, 1716.83 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 272000/619741 [03:15<02:54, 1996.23 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 273000/619741 [03:15<02:42, 2128.13 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 278000/619741 [03:15<04:06, 1387.80 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 279000/619741 [03:15<03:29, 1624.56 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 280000/619741 [03:16<02:52, 1966.31 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 275000/619741 [03:16<04:46, 1201.60 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 281000/619741 [03:16<02:16, 2474.60 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 274000/619741 [03:17<03:50, 1497.64 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 275000/619741 [03:17<03:07, 1843.45 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 276000/619741 [03:17<05:16, 1086.11 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 276000/619741 [03:18<03:32, 1619.16 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 278000/619741 [03:17<03:09, 1808.02 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 277000/619741 [03:18<02:58, 1924.59 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 279000/619741 [03:17<02:39, 2141.46 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 280000/619741 [03:18<02:37, 2155.37 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 282000/619741 [03:18<05:11, 1083.75 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 278000/619741 [03:19<03:31, 1619.20 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 281000/619741 [03:18<02:49, 1992.83 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 279000/619741 [03:19<02:55, 1940.46 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 280000/619741 [03:19<02:15, 2502.90 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 281000/619741 [03:20<02:10, 2594.93 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 282000/619741 [03:19<02:55, 1924.35 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 283000/619741 [03:19<05:22, 1045.33 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 282000/619741 [03:20<01:43, 3263.45 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 284000/619741 [03:20<05:04, 1103.53 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 285000/619741 [03:20<04:30, 1236.84 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 283000/619741 [03:21<05:14, 1069.27 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 284000/619741 [03:21<04:21, 1283.45 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 285000/619741 [03:21<03:31, 1581.47 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 286000/619741 [03:22<05:22, 1033.33 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 287000/619741 [03:22<04:48, 1153.83 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 283000/619741 [03:23<07:11, 781.13 examples/s] Running tokenizer on dataset (num_proc=20):  46%|████▋     | 288000/619741 [03:23<03:58, 1392.32 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 284000/619741 [03:23<05:15, 1063.35 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 285000/619741 [03:24<04:34, 1217.40 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 289000/619741 [03:23<03:51, 1427.95 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 290000/619741 [03:23<03:02, 1810.36 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 286000/619741 [03:24<05:51, 949.37 examples/s] Running tokenizer on dataset (num_proc=20):  46%|████▌     | 286000/619741 [03:25<04:27, 1245.42 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 292000/619741 [03:24<02:31, 2169.37 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 287000/619741 [03:25<05:52, 942.87 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 293000/619741 [03:25<02:32, 2139.52 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 288000/619741 [03:25<05:02, 1094.99 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 289000/619741 [03:25<03:52, 1424.64 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 294000/619741 [03:26<03:18, 1640.75 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 295000/619741 [03:26<02:37, 2067.86 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 290000/619741 [03:26<03:30, 1564.18 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 287000/619741 [03:27<06:31, 850.39 examples/s] Running tokenizer on dataset (num_proc=20):  48%|████▊     | 296000/619741 [03:26<02:23, 2248.63 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 291000/619741 [03:26<02:59, 1828.26 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 297000/619741 [03:26<01:58, 2721.93 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 288000/619741 [03:27<05:32, 998.69 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 289000/619741 [03:28<04:26, 1243.14 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 290000/619741 [03:28<03:27, 1585.36 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 291000/619741 [03:28<02:45, 1991.81 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 293000/619741 [03:27<03:11, 1706.38 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 298000/619741 [03:28<03:27, 1551.47 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 294000/619741 [03:28<02:42, 2002.38 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 299000/619741 [03:28<03:08, 1704.94 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 292000/619741 [03:29<03:17, 1661.93 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 293000/619741 [03:30<03:13, 1686.92 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 295000/619741 [03:29<03:36, 1498.47 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 300000/619741 [03:29<03:41, 1446.48 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 296000/619741 [03:29<03:32, 1524.23 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 295000/619741 [03:31<03:08, 1725.78 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 296000/619741 [03:31<02:41, 2004.38 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 297000/619741 [03:30<03:39, 1472.59 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▊     | 302000/619741 [03:30<03:29, 1519.14 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 298000/619741 [03:31<03:11, 1677.39 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 299000/619741 [03:31<02:25, 2202.54 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 297000/619741 [03:32<03:31, 1526.19 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 298000/619741 [03:32<02:48, 1914.41 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 300000/619741 [03:31<02:55, 1826.49 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 299000/619741 [03:33<02:35, 2057.64 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 300000/619741 [03:33<02:02, 2619.40 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▊     | 301000/619741 [03:32<02:59, 1779.84 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▊     | 301000/619741 [03:34<03:02, 1742.93 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 303000/619741 [03:33<06:11, 851.62 examples/s] Running tokenizer on dataset (num_proc=20):  49%|████▉     | 304000/619741 [03:33<05:01, 1047.79 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 303000/619741 [03:34<03:32, 1492.80 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 305000/619741 [03:35<05:59, 876.24 examples/s] Running tokenizer on dataset (num_proc=20):  50%|████▉     | 307000/619741 [03:35<03:30, 1488.32 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 308000/619741 [03:35<02:50, 1827.32 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 304000/619741 [03:35<04:55, 1069.75 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 309000/619741 [03:36<02:27, 2099.63 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 305000/619741 [03:36<04:19, 1214.27 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 303000/619741 [03:37<05:20, 989.63 examples/s] Running tokenizer on dataset (num_proc=20):  49%|████▉     | 304000/619741 [03:37<04:38, 1135.10 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 306000/619741 [03:37<04:09, 1255.17 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 305000/619741 [03:38<03:52, 1351.54 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 306000/619741 [03:38<03:01, 1731.90 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 310000/619741 [03:37<03:46, 1364.83 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 307000/619741 [03:37<03:38, 1432.86 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 311000/619741 [03:37<03:02, 1687.42 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 312000/619741 [03:38<02:40, 1913.99 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 308000/619741 [03:38<04:20, 1197.16 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 313000/619741 [03:39<03:32, 1443.56 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 309000/619741 [03:39<03:48, 1357.19 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 310000/619741 [03:39<03:21, 1535.26 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 315000/619741 [03:39<02:36, 1944.65 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 311000/619741 [03:39<02:39, 1939.76 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 312000/619741 [03:40<02:18, 2216.07 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 317000/619741 [03:40<02:05, 2408.71 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 307000/619741 [03:41<06:38, 784.64 examples/s] Running tokenizer on dataset (num_proc=20):  50%|████▉     | 308000/619741 [03:42<05:51, 885.96 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 313000/619741 [03:42<04:24, 1158.79 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 318000/619741 [03:42<03:49, 1315.74 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 309000/619741 [03:43<05:40, 913.91 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 314000/619741 [03:42<03:43, 1370.83 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 319000/619741 [03:42<03:14, 1548.10 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 320000/619741 [03:42<02:33, 1957.12 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 311000/619741 [03:43<03:34, 1436.79 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 315000/619741 [03:42<03:10, 1598.16 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 316000/619741 [03:42<02:27, 2056.05 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 312000/619741 [03:43<03:05, 1657.39 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 321000/619741 [03:43<03:36, 1381.64 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 317000/619741 [03:44<03:23, 1488.70 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 313000/619741 [03:44<03:39, 1396.04 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 314000/619741 [03:45<02:49, 1799.56 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 322000/619741 [03:44<03:13, 1538.72 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 315000/619741 [03:45<02:24, 2108.18 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 318000/619741 [03:44<03:11, 1574.04 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 317000/619741 [03:45<01:47, 2824.40 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 319000/619741 [03:45<02:48, 1784.60 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 323000/619741 [03:45<04:14, 1164.28 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 318000/619741 [03:46<02:38, 1899.98 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 319000/619741 [03:46<02:08, 2347.71 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 320000/619741 [03:46<03:42, 1346.77 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 320000/619741 [03:47<02:05, 2387.71 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 324000/619741 [03:46<04:14, 1162.91 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 322000/619741 [03:47<01:33, 3181.69 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 321000/619741 [03:47<03:53, 1278.87 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 325000/619741 [03:47<04:22, 1122.34 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 322000/619741 [03:47<04:03, 1222.31 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 323000/619741 [03:48<02:30, 1977.18 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 323000/619741 [03:48<03:18, 1494.44 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 324000/619741 [03:48<03:17, 1500.97 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 325000/619741 [03:49<02:47, 1758.26 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 327000/619741 [03:49<04:37, 1056.41 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 328000/619741 [03:50<03:54, 1244.60 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 326000/619741 [03:50<03:10, 1540.35 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 329000/619741 [03:50<03:16, 1476.09 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 324000/619741 [03:51<04:51, 1013.94 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 327000/619741 [03:50<03:26, 1415.51 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 325000/619741 [03:52<05:00, 981.98 examples/s] Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 330000/619741 [03:51<04:19, 1115.34 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 328000/619741 [03:52<04:01, 1208.56 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 326000/619741 [03:52<04:33, 1074.76 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 332000/619741 [03:52<02:52, 1671.73 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 334000/619741 [03:52<02:04, 2303.05 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 329000/619741 [03:52<04:00, 1206.42 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 335000/619741 [03:53<02:04, 2295.31 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 330000/619741 [03:53<03:34, 1350.50 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 336000/619741 [03:53<02:11, 2153.54 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 331000/619741 [03:53<03:00, 1597.16 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 332000/619741 [03:54<02:48, 1711.96 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 337000/619741 [03:54<03:10, 1485.90 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 327000/619741 [03:55<07:05, 687.80 examples/s] Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 333000/619741 [03:55<03:20, 1430.46 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 328000/619741 [03:56<05:32, 877.61 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 329000/619741 [03:56<04:04, 1190.20 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 338000/619741 [03:55<03:24, 1380.63 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 330000/619741 [03:57<04:05, 1181.51 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 331000/619741 [03:57<03:01, 1592.96 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 334000/619741 [03:56<04:08, 1148.11 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 332000/619741 [03:57<02:19, 2061.22 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 339000/619741 [03:56<03:38, 1283.34 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 335000/619741 [03:56<03:18, 1438.05 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 333000/619741 [03:57<02:03, 2315.23 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 334000/619741 [03:58<02:04, 2301.22 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 336000/619741 [03:57<03:22, 1402.83 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 340000/619741 [03:57<04:04, 1141.89 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 341000/619741 [03:58<03:48, 1219.13 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 335000/619741 [03:59<03:31, 1347.35 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 337000/619741 [03:58<04:10, 1127.09 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 342000/619741 [03:58<03:10, 1454.88 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 343000/619741 [03:59<02:27, 1870.00 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 336000/619741 [03:59<02:57, 1602.40 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 344000/619741 [03:59<01:57, 2350.31 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 338000/619741 [03:59<03:45, 1248.81 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 337000/619741 [04:00<03:05, 1524.90 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 340000/619741 [04:00<02:40, 1745.27 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 338000/619741 [04:00<02:32, 1847.62 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 339000/619741 [04:01<02:04, 2258.56 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 340000/619741 [04:01<02:05, 2235.95 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 345000/619741 [04:00<03:33, 1289.22 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 341000/619741 [04:01<01:54, 2443.18 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 346000/619741 [04:01<02:58, 1534.92 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 342000/619741 [04:02<02:02, 2273.15 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 343000/619741 [04:02<01:46, 2594.56 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 341000/619741 [04:02<04:26, 1046.83 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 342000/619741 [04:02<03:40, 1257.14 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 347000/619741 [04:03<04:32, 999.90 examples/s] Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 345000/619741 [04:03<02:15, 2026.43 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 346000/619741 [04:03<02:02, 2225.87 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 344000/619741 [04:04<03:31, 1306.41 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 348000/619741 [04:03<04:20, 1041.63 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 349000/619741 [04:04<03:52, 1164.61 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 345000/619741 [04:05<03:50, 1191.25 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 347000/619741 [04:04<02:40, 1694.11 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 348000/619741 [04:05<02:51, 1581.47 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 350000/619741 [04:05<03:54, 1149.31 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 346000/619741 [04:06<04:27, 1024.52 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 352000/619741 [04:05<02:39, 1680.15 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 353000/619741 [04:06<02:21, 1884.22 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 349000/619741 [04:06<03:25, 1315.57 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 354000/619741 [04:06<02:06, 2106.02 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 347000/619741 [04:07<04:08, 1099.74 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 356000/619741 [04:07<01:44, 2514.92 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 350000/619741 [04:07<03:47, 1184.72 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 351000/619741 [04:07<03:01, 1478.95 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 348000/619741 [04:08<04:47, 943.66 examples/s] Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 357000/619741 [04:08<02:43, 1610.19 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 352000/619741 [04:08<03:25, 1303.45 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 353000/619741 [04:09<02:48, 1587.30 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 349000/619741 [04:09<04:56, 913.89 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 350000/619741 [04:10<03:56, 1140.01 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 351000/619741 [04:10<03:04, 1459.84 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 358000/619741 [04:09<03:33, 1224.38 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 352000/619741 [04:10<02:28, 1806.67 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 354000/619741 [04:10<03:30, 1260.43 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 359000/619741 [04:11<03:50, 1130.64 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 355000/619741 [04:11<03:33, 1237.19 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 356000/619741 [04:11<02:42, 1618.99 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 353000/619741 [04:12<03:49, 1159.96 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 361000/619741 [04:12<03:12, 1340.83 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 357000/619741 [04:12<03:01, 1444.48 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 354000/619741 [04:12<03:23, 1303.12 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 362000/619741 [04:12<02:33, 1676.25 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 363000/619741 [04:12<02:11, 1951.46 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 355000/619741 [04:13<03:00, 1468.45 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 358000/619741 [04:12<02:51, 1524.48 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 364000/619741 [04:12<02:02, 2093.17 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 359000/619741 [04:13<02:24, 1806.55 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 365000/619741 [04:13<01:49, 2323.09 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 356000/619741 [04:14<02:58, 1478.42 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 357000/619741 [04:14<02:46, 1574.75 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 360000/619741 [04:14<03:07, 1386.26 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 366000/619741 [04:14<02:27, 1725.53 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 359000/619741 [04:15<01:57, 2209.96 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 360000/619741 [04:15<01:41, 2546.93 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 361000/619741 [04:15<01:48, 2390.81 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 363000/619741 [04:16<01:29, 2872.59 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 361000/619741 [04:15<04:12, 1024.74 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 364000/619741 [04:16<01:36, 2649.36 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 362000/619741 [04:16<03:37, 1183.54 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 363000/619741 [04:16<03:02, 1410.26 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 367000/619741 [04:16<04:47, 879.02 examples/s] Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 364000/619741 [04:16<02:15, 1884.70 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 365000/619741 [04:16<01:45, 2416.64 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 368000/619741 [04:16<03:46, 1111.90 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 365000/619741 [04:18<02:40, 1590.59 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 369000/619741 [04:17<03:32, 1180.08 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 366000/619741 [04:18<02:41, 1570.83 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 370000/619741 [04:18<02:55, 1422.64 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 368000/619741 [04:18<01:57, 2139.43 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 369000/619741 [04:18<01:39, 2528.81 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 366000/619741 [04:20<03:59, 1060.47 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 371000/619741 [04:19<04:14, 976.68 examples/s] Running tokenizer on dataset (num_proc=20):  60%|██████    | 372000/619741 [04:20<03:17, 1256.89 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 373000/619741 [04:20<02:26, 1682.95 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 367000/619741 [04:21<04:07, 1022.26 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 374000/619741 [04:20<01:58, 2080.95 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 370000/619741 [04:20<03:18, 1260.70 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 368000/619741 [04:21<03:29, 1202.19 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 375000/619741 [04:20<02:01, 2007.97 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 376000/619741 [04:21<02:00, 2017.56 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 377000/619741 [04:21<01:49, 2218.19 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 371000/619741 [04:21<03:42, 1119.69 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 372000/619741 [04:22<03:01, 1362.87 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 378000/619741 [04:22<01:43, 2332.63 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 373000/619741 [04:22<02:34, 1598.87 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 374000/619741 [04:22<02:20, 1751.69 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 369000/619741 [04:23<05:09, 810.16 examples/s] Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 370000/619741 [04:23<03:47, 1098.34 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 379000/619741 [04:23<02:31, 1593.56 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 371000/619741 [04:24<03:28, 1194.55 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 380000/619741 [04:24<02:54, 1376.75 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 372000/619741 [04:25<03:03, 1353.69 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 375000/619741 [04:24<03:17, 1241.41 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 373000/619741 [04:25<02:41, 1527.19 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 377000/619741 [04:25<02:30, 1614.77 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 374000/619741 [04:26<03:10, 1292.96 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 381000/619741 [04:26<04:11, 949.34 examples/s] Running tokenizer on dataset (num_proc=20):  61%|██████    | 376000/619741 [04:27<02:14, 1813.26 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 383000/619741 [04:26<02:35, 1519.92 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 379000/619741 [04:26<02:51, 1407.36 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 384000/619741 [04:27<02:46, 1415.50 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 377000/619741 [04:28<02:48, 1442.30 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 380000/619741 [04:27<02:58, 1343.67 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 386000/619741 [04:27<01:56, 2000.69 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 378000/619741 [04:28<02:23, 1687.65 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 379000/619741 [04:28<01:55, 2089.92 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 381000/619741 [04:28<02:33, 1554.18 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 381000/619741 [04:29<01:40, 2384.06 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 387000/619741 [04:29<02:51, 1358.90 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 383000/619741 [04:30<01:34, 2501.00 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 384000/619741 [04:30<01:31, 2563.06 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 382000/619741 [04:29<03:43, 1062.26 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 383000/619741 [04:30<03:13, 1221.26 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 388000/619741 [04:30<03:09, 1223.46 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 384000/619741 [04:30<02:27, 1603.12 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 389000/619741 [04:30<02:27, 1568.75 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 390000/619741 [04:31<02:29, 1538.01 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 385000/619741 [04:32<02:40, 1460.71 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 386000/619741 [04:31<02:11, 1772.37 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 387000/619741 [04:31<02:02, 1899.34 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 386000/619741 [04:32<02:33, 1524.60 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 388000/619741 [04:32<02:09, 1785.79 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 391000/619741 [04:32<03:31, 1082.20 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 389000/619741 [04:32<01:56, 1976.75 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 387000/619741 [04:33<02:54, 1333.43 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 390000/619741 [04:32<01:33, 2445.95 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 393000/619741 [04:33<02:26, 1545.67 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 388000/619741 [04:34<03:14, 1193.77 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 394000/619741 [04:34<02:27, 1531.50 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 389000/619741 [04:35<02:30, 1529.82 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 395000/619741 [04:34<02:33, 1460.72 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 391000/619741 [04:35<03:23, 1126.00 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 396000/619741 [04:35<02:26, 1525.05 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 392000/619741 [04:35<02:59, 1266.86 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 397000/619741 [04:35<02:04, 1786.27 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 398000/619741 [04:35<01:37, 2270.33 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 393000/619741 [04:35<02:28, 1527.51 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 390000/619741 [04:36<03:45, 1018.31 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 399000/619741 [04:36<01:39, 2213.06 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 394000/619741 [04:37<03:27, 1088.79 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 391000/619741 [04:38<04:34, 833.97 examples/s] Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 400000/619741 [04:37<02:44, 1338.39 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 395000/619741 [04:37<02:50, 1318.65 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 396000/619741 [04:38<02:33, 1461.68 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 392000/619741 [04:39<04:03, 936.95 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 397000/619741 [04:38<02:01, 1829.08 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 393000/619741 [04:39<03:00, 1256.45 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 401000/619741 [04:39<03:20, 1092.31 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 394000/619741 [04:40<02:45, 1361.43 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 398000/619741 [04:39<02:23, 1542.43 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 402000/619741 [04:40<03:24, 1062.96 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 399000/619741 [04:40<02:40, 1374.53 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 395000/619741 [04:41<03:27, 1085.55 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 403000/619741 [04:40<03:04, 1173.91 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 400000/619741 [04:40<02:27, 1486.89 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 397000/619741 [04:41<02:09, 1717.95 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 400000/619741 [04:42<01:15, 2904.24 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 401000/619741 [04:41<02:13, 1641.27 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 404000/619741 [04:41<03:04, 1167.63 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 406000/619741 [04:42<02:05, 1696.78 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 408000/619741 [04:42<01:23, 2542.45 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 401000/619741 [04:43<01:50, 1971.29 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 409000/619741 [04:42<01:19, 2645.25 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 403000/619741 [04:43<01:21, 2664.07 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 404000/619741 [04:43<01:19, 2723.74 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 402000/619741 [04:43<03:59, 909.52 examples/s] Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 406000/619741 [04:44<01:15, 2827.58 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 403000/619741 [04:43<02:58, 1214.12 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 404000/619741 [04:44<02:19, 1551.30 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 410000/619741 [04:44<02:14, 1558.78 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 405000/619741 [04:44<01:56, 1843.90 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 406000/619741 [04:44<01:41, 2104.67 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 407000/619741 [04:44<01:25, 2489.08 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 408000/619741 [04:46<02:25, 1454.92 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 411000/619741 [04:46<03:35, 969.69 examples/s] Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 412000/619741 [04:46<02:50, 1215.94 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 409000/619741 [04:46<02:08, 1636.57 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 413000/619741 [04:47<02:29, 1385.60 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 414000/619741 [04:48<03:08, 1091.92 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 408000/619741 [04:49<03:42, 950.81 examples/s] Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 411000/619741 [04:48<02:37, 1327.35 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 415000/619741 [04:48<02:21, 1451.06 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 416000/619741 [04:48<01:58, 1718.97 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 412000/619741 [04:49<02:26, 1419.05 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 409000/619741 [04:49<03:25, 1025.11 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 410000/619741 [04:50<02:44, 1275.85 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 417000/619741 [04:49<01:48, 1860.48 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 413000/619741 [04:49<02:08, 1604.99 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 418000/619741 [04:49<01:55, 1751.26 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 411000/619741 [04:51<02:51, 1219.14 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 414000/619741 [04:51<02:59, 1144.24 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 420000/619741 [04:51<01:56, 1713.52 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 416000/619741 [04:52<02:24, 1407.26 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 417000/619741 [04:52<01:59, 1700.49 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 421000/619741 [04:52<02:25, 1370.47 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 412000/619741 [04:53<04:03, 853.04 examples/s] Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 422000/619741 [04:52<01:58, 1671.64 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 413000/619741 [04:53<03:08, 1098.45 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 414000/619741 [04:53<02:34, 1328.93 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 415000/619741 [04:53<02:02, 1672.73 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 423000/619741 [04:53<02:25, 1355.75 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 416000/619741 [04:54<02:11, 1554.34 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 418000/619741 [04:54<03:00, 1117.43 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 417000/619741 [04:55<01:52, 1794.97 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 419000/619741 [04:54<02:43, 1230.88 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 424000/619741 [04:55<02:57, 1100.22 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 425000/619741 [04:55<02:14, 1453.02 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 418000/619741 [04:56<02:16, 1482.55 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 420000/619741 [04:55<02:32, 1305.71 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 419000/619741 [04:56<01:45, 1894.84 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 426000/619741 [04:55<01:47, 1807.92 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 420000/619741 [04:56<01:23, 2386.80 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 427000/619741 [04:55<01:36, 2002.71 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 421000/619741 [04:55<02:26, 1358.04 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 428000/619741 [04:56<01:32, 2066.41 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 429000/619741 [04:56<01:16, 2491.82 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 421000/619741 [04:57<01:53, 1753.34 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 422000/619741 [04:57<01:29, 2199.97 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 423000/619741 [04:57<01:21, 2411.53 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 424000/619741 [04:57<01:08, 2869.03 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 422000/619741 [04:57<03:08, 1046.67 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 425000/619741 [04:58<01:04, 3041.72 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 426000/619741 [04:58<00:56, 3439.22 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 423000/619741 [04:57<02:38, 1241.57 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 430000/619741 [04:57<02:22, 1334.91 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 431000/619741 [04:58<01:53, 1655.92 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 424000/619741 [04:58<02:33, 1273.57 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 426000/619741 [04:58<01:29, 2156.22 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 428000/619741 [04:59<01:35, 2003.65 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 427000/619741 [04:59<01:35, 2016.48 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 432000/619741 [05:00<02:59, 1044.78 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 428000/619741 [05:00<01:45, 1817.80 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 429000/619741 [05:00<01:54, 1669.14 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 433000/619741 [05:01<03:25, 909.07 examples/s] Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 430000/619741 [05:01<02:03, 1534.90 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 431000/619741 [05:01<01:48, 1738.20 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 434000/619741 [05:02<02:56, 1054.26 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 435000/619741 [05:02<02:22, 1298.76 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 432000/619741 [05:02<02:04, 1512.50 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 429000/619741 [05:03<04:17, 739.75 examples/s] Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 433000/619741 [05:03<01:42, 1817.01 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 430000/619741 [05:04<03:17, 958.57 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 436000/619741 [05:03<02:45, 1110.42 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 431000/619741 [05:04<03:00, 1043.19 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 438000/619741 [05:04<01:46, 1701.61 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 439000/619741 [05:04<01:25, 2122.97 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 434000/619741 [05:04<02:33, 1210.57 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 435000/619741 [05:05<02:13, 1385.96 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 432000/619741 [05:06<03:22, 926.05 examples/s] Running tokenizer on dataset (num_proc=20):  71%|███████   | 441000/619741 [05:05<01:39, 1789.80 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 433000/619741 [05:06<02:43, 1139.05 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████▏  | 442000/619741 [05:06<01:42, 1736.74 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 434000/619741 [05:07<02:32, 1221.04 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 435000/619741 [05:07<01:54, 1612.08 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 436000/619741 [05:07<01:29, 2053.54 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████▏  | 443000/619741 [05:07<02:11, 1345.37 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 437000/619741 [05:07<02:54, 1047.51 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 437000/619741 [05:08<01:57, 1559.43 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 438000/619741 [05:07<02:29, 1217.94 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 444000/619741 [05:08<02:09, 1351.87 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 445000/619741 [05:08<01:48, 1605.60 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 438000/619741 [05:09<02:06, 1434.70 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 439000/619741 [05:08<02:26, 1235.68 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 446000/619741 [05:08<01:34, 1830.92 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 440000/619741 [05:08<01:56, 1546.72 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 447000/619741 [05:08<01:15, 2284.24 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 448000/619741 [05:09<01:16, 2231.21 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 439000/619741 [05:10<02:18, 1309.53 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 440000/619741 [05:10<01:42, 1759.08 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 449000/619741 [05:09<01:04, 2640.41 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 441000/619741 [05:09<02:13, 1335.32 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████▏  | 442000/619741 [05:10<02:01, 1457.71 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 441000/619741 [05:11<02:01, 1467.29 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████▏  | 443000/619741 [05:10<01:41, 1744.24 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████▏  | 442000/619741 [05:12<02:15, 1311.49 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████▏  | 443000/619741 [05:12<01:43, 1709.56 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 444000/619741 [05:11<02:00, 1453.03 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 445000/619741 [05:12<01:07, 2580.95 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 446000/619741 [05:12<00:55, 3111.48 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 450000/619741 [05:12<03:10, 892.84 examples/s] Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 445000/619741 [05:12<02:11, 1332.27 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 447000/619741 [05:12<01:20, 2138.42 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 447000/619741 [05:13<01:23, 2080.13 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 448000/619741 [05:13<01:11, 2389.73 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 448000/619741 [05:14<01:18, 2179.27 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 452000/619741 [05:14<02:39, 1050.94 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 453000/619741 [05:14<02:10, 1278.75 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 454000/619741 [05:14<02:01, 1359.57 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 449000/619741 [05:15<02:18, 1233.06 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 449000/619741 [05:16<02:59, 951.16 examples/s] Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 450000/619741 [05:16<02:27, 1153.75 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 450000/619741 [05:17<02:32, 1109.70 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 451000/619741 [05:16<02:08, 1309.90 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 455000/619741 [05:17<03:02, 902.86 examples/s] Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 451000/619741 [05:18<02:25, 1161.10 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 452000/619741 [05:17<02:07, 1312.61 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 453000/619741 [05:17<01:38, 1694.62 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 456000/619741 [05:17<02:39, 1028.12 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 457000/619741 [05:17<01:58, 1376.63 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 458000/619741 [05:17<01:31, 1777.19 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 454000/619741 [05:18<01:43, 1603.84 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 452000/619741 [05:19<02:29, 1118.72 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 459000/619741 [05:18<01:23, 1935.84 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 455000/619741 [05:18<01:23, 1966.96 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 456000/619741 [05:19<01:35, 1711.04 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 453000/619741 [05:20<02:35, 1071.98 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 460000/619741 [05:19<01:55, 1378.18 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 454000/619741 [05:20<02:04, 1326.19 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 461000/619741 [05:19<01:28, 1794.49 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 455000/619741 [05:20<01:39, 1651.93 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 462000/619741 [05:20<01:21, 1936.88 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 456000/619741 [05:21<01:30, 1818.52 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 463000/619741 [05:20<01:09, 2244.76 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 465000/619741 [05:21<00:58, 2636.73 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 457000/619741 [05:22<02:14, 1210.31 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 466000/619741 [05:22<01:25, 1807.12 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 458000/619741 [05:22<02:38, 1018.89 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 467000/619741 [05:22<01:13, 2076.73 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 459000/619741 [05:22<02:17, 1165.45 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 460000/619741 [05:22<01:54, 1393.61 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 458000/619741 [05:23<02:38, 1017.93 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 461000/619741 [05:23<01:31, 1728.87 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 468000/619741 [05:23<01:36, 1577.00 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 462000/619741 [05:23<01:23, 1888.48 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 463000/619741 [05:23<01:19, 1975.00 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 459000/619741 [05:25<02:48, 951.69 examples/s] Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 461000/619741 [05:25<01:59, 1327.76 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 462000/619741 [05:26<01:41, 1546.76 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 463000/619741 [05:26<01:23, 1878.76 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 464000/619741 [05:26<01:06, 2332.08 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 470000/619741 [05:25<02:13, 1120.35 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 466000/619741 [05:26<00:44, 3471.68 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 464000/619741 [05:26<02:43, 954.11 examples/s] Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 465000/619741 [05:26<02:01, 1275.35 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 466000/619741 [05:27<01:47, 1424.59 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 471000/619741 [05:27<02:24, 1032.15 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 467000/619741 [05:27<01:13, 2087.64 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 468000/619741 [05:28<00:59, 2545.23 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 469000/619741 [05:28<00:49, 3049.48 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 467000/619741 [05:27<01:40, 1524.14 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 472000/619741 [05:27<02:18, 1067.79 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 473000/619741 [05:28<01:45, 1396.83 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 469000/619741 [05:28<01:21, 1846.26 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 474000/619741 [05:28<01:43, 1401.78 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 470000/619741 [05:29<01:51, 1339.77 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 471000/619741 [05:29<01:28, 1679.25 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 470000/619741 [05:31<02:36, 959.49 examples/s] Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 475000/619741 [05:30<02:43, 883.18 examples/s] Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 477000/619741 [05:31<01:37, 1463.35 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 472000/619741 [05:31<02:06, 1165.91 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 474000/619741 [05:31<01:19, 1842.14 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 478000/619741 [05:31<01:33, 1511.67 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 471000/619741 [05:33<03:10, 781.46 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 479000/619741 [05:32<01:26, 1629.04 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 475000/619741 [05:32<01:18, 1842.63 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 473000/619741 [05:33<01:52, 1299.48 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 480000/619741 [05:32<01:20, 1744.38 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 481000/619741 [05:32<01:03, 2201.04 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 475000/619741 [05:33<01:22, 1745.49 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 482000/619741 [05:33<01:10, 1947.22 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 483000/619741 [05:33<00:59, 2283.46 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 476000/619741 [05:33<01:53, 1268.23 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 477000/619741 [05:33<01:27, 1627.73 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 476000/619741 [05:35<01:41, 1413.42 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 484000/619741 [05:34<01:18, 1730.14 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 477000/619741 [05:35<01:48, 1311.27 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 485000/619741 [05:35<01:28, 1527.95 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 478000/619741 [05:35<02:14, 1055.95 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 479000/619741 [05:36<01:52, 1256.30 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 480000/619741 [05:36<01:38, 1414.67 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 486000/619741 [05:36<01:45, 1262.37 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 478000/619741 [05:37<02:23, 986.63 examples/s] Running tokenizer on dataset (num_proc=20):  79%|███████▊  | 487000/619741 [05:37<01:29, 1476.13 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 479000/619741 [05:38<01:58, 1187.50 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▊  | 488000/619741 [05:37<01:21, 1617.33 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 482000/619741 [05:37<01:29, 1530.79 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 489000/619741 [05:38<01:14, 1744.18 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 483000/619741 [05:38<01:21, 1675.51 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 480000/619741 [05:39<02:05, 1109.35 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 481000/619741 [05:39<01:49, 1268.88 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 490000/619741 [05:38<01:27, 1484.05 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 482000/619741 [05:40<01:31, 1499.74 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 484000/619741 [05:40<00:56, 2402.71 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 484000/619741 [05:39<01:43, 1317.45 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 485000/619741 [05:40<00:49, 2714.82 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 486000/619741 [05:41<01:06, 2014.36 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 491000/619741 [05:40<02:05, 1029.43 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 485000/619741 [05:40<02:04, 1082.49 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 486000/619741 [05:41<01:46, 1259.02 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▊  | 487000/619741 [05:41<01:19, 1670.00 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 492000/619741 [05:41<02:07, 1004.70 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▊  | 487000/619741 [05:42<01:29, 1478.66 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▊  | 488000/619741 [05:42<01:14, 1773.72 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 493000/619741 [05:41<01:40, 1260.12 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 489000/619741 [05:42<01:13, 1771.62 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 494000/619741 [05:43<02:16, 920.77 examples/s] Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 495000/619741 [05:44<01:49, 1135.19 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 490000/619741 [05:44<01:56, 1116.27 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 490000/619741 [05:45<01:52, 1153.09 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 496000/619741 [05:45<01:50, 1120.71 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 497000/619741 [05:45<01:25, 1441.07 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 491000/619741 [05:45<01:57, 1096.14 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 498000/619741 [05:45<01:09, 1751.37 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 491000/619741 [05:46<02:09, 997.40 examples/s] Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 493000/619741 [05:45<01:21, 1549.72 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 492000/619741 [05:46<01:41, 1252.88 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 499000/619741 [05:46<01:13, 1646.65 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 495000/619741 [05:46<01:04, 1925.35 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 493000/619741 [05:47<01:32, 1370.79 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 500000/619741 [05:46<01:04, 1852.90 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 502000/619741 [05:46<00:38, 3092.78 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 494000/619741 [05:47<01:20, 1563.53 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 496000/619741 [05:47<01:05, 1894.77 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 495000/619741 [05:48<01:05, 1891.92 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 496000/619741 [05:48<00:50, 2440.60 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 503000/619741 [05:47<00:48, 2383.30 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 497000/619741 [05:48<01:16, 1607.98 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████▏ | 504000/619741 [05:48<01:18, 1468.09 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 497000/619741 [05:49<01:30, 1352.84 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 498000/619741 [05:49<01:26, 1401.99 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 499000/619741 [05:50<01:37, 1234.89 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 501000/619741 [05:50<01:01, 1937.15 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 502000/619741 [05:50<00:49, 2373.19 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 506000/619741 [05:50<01:28, 1280.19 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 498000/619741 [05:52<02:25, 835.61 examples/s] Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 507000/619741 [05:51<01:20, 1392.09 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 503000/619741 [05:51<01:04, 1814.25 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 508000/619741 [05:51<01:04, 1730.15 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 499000/619741 [05:52<01:52, 1077.24 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 509000/619741 [05:51<00:53, 2085.45 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 500000/619741 [05:52<01:33, 1274.70 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 501000/619741 [05:53<01:23, 1422.42 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████▏ | 504000/619741 [05:52<01:20, 1434.72 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 502000/619741 [05:53<01:01, 1904.47 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 503000/619741 [05:54<01:06, 1765.02 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████▏ | 505000/619741 [05:53<01:34, 1212.73 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 510000/619741 [05:53<01:43, 1059.02 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████▏ | 504000/619741 [05:54<01:08, 1690.58 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 511000/619741 [05:54<01:23, 1301.45 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 506000/619741 [05:54<01:35, 1191.97 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████▏ | 505000/619741 [05:55<01:19, 1443.92 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 506000/619741 [05:56<01:13, 1538.81 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 507000/619741 [05:56<00:58, 1924.23 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 507000/619741 [05:55<01:43, 1087.47 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 512000/619741 [05:55<01:47, 1003.80 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 513000/619741 [05:55<01:22, 1293.16 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 508000/619741 [05:56<00:59, 1872.44 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 508000/619741 [05:56<01:32, 1209.26 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 509000/619741 [05:57<01:02, 1764.06 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 514000/619741 [05:57<01:31, 1156.10 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 515000/619741 [05:57<01:29, 1171.50 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 516000/619741 [05:58<01:11, 1460.18 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 510000/619741 [05:58<01:35, 1143.56 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 510000/619741 [05:59<01:32, 1189.21 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 512000/619741 [05:58<01:01, 1763.60 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 517000/619741 [05:58<01:03, 1619.22 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 513000/619741 [05:58<00:54, 1952.37 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 514000/619741 [05:58<00:48, 2158.95 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 519000/619741 [05:59<00:47, 2116.43 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 511000/619741 [06:00<01:37, 1111.46 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 521000/619741 [06:00<00:48, 2015.85 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 512000/619741 [06:01<01:41, 1062.84 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 515000/619741 [06:00<01:15, 1378.27 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 513000/619741 [06:01<01:18, 1363.27 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 522000/619741 [06:01<00:56, 1718.82 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 516000/619741 [06:01<01:16, 1356.29 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 514000/619741 [06:02<01:13, 1435.99 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 524000/619741 [06:01<00:36, 2634.07 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 515000/619741 [06:02<01:01, 1705.56 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 517000/619741 [06:01<01:05, 1573.86 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 525000/619741 [06:01<00:39, 2381.00 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 516000/619741 [06:02<01:02, 1672.28 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 526000/619741 [06:02<00:50, 1858.61 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 517000/619741 [06:03<01:04, 1586.30 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 518000/619741 [06:02<01:24, 1201.81 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 527000/619741 [06:03<00:53, 1736.65 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 520000/619741 [06:03<01:04, 1542.45 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 528000/619741 [06:04<01:05, 1407.83 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 521000/619741 [06:04<01:07, 1463.41 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 518000/619741 [06:05<01:50, 919.62 examples/s] Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 522000/619741 [06:05<01:03, 1538.65 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 519000/619741 [06:06<01:26, 1160.65 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 523000/619741 [06:05<01:03, 1533.30 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 520000/619741 [06:06<01:11, 1388.44 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 521000/619741 [06:06<01:01, 1599.24 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 524000/619741 [06:06<00:58, 1648.84 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 522000/619741 [06:07<00:54, 1786.80 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 529000/619741 [06:07<01:56, 778.06 examples/s] Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 525000/619741 [06:07<01:13, 1294.95 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 524000/619741 [06:08<00:58, 1639.70 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 525000/619741 [06:08<00:46, 2057.34 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 530000/619741 [06:08<01:44, 858.44 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 526000/619741 [06:08<01:24, 1112.23 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 527000/619741 [06:08<01:07, 1370.20 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 526000/619741 [06:10<01:05, 1422.01 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 531000/619741 [06:09<01:44, 852.52 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 527000/619741 [06:10<00:50, 1848.59 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 532000/619741 [06:09<01:22, 1066.24 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 528000/619741 [06:09<01:10, 1306.83 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 533000/619741 [06:09<01:03, 1361.13 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 534000/619741 [06:10<00:48, 1752.18 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 529000/619741 [06:10<01:00, 1510.44 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 528000/619741 [06:11<00:56, 1633.88 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▋ | 534987/619741 [06:10<00:38, 2190.31 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 530000/619741 [06:10<00:54, 1651.75 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 530000/619741 [06:12<00:49, 1796.04 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 531000/619741 [06:11<00:57, 1549.09 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▋ | 535987/619741 [06:11<00:57, 1463.46 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 532000/619741 [06:11<00:46, 1901.70 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 531000/619741 [06:12<00:48, 1820.32 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 536987/619741 [06:11<00:45, 1810.34 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 533000/619741 [06:12<00:40, 2123.93 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 537987/619741 [06:12<01:01, 1336.17 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 538987/619741 [06:13<00:45, 1783.38 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 539987/619741 [06:13<00:35, 2254.06 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 534000/619741 [06:13<01:04, 1333.24 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 540987/619741 [06:13<00:30, 2618.57 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 541987/619741 [06:13<00:23, 3347.54 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 532000/619741 [06:14<01:23, 1048.15 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 542987/619741 [06:13<00:21, 3546.47 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 533000/619741 [06:15<01:15, 1156.23 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▋ | 535000/619741 [06:14<01:16, 1104.01 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 543987/619741 [06:14<00:36, 2066.41 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▋ | 536000/619741 [06:15<01:15, 1110.49 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 534000/619741 [06:16<01:25, 1005.52 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 544987/619741 [06:15<00:50, 1478.98 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 539000/619741 [06:16<00:40, 1976.34 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 545987/619741 [06:16<00:41, 1776.83 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 546987/619741 [06:17<00:50, 1430.18 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▋ | 536000/619741 [06:18<01:13, 1135.50 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 540000/619741 [06:17<00:56, 1403.91 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 547987/619741 [06:18<00:52, 1370.19 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 541000/619741 [06:18<00:57, 1372.01 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 537000/619741 [06:19<01:21, 1012.70 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 538000/619741 [06:19<01:04, 1259.31 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 542000/619741 [06:18<00:51, 1515.61 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 539000/619741 [06:20<00:55, 1466.45 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 540000/619741 [06:20<00:44, 1780.10 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 543000/619741 [06:19<00:51, 1478.16 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 541000/619741 [06:21<00:48, 1609.25 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 543000/619741 [06:21<00:32, 2338.44 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 544000/619741 [06:21<01:07, 1120.65 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 544000/619741 [06:22<00:35, 2123.29 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 548987/619741 [06:21<01:47, 660.53 examples/s] Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 545000/619741 [06:21<01:06, 1118.25 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 545000/619741 [06:22<00:42, 1772.51 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 549987/619741 [06:22<01:37, 714.66 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 546000/619741 [06:23<00:43, 1704.95 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 550987/619741 [06:22<01:15, 910.86 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 546000/619741 [06:23<01:11, 1035.60 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 548000/619741 [06:23<00:31, 2279.33 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 551987/619741 [06:23<01:03, 1069.63 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 552987/619741 [06:23<00:46, 1447.50 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 548000/619741 [06:23<00:48, 1494.24 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 549000/619741 [06:25<00:41, 1695.04 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 549000/619741 [06:24<00:45, 1562.90 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 553987/619741 [06:24<00:49, 1335.50 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 550000/619741 [06:24<00:42, 1658.83 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 551000/619741 [06:25<00:34, 1980.79 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 555987/619741 [06:25<00:35, 1815.05 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 550000/619741 [06:26<00:48, 1438.04 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 556987/619741 [06:25<00:29, 2119.78 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 552000/619741 [06:25<00:38, 1781.26 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 553000/619741 [06:25<00:31, 2118.18 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 557987/619741 [06:26<00:40, 1532.88 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 554000/619741 [06:26<00:32, 2010.89 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 558987/619741 [06:26<00:33, 1828.83 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 559987/619741 [06:27<00:28, 2121.53 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 560987/619741 [06:27<00:27, 2119.86 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 552000/619741 [06:28<01:04, 1049.40 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 555000/619741 [06:28<00:57, 1122.66 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 561987/619741 [06:28<00:35, 1612.22 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 556000/619741 [06:28<00:46, 1377.56 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 553000/619741 [06:29<01:03, 1044.99 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 563987/619741 [06:28<00:23, 2355.13 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 557000/619741 [06:29<00:38, 1634.00 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 554000/619741 [06:30<00:53, 1224.53 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 558000/619741 [06:29<00:38, 1607.42 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 559000/619741 [06:29<00:29, 2087.69 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 555000/619741 [06:31<00:55, 1166.44 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 560000/619741 [06:30<00:26, 2216.66 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 556000/619741 [06:31<00:46, 1365.34 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 564987/619741 [06:30<00:41, 1331.74 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 561000/619741 [06:30<00:29, 2021.20 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 562000/619741 [06:30<00:22, 2571.48 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████▏| 565987/619741 [06:30<00:34, 1562.78 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████▏| 566987/619741 [06:31<00:30, 1724.52 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 563000/619741 [06:31<00:26, 2140.47 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 557000/619741 [06:32<00:51, 1217.88 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 558000/619741 [06:32<00:44, 1386.46 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 559000/619741 [06:33<00:33, 1801.94 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 560000/619741 [06:33<00:28, 2093.98 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 561000/619741 [06:34<00:38, 1537.05 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 562000/619741 [06:34<00:30, 1900.38 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 564000/619741 [06:34<01:05, 853.25 examples/s] Running tokenizer on dataset (num_proc=20):  91%|█████████ | 563000/619741 [06:35<00:35, 1608.75 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 565000/619741 [06:35<01:01, 897.23 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 565000/619741 [06:36<00:30, 1803.89 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████▏| 566000/619741 [06:36<00:54, 986.69 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 567987/619741 [06:36<01:31, 565.41 examples/s] Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 570987/619741 [06:36<00:42, 1156.22 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████▏| 566000/619741 [06:37<00:35, 1523.45 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████▏| 567000/619741 [06:37<00:27, 1887.08 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 568000/619741 [06:37<00:22, 2349.66 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████▏| 567000/619741 [06:37<00:50, 1034.90 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 571987/619741 [06:37<00:42, 1131.46 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 568000/619741 [06:37<00:44, 1153.63 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 569000/619741 [06:37<00:35, 1442.70 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 569987/619741 [06:38<00:23, 2148.08 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 572987/619741 [06:38<00:46, 1011.26 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 570000/619741 [06:39<00:39, 1260.52 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 575987/619741 [06:39<00:23, 1877.15 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 572000/619741 [06:39<00:21, 2208.30 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 570974/619741 [06:40<00:34, 1414.39 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 573000/619741 [06:39<00:22, 2064.94 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 576987/619741 [06:39<00:23, 1806.08 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 574000/619741 [06:39<00:17, 2596.15 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 577987/619741 [06:39<00:20, 2080.66 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 575000/619741 [06:39<00:14, 3059.73 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 578987/619741 [06:40<00:16, 2489.89 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▎| 579987/619741 [06:40<00:16, 2369.37 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▎| 580987/619741 [06:41<00:16, 2292.42 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 576000/619741 [06:41<00:26, 1635.69 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 577000/619741 [06:41<00:20, 2064.89 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 571974/619741 [06:42<00:52, 908.64 examples/s] Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 581987/619741 [06:41<00:20, 1865.88 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 583987/619741 [06:42<00:12, 2939.12 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 572974/619741 [06:43<00:47, 985.36 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 573974/619741 [06:43<00:35, 1289.70 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 578987/619741 [06:42<00:22, 1820.70 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 574974/619741 [06:43<00:27, 1647.50 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 575974/619741 [06:44<00:33, 1304.93 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▎| 579987/619741 [06:44<00:32, 1229.01 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▎| 580987/619741 [06:44<00:27, 1418.21 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 584987/619741 [06:44<00:31, 1088.72 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 585987/619741 [06:44<00:24, 1377.76 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 581987/619741 [06:45<00:22, 1675.27 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 576974/619741 [06:45<00:37, 1140.14 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 577974/619741 [06:46<00:33, 1253.07 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 582987/619741 [06:46<00:26, 1386.76 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 578974/619741 [06:47<00:30, 1339.92 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▎| 579974/619741 [06:47<00:22, 1739.35 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▎| 580974/619741 [06:47<00:19, 1991.03 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 583987/619741 [06:46<00:26, 1336.96 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 581974/619741 [06:47<00:16, 2327.23 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 584987/619741 [06:47<00:27, 1278.04 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 582974/619741 [06:48<00:20, 1827.60 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 585987/619741 [06:47<00:20, 1663.54 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 586987/619741 [06:48<00:18, 1766.58 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 583974/619741 [06:49<00:19, 1840.42 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 586987/619741 [06:48<00:51, 632.09 examples/s] Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 587987/619741 [06:49<00:39, 802.82 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 587987/619741 [06:49<00:26, 1187.53 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 589987/619741 [06:50<00:26, 1116.74 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 585974/619741 [06:51<00:24, 1351.91 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 588974/619741 [06:50<00:22, 1347.66 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 590987/619741 [06:50<00:23, 1237.66 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 591974/619741 [06:50<00:18, 1542.46 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 586974/619741 [06:51<00:21, 1500.51 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 587974/619741 [06:51<00:18, 1753.70 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 588974/619741 [06:52<00:15, 2043.19 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 589961/619741 [06:52<00:14, 2054.98 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 592961/619741 [06:52<00:23, 1117.71 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 589974/619741 [06:52<00:33, 882.93 examples/s] Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 594948/619741 [06:52<00:13, 1848.33 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 592974/619741 [06:53<00:16, 1646.54 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 595935/619741 [06:53<00:13, 1741.92 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 596922/619741 [06:53<00:10, 2175.88 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 597922/619741 [06:53<00:08, 2593.65 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 594974/619741 [06:53<00:12, 1964.99 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 599922/619741 [06:53<00:06, 3190.44 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 595961/619741 [06:53<00:10, 2184.31 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 600922/619741 [06:54<00:05, 3702.72 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 596948/619741 [06:54<00:09, 2494.15 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 601922/619741 [06:54<00:05, 3530.24 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 602922/619741 [06:55<00:06, 2611.55 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 597935/619741 [06:55<00:11, 1844.25 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 591961/619741 [06:56<00:30, 904.60 examples/s] Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 592961/619741 [06:56<00:23, 1149.12 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 598922/619741 [06:56<00:15, 1329.05 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 593961/619741 [06:57<00:21, 1191.22 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 599909/619741 [06:56<00:11, 1658.18 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 594948/619741 [06:57<00:17, 1386.67 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 600896/619741 [06:56<00:09, 1926.35 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 603909/619741 [06:57<00:12, 1227.20 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 604896/619741 [06:57<00:09, 1612.48 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 601896/619741 [06:57<00:08, 2207.84 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 602896/619741 [06:57<00:06, 2423.03 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 595948/619741 [06:58<00:18, 1320.15 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 603896/619741 [06:58<00:06, 2264.50 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 605883/619741 [06:58<00:12, 1146.87 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 606870/619741 [06:58<00:08, 1447.77 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 604883/619741 [06:58<00:08, 1823.41 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 596948/619741 [06:59<00:20, 1134.79 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 597935/619741 [07:00<00:15, 1426.90 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 598935/619741 [07:00<00:11, 1771.79 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 601935/619741 [07:00<00:04, 3677.55 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 605870/619741 [07:00<00:10, 1338.24 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 603909/619741 [07:01<00:04, 3292.84 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 604909/619741 [07:01<00:03, 3723.97 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 606870/619741 [07:00<00:10, 1281.40 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 607857/619741 [07:01<00:13, 885.19 examples/s] Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 608844/619741 [07:01<00:09, 1166.03 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 609831/619741 [07:01<00:06, 1523.24 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 605909/619741 [07:02<00:06, 2146.72 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 607857/619741 [07:01<00:09, 1262.39 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 606909/619741 [07:03<00:06, 1873.69 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▊| 610818/619741 [07:02<00:06, 1344.05 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 609831/619741 [07:02<00:05, 1748.76 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▊| 610831/619741 [07:02<00:04, 2002.90 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 612792/619741 [07:02<00:03, 2037.74 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 613779/619741 [07:02<00:02, 2449.43 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 607896/619741 [07:03<00:06, 1770.38 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▊| 611831/619741 [07:03<00:04, 1765.89 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 614766/619741 [07:03<00:02, 1927.01 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 612818/619741 [07:03<00:03, 1950.37 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 608883/619741 [07:05<00:08, 1208.99 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 615753/619741 [07:04<00:02, 1526.03 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 609870/619741 [07:05<00:06, 1488.13 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 613818/619741 [07:05<00:04, 1373.08 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 616753/619741 [07:05<00:01, 1621.84 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 614805/619741 [07:05<00:02, 1660.96 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▊| 610858/619741 [07:06<00:06, 1425.22 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▊| 611845/619741 [07:06<00:04, 1607.35 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 612832/619741 [07:06<00:03, 1990.63 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 613819/619741 [07:07<00:02, 2361.42 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 614806/619741 [07:07<00:01, 2665.35 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 615793/619741 [07:06<00:03, 1183.11 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 615793/619741 [07:08<00:02, 1832.40 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 617767/619741 [07:09<00:00, 2106.06 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 616780/619741 [07:08<00:03, 894.03 examples/s] Running tokenizer on dataset (num_proc=20): 100%|██████████| 619741/619741 [07:10<00:00, 2121.41 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 617767/619741 [07:09<00:02, 978.52 examples/s]Running tokenizer on dataset (num_proc=20): 100%|██████████| 619741/619741 [07:10<00:00, 1439.29 examples/s]
Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 618754/619741 [07:10<00:00, 990.70 examples/s]Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 487956
})
Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 617753/619741 [07:11<00:04, 424.37 examples/s] Running tokenizer on dataset (num_proc=20): 100%|██████████| 619741/619741 [07:12<00:00, 800.38 examples/s]Running tokenizer on dataset (num_proc=20): 100%|██████████| 619741/619741 [07:12<00:00, 1432.30 examples/s]
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 487956
})
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yangdezhao/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.513521909713745 seconds
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yangdezhao/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4645650386810303 seconds
Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 618753/619741 [07:18<00:03, 283.08 examples/s]Running tokenizer on dataset (num_proc=20): 100%|██████████| 619741/619741 [07:25<00:00, 219.76 examples/s]Running tokenizer on dataset (num_proc=20): 100%|██████████| 619741/619741 [07:25<00:00, 1389.75 examples/s]
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 487956
})
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yangdezhao/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.461240530014038 seconds
[2024-02-26 09:43:57,894] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-02-26 09:43:57,898] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-02-26 09:43:57,898] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-02-26 09:43:57,906] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-02-26 09:43:57,907] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-02-26 09:43:57,907] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-02-26 09:43:57,907] [INFO] [stage_1_and_2.py:143:__init__] Reduce bucket size 500000000
[2024-02-26 09:43:57,907] [INFO] [stage_1_and_2.py:144:__init__] Allgather bucket size 500000000
[2024-02-26 09:43:57,907] [INFO] [stage_1_and_2.py:145:__init__] CPU Offload: True
[2024-02-26 09:43:57,907] [INFO] [stage_1_and_2.py:146:__init__] Round robin gradient partitioning: False
[2024-02-26 09:44:21,737] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
[2024-02-26 09:44:21,738] [INFO] [utils.py:792:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-02-26 09:44:21,738] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 86.71 GB, percent = 34.5%
[2024-02-26 09:44:27,177] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
[2024-02-26 09:44:27,178] [INFO] [utils.py:792:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-02-26 09:44:27,178] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 111.67 GB, percent = 44.4%
[2024-02-26 09:44:27,178] [INFO] [stage_1_and_2.py:533:__init__] optimizer state initialized
[2024-02-26 09:44:27,375] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
[2024-02-26 09:44:27,376] [INFO] [utils.py:792:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-02-26 09:44:27,376] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 111.67 GB, percent = 44.4%
[2024-02-26 09:44:27,378] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-02-26 09:44:27,378] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-02-26 09:44:27,378] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-02-26 09:44:27,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2024-02-26 09:44:27,380] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2024-02-26 09:44:27,380] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-02-26 09:44:27,380] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-02-26 09:44:27,380] [INFO] [config.py:988:print]   amp_enabled .................. False
[2024-02-26 09:44:27,380] [INFO] [config.py:988:print]   amp_params ................... False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   bfloat16_enabled ............. True
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3fdc72d060>
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   communication_data_type ...... None
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   disable_allgather ............ False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   dump_state ................... False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   fp16_enabled ................. False
[2024-02-26 09:44:27,381] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   global_rank .................. 0
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 4
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 1
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   loss_scale ................... 1.0
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   optimizer_name ............... None
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   optimizer_params ............. None
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   pld_enabled .................. False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   pld_params ................... False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   scheduler_name ............... None
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   scheduler_params ............. None
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   sparse_attention ............. None
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   steps_per_print .............. inf
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   train_batch_size ............. 192
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  12
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   world_size ................... 4
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  True
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   zero_enabled ................. True
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2024-02-26 09:44:27,382] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2
[2024-02-26 09:44:27,383] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 192, 
    "train_micro_batch_size_per_gpu": 12, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1747] 2024-02-26 09:44:27,383 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-26 09:44:27,383 >>   Num examples = 487,956
[INFO|trainer.py:1749] 2024-02-26 09:44:27,383 >>   Num Epochs = 1
[INFO|trainer.py:1750] 2024-02-26 09:44:27,383 >>   Instantaneous batch size per device = 12
[INFO|trainer.py:1753] 2024-02-26 09:44:27,383 >>   Total train batch size (w. parallel, distributed & accumulation) = 192
[INFO|trainer.py:1754] 2024-02-26 09:44:27,383 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1755] 2024-02-26 09:44:27,383 >>   Total optimization steps = 2,541
[INFO|trainer.py:1756] 2024-02-26 09:44:27,385 >>   Number of trainable parameters = 4,328,783,872
  0%|          | 0/2541 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/2541 [01:07<47:20:16, 67.09s/it]  0%|          | 2/2541 [02:10<45:52:58, 65.06s/it]  0%|          | 3/2541 [03:14<45:17:46, 64.25s/it]  0%|          | 4/2541 [04:16<44:49:08, 63.60s/it]  0%|          | 5/2541 [05:18<44:22:54, 63.00s/it]  0%|          | 6/2541 [06:20<44:10:46, 62.74s/it]  0%|          | 7/2541 [07:22<43:59:12, 62.49s/it]  0%|          | 8/2541 [08:24<43:46:05, 62.21s/it]  0%|          | 9/2541 [09:25<43:34:18, 61.95s/it]  0%|          | 10/2541 [10:27<43:28:39, 61.84s/it]                                                    {'loss': 3.2655, 'learning_rate': 4.9998089289473884e-05, 'epoch': 0.0}
  0%|          | 10/2541 [10:27<43:28:39, 61.84s/it]  0%|          | 11/2541 [11:28<43:20:38, 61.68s/it]  0%|          | 12/2541 [12:30<43:18:05, 61.64s/it]  1%|          | 13/2541 [13:31<43:13:36, 61.56s/it]  1%|          | 14/2541 [14:33<43:11:43, 61.54s/it]  1%|          | 15/2541 [15:34<43:04:59, 61.40s/it]  1%|          | 16/2541 [16:35<43:03:54, 61.40s/it]  1%|          | 17/2541 [17:36<43:02:48, 61.40s/it]  1%|          | 18/2541 [18:38<43:04:40, 61.47s/it]  1%|          | 19/2541 [19:39<43:00:09, 61.38s/it]  1%|          | 20/2541 [20:41<42:58:20, 61.36s/it]                                                    {'loss': 2.6039, 'learning_rate': 4.999235744996069e-05, 'epoch': 0.01}
  1%|          | 20/2541 [20:41<42:58:20, 61.36s/it]  1%|          | 21/2541 [21:42<42:54:32, 61.30s/it]  1%|          | 22/2541 [22:43<42:48:36, 61.18s/it]  1%|          | 23/2541 [23:44<42:53:04, 61.31s/it]  1%|          | 24/2541 [24:46<42:51:18, 61.29s/it]  1%|          | 25/2541 [25:47<42:48:21, 61.25s/it]  1%|          | 26/2541 [26:48<42:44:38, 61.18s/it]  1%|          | 27/2541 [27:49<42:40:40, 61.11s/it]  1%|          | 28/2541 [28:50<42:44:38, 61.23s/it]  1%|          | 29/2541 [29:51<42:43:36, 61.23s/it]  1%|          | 30/2541 [30:52<42:38:52, 61.14s/it]                                                    {'loss': 2.4081, 'learning_rate': 4.998280535761132e-05, 'epoch': 0.01}
  1%|          | 30/2541 [30:52<42:38:52, 61.14s/it]  1%|          | 31/2541 [31:53<42:33:03, 61.03s/it]  1%|▏         | 32/2541 [32:54<42:29:15, 60.96s/it]  1%|▏         | 33/2541 [33:55<42:29:56, 61.00s/it]  1%|▏         | 34/2541 [34:56<42:28:27, 60.99s/it]  1%|▏         | 35/2541 [35:57<42:31:17, 61.08s/it]  1%|▏         | 36/2541 [36:58<42:28:11, 61.03s/it]  1%|▏         | 37/2541 [37:59<42:26:26, 61.02s/it]  1%|▏         | 38/2541 [39:00<42:23:38, 60.97s/it]  2%|▏         | 39/2541 [40:01<42:20:06, 60.91s/it]  2%|▏         | 40/2541 [41:02<42:19:28, 60.92s/it]                                                    {'loss': 2.2725, 'learning_rate': 4.996943447252843e-05, 'epoch': 0.02}
  2%|▏         | 40/2541 [41:02<42:19:28, 60.92s/it]  2%|▏         | 41/2541 [42:03<42:24:29, 61.07s/it]  2%|▏         | 42/2541 [43:04<42:22:18, 61.04s/it]  2%|▏         | 43/2541 [44:05<42:19:57, 61.01s/it]  2%|▏         | 44/2541 [45:07<42:27:44, 61.22s/it]  2%|▏         | 45/2541 [46:08<42:24:42, 61.17s/it]  2%|▏         | 46/2541 [47:09<42:20:19, 61.09s/it]  2%|▏         | 47/2541 [48:10<42:21:53, 61.15s/it]  2%|▏         | 48/2541 [49:11<42:18:31, 61.10s/it]  2%|▏         | 49/2541 [50:12<42:14:18, 61.02s/it]  2%|▏         | 50/2541 [51:13<42:10:35, 60.95s/it]                                                    {'loss': 2.1717, 'learning_rate': 4.9952246838543304e-05, 'epoch': 0.02}
  2%|▏         | 50/2541 [51:13<42:10:35, 60.95s/it]  2%|▏         | 51/2541 [52:13<42:07:56, 60.91s/it]  2%|▏         | 52/2541 [53:15<42:10:30, 61.00s/it]  2%|▏         | 53/2541 [54:15<42:06:46, 60.94s/it]  2%|▏         | 54/2541 [55:16<42:05:29, 60.93s/it]  2%|▏         | 55/2541 [56:17<42:05:15, 60.95s/it]  2%|▏         | 56/2541 [57:18<42:03:41, 60.93s/it]  2%|▏         | 57/2541 [58:19<42:01:22, 60.90s/it]  2%|▏         | 58/2541 [59:20<42:04:22, 61.00s/it]  2%|▏         | 59/2541 [1:00:21<42:05:00, 61.04s/it]  2%|▏         | 60/2541 [1:01:22<42:02:33, 61.01s/it]                                                      {'loss': 2.0936, 'learning_rate': 4.99312450829034e-05, 'epoch': 0.02}
  2%|▏         | 60/2541 [1:01:22<42:02:33, 61.01s/it]  2%|▏         | 61/2541 [1:02:23<42:00:12, 60.97s/it]  2%|▏         | 62/2541 [1:03:24<41:58:41, 60.96s/it]  2%|▏         | 63/2541 [1:04:25<41:55:26, 60.91s/it]  3%|▎         | 64/2541 [1:05:26<41:56:31, 60.96s/it]  3%|▎         | 65/2541 [1:06:27<41:55:07, 60.95s/it]  3%|▎         | 66/2541 [1:07:28<41:54:00, 60.95s/it]  3%|▎         | 67/2541 [1:08:29<41:51:15, 60.90s/it]  3%|▎         | 68/2541 [1:09:30<41:51:44, 60.94s/it]  3%|▎         | 69/2541 [1:10:31<41:50:22, 60.93s/it]  3%|▎         | 70/2541 [1:11:32<41:55:14, 61.07s/it]                                                      {'loss': 2.0163, 'learning_rate': 4.990643241587075e-05, 'epoch': 0.03}
  3%|▎         | 70/2541 [1:11:32<41:55:14, 61.07s/it]  3%|▎         | 71/2541 [1:12:33<41:51:12, 61.00s/it]  3%|▎         | 72/2541 [1:13:34<41:46:52, 60.92s/it]  3%|▎         | 73/2541 [1:14:34<41:45:17, 60.91s/it]  3%|▎         | 74/2541 [1:15:35<41:42:51, 60.87s/it]  3%|▎         | 75/2541 [1:16:36<41:44:20, 60.93s/it]  3%|▎         | 76/2541 [1:17:37<41:41:52, 60.90s/it]  3%|▎         | 77/2541 [1:18:38<41:40:22, 60.89s/it]  3%|▎         | 78/2541 [1:19:39<41:39:20, 60.89s/it]  3%|▎         | 79/2541 [1:20:40<41:38:25, 60.89s/it]  3%|▎         | 80/2541 [1:21:41<41:35:50, 60.85s/it]                                                      {'loss': 1.9679, 'learning_rate': 4.987781263023128e-05, 'epoch': 0.03}
  3%|▎         | 80/2541 [1:21:41<41:35:50, 60.85s/it]  3%|▎         | 81/2541 [1:22:42<41:38:04, 60.93s/it]  3%|▎         | 82/2541 [1:23:43<41:39:15, 60.98s/it]  3%|▎         | 83/2541 [1:24:44<41:36:57, 60.95s/it]  3%|▎         | 84/2541 [1:25:44<41:34:11, 60.91s/it]  3%|▎         | 85/2541 [1:26:45<41:31:44, 60.87s/it]  3%|▎         | 86/2541 [1:27:46<41:28:56, 60.83s/it]  3%|▎         | 87/2541 [1:28:47<41:31:43, 60.92s/it]  3%|▎         | 88/2541 [1:29:48<41:31:58, 60.95s/it]  4%|▎         | 89/2541 [1:30:49<41:30:26, 60.94s/it]  4%|▎         | 90/2541 [1:31:50<41:28:11, 60.91s/it]                                                      {'loss': 1.9264, 'learning_rate': 4.984539010071506e-05, 'epoch': 0.04}
  4%|▎         | 90/2541 [1:31:50<41:28:11, 60.91s/it]  4%|▎         | 91/2541 [1:32:51<41:26:58, 60.91s/it]  4%|▎         | 92/2541 [1:33:52<41:25:46, 60.90s/it]  4%|▎         | 93/2541 [1:34:53<41:28:56, 61.00s/it]  4%|▎         | 94/2541 [1:35:54<41:27:17, 60.99s/it]  4%|▎         | 95/2541 [1:36:55<41:25:38, 60.97s/it]  4%|▍         | 96/2541 [1:37:56<41:22:24, 60.92s/it]  4%|▍         | 97/2541 [1:38:56<41:20:27, 60.90s/it]  4%|▍         | 98/2541 [1:39:58<41:23:54, 61.00s/it]  4%|▍         | 99/2541 [1:40:59<41:21:34, 60.97s/it]  4%|▍         | 100/2541 [1:41:59<41:18:31, 60.92s/it]                                                       {'loss': 1.8875, 'learning_rate': 4.9809169783327544e-05, 'epoch': 0.04}
  4%|▍         | 100/2541 [1:41:59<41:18:31, 60.92s/it]  4%|▍         | 101/2541 [1:43:00<41:17:16, 60.92s/it]  4%|▍         | 102/2541 [1:44:01<41:16:41, 60.93s/it]  4%|▍         | 103/2541 [1:45:02<41:15:06, 60.91s/it]  4%|▍         | 104/2541 [1:46:03<41:18:34, 61.02s/it]  4%|▍         | 105/2541 [1:47:05<41:18:35, 61.05s/it]  4%|▍         | 106/2541 [1:48:05<41:16:19, 61.02s/it]  4%|▍         | 107/2541 [1:49:06<41:14:22, 61.00s/it]  4%|▍         | 108/2541 [1:50:07<41:11:01, 60.94s/it]  4%|▍         | 109/2541 [1:51:08<41:10:28, 60.95s/it]  4%|▍         | 110/2541 [1:52:09<41:12:30, 61.02s/it]                                                       {'loss': 1.8595, 'learning_rate': 4.976915721459209e-05, 'epoch': 0.04}
  4%|▍         | 110/2541 [1:52:09<41:12:30, 61.02s/it]  4%|▍         | 111/2541 [1:53:11<41:13:43, 61.08s/it]  4%|▍         | 112/2541 [1:54:11<41:09:08, 60.99s/it]  4%|▍         | 113/2541 [1:55:12<41:07:27, 60.98s/it]  4%|▍         | 114/2541 [1:56:13<41:05:48, 60.96s/it]  5%|▍         | 115/2541 [1:57:14<41:03:37, 60.93s/it]  5%|▍         | 116/2541 [1:58:15<41:06:06, 61.02s/it]  5%|▍         | 117/2541 [1:59:16<41:04:20, 61.00s/it]  5%|▍         | 118/2541 [2:00:17<41:02:27, 60.98s/it]  5%|▍         | 119/2541 [2:01:18<40:59:12, 60.92s/it]  5%|▍         | 120/2541 [2:02:19<40:58:25, 60.93s/it]                                                       {'loss': 1.8398, 'learning_rate': 4.972535851070358e-05, 'epoch': 0.05}
  5%|▍         | 120/2541 [2:02:19<40:58:25, 60.93s/it]  5%|▍         | 121/2541 [2:03:20<40:58:14, 60.95s/it]  5%|▍         | 122/2541 [2:04:21<41:01:44, 61.06s/it]  5%|▍         | 123/2541 [2:05:22<40:58:54, 61.01s/it]  5%|▍         | 124/2541 [2:06:23<40:58:07, 61.02s/it]  5%|▍         | 125/2541 [2:07:24<40:57:39, 61.03s/it]  5%|▍         | 126/2541 [2:08:25<40:54:36, 60.98s/it]  5%|▍         | 127/2541 [2:09:26<40:52:16, 60.95s/it]  5%|▌         | 128/2541 [2:10:27<40:55:58, 61.07s/it]  5%|▌         | 129/2541 [2:11:28<40:52:36, 61.01s/it]  5%|▌         | 130/2541 [2:12:29<40:50:30, 60.98s/it]                                                       {'loss': 1.805, 'learning_rate': 4.967778036659358e-05, 'epoch': 0.05}
  5%|▌         | 130/2541 [2:12:29<40:50:30, 60.98s/it]  5%|▌         | 131/2541 [2:13:30<40:49:32, 60.98s/it]  5%|▌         | 132/2541 [2:14:31<40:49:42, 61.01s/it]  5%|▌         | 133/2541 [2:15:32<40:48:11, 61.00s/it]  5%|▌         | 134/2541 [2:16:33<40:49:23, 61.06s/it]  5%|▌         | 135/2541 [2:17:34<40:47:20, 61.03s/it]  5%|▌         | 136/2541 [2:18:35<40:45:21, 61.01s/it]  5%|▌         | 137/2541 [2:19:36<40:42:40, 60.97s/it]  5%|▌         | 138/2541 [2:20:37<40:40:33, 60.94s/it]  5%|▌         | 139/2541 [2:21:38<40:43:53, 61.05s/it]  6%|▌         | 140/2541 [2:22:39<40:40:59, 61.00s/it]                                                       {'loss': 1.7821, 'learning_rate': 4.962643005490697e-05, 'epoch': 0.06}
  6%|▌         | 140/2541 [2:22:39<40:40:59, 61.00s/it]  6%|▌         | 141/2541 [2:23:40<40:40:11, 61.00s/it]  6%|▌         | 142/2541 [2:24:41<40:38:42, 60.99s/it]  6%|▌         | 143/2541 [2:25:42<40:39:00, 61.03s/it]  6%|▌         | 144/2541 [2:26:43<40:39:19, 61.06s/it]  6%|▌         | 145/2541 [2:27:44<40:35:03, 60.98s/it]  6%|▌         | 146/2541 [2:28:46<40:37:45, 61.07s/it]  6%|▌         | 147/2541 [2:29:46<40:34:00, 61.00s/it]  6%|▌         | 148/2541 [2:30:47<40:33:14, 61.01s/it]  6%|▌         | 149/2541 [2:31:48<40:30:55, 60.98s/it]  6%|▌         | 150/2541 [2:32:49<40:30:36, 60.99s/it]                                                       {'loss': 1.7676, 'learning_rate': 4.957131542489021e-05, 'epoch': 0.06}
  6%|▌         | 150/2541 [2:32:49<40:30:36, 60.99s/it]  6%|▌         | 151/2541 [2:33:51<40:33:11, 61.08s/it]  6%|▌         | 152/2541 [2:34:51<40:29:25, 61.02s/it]  6%|▌         | 153/2541 [2:35:53<40:28:44, 61.02s/it]  6%|▌         | 154/2541 [2:36:54<40:27:01, 61.01s/it]  6%|▌         | 155/2541 [2:37:54<40:24:28, 60.97s/it]  6%|▌         | 156/2541 [2:38:55<40:22:39, 60.95s/it]  6%|▌         | 157/2541 [2:39:57<40:26:25, 61.07s/it]  6%|▌         | 158/2541 [2:40:58<40:24:33, 61.05s/it]  6%|▋         | 159/2541 [2:41:59<40:23:44, 61.05s/it]  6%|▋         | 160/2541 [2:43:00<40:21:56, 61.03s/it]                                                       {'loss': 1.7398, 'learning_rate': 4.951244490119161e-05, 'epoch': 0.06}
  6%|▋         | 160/2541 [2:43:00<40:21:56, 61.03s/it]  6%|▋         | 161/2541 [2:44:01<40:20:46, 61.03s/it]  6%|▋         | 162/2541 [2:45:02<40:21:50, 61.08s/it]  6%|▋         | 163/2541 [2:46:03<40:22:35, 61.13s/it]  6%|▋         | 164/2541 [2:47:04<40:19:21, 61.07s/it]  6%|▋         | 165/2541 [2:48:05<40:16:44, 61.03s/it]  7%|▋         | 166/2541 [2:49:06<40:14:51, 61.01s/it]  7%|▋         | 167/2541 [2:50:07<40:18:08, 61.12s/it]  7%|▋         | 168/2541 [2:51:08<40:17:04, 61.11s/it]  7%|▋         | 169/2541 [2:52:09<40:12:28, 61.02s/it]  7%|▋         | 170/2541 [2:53:10<40:10:08, 60.99s/it]                                                       {'loss': 1.7251, 'learning_rate': 4.944982748257351e-05, 'epoch': 0.07}
  7%|▋         | 170/2541 [2:53:10<40:10:08, 60.99s/it]  7%|▋         | 171/2541 [2:54:11<40:08:58, 60.99s/it]  7%|▋         | 172/2541 [2:55:12<40:07:50, 60.98s/it]  7%|▋         | 173/2541 [2:56:13<40:05:36, 60.95s/it]  7%|▋         | 174/2541 [2:57:14<40:10:08, 61.09s/it]  7%|▋         | 175/2541 [2:58:15<40:08:27, 61.08s/it]  7%|▋         | 176/2541 [2:59:16<40:05:02, 61.02s/it]  7%|▋         | 177/2541 [3:00:17<40:04:52, 61.04s/it]  7%|▋         | 178/2541 [3:01:18<40:04:12, 61.05s/it]  7%|▋         | 179/2541 [3:02:19<40:01:42, 61.01s/it]  7%|▋         | 180/2541 [3:03:21<40:05:21, 61.13s/it]                                                       {'loss': 1.717, 'learning_rate': 4.9383472740536785e-05, 'epoch': 0.07}
  7%|▋         | 180/2541 [3:03:21<40:05:21, 61.13s/it]  7%|▋         | 181/2541 [3:04:22<40:03:23, 61.10s/it]  7%|▋         | 182/2541 [3:05:23<40:02:10, 61.10s/it]  7%|▋         | 183/2541 [3:06:24<39:58:29, 61.03s/it]  7%|▋         | 184/2541 [3:07:25<39:57:52, 61.04s/it]  7%|▋         | 185/2541 [3:08:26<40:02:08, 61.18s/it]  7%|▋         | 186/2541 [3:09:28<40:01:24, 61.18s/it]  7%|▋         | 187/2541 [3:10:29<39:58:00, 61.12s/it]  7%|▋         | 188/2541 [3:11:30<39:56:37, 61.11s/it]  7%|▋         | 189/2541 [3:12:31<39:52:58, 61.05s/it]  7%|▋         | 190/2541 [3:13:31<39:49:58, 60.99s/it]                                                       {'loss': 1.6935, 'learning_rate': 4.931339081785775e-05, 'epoch': 0.07}
  7%|▋         | 190/2541 [3:13:31<39:49:58, 60.99s/it]  8%|▊         | 191/2541 [3:14:33<39:52:19, 61.08s/it]  8%|▊         | 192/2541 [3:15:34<39:50:51, 61.07s/it]  8%|▊         | 193/2541 [3:16:35<39:47:25, 61.01s/it]  8%|▊         | 194/2541 [3:17:36<39:48:41, 61.07s/it]  8%|▊         | 195/2541 [3:18:37<39:47:20, 61.06s/it]  8%|▊         | 196/2541 [3:19:38<39:45:45, 61.04s/it]  8%|▊         | 197/2541 [3:20:39<39:46:23, 61.08s/it]  8%|▊         | 198/2541 [3:21:40<39:45:40, 61.09s/it]  8%|▊         | 199/2541 [3:22:41<39:44:24, 61.09s/it]  8%|▊         | 200/2541 [3:23:42<39:42:03, 61.05s/it]                                                       {'loss': 1.6773, 'learning_rate': 4.923959242703781e-05, 'epoch': 0.08}
  8%|▊         | 200/2541 [3:23:42<39:42:03, 61.05s/it]  8%|▊         | 201/2541 [3:24:43<39:40:26, 61.04s/it]  8%|▊         | 202/2541 [3:25:44<39:38:59, 61.03s/it]  8%|▊         | 203/2541 [3:26:46<39:41:26, 61.11s/it]  8%|▊         | 204/2541 [3:27:46<39:37:43, 61.05s/it]  8%|▊         | 205/2541 [3:28:47<39:35:43, 61.02s/it]  8%|▊         | 206/2541 [3:29:48<39:33:36, 60.99s/it]  8%|▊         | 207/2541 [3:30:49<39:31:54, 60.97s/it]  8%|▊         | 208/2541 [3:31:50<39:33:36, 61.04s/it]  8%|▊         | 209/2541 [3:32:52<39:34:15, 61.09s/it]  8%|▊         | 210/2541 [3:33:53<39:33:27, 61.09s/it]                                                       {'loss': 1.6825, 'learning_rate': 4.916208884866593e-05, 'epoch': 0.08}
  8%|▊         | 210/2541 [3:33:53<39:33:27, 61.09s/it]  8%|▊         | 211/2541 [3:34:54<39:31:35, 61.07s/it]  8%|▊         | 212/2541 [3:35:55<39:29:10, 61.03s/it]  8%|▊         | 213/2541 [3:36:56<39:30:25, 61.09s/it]  8%|▊         | 214/2541 [3:37:57<39:30:52, 61.13s/it]  8%|▊         | 215/2541 [3:38:58<39:26:51, 61.05s/it]  9%|▊         | 216/2541 [3:39:59<39:24:50, 61.03s/it]  9%|▊         | 217/2541 [3:41:00<39:24:00, 61.03s/it]  9%|▊         | 218/2541 [3:42:01<39:22:48, 61.03s/it]  9%|▊         | 219/2541 [3:43:02<39:19:52, 60.98s/it]  9%|▊         | 220/2541 [3:44:03<39:22:07, 61.06s/it]                                                       {'loss': 1.6734, 'learning_rate': 4.908089192969434e-05, 'epoch': 0.09}
  9%|▊         | 220/2541 [3:44:03<39:22:07, 61.06s/it]  9%|▊         | 221/2541 [3:45:04<39:21:04, 61.06s/it]  9%|▊         | 222/2541 [3:46:05<39:19:46, 61.06s/it]  9%|▉         | 223/2541 [3:47:06<39:16:41, 61.00s/it]  9%|▉         | 224/2541 [3:48:07<39:14:30, 60.97s/it]  9%|▉         | 225/2541 [3:49:08<39:14:05, 60.99s/it]  9%|▉         | 226/2541 [3:50:09<39:15:34, 61.05s/it]  9%|▉         | 227/2541 [3:51:10<39:11:51, 60.98s/it]  9%|▉         | 228/2541 [3:52:11<39:12:14, 61.02s/it]  9%|▉         | 229/2541 [3:53:12<39:10:50, 61.01s/it]  9%|▉         | 230/2541 [3:54:13<39:09:35, 61.00s/it]                                                       {'loss': 1.6591, 'learning_rate': 4.899601408162767e-05, 'epoch': 0.09}
  9%|▉         | 230/2541 [3:54:13<39:09:35, 61.00s/it]  9%|▉         | 231/2541 [3:55:14<39:07:39, 60.98s/it]  9%|▉         | 232/2541 [3:56:15<39:11:13, 61.10s/it]  9%|▉         | 233/2541 [3:57:16<39:09:23, 61.08s/it]  9%|▉         | 234/2541 [3:58:18<39:08:16, 61.07s/it]  9%|▉         | 235/2541 [3:59:19<39:08:36, 61.11s/it]  9%|▉         | 236/2541 [4:00:20<39:07:14, 61.10s/it]  9%|▉         | 237/2541 [4:01:21<39:09:16, 61.18s/it]  9%|▉         | 238/2541 [4:02:22<39:07:57, 61.17s/it]  9%|▉         | 239/2541 [4:03:23<39:04:19, 61.10s/it]  9%|▉         | 240/2541 [4:04:24<39:03:11, 61.10s/it]                                                       {'loss': 1.6536, 'learning_rate': 4.8907468278625747e-05, 'epoch': 0.09}
  9%|▉         | 240/2541 [4:04:24<39:03:11, 61.10s/it]  9%|▉         | 241/2541 [4:05:25<39:00:17, 61.05s/it] 10%|▉         | 242/2541 [4:06:26<38:57:28, 61.00s/it] 10%|▉         | 243/2541 [4:07:27<38:59:12, 61.08s/it] 10%|▉         | 244/2541 [4:08:29<39:00:38, 61.14s/it] 10%|▉         | 245/2541 [4:09:30<38:58:08, 61.10s/it] 10%|▉         | 246/2541 [4:10:31<38:53:56, 61.02s/it] 10%|▉         | 247/2541 [4:11:32<38:52:35, 61.01s/it] 10%|▉         | 248/2541 [4:12:32<38:50:48, 60.99s/it] 10%|▉         | 249/2541 [4:13:34<38:53:31, 61.09s/it] 10%|▉         | 250/2541 [4:14:35<38:54:20, 61.13s/it]                                                       {'loss': 1.6502, 'learning_rate': 4.881526805552039e-05, 'epoch': 0.1}
 10%|▉         | 250/2541 [4:14:35<38:54:20, 61.13s/it] 10%|▉         | 251/2541 [4:15:36<38:51:17, 61.08s/it] 10%|▉         | 252/2541 [4:16:37<38:50:11, 61.08s/it] 10%|▉         | 253/2541 [4:17:38<38:47:48, 61.04s/it] 10%|▉         | 254/2541 [4:18:39<38:46:10, 61.03s/it] 10%|█         | 255/2541 [4:19:40<38:48:49, 61.12s/it] 10%|█         | 256/2541 [4:20:41<38:46:59, 61.10s/it] 10%|█         | 257/2541 [4:21:42<38:45:13, 61.08s/it] 10%|█         | 258/2541 [4:22:43<38:43:24, 61.06s/it] 10%|█         | 259/2541 [4:23:45<38:43:01, 61.08s/it] 10%|█         | 260/2541 [4:24:46<38:46:02, 61.18s/it]                                                       {'loss': 1.6248, 'learning_rate': 4.871942750574654e-05, 'epoch': 0.1}
 10%|█         | 260/2541 [4:24:46<38:46:02, 61.18s/it] 10%|█         | 261/2541 [4:25:47<38:43:27, 61.14s/it] 10%|█         | 262/2541 [4:26:48<38:40:05, 61.08s/it] 10%|█         | 263/2541 [4:27:49<38:38:18, 61.06s/it] 10%|█         | 264/2541 [4:28:50<38:36:59, 61.05s/it] 10%|█         | 265/2541 [4:29:51<38:34:24, 61.01s/it] 10%|█         | 266/2541 [4:30:52<38:36:00, 61.08s/it] 11%|█         | 267/2541 [4:31:53<38:36:31, 61.12s/it] 11%|█         | 268/2541 [4:32:54<38:32:42, 61.05s/it] 11%|█         | 269/2541 [4:33:55<38:31:18, 61.04s/it] 11%|█         | 270/2541 [4:34:56<38:30:53, 61.05s/it]                                                       {'loss': 1.6258, 'learning_rate': 4.861996127918798e-05, 'epoch': 0.11}
 11%|█         | 270/2541 [4:34:56<38:30:53, 61.05s/it] 11%|█         | 271/2541 [4:35:57<38:28:32, 61.02s/it] 11%|█         | 272/2541 [4:36:59<38:30:11, 61.09s/it] 11%|█         | 273/2541 [4:38:00<38:30:58, 61.14s/it] 11%|█         | 274/2541 [4:39:01<38:27:20, 61.07s/it] 11%|█         | 275/2541 [4:40:02<38:25:59, 61.06s/it] 11%|█         | 276/2541 [4:41:03<38:25:34, 61.07s/it] 11%|█         | 277/2541 [4:42:04<38:23:43, 61.05s/it] 11%|█         | 278/2541 [4:43:05<38:25:37, 61.13s/it] 11%|█         | 279/2541 [4:44:06<38:24:26, 61.13s/it] 11%|█         | 280/2541 [4:45:08<38:29:22, 61.28s/it]                                                       {'loss': 1.6152, 'learning_rate': 4.8516884579938007e-05, 'epoch': 0.11}
 11%|█         | 280/2541 [4:45:08<38:29:22, 61.28s/it] 11%|█         | 281/2541 [4:46:09<38:25:15, 61.20s/it] 11%|█         | 282/2541 [4:47:10<38:21:48, 61.14s/it] 11%|█         | 283/2541 [4:48:11<38:22:39, 61.19s/it] 11%|█         | 284/2541 [4:49:12<38:19:50, 61.14s/it] 11%|█         | 285/2541 [4:50:13<38:15:19, 61.05s/it] 11%|█▏        | 286/2541 [4:51:14<38:12:49, 61.01s/it] 11%|█▏        | 287/2541 [4:52:15<38:10:48, 60.98s/it] 11%|█▏        | 288/2541 [4:53:16<38:10:02, 60.99s/it] 11%|█▏        | 289/2541 [4:54:17<38:13:00, 61.09s/it] 11%|█▏        | 290/2541 [4:55:19<38:13:06, 61.12s/it]                                                       {'loss': 1.6134, 'learning_rate': 4.8410213163975335e-05, 'epoch': 0.11}
 11%|█▏        | 290/2541 [4:55:19<38:13:06, 61.12s/it] 11%|█▏        | 291/2541 [4:56:19<38:09:20, 61.05s/it] 11%|█▏        | 292/2541 [4:57:20<38:06:59, 61.01s/it] 12%|█▏        | 293/2541 [4:58:21<38:07:16, 61.05s/it] 12%|█▏        | 294/2541 [4:59:22<38:04:29, 61.00s/it] 12%|█▏        | 295/2541 [5:00:24<38:06:28, 61.08s/it] 12%|█▏        | 296/2541 [5:01:25<38:07:27, 61.13s/it] 12%|█▏        | 297/2541 [5:02:26<38:04:55, 61.09s/it] 12%|█▏        | 298/2541 [5:03:27<38:03:49, 61.09s/it] 12%|█▏        | 299/2541 [5:04:28<38:03:30, 61.11s/it] 12%|█▏        | 300/2541 [5:05:29<38:02:05, 61.10s/it]                                                       {'loss': 1.606, 'learning_rate': 4.8299963336755784e-05, 'epoch': 0.12}
 12%|█▏        | 300/2541 [5:05:29<38:02:05, 61.10s/it] 12%|█▏        | 301/2541 [5:06:30<38:02:47, 61.15s/it] 12%|█▏        | 302/2541 [5:07:31<38:00:07, 61.10s/it] 12%|█▏        | 303/2541 [5:08:32<37:57:48, 61.07s/it] 12%|█▏        | 304/2541 [5:09:33<37:55:53, 61.04s/it] 12%|█▏        | 305/2541 [5:10:34<37:54:34, 61.04s/it] 12%|█▏        | 306/2541 [5:11:36<37:55:15, 61.08s/it] 12%|█▏        | 307/2541 [5:12:37<37:54:48, 61.10s/it] 12%|█▏        | 308/2541 [5:13:38<37:53:17, 61.08s/it] 12%|█▏        | 309/2541 [5:14:39<37:51:39, 61.07s/it] 12%|█▏        | 310/2541 [5:15:40<37:50:08, 61.05s/it]                                                       {'loss': 1.5991, 'learning_rate': 4.8186151950719764e-05, 'epoch': 0.12}
 12%|█▏        | 310/2541 [5:15:40<37:50:08, 61.05s/it] 12%|█▏        | 311/2541 [5:16:41<37:51:32, 61.12s/it] 12%|█▏        | 312/2541 [5:17:42<37:48:08, 61.05s/it] 12%|█▏        | 313/2541 [5:18:43<37:50:18, 61.14s/it] 12%|█▏        | 314/2541 [5:19:44<37:48:08, 61.11s/it] 12%|█▏        | 315/2541 [5:20:46<37:47:35, 61.12s/it] 12%|█▏        | 316/2541 [5:21:47<37:45:05, 61.08s/it] 12%|█▏        | 317/2541 [5:22:47<37:41:22, 61.01s/it] 13%|█▎        | 318/2541 [5:23:49<37:44:17, 61.11s/it] 13%|█▎        | 319/2541 [5:24:50<37:46:17, 61.20s/it] 13%|█▎        | 320/2541 [5:25:51<37:46:24, 61.23s/it]                                                       {'loss': 1.5922, 'learning_rate': 4.8068796402716345e-05, 'epoch': 0.13}
 13%|█▎        | 320/2541 [5:25:51<37:46:24, 61.23s/it] 13%|█▎        | 321/2541 [5:26:52<37:43:08, 61.17s/it] 13%|█▎        | 322/2541 [5:27:54<37:42:09, 61.17s/it] 13%|█▎        | 323/2541 [5:28:55<37:38:41, 61.10s/it] 13%|█▎        | 324/2541 [5:29:56<37:40:14, 61.17s/it] 13%|█▎        | 325/2541 [5:30:57<37:37:38, 61.13s/it] 13%|█▎        | 326/2541 [5:31:58<37:35:50, 61.11s/it] 13%|█▎        | 327/2541 [5:32:59<37:33:29, 61.07s/it] 13%|█▎        | 328/2541 [5:34:00<37:31:40, 61.05s/it] 13%|█▎        | 329/2541 [5:35:01<37:32:42, 61.10s/it] 13%|█▎        | 330/2541 [5:36:02<37:32:24, 61.12s/it]                                                       {'loss': 1.5946, 'learning_rate': 4.794791463134399e-05, 'epoch': 0.13}
 13%|█▎        | 330/2541 [5:36:02<37:32:24, 61.12s/it] 13%|█▎        | 331/2541 [5:37:03<37:29:57, 61.08s/it] 13%|█▎        | 332/2541 [5:38:04<37:28:04, 61.06s/it] 13%|█▎        | 333/2541 [5:39:05<37:25:21, 61.02s/it] 13%|█▎        | 334/2541 [5:40:06<37:25:34, 61.05s/it] 13%|█▎        | 335/2541 [5:41:07<37:23:55, 61.03s/it] 13%|█▎        | 336/2541 [5:42:09<37:25:56, 61.11s/it] 13%|█▎        | 337/2541 [5:43:10<37:24:03, 61.09s/it] 13%|█▎        | 338/2541 [5:44:11<37:22:17, 61.07s/it] 13%|█▎        | 339/2541 [5:45:12<37:21:06, 61.07s/it] 13%|█▎        | 340/2541 [5:46:13<37:18:38, 61.03s/it]                                                       {'loss': 1.5791, 'learning_rate': 4.782352511420853e-05, 'epoch': 0.13}
 13%|█▎        | 340/2541 [5:46:13<37:18:38, 61.03s/it] 13%|█▎        | 341/2541 [5:47:14<37:17:53, 61.03s/it] 13%|█▎        | 342/2541 [5:48:15<37:19:16, 61.10s/it] 13%|█▎        | 343/2541 [5:49:16<37:18:51, 61.12s/it] 14%|█▎        | 344/2541 [5:50:17<37:16:45, 61.09s/it] 14%|█▎        | 345/2541 [5:51:18<37:16:55, 61.12s/it] 14%|█▎        | 346/2541 [5:52:19<37:14:25, 61.08s/it] 14%|█▎        | 347/2541 [5:53:21<37:14:15, 61.10s/it] 14%|█▎        | 348/2541 [5:54:22<37:15:17, 61.16s/it] 14%|█▎        | 349/2541 [5:55:23<37:12:13, 61.10s/it] 14%|█▍        | 350/2541 [5:56:24<37:09:09, 61.04s/it]                                                       {'loss': 1.5774, 'learning_rate': 4.769564686509877e-05, 'epoch': 0.14}
 14%|█▍        | 350/2541 [5:56:24<37:09:09, 61.04s/it] 14%|█▍        | 351/2541 [5:57:25<37:07:01, 61.01s/it] 14%|█▍        | 352/2541 [5:58:26<37:08:28, 61.08s/it] 14%|█▍        | 353/2541 [5:59:27<37:10:30, 61.17s/it] 14%|█▍        | 354/2541 [6:00:28<37:07:53, 61.12s/it] 14%|█▍        | 355/2541 [6:01:29<37:05:22, 61.08s/it] 14%|█▍        | 356/2541 [6:02:30<37:02:50, 61.04s/it] 14%|█▍        | 357/2541 [6:03:31<37:01:43, 61.04s/it] 14%|█▍        | 358/2541 [6:04:32<36:59:08, 60.99s/it] 14%|█▍        | 359/2541 [6:05:33<37:01:55, 61.10s/it] 14%|█▍        | 360/2541 [6:06:34<36:59:44, 61.07s/it]                                                       {'loss': 1.5641, 'learning_rate': 4.7564299431080016e-05, 'epoch': 0.14}
 14%|█▍        | 360/2541 [6:06:34<36:59:44, 61.07s/it] 14%|█▍        | 361/2541 [6:07:36<36:58:17, 61.05s/it] 14%|█▍        | 362/2541 [6:08:36<36:56:08, 61.02s/it] 14%|█▍        | 363/2541 [6:09:37<36:54:04, 60.99s/it] 14%|█▍        | 364/2541 [6:10:39<36:55:38, 61.06s/it] 14%|█▍        | 365/2541 [6:11:40<36:57:41, 61.15s/it] 14%|█▍        | 366/2541 [6:12:41<36:54:57, 61.10s/it] 14%|█▍        | 367/2541 [6:13:42<36:51:26, 61.03s/it] 14%|█▍        | 368/2541 [6:14:43<36:51:50, 61.07s/it] 15%|█▍        | 369/2541 [6:15:44<36:50:33, 61.07s/it] 15%|█▍        | 370/2541 [6:16:45<36:51:22, 61.12s/it]                                                       {'loss': 1.5607, 'learning_rate': 4.742950288950626e-05, 'epoch': 0.15}
 15%|█▍        | 370/2541 [6:16:45<36:51:22, 61.12s/it] 15%|█▍        | 371/2541 [6:17:46<36:51:13, 61.14s/it] 15%|█▍        | 372/2541 [6:18:47<36:47:46, 61.07s/it] 15%|█▍        | 373/2541 [6:19:48<36:46:08, 61.06s/it] 15%|█▍        | 374/2541 [6:20:49<36:43:37, 61.01s/it] 15%|█▍        | 375/2541 [6:21:50<36:42:44, 61.02s/it] 15%|█▍        | 376/2541 [6:22:52<36:44:45, 61.10s/it] 15%|█▍        | 377/2541 [6:23:53<36:43:07, 61.09s/it] 15%|█▍        | 378/2541 [6:24:54<36:41:40, 61.07s/it] 15%|█▍        | 379/2541 [6:25:55<36:41:49, 61.11s/it] 15%|█▍        | 380/2541 [6:26:56<36:42:05, 61.14s/it]                                                       {'loss': 1.5702, 'learning_rate': 4.7291277844951173e-05, 'epoch': 0.15}
 15%|█▍        | 380/2541 [6:26:56<36:42:05, 61.14s/it] 15%|█▍        | 381/2541 [6:27:57<36:41:12, 61.14s/it] 15%|█▌        | 382/2541 [6:28:59<36:42:50, 61.22s/it] 15%|█▌        | 383/2541 [6:30:00<36:39:19, 61.15s/it] 15%|█▌        | 384/2541 [6:31:01<36:36:53, 61.11s/it] 15%|█▌        | 385/2541 [6:32:02<36:34:50, 61.08s/it] 15%|█▌        | 386/2541 [6:33:03<36:32:56, 61.06s/it] 15%|█▌        | 387/2541 [6:34:04<36:30:34, 61.02s/it] 15%|█▌        | 388/2541 [6:35:05<36:33:34, 61.13s/it] 15%|█▌        | 389/2541 [6:36:06<36:31:01, 61.09s/it] 15%|█▌        | 390/2541 [6:37:07<36:28:32, 61.05s/it]                                                       {'loss': 1.5563, 'learning_rate': 4.714964542605855e-05, 'epoch': 0.15}
 15%|█▌        | 390/2541 [6:37:07<36:28:32, 61.05s/it] 15%|█▌        | 391/2541 [6:38:08<36:27:22, 61.04s/it] 15%|█▌        | 392/2541 [6:39:09<36:25:47, 61.03s/it] 15%|█▌        | 393/2541 [6:40:10<36:25:46, 61.06s/it] 16%|█▌        | 394/2541 [6:41:11<36:26:38, 61.11s/it] 16%|█▌        | 395/2541 [6:42:12<36:24:49, 61.09s/it] 16%|█▌        | 396/2541 [6:43:13<36:24:11, 61.10s/it] 16%|█▌        | 397/2541 [6:44:14<36:21:44, 61.06s/it] 16%|█▌        | 398/2541 [6:45:16<36:25:23, 61.19s/it] 16%|█▌        | 399/2541 [6:46:17<36:26:55, 61.26s/it] 16%|█▌        | 400/2541 [6:47:19<36:25:25, 61.24s/it]                                                       {'loss': 1.5475, 'learning_rate': 4.7004627282312694e-05, 'epoch': 0.16}
 16%|█▌        | 400/2541 [6:47:19<36:25:25, 61.24s/it] 16%|█▌        | 401/2541 [6:48:20<36:23:37, 61.22s/it] 16%|█▌        | 402/2541 [6:49:21<36:20:16, 61.16s/it] 16%|█▌        | 403/2541 [6:50:22<36:19:40, 61.17s/it] 16%|█▌        | 404/2541 [6:51:23<36:16:15, 61.10s/it] 16%|█▌        | 405/2541 [6:52:24<36:17:46, 61.17s/it] 16%|█▌        | 406/2541 [6:53:25<36:15:45, 61.15s/it] 16%|█▌        | 407/2541 [6:54:26<36:13:53, 61.12s/it] 16%|█▌        | 408/2541 [6:55:27<36:11:22, 61.08s/it] 16%|█▌        | 409/2541 [6:56:28<36:09:58, 61.07s/it] 16%|█▌        | 410/2541 [6:57:29<36:07:22, 61.02s/it]                                                       {'loss': 1.5411, 'learning_rate': 4.68562455807291e-05, 'epoch': 0.16}
 16%|█▌        | 410/2541 [6:57:29<36:07:22, 61.02s/it] 16%|█▌        | 411/2541 [6:58:31<36:12:12, 61.19s/it] 16%|█▌        | 412/2541 [6:59:32<36:08:39, 61.12s/it] 16%|█▋        | 413/2541 [7:00:33<36:06:30, 61.09s/it] 16%|█▋        | 414/2541 [7:01:34<36:05:14, 61.08s/it] 16%|█▋        | 415/2541 [7:02:35<36:02:45, 61.04s/it] 16%|█▋        | 416/2541 [7:03:36<36:03:28, 61.09s/it] 16%|█▋        | 417/2541 [7:04:37<36:04:45, 61.15s/it] 16%|█▋        | 418/2541 [7:05:38<36:02:49, 61.13s/it] 16%|█▋        | 419/2541 [7:06:39<36:00:54, 61.10s/it] 17%|█▋        | 420/2541 [7:07:40<35:58:06, 61.05s/it]                                                       {'loss': 1.5283, 'learning_rate': 4.67045230024661e-05, 'epoch': 0.17}
 17%|█▋        | 420/2541 [7:07:40<35:58:06, 61.05s/it] 17%|█▋        | 421/2541 [7:08:41<35:56:53, 61.04s/it] 17%|█▋        | 422/2541 [7:09:43<35:58:12, 61.11s/it] 17%|█▋        | 423/2541 [7:10:44<35:56:11, 61.08s/it] 17%|█▋        | 424/2541 [7:11:45<35:53:49, 61.04s/it] 17%|█▋        | 425/2541 [7:12:46<35:52:58, 61.05s/it] 17%|█▋        | 426/2541 [7:13:47<35:50:49, 61.02s/it] 17%|█▋        | 427/2541 [7:14:48<35:50:38, 61.04s/it] 17%|█▋        | 428/2541 [7:15:49<35:50:17, 61.06s/it] 17%|█▋        | 429/2541 [7:16:50<35:50:07, 61.08s/it] 17%|█▋        | 430/2541 [7:17:51<35:48:38, 61.07s/it]                                                       {'loss': 1.5403, 'learning_rate': 4.654948273935788e-05, 'epoch': 0.17}
 17%|█▋        | 430/2541 [7:17:51<35:48:38, 61.07s/it] 17%|█▋        | 431/2541 [7:18:52<35:46:34, 61.04s/it] 17%|█▋        | 432/2541 [7:19:53<35:45:08, 61.03s/it] 17%|█▋        | 433/2541 [7:20:54<35:43:27, 61.01s/it] 17%|█▋        | 434/2541 [7:21:55<35:43:51, 61.05s/it] 17%|█▋        | 435/2541 [7:22:56<35:45:19, 61.12s/it] 17%|█▋        | 436/2541 [7:23:57<35:43:27, 61.10s/it] 17%|█▋        | 437/2541 [7:24:58<35:40:08, 61.03s/it] 17%|█▋        | 438/2541 [7:25:59<35:38:00, 61.00s/it] 17%|█▋        | 439/2541 [7:27:00<35:36:04, 60.97s/it] 17%|█▋        | 440/2541 [7:28:01<35:37:47, 61.05s/it]                                                       {'loss': 1.533, 'learning_rate': 4.639114849036944e-05, 'epoch': 0.17}
 17%|█▋        | 440/2541 [7:28:01<35:37:47, 61.05s/it] 17%|█▋        | 441/2541 [7:29:02<35:35:38, 61.02s/it] 17%|█▋        | 442/2541 [7:30:03<35:34:37, 61.02s/it] 17%|█▋        | 443/2541 [7:31:04<35:33:49, 61.02s/it] 17%|█▋        | 444/2541 [7:32:05<35:31:40, 60.99s/it] 18%|█▊        | 445/2541 [7:33:07<35:34:07, 61.09s/it] 18%|█▊        | 446/2541 [7:34:08<35:31:27, 61.04s/it] 18%|█▊        | 447/2541 [7:35:09<35:29:46, 61.03s/it] 18%|█▊        | 448/2541 [7:36:10<35:29:12, 61.04s/it] 18%|█▊        | 449/2541 [7:37:11<35:28:33, 61.05s/it] 18%|█▊        | 450/2541 [7:38:12<35:26:32, 61.02s/it]                                                       {'loss': 1.5353, 'learning_rate': 4.622954445797409e-05, 'epoch': 0.18}
 18%|█▊        | 450/2541 [7:38:12<35:26:32, 61.02s/it] 18%|█▊        | 451/2541 [7:39:13<35:28:16, 61.10s/it] 18%|█▊        | 452/2541 [7:40:14<35:27:47, 61.11s/it] 18%|█▊        | 453/2541 [7:41:15<35:27:23, 61.13s/it] 18%|█▊        | 454/2541 [7:42:16<35:25:11, 61.10s/it] 18%|█▊        | 455/2541 [7:43:17<35:24:29, 61.11s/it] 18%|█▊        | 456/2541 [7:44:18<35:23:00, 61.09s/it] 18%|█▊        | 457/2541 [7:45:20<35:23:59, 61.15s/it] 18%|█▊        | 458/2541 [7:46:21<35:24:00, 61.18s/it] 18%|█▊        | 459/2541 [7:47:22<35:21:22, 61.13s/it] 18%|█▊        | 460/2541 [7:48:23<35:20:19, 61.13s/it]                                                       {'loss': 1.514, 'learning_rate': 4.606469534445389e-05, 'epoch': 0.18}
 18%|█▊        | 460/2541 [7:48:23<35:20:19, 61.13s/it] 18%|█▊        | 461/2541 [7:49:24<35:18:33, 61.11s/it] 18%|█▊        | 462/2541 [7:50:25<35:16:59, 61.10s/it] 18%|█▊        | 463/2541 [7:51:27<35:19:43, 61.20s/it] 18%|█▊        | 464/2541 [7:52:28<35:17:30, 61.17s/it] 18%|█▊        | 465/2541 [7:53:29<35:15:25, 61.14s/it] 18%|█▊        | 466/2541 [7:54:30<35:13:10, 61.10s/it] 18%|█▊        | 467/2541 [7:55:31<35:13:13, 61.13s/it] 18%|█▊        | 468/2541 [7:56:33<35:16:53, 61.27s/it] 18%|█▊        | 469/2541 [7:57:34<35:14:30, 61.23s/it] 18%|█▊        | 470/2541 [7:58:35<35:13:29, 61.23s/it]                                                       {'loss': 1.5395, 'learning_rate': 4.589662634812375e-05, 'epoch': 0.18}
 18%|█▊        | 470/2541 [7:58:35<35:13:29, 61.23s/it] 19%|█▊        | 471/2541 [7:59:36<35:11:36, 61.21s/it] 19%|█▊        | 472/2541 [8:00:37<35:09:58, 61.19s/it] 19%|█▊        | 473/2541 [8:01:39<35:10:34, 61.24s/it] 19%|█▊        | 474/2541 [8:02:40<35:11:06, 61.28s/it] 19%|█▊        | 475/2541 [8:03:41<35:09:25, 61.26s/it] 19%|█▊        | 476/2541 [8:04:42<35:05:23, 61.17s/it] 19%|█▉        | 477/2541 [8:05:43<35:01:35, 61.09s/it] 19%|█▉        | 478/2541 [8:06:44<34:58:02, 61.02s/it] 19%|█▉        | 479/2541 [8:07:45<34:56:39, 61.01s/it] 19%|█▉        | 480/2541 [8:08:47<35:00:55, 61.16s/it]                                                       {'loss': 1.524, 'learning_rate': 4.572536315947971e-05, 'epoch': 0.19}
 19%|█▉        | 480/2541 [8:08:47<35:00:55, 61.16s/it] 19%|█▉        | 481/2541 [8:09:48<34:59:58, 61.16s/it] 19%|█▉        | 482/2541 [8:10:49<34:57:04, 61.11s/it] 19%|█▉        | 483/2541 [8:11:50<34:54:03, 61.05s/it] 19%|█▉        | 484/2541 [8:12:51<34:52:58, 61.05s/it] 19%|█▉        | 485/2541 [8:13:52<34:52:06, 61.05s/it] 19%|█▉        | 486/2541 [8:14:53<34:53:40, 61.13s/it] 19%|█▉        | 487/2541 [8:15:54<34:52:23, 61.12s/it] 19%|█▉        | 488/2541 [8:16:55<34:49:31, 61.07s/it] 19%|█▉        | 489/2541 [8:17:56<34:49:06, 61.09s/it] 19%|█▉        | 490/2541 [8:18:57<34:47:06, 61.06s/it]                                                       {'loss': 1.5147, 'learning_rate': 4.5550931957271943e-05, 'epoch': 0.19}
 19%|█▉        | 490/2541 [8:18:57<34:47:06, 61.06s/it] 19%|█▉        | 491/2541 [8:19:58<34:47:53, 61.11s/it] 19%|█▉        | 492/2541 [8:21:00<34:48:59, 61.17s/it] 19%|█▉        | 493/2541 [8:22:01<34:47:23, 61.15s/it] 19%|█▉        | 494/2541 [8:23:02<34:45:04, 61.12s/it] 19%|█▉        | 495/2541 [8:24:03<34:42:20, 61.07s/it] 20%|█▉        | 496/2541 [8:25:04<34:40:08, 61.03s/it] 20%|█▉        | 497/2541 [8:26:05<34:37:58, 61.00s/it] 20%|█▉        | 498/2541 [8:27:06<34:39:49, 61.08s/it] 20%|█▉        | 499/2541 [8:28:07<34:38:29, 61.07s/it] 20%|█▉        | 500/2541 [8:29:08<34:37:23, 61.07s/it]                                                       {'loss': 1.5063, 'learning_rate': 4.537335940450318e-05, 'epoch': 0.2}
 20%|█▉        | 500/2541 [8:29:08<34:37:23, 61.07s/it] 20%|█▉        | 501/2541 [8:30:09<34:35:28, 61.04s/it] 20%|█▉        | 502/2541 [8:31:10<34:34:45, 61.05s/it] 20%|█▉        | 503/2541 [8:32:11<34:33:14, 61.04s/it] 20%|█▉        | 504/2541 [8:33:13<34:35:27, 61.13s/it] 20%|█▉        | 505/2541 [8:34:14<34:34:14, 61.13s/it] 20%|█▉        | 506/2541 [8:35:15<34:32:09, 61.10s/it] 20%|█▉        | 507/2541 [8:36:16<34:30:45, 61.08s/it] 20%|█▉        | 508/2541 [8:37:17<34:29:22, 61.07s/it] 20%|██        | 509/2541 [8:38:18<34:31:18, 61.16s/it] 20%|██        | 510/2541 [8:39:19<34:28:55, 61.12s/it]                                                       {'loss': 1.5098, 'learning_rate': 4.519267264435309e-05, 'epoch': 0.2}
 20%|██        | 510/2541 [8:39:19<34:28:55, 61.12s/it] 20%|██        | 511/2541 [8:40:20<34:25:24, 61.05s/it] 20%|██        | 512/2541 [8:41:21<34:25:27, 61.08s/it] 20%|██        | 513/2541 [8:42:22<34:23:51, 61.06s/it] 20%|██        | 514/2541 [8:43:23<34:23:58, 61.09s/it] 20%|██        | 515/2541 [8:44:25<34:23:50, 61.12s/it] 20%|██        | 516/2541 [8:45:26<34:22:17, 61.11s/it] 20%|██        | 517/2541 [8:46:27<34:20:07, 61.07s/it] 20%|██        | 518/2541 [8:47:28<34:18:12, 61.04s/it] 20%|██        | 519/2541 [8:48:29<34:18:10, 61.07s/it] 20%|██        | 520/2541 [8:49:30<34:15:55, 61.04s/it]                                                       {'loss': 1.5062, 'learning_rate': 4.5008899296029225e-05, 'epoch': 0.2}
 20%|██        | 520/2541 [8:49:30<34:15:55, 61.04s/it] 21%|██        | 521/2541 [8:50:31<34:17:19, 61.11s/it] 21%|██        | 522/2541 [8:51:32<34:14:21, 61.05s/it] 21%|██        | 523/2541 [8:52:33<34:12:04, 61.01s/it] 21%|██        | 524/2541 [8:53:34<34:11:32, 61.03s/it] 21%|██        | 525/2541 [8:54:35<34:09:35, 61.00s/it] 21%|██        | 526/2541 [8:55:36<34:07:37, 60.97s/it] 21%|██        | 527/2541 [8:56:37<34:10:01, 61.07s/it] 21%|██        | 528/2541 [8:57:38<34:08:22, 61.05s/it] 21%|██        | 529/2541 [8:58:39<34:07:32, 61.06s/it] 21%|██        | 530/2541 [8:59:40<34:06:23, 61.06s/it]                                                       {'loss': 1.5034, 'learning_rate': 4.482206745054528e-05, 'epoch': 0.21}
 21%|██        | 530/2541 [8:59:40<34:06:23, 61.06s/it] 21%|██        | 531/2541 [9:00:41<34:05:27, 61.06s/it] 21%|██        | 532/2541 [9:01:42<34:06:17, 61.11s/it] 21%|██        | 533/2541 [9:02:44<34:05:49, 61.13s/it] 21%|██        | 534/2541 [9:03:45<34:04:20, 61.12s/it] 21%|██        | 535/2541 [9:04:46<34:02:22, 61.09s/it] 21%|██        | 536/2541 [9:05:47<34:02:54, 61.13s/it] 21%|██        | 537/2541 [9:06:48<34:02:08, 61.14s/it] 21%|██        | 538/2541 [9:07:49<34:01:57, 61.17s/it] 21%|██        | 539/2541 [9:08:50<34:00:32, 61.16s/it] 21%|██▏       | 540/2541 [9:09:51<33:57:26, 61.09s/it]                                                       {'loss': 1.4992, 'learning_rate': 4.463220566642715e-05, 'epoch': 0.21}
 21%|██▏       | 540/2541 [9:09:51<33:57:26, 61.09s/it] 21%|██▏       | 541/2541 [9:10:52<33:54:02, 61.02s/it] 21%|██▏       | 542/2541 [9:11:53<33:52:58, 61.02s/it] 21%|██▏       | 543/2541 [9:12:54<33:52:58, 61.05s/it] 21%|██▏       | 544/2541 [9:13:56<33:55:16, 61.15s/it] 21%|██▏       | 545/2541 [9:14:57<33:53:09, 61.12s/it] 21%|██▏       | 546/2541 [9:15:58<33:52:32, 61.13s/it] 22%|██▏       | 547/2541 [9:16:59<33:49:27, 61.07s/it] 22%|██▏       | 548/2541 [9:18:00<33:48:19, 61.06s/it] 22%|██▏       | 549/2541 [9:19:01<33:46:21, 61.04s/it] 22%|██▏       | 550/2541 [9:20:02<33:49:46, 61.17s/it]                                                       {'loss': 1.4952, 'learning_rate': 4.4439342965347595e-05, 'epoch': 0.22}
 22%|██▏       | 550/2541 [9:20:02<33:49:46, 61.17s/it] 22%|██▏       | 551/2541 [9:21:03<33:47:39, 61.14s/it] 22%|██▏       | 552/2541 [9:22:05<33:46:05, 61.12s/it] 22%|██▏       | 553/2541 [9:23:06<33:44:33, 61.10s/it] 22%|██▏       | 554/2541 [9:24:07<33:44:05, 61.12s/it] 22%|██▏       | 555/2541 [9:25:08<33:43:50, 61.14s/it] 22%|██▏       | 556/2541 [9:26:09<33:44:11, 61.18s/it] 22%|██▏       | 557/2541 [9:27:10<33:41:13, 61.13s/it] 22%|██▏       | 558/2541 [9:28:11<33:38:44, 61.08s/it] 22%|██▏       | 559/2541 [9:29:12<33:38:14, 61.10s/it] 22%|██▏       | 560/2541 [9:30:13<33:36:01, 61.06s/it]                                                       {'loss': 1.4927, 'learning_rate': 4.424350882769005e-05, 'epoch': 0.22}
 22%|██▏       | 560/2541 [9:30:13<33:36:01, 61.06s/it] 22%|██▏       | 561/2541 [9:31:15<33:37:15, 61.13s/it] 22%|██▏       | 562/2541 [9:32:16<33:34:40, 61.08s/it] 22%|██▏       | 563/2541 [9:33:17<33:32:02, 61.03s/it] 22%|██▏       | 564/2541 [9:34:18<33:31:15, 61.04s/it] 22%|██▏       | 565/2541 [9:35:19<33:30:42, 61.05s/it] 22%|██▏       | 566/2541 [9:36:20<33:29:28, 61.05s/it] 22%|██▏       | 567/2541 [9:37:21<33:31:57, 61.15s/it] 22%|██▏       | 568/2541 [9:38:22<33:29:37, 61.11s/it] 22%|██▏       | 569/2541 [9:39:23<33:27:30, 61.08s/it] 22%|██▏       | 570/2541 [9:40:24<33:25:44, 61.06s/it]                                                       {'loss': 1.4952, 'learning_rate': 4.4044733188042384e-05, 'epoch': 0.22}
 22%|██▏       | 570/2541 [9:40:24<33:25:44, 61.06s/it] 22%|██▏       | 571/2541 [9:41:25<33:22:38, 60.99s/it] 23%|██▎       | 572/2541 [9:42:26<33:21:40, 61.00s/it] 23%|██▎       | 573/2541 [9:43:27<33:23:14, 61.07s/it] 23%|██▎       | 574/2541 [9:44:28<33:22:12, 61.07s/it] 23%|██▎       | 575/2541 [9:45:29<33:20:48, 61.06s/it] 23%|██▎       | 576/2541 [9:46:30<33:19:41, 61.06s/it] 23%|██▎       | 577/2541 [9:47:31<33:18:26, 61.05s/it] 23%|██▎       | 578/2541 [9:48:33<33:17:54, 61.07s/it] 23%|██▎       | 579/2541 [9:49:34<33:17:47, 61.09s/it] 23%|██▎       | 580/2541 [9:50:35<33:16:08, 61.08s/it]                                                       {'loss': 1.4778, 'learning_rate': 4.384304643062114e-05, 'epoch': 0.23}
 23%|██▎       | 580/2541 [9:50:35<33:16:08, 61.08s/it] 23%|██▎       | 581/2541 [9:51:36<33:15:41, 61.09s/it] 23%|██▎       | 582/2541 [9:52:37<33:13:24, 61.05s/it] 23%|██▎       | 583/2541 [9:53:38<33:14:09, 61.11s/it] 23%|██▎       | 584/2541 [9:54:39<33:14:59, 61.16s/it] 23%|██▎       | 585/2541 [9:55:40<33:12:29, 61.12s/it] 23%|██▎       | 586/2541 [9:56:41<33:09:22, 61.05s/it] 23%|██▎       | 587/2541 [9:57:42<33:06:53, 61.01s/it] 23%|██▎       | 588/2541 [9:58:43<33:04:51, 60.98s/it] 23%|██▎       | 589/2541 [9:59:44<33:04:27, 61.00s/it] 23%|██▎       | 590/2541 [10:00:45<33:06:37, 61.10s/it]                                                        {'loss': 1.4981, 'learning_rate': 4.3638479384627167e-05, 'epoch': 0.23}
 23%|██▎       | 590/2541 [10:00:45<33:06:37, 61.10s/it] 23%|██▎       | 591/2541 [10:01:46<33:03:40, 61.04s/it] 23%|██▎       | 592/2541 [10:02:47<33:01:28, 61.00s/it] 23%|██▎       | 593/2541 [10:03:48<33:00:42, 61.01s/it] 23%|██▎       | 594/2541 [10:04:49<32:58:06, 60.96s/it] 23%|██▎       | 595/2541 [10:05:50<32:57:38, 60.98s/it] 23%|██▎       | 596/2541 [10:06:51<33:00:15, 61.09s/it] 23%|██▎       | 597/2541 [10:07:53<32:59:51, 61.11s/it] 24%|██▎       | 598/2541 [10:08:54<32:57:36, 61.07s/it] 24%|██▎       | 599/2541 [10:09:55<32:55:38, 61.04s/it] 24%|██▎       | 600/2541 [10:10:56<32:55:27, 61.07s/it]                                                        {'loss': 1.4854, 'learning_rate': 4.34310633195331e-05, 'epoch': 0.24}
 24%|██▎       | 600/2541 [10:10:56<32:55:27, 61.07s/it] 24%|██▎       | 601/2541 [10:11:57<32:53:07, 61.02s/it] 24%|██▎       | 602/2541 [10:12:58<32:55:30, 61.13s/it] 24%|██▎       | 603/2541 [10:13:59<32:53:16, 61.09s/it] 24%|██▍       | 604/2541 [10:15:00<32:52:07, 61.09s/it] 24%|██▍       | 605/2541 [10:16:01<32:50:57, 61.08s/it] 24%|██▍       | 606/2541 [10:17:02<32:49:09, 61.06s/it] 24%|██▍       | 607/2541 [10:18:04<32:53:34, 61.23s/it] 24%|██▍       | 608/2541 [10:19:05<32:50:29, 61.16s/it] 24%|██▍       | 609/2541 [10:20:06<32:48:02, 61.12s/it] 24%|██▍       | 610/2541 [10:21:07<32:45:30, 61.07s/it]                                                        {'loss': 1.4803, 'learning_rate': 4.322082994030365e-05, 'epoch': 0.24}
 24%|██▍       | 610/2541 [10:21:07<32:45:30, 61.07s/it] 24%|██▍       | 611/2541 [10:22:08<32:44:34, 61.07s/it] 24%|██▍       | 612/2541 [10:23:09<32:42:04, 61.03s/it] 24%|██▍       | 613/2541 [10:24:10<32:43:07, 61.09s/it] 24%|██▍       | 614/2541 [10:25:11<32:44:12, 61.16s/it] 24%|██▍       | 615/2541 [10:26:13<32:43:12, 61.16s/it] 24%|██▍       | 616/2541 [10:27:13<32:38:56, 61.06s/it] 24%|██▍       | 617/2541 [10:28:14<32:38:10, 61.07s/it] 24%|██▍       | 618/2541 [10:29:15<32:35:44, 61.02s/it] 24%|██▍       | 619/2541 [10:30:17<32:36:49, 61.09s/it] 24%|██▍       | 620/2541 [10:31:18<32:35:37, 61.08s/it]                                                        {'loss': 1.4874, 'learning_rate': 4.300781138254927e-05, 'epoch': 0.24}
 24%|██▍       | 620/2541 [10:31:18<32:35:37, 61.08s/it] 24%|██▍       | 621/2541 [10:32:19<32:32:58, 61.03s/it] 24%|██▍       | 622/2541 [10:33:20<32:31:30, 61.02s/it] 25%|██▍       | 623/2541 [10:34:20<32:29:27, 60.98s/it] 25%|██▍       | 624/2541 [10:35:21<32:29:03, 61.00s/it] 25%|██▍       | 625/2541 [10:36:23<32:30:50, 61.09s/it] 25%|██▍       | 626/2541 [10:37:24<32:29:35, 61.08s/it] 25%|██▍       | 627/2541 [10:38:25<32:26:45, 61.03s/it] 25%|██▍       | 628/2541 [10:39:26<32:24:41, 60.99s/it] 25%|██▍       | 629/2541 [10:40:27<32:25:34, 61.05s/it] 25%|██▍       | 630/2541 [10:41:28<32:26:12, 61.11s/it]                                                        {'loss': 1.4726, 'learning_rate': 4.2792040207614005e-05, 'epoch': 0.25}
 25%|██▍       | 630/2541 [10:41:28<32:26:12, 61.11s/it] 25%|██▍       | 631/2541 [10:42:29<32:24:24, 61.08s/it] 25%|██▍       | 632/2541 [10:43:30<32:22:23, 61.05s/it] 25%|██▍       | 633/2541 [10:44:31<32:20:37, 61.03s/it] 25%|██▍       | 634/2541 [10:45:32<32:19:37, 61.03s/it] 25%|██▍       | 635/2541 [10:46:33<32:18:24, 61.02s/it] 25%|██▌       | 636/2541 [10:47:34<32:19:16, 61.08s/it] 25%|██▌       | 637/2541 [10:48:36<32:19:32, 61.12s/it] 25%|██▌       | 638/2541 [10:49:36<32:17:00, 61.07s/it] 25%|██▌       | 639/2541 [10:50:37<32:14:07, 61.01s/it] 25%|██▌       | 640/2541 [10:51:38<32:12:29, 60.99s/it]                                                        {'loss': 1.4731, 'learning_rate': 4.2573549397598264e-05, 'epoch': 0.25}
 25%|██▌       | 640/2541 [10:51:38<32:12:29, 60.99s/it] 25%|██▌       | 641/2541 [10:52:39<32:11:24, 60.99s/it] 25%|██▌       | 642/2541 [10:53:40<32:11:56, 61.04s/it] 25%|██▌       | 643/2541 [10:54:42<32:13:47, 61.13s/it] 25%|██▌       | 644/2541 [10:55:43<32:10:44, 61.07s/it] 25%|██▌       | 645/2541 [10:56:44<32:10:58, 61.11s/it] 25%|██▌       | 646/2541 [10:57:45<32:09:55, 61.11s/it] 25%|██▌       | 647/2541 [10:58:46<32:07:48, 61.07s/it] 26%|██▌       | 648/2541 [10:59:47<32:10:37, 61.19s/it] 26%|██▌       | 649/2541 [11:00:48<32:07:50, 61.14s/it] 26%|██▌       | 650/2541 [11:01:49<32:05:29, 61.09s/it]                                                        {'loss': 1.473, 'learning_rate': 4.2352372350317304e-05, 'epoch': 0.26}
 26%|██▌       | 650/2541 [11:01:49<32:05:29, 61.09s/it] 26%|██▌       | 651/2541 [11:02:51<32:05:22, 61.12s/it] 26%|██▌       | 652/2541 [11:03:52<32:04:50, 61.14s/it] 26%|██▌       | 653/2541 [11:04:53<32:07:08, 61.24s/it] 26%|██▌       | 654/2541 [11:05:55<32:06:58, 61.27s/it] 26%|██▌       | 655/2541 [11:06:56<32:03:12, 61.18s/it] 26%|██▌       | 656/2541 [11:07:57<32:02:30, 61.19s/it] 26%|██▌       | 657/2541 [11:08:58<32:00:14, 61.15s/it] 26%|██▌       | 658/2541 [11:09:59<31:58:33, 61.13s/it] 26%|██▌       | 659/2541 [11:11:00<31:58:55, 61.18s/it] 26%|██▌       | 660/2541 [11:12:01<31:57:55, 61.18s/it]                                                        {'loss': 1.4728, 'learning_rate': 4.212854287419611e-05, 'epoch': 0.26}
 26%|██▌       | 660/2541 [11:12:01<31:57:55, 61.18s/it] 26%|██▌       | 661/2541 [11:13:03<31:57:03, 61.18s/it] 26%|██▌       | 662/2541 [11:14:04<31:54:00, 61.12s/it] 26%|██▌       | 663/2541 [11:15:05<31:51:31, 61.07s/it] 26%|██▌       | 664/2541 [11:16:06<31:49:21, 61.03s/it] 26%|██▌       | 665/2541 [11:17:07<31:50:34, 61.11s/it] 26%|██▌       | 666/2541 [11:18:08<31:49:08, 61.09s/it] 26%|██▌       | 667/2541 [11:19:09<31:46:19, 61.03s/it] 26%|██▋       | 668/2541 [11:20:10<31:46:24, 61.07s/it] 26%|██▋       | 669/2541 [11:21:11<31:44:26, 61.04s/it] 26%|██▋       | 670/2541 [11:22:12<31:43:08, 61.03s/it]                                                        {'loss': 1.4745, 'learning_rate': 4.190209518310156e-05, 'epoch': 0.26}
 26%|██▋       | 670/2541 [11:22:12<31:43:08, 61.03s/it] 26%|██▋       | 671/2541 [11:23:13<31:45:51, 61.15s/it] 26%|██▋       | 672/2541 [11:24:14<31:43:48, 61.12s/it] 26%|██▋       | 673/2541 [11:25:15<31:42:19, 61.10s/it] 27%|██▋       | 674/2541 [11:26:17<31:42:06, 61.13s/it] 27%|██▋       | 675/2541 [11:27:17<31:38:36, 61.05s/it] 27%|██▋       | 676/2541 [11:28:19<31:40:28, 61.14s/it] 27%|██▋       | 677/2541 [11:29:20<31:40:28, 61.17s/it] 27%|██▋       | 678/2541 [11:30:21<31:37:13, 61.10s/it] 27%|██▋       | 679/2541 [11:31:22<31:33:39, 61.02s/it] 27%|██▋       | 680/2541 [11:32:23<31:32:30, 61.02s/it]                                                        {'loss': 1.4672, 'learning_rate': 4.167306389111262e-05, 'epoch': 0.27}
 27%|██▋       | 680/2541 [11:32:23<31:32:30, 61.02s/it] 27%|██▋       | 681/2541 [11:33:24<31:30:46, 60.99s/it] 27%|██▋       | 682/2541 [11:34:25<31:29:58, 61.00s/it] 27%|██▋       | 683/2541 [11:35:26<31:33:36, 61.15s/it] 27%|██▋       | 684/2541 [11:36:27<31:30:48, 61.09s/it] 27%|██▋       | 685/2541 [11:37:28<31:28:41, 61.06s/it] 27%|██▋       | 686/2541 [11:38:29<31:26:38, 61.02s/it] 27%|██▋       | 687/2541 [11:39:30<31:24:36, 60.99s/it] 27%|██▋       | 688/2541 [11:40:31<31:23:46, 61.00s/it] 27%|██▋       | 689/2541 [11:41:33<31:26:55, 61.13s/it] 27%|██▋       | 690/2541 [11:42:34<31:25:02, 61.10s/it]                                                        {'loss': 1.4702, 'learning_rate': 4.1441484007229314e-05, 'epoch': 0.27}
 27%|██▋       | 690/2541 [11:42:34<31:25:02, 61.10s/it] 27%|██▋       | 691/2541 [11:43:35<31:23:27, 61.09s/it] 27%|██▋       | 692/2541 [11:44:36<31:21:15, 61.05s/it] 27%|██▋       | 693/2541 [11:45:37<31:19:06, 61.01s/it] 27%|██▋       | 694/2541 [11:46:38<31:21:01, 61.11s/it] 27%|██▋       | 695/2541 [11:47:39<31:20:16, 61.11s/it] 27%|██▋       | 696/2541 [11:48:40<31:19:56, 61.14s/it] 27%|██▋       | 697/2541 [11:49:41<31:17:10, 61.08s/it] 27%|██▋       | 698/2541 [11:50:42<31:14:59, 61.04s/it] 28%|██▊       | 699/2541 [11:51:43<31:16:55, 61.14s/it] 28%|██▊       | 700/2541 [11:52:44<31:14:37, 61.10s/it]                                                        {'loss': 1.4569, 'learning_rate': 4.12073909300214e-05, 'epoch': 0.28}
 28%|██▊       | 700/2541 [11:52:44<31:14:37, 61.10s/it] 28%|██▊       | 701/2541 [11:53:45<31:12:44, 61.07s/it] 28%|██▊       | 702/2541 [11:54:46<31:10:17, 61.02s/it] 28%|██▊       | 703/2541 [11:55:47<31:08:24, 60.99s/it] 28%|██▊       | 704/2541 [11:56:48<31:06:37, 60.97s/it] 28%|██▊       | 705/2541 [11:57:49<31:05:43, 60.97s/it] 28%|██▊       | 706/2541 [11:58:50<31:06:49, 61.04s/it] 28%|██▊       | 707/2541 [11:59:51<31:04:32, 61.00s/it] 28%|██▊       | 708/2541 [12:00:53<31:05:58, 61.08s/it] 28%|██▊       | 709/2541 [12:01:54<31:04:02, 61.05s/it] 28%|██▊       | 710/2541 [12:02:55<31:02:34, 61.03s/it]                                                        {'loss': 1.4625, 'learning_rate': 4.097082044221741e-05, 'epoch': 0.28}
 28%|██▊       | 710/2541 [12:02:55<31:02:34, 61.03s/it] 28%|██▊       | 711/2541 [12:03:56<31:01:18, 61.03s/it] 28%|██▊       | 712/2541 [12:04:57<31:03:06, 61.12s/it] 28%|██▊       | 713/2541 [12:05:58<31:00:45, 61.07s/it] 28%|██▊       | 714/2541 [12:06:59<30:58:41, 61.04s/it] 28%|██▊       | 715/2541 [12:08:00<31:00:20, 61.13s/it] 28%|██▊       | 716/2541 [12:09:01<30:58:40, 61.11s/it] 28%|██▊       | 717/2541 [12:10:03<31:01:05, 61.22s/it] 28%|██▊       | 718/2541 [12:11:04<31:00:32, 61.24s/it] 28%|██▊       | 719/2541 [12:12:05<30:59:34, 61.24s/it] 28%|██▊       | 720/2541 [12:13:06<30:55:45, 61.15s/it]                                                        {'loss': 1.4568, 'learning_rate': 4.073180870523503e-05, 'epoch': 0.28}
 28%|██▊       | 720/2541 [12:13:06<30:55:45, 61.15s/it] 28%|██▊       | 721/2541 [12:14:07<30:52:16, 61.06s/it] 28%|██▊       | 722/2541 [12:15:08<30:52:16, 61.10s/it] 28%|██▊       | 723/2541 [12:16:09<30:52:56, 61.15s/it] 28%|██▊       | 724/2541 [12:17:10<30:50:12, 61.10s/it] 29%|██▊       | 725/2541 [12:18:11<30:48:43, 61.08s/it] 29%|██▊       | 726/2541 [12:19:13<30:47:30, 61.07s/it] 29%|██▊       | 727/2541 [12:20:13<30:45:12, 61.03s/it] 29%|██▊       | 728/2541 [12:21:14<30:43:54, 61.02s/it] 29%|██▊       | 729/2541 [12:22:16<30:46:11, 61.13s/it] 29%|██▊       | 730/2541 [12:23:17<30:45:33, 61.14s/it]                                                        {'loss': 1.4558, 'learning_rate': 4.049039225365362e-05, 'epoch': 0.29}
 29%|██▊       | 730/2541 [12:23:17<30:45:33, 61.14s/it] 29%|██▉       | 731/2541 [12:24:18<30:44:31, 61.14s/it] 29%|██▉       | 732/2541 [12:25:19<30:43:10, 61.13s/it] 29%|██▉       | 733/2541 [12:26:20<30:41:42, 61.12s/it] 29%|██▉       | 734/2541 [12:27:22<30:42:35, 61.18s/it] 29%|██▉       | 735/2541 [12:28:23<30:46:09, 61.33s/it] 29%|██▉       | 736/2541 [12:29:24<30:41:55, 61.23s/it] 29%|██▉       | 737/2541 [12:30:26<30:40:26, 61.21s/it] 29%|██▉       | 738/2541 [12:31:27<30:38:13, 61.17s/it] 29%|██▉       | 739/2541 [12:32:28<30:35:01, 61.10s/it] 29%|██▉       | 740/2541 [12:33:29<30:36:42, 61.19s/it]                                                        {'loss': 1.455, 'learning_rate': 4.024660798962957e-05, 'epoch': 0.29}
 29%|██▉       | 740/2541 [12:33:29<30:36:42, 61.19s/it] 29%|██▉       | 741/2541 [12:34:30<30:36:49, 61.23s/it] 29%|██▉       | 742/2541 [12:35:31<30:33:43, 61.16s/it] 29%|██▉       | 743/2541 [12:36:32<30:31:26, 61.12s/it] 29%|██▉       | 744/2541 [12:37:33<30:29:39, 61.09s/it] 29%|██▉       | 745/2541 [12:38:34<30:28:21, 61.08s/it] 29%|██▉       | 746/2541 [12:39:36<30:28:21, 61.11s/it] 29%|██▉       | 747/2541 [12:40:36<30:24:24, 61.02s/it] 29%|██▉       | 748/2541 [12:41:37<30:22:44, 61.00s/it] 29%|██▉       | 749/2541 [12:42:38<30:22:26, 61.02s/it] 30%|██▉       | 750/2541 [12:43:39<30:21:12, 61.01s/it]                                                        {'loss': 1.4587, 'learning_rate': 4.000049317725565e-05, 'epoch': 0.3}
 30%|██▉       | 750/2541 [12:43:39<30:21:12, 61.01s/it] 30%|██▉       | 751/2541 [12:44:40<30:20:11, 61.01s/it] 30%|██▉       | 752/2541 [12:45:42<30:21:55, 61.10s/it] 30%|██▉       | 753/2541 [12:46:43<30:20:34, 61.09s/it] 30%|██▉       | 754/2541 [12:47:44<30:19:09, 61.08s/it] 30%|██▉       | 755/2541 [12:48:45<30:17:25, 61.06s/it] 30%|██▉       | 756/2541 [12:49:46<30:14:49, 61.00s/it] 30%|██▉       | 757/2541 [12:50:47<30:12:49, 60.97s/it] 30%|██▉       | 758/2541 [12:51:48<30:15:35, 61.10s/it] 30%|██▉       | 759/2541 [12:52:49<30:14:51, 61.11s/it] 30%|██▉       | 760/2541 [12:53:50<30:11:54, 61.04s/it]                                                        {'loss': 1.4585, 'learning_rate': 3.975208543686486e-05, 'epoch': 0.3}
 30%|██▉       | 760/2541 [12:53:50<30:11:54, 61.04s/it] 30%|██▉       | 761/2541 [12:54:51<30:11:27, 61.06s/it] 30%|██▉       | 762/2541 [12:55:52<30:10:09, 61.05s/it] 30%|███       | 763/2541 [12:56:53<30:11:28, 61.13s/it] 30%|███       | 764/2541 [12:57:55<30:11:09, 61.15s/it] 30%|███       | 765/2541 [12:58:56<30:07:32, 61.07s/it] 30%|███       | 766/2541 [12:59:57<30:06:24, 61.06s/it] 30%|███       | 767/2541 [13:00:57<30:03:51, 61.01s/it] 30%|███       | 768/2541 [13:01:59<30:05:02, 61.08s/it] 30%|███       | 769/2541 [13:03:00<30:05:03, 61.12s/it] 30%|███       | 770/2541 [13:04:01<30:03:06, 61.09s/it]                                                        {'loss': 1.4453, 'learning_rate': 3.9501422739279956e-05, 'epoch': 0.3}
 30%|███       | 770/2541 [13:04:01<30:03:06, 61.09s/it] 30%|███       | 771/2541 [13:05:02<30:00:43, 61.04s/it] 30%|███       | 772/2541 [13:06:03<29:59:32, 61.04s/it] 30%|███       | 773/2541 [13:07:04<29:59:18, 61.06s/it] 30%|███       | 774/2541 [13:08:05<29:57:39, 61.04s/it] 30%|███       | 775/2541 [13:09:06<29:59:36, 61.14s/it] 31%|███       | 776/2541 [13:10:07<29:56:05, 61.06s/it] 31%|███       | 777/2541 [13:11:08<29:54:39, 61.04s/it] 31%|███       | 778/2541 [13:12:09<29:52:19, 61.00s/it] 31%|███       | 779/2541 [13:13:10<29:51:27, 61.00s/it] 31%|███       | 780/2541 [13:14:11<29:50:02, 60.99s/it]                                                        {'loss': 1.453, 'learning_rate': 3.924854340000931e-05, 'epoch': 0.31}
 31%|███       | 780/2541 [13:14:11<29:50:02, 60.99s/it] 31%|███       | 781/2541 [13:15:12<29:52:21, 61.10s/it] 31%|███       | 782/2541 [13:16:13<29:49:47, 61.05s/it] 31%|███       | 783/2541 [13:17:14<29:48:46, 61.05s/it] 31%|███       | 784/2541 [13:18:15<29:46:19, 61.00s/it] 31%|███       | 785/2541 [13:19:16<29:45:16, 61.00s/it] 31%|███       | 786/2541 [13:20:17<29:42:43, 60.95s/it] 31%|███       | 787/2541 [13:21:18<29:44:37, 61.05s/it] 31%|███       | 788/2541 [13:22:19<29:43:20, 61.04s/it] 31%|███       | 789/2541 [13:23:21<29:42:36, 61.05s/it] 31%|███       | 790/2541 [13:24:22<29:41:11, 61.03s/it]                                                        {'loss': 1.4525, 'learning_rate': 3.899348607339016e-05, 'epoch': 0.31}
 31%|███       | 790/2541 [13:24:22<29:41:11, 61.03s/it] 31%|███       | 791/2541 [13:25:23<29:42:52, 61.13s/it] 31%|███       | 792/2541 [13:26:24<29:43:07, 61.17s/it] 31%|███       | 793/2541 [13:27:25<29:40:58, 61.13s/it] 31%|███       | 794/2541 [13:28:26<29:40:29, 61.15s/it] 31%|███▏      | 795/2541 [13:29:28<29:39:44, 61.16s/it] 31%|███▏      | 796/2541 [13:30:29<29:37:35, 61.12s/it] 31%|███▏      | 797/2541 [13:31:30<29:34:58, 61.07s/it] 31%|███▏      | 798/2541 [13:32:31<29:36:10, 61.14s/it] 31%|███▏      | 799/2541 [13:33:32<29:37:42, 61.23s/it] 31%|███▏      | 800/2541 [13:34:33<29:35:32, 61.19s/it]                                                        {'loss': 1.4483, 'learning_rate': 3.8736289746680014e-05, 'epoch': 0.31}
 31%|███▏      | 800/2541 [13:34:33<29:35:32, 61.19s/it] 32%|███▏      | 801/2541 [13:35:34<29:32:12, 61.11s/it] 32%|███▏      | 802/2541 [13:36:35<29:31:37, 61.13s/it] 32%|███▏      | 803/2541 [13:37:36<29:28:51, 61.07s/it] 32%|███▏      | 804/2541 [13:38:37<29:28:11, 61.08s/it] 32%|███▏      | 805/2541 [13:39:39<29:31:10, 61.22s/it] 32%|███▏      | 806/2541 [13:40:40<29:29:59, 61.21s/it] 32%|███▏      | 807/2541 [13:41:41<29:27:16, 61.15s/it] 32%|███▏      | 808/2541 [13:42:42<29:26:21, 61.16s/it] 32%|███▏      | 809/2541 [13:43:44<29:25:30, 61.16s/it] 32%|███▏      | 810/2541 [13:44:45<29:25:46, 61.21s/it]                                                        {'loss': 1.4406, 'learning_rate': 3.8476993734097155e-05, 'epoch': 0.32}
 32%|███▏      | 810/2541 [13:44:45<29:25:46, 61.21s/it] 32%|███▏      | 811/2541 [13:45:46<29:22:05, 61.11s/it] 32%|███▏      | 812/2541 [13:46:47<29:20:30, 61.09s/it] 32%|███▏      | 813/2541 [13:47:48<29:21:07, 61.15s/it] 32%|███▏      | 814/2541 [13:48:49<29:19:58, 61.15s/it] 32%|███▏      | 815/2541 [13:49:51<29:22:55, 61.28s/it] 32%|███▏      | 816/2541 [13:50:52<29:19:50, 61.21s/it] 32%|███▏      | 817/2541 [13:51:53<29:17:46, 61.18s/it] 32%|███▏      | 818/2541 [13:52:54<29:13:57, 61.08s/it] 32%|███▏      | 819/2541 [13:53:55<29:10:54, 61.01s/it] 32%|███▏      | 820/2541 [13:54:56<29:10:48, 61.04s/it]                                                        {'loss': 1.451, 'learning_rate': 3.8215637670811234e-05, 'epoch': 0.32}
 32%|███▏      | 820/2541 [13:54:56<29:10:48, 61.04s/it] 32%|███▏      | 821/2541 [13:55:57<29:11:00, 61.08s/it] 32%|███▏      | 822/2541 [13:56:58<29:11:55, 61.15s/it] 32%|███▏      | 823/2541 [13:57:59<29:09:37, 61.10s/it] 32%|███▏      | 824/2541 [13:59:00<29:09:10, 61.12s/it] 32%|███▏      | 825/2541 [14:00:02<29:07:54, 61.12s/it] 33%|███▎      | 826/2541 [14:01:03<29:06:27, 61.10s/it] 33%|███▎      | 827/2541 [14:02:04<29:07:30, 61.17s/it] 33%|███▎      | 828/2541 [14:03:05<29:07:43, 61.22s/it] 33%|███▎      | 829/2541 [14:04:06<29:05:39, 61.18s/it] 33%|███▎      | 830/2541 [14:05:07<29:04:07, 61.16s/it]                                                        {'loss': 1.4449, 'learning_rate': 3.795226150688475e-05, 'epoch': 0.33}
 33%|███▎      | 830/2541 [14:05:07<29:04:07, 61.16s/it] 33%|███▎      | 831/2541 [14:06:09<29:01:53, 61.12s/it] 33%|███▎      | 832/2541 [14:07:10<28:59:53, 61.08s/it] 33%|███▎      | 833/2541 [14:08:11<29:04:18, 61.28s/it] 33%|███▎      | 834/2541 [14:09:12<29:00:48, 61.19s/it] 33%|███▎      | 835/2541 [14:10:13<28:59:02, 61.16s/it] 33%|███▎      | 836/2541 [14:11:14<28:57:23, 61.14s/it] 33%|███▎      | 837/2541 [14:12:16<28:56:18, 61.14s/it] 33%|███▎      | 838/2541 [14:13:17<28:56:47, 61.19s/it] 33%|███▎      | 839/2541 [14:14:18<28:55:09, 61.17s/it] 33%|███▎      | 840/2541 [14:15:19<28:52:26, 61.11s/it]                                                        {'loss': 1.436, 'learning_rate': 3.768690550116639e-05, 'epoch': 0.33}
 33%|███▎      | 840/2541 [14:15:19<28:52:26, 61.11s/it] 33%|███▎      | 841/2541 [14:16:20<28:50:23, 61.07s/it] 33%|███▎      | 842/2541 [14:17:21<28:48:29, 61.04s/it] 33%|███▎      | 843/2541 [14:18:22<28:48:11, 61.07s/it] 33%|███▎      | 844/2541 [14:19:24<28:51:11, 61.21s/it] 33%|███▎      | 845/2541 [14:20:25<28:50:58, 61.24s/it] 33%|███▎      | 846/2541 [14:21:26<28:48:46, 61.20s/it] 33%|███▎      | 847/2541 [14:22:27<28:44:59, 61.10s/it] 33%|███▎      | 848/2541 [14:23:28<28:42:16, 61.04s/it] 33%|███▎      | 849/2541 [14:24:29<28:40:39, 61.02s/it] 33%|███▎      | 850/2541 [14:25:30<28:41:32, 61.08s/it]                                                        {'loss': 1.4325, 'learning_rate': 3.741961021513724e-05, 'epoch': 0.33}
 33%|███▎      | 850/2541 [14:25:30<28:41:32, 61.08s/it] 33%|███▎      | 851/2541 [14:26:31<28:41:22, 61.11s/it] 34%|███▎      | 852/2541 [14:27:32<28:41:56, 61.17s/it] 34%|███▎      | 853/2541 [14:28:33<28:39:22, 61.12s/it] 34%|███▎      | 854/2541 [14:29:35<28:39:01, 61.14s/it] 34%|███▎      | 855/2541 [14:30:36<28:36:30, 61.09s/it] 34%|███▎      | 856/2541 [14:31:37<28:39:20, 61.22s/it] 34%|███▎      | 857/2541 [14:32:38<28:36:21, 61.15s/it] 34%|███▍      | 858/2541 [14:33:39<28:35:08, 61.15s/it] 34%|███▍      | 859/2541 [14:34:40<28:32:01, 61.07s/it] 34%|███▍      | 860/2541 [14:35:41<28:30:29, 61.05s/it]                                                        {'loss': 1.4293, 'learning_rate': 3.715041650671062e-05, 'epoch': 0.34}
 34%|███▍      | 860/2541 [14:35:41<28:30:29, 61.05s/it] 34%|███▍      | 861/2541 [14:36:42<28:29:48, 61.06s/it] 34%|███▍      | 862/2541 [14:37:43<28:29:14, 61.08s/it] 34%|███▍      | 863/2541 [14:38:44<28:27:27, 61.05s/it] 34%|███▍      | 864/2541 [14:39:45<28:26:16, 61.05s/it] 34%|███▍      | 865/2541 [14:40:46<28:23:40, 60.99s/it] 34%|███▍      | 866/2541 [14:41:47<28:24:03, 61.04s/it] 34%|███▍      | 867/2541 [14:42:48<28:22:53, 61.04s/it] 34%|███▍      | 868/2541 [14:43:50<28:24:07, 61.12s/it] 34%|███▍      | 869/2541 [14:44:51<28:22:11, 61.08s/it] 34%|███▍      | 870/2541 [14:45:52<28:20:56, 61.08s/it]                                                        {'loss': 1.4361, 'learning_rate': 3.6879365523986706e-05, 'epoch': 0.34}
 34%|███▍      | 870/2541 [14:45:52<28:20:56, 61.08s/it] 34%|███▍      | 871/2541 [14:46:53<28:20:31, 61.10s/it] 34%|███▍      | 872/2541 [14:47:54<28:20:10, 61.12s/it] 34%|███▍      | 873/2541 [14:48:55<28:18:43, 61.11s/it] 34%|███▍      | 874/2541 [14:49:57<28:19:51, 61.18s/it] 34%|███▍      | 875/2541 [14:50:57<28:16:46, 61.11s/it] 34%|███▍      | 876/2541 [14:51:58<28:14:37, 61.07s/it] 35%|███▍      | 877/2541 [14:53:00<28:13:30, 61.06s/it] 35%|███▍      | 878/2541 [14:54:01<28:12:09, 61.05s/it] 35%|███▍      | 879/2541 [14:55:02<28:13:17, 61.13s/it] 35%|███▍      | 880/2541 [14:56:03<28:12:57, 61.15s/it]                                                        {'loss': 1.432, 'learning_rate': 3.660649869896276e-05, 'epoch': 0.35}
 35%|███▍      | 880/2541 [14:56:03<28:12:57, 61.15s/it] 35%|███▍      | 881/2541 [14:57:04<28:11:35, 61.14s/it] 35%|███▍      | 882/2541 [14:58:05<28:10:19, 61.13s/it] 35%|███▍      | 883/2541 [14:59:06<28:08:18, 61.10s/it] 35%|███▍      | 884/2541 [15:00:08<28:10:13, 61.20s/it] 35%|███▍      | 885/2541 [15:01:09<28:07:34, 61.14s/it] 35%|███▍      | 886/2541 [15:02:10<28:05:08, 61.09s/it] 35%|███▍      | 887/2541 [15:03:11<28:03:04, 61.05s/it] 35%|███▍      | 888/2541 [15:04:12<28:02:54, 61.09s/it] 35%|███▍      | 889/2541 [15:05:13<28:00:04, 61.02s/it] 35%|███▌      | 890/2541 [15:06:14<27:59:31, 61.04s/it]                                                        {'loss': 1.4366, 'learning_rate': 3.633185774119998e-05, 'epoch': 0.35}
 35%|███▌      | 890/2541 [15:06:14<27:59:31, 61.04s/it] 35%|███▌      | 891/2541 [15:07:15<28:01:48, 61.16s/it] 35%|███▌      | 892/2541 [15:08:16<27:59:20, 61.10s/it] 35%|███▌      | 893/2541 [15:09:17<27:57:35, 61.08s/it] 35%|███▌      | 894/2541 [15:10:18<27:55:45, 61.05s/it] 35%|███▌      | 895/2541 [15:11:19<27:55:31, 61.08s/it] 35%|███▌      | 896/2541 [15:12:20<27:52:40, 61.01s/it] 35%|███▌      | 897/2541 [15:13:22<27:54:24, 61.11s/it] 35%|███▌      | 898/2541 [15:14:23<27:52:24, 61.07s/it] 35%|███▌      | 899/2541 [15:15:24<27:51:20, 61.07s/it] 35%|███▌      | 900/2541 [15:16:25<27:49:31, 61.04s/it]                                                        {'loss': 1.4348, 'learning_rate': 3.605548463144786e-05, 'epoch': 0.35}
 35%|███▌      | 900/2541 [15:16:25<27:49:31, 61.04s/it] 35%|███▌      | 901/2541 [15:17:25<27:47:17, 61.00s/it] 35%|███▌      | 902/2541 [15:18:27<27:49:31, 61.12s/it] 36%|███▌      | 903/2541 [15:19:28<27:48:43, 61.13s/it] 36%|███▌      | 904/2541 [15:20:29<27:45:45, 61.05s/it] 36%|███▌      | 905/2541 [15:21:30<27:44:28, 61.04s/it] 36%|███▌      | 906/2541 [15:22:31<27:43:39, 61.05s/it] 36%|███▌      | 907/2541 [15:23:32<27:45:29, 61.16s/it] 36%|███▌      | 908/2541 [15:24:34<27:45:19, 61.19s/it] 36%|███▌      | 909/2541 [15:25:35<27:43:43, 61.17s/it] 36%|███▌      | 910/2541 [15:26:36<27:42:27, 61.16s/it]                                                        {'loss': 1.4263, 'learning_rate': 3.577742161522721e-05, 'epoch': 0.36}
 36%|███▌      | 910/2541 [15:26:36<27:42:27, 61.16s/it] 36%|███▌      | 911/2541 [15:27:37<27:38:40, 61.06s/it] 36%|███▌      | 912/2541 [15:28:38<27:36:11, 61.00s/it] 36%|███▌      | 913/2541 [15:29:39<27:37:41, 61.09s/it] 36%|███▌      | 914/2541 [15:30:40<27:38:51, 61.17s/it] 36%|███▌      | 915/2541 [15:31:41<27:36:03, 61.11s/it] 36%|███▌      | 916/2541 [15:32:43<27:36:31, 61.16s/it] 36%|███▌      | 917/2541 [15:33:44<27:34:13, 61.12s/it] 36%|███▌      | 918/2541 [15:34:44<27:31:40, 61.06s/it] 36%|███▌      | 919/2541 [15:35:46<27:30:53, 61.07s/it] 36%|███▌      | 920/2541 [15:36:47<27:31:57, 61.15s/it]                                                        {'loss': 1.4283, 'learning_rate': 3.549771119637258e-05, 'epoch': 0.36}
 36%|███▌      | 920/2541 [15:36:47<27:31:57, 61.15s/it] 36%|███▌      | 921/2541 [15:37:48<27:28:42, 61.06s/it] 36%|███▋      | 922/2541 [15:38:49<27:27:02, 61.04s/it] 36%|███▋      | 923/2541 [15:39:50<27:27:13, 61.08s/it] 36%|███▋      | 924/2541 [15:40:51<27:24:53, 61.04s/it] 36%|███▋      | 925/2541 [15:41:52<27:26:23, 61.13s/it] 36%|███▋      | 926/2541 [15:42:53<27:26:50, 61.18s/it] 36%|███▋      | 927/2541 [15:43:55<27:26:04, 61.19s/it] 37%|███▋      | 928/2541 [15:44:56<27:23:15, 61.13s/it] 37%|███▋      | 929/2541 [15:45:57<27:22:02, 61.12s/it] 37%|███▋      | 930/2541 [15:46:58<27:20:02, 61.08s/it]                                                        {'loss': 1.4322, 'learning_rate': 3.52163961305353e-05, 'epoch': 0.37}
 37%|███▋      | 930/2541 [15:46:58<27:20:02, 61.08s/it] 37%|███▋      | 931/2541 [15:47:59<27:20:50, 61.15s/it] 37%|███▋      | 932/2541 [15:49:00<27:18:42, 61.11s/it] 37%|███▋      | 933/2541 [15:50:01<27:17:28, 61.10s/it] 37%|███▋      | 934/2541 [15:51:02<27:16:30, 61.10s/it] 37%|███▋      | 935/2541 [15:52:03<27:14:47, 61.08s/it] 37%|███▋      | 936/2541 [15:53:04<27:12:32, 61.03s/it] 37%|███▋      | 937/2541 [15:54:06<27:15:01, 61.16s/it] 37%|███▋      | 938/2541 [15:55:07<27:13:13, 61.13s/it] 37%|███▋      | 939/2541 [15:56:08<27:11:02, 61.09s/it] 37%|███▋      | 940/2541 [15:57:09<27:09:25, 61.07s/it]                                                        {'loss': 1.4233, 'learning_rate': 3.493351941864798e-05, 'epoch': 0.37}
 37%|███▋      | 940/2541 [15:57:09<27:09:25, 61.07s/it] 37%|███▋      | 941/2541 [15:58:10<27:08:55, 61.08s/it] 37%|███▋      | 942/2541 [15:59:11<27:08:04, 61.09s/it] 37%|███▋      | 943/2541 [16:00:12<27:10:33, 61.22s/it] 37%|███▋      | 944/2541 [16:01:14<27:08:43, 61.19s/it] 37%|███▋      | 945/2541 [16:02:15<27:08:28, 61.22s/it] 37%|███▋      | 946/2541 [16:03:16<27:05:28, 61.15s/it] 37%|███▋      | 947/2541 [16:04:17<27:02:36, 61.08s/it] 37%|███▋      | 948/2541 [16:05:18<27:03:02, 61.13s/it] 37%|███▋      | 949/2541 [16:06:19<27:02:31, 61.15s/it] 37%|███▋      | 950/2541 [16:07:20<26:59:25, 61.07s/it]                                                        {'loss': 1.4216, 'learning_rate': 3.464912430035148e-05, 'epoch': 0.37}
 37%|███▋      | 950/2541 [16:07:20<26:59:25, 61.07s/it] 37%|███▋      | 951/2541 [16:08:21<26:58:14, 61.07s/it] 37%|███▋      | 952/2541 [16:09:22<26:56:35, 61.04s/it] 38%|███▊      | 953/2541 [16:10:23<26:57:00, 61.10s/it] 38%|███▊      | 954/2541 [16:11:25<26:56:49, 61.13s/it] 38%|███▊      | 955/2541 [16:12:26<26:55:53, 61.13s/it] 38%|███▊      | 956/2541 [16:13:27<26:55:36, 61.16s/it] 38%|███▊      | 957/2541 [16:14:28<26:53:33, 61.12s/it] 38%|███▊      | 958/2541 [16:15:29<26:51:56, 61.10s/it] 38%|███▊      | 959/2541 [16:16:30<26:50:41, 61.09s/it] 38%|███▊      | 960/2541 [16:17:31<26:51:49, 61.17s/it]                                                        {'loss': 1.4251, 'learning_rate': 3.436325424738549e-05, 'epoch': 0.38}
 38%|███▊      | 960/2541 [16:17:31<26:51:49, 61.17s/it] 38%|███▊      | 961/2541 [16:18:32<26:48:35, 61.09s/it] 38%|███▊      | 962/2541 [16:19:33<26:47:01, 61.07s/it] 38%|███▊      | 963/2541 [16:20:34<26:45:07, 61.03s/it] 38%|███▊      | 964/2541 [16:21:35<26:43:56, 61.02s/it] 38%|███▊      | 965/2541 [16:22:36<26:42:19, 61.00s/it] 38%|███▊      | 966/2541 [16:23:38<26:44:00, 61.10s/it] 38%|███▊      | 967/2541 [16:24:39<26:42:16, 61.08s/it] 38%|███▊      | 968/2541 [16:25:40<26:39:58, 61.03s/it] 38%|███▊      | 969/2541 [16:26:41<26:38:21, 61.01s/it] 38%|███▊      | 970/2541 [16:27:42<26:37:27, 61.01s/it]                                                        {'loss': 1.4212, 'learning_rate': 3.407595295694356e-05, 'epoch': 0.38}
 38%|███▊      | 970/2541 [16:27:42<26:37:27, 61.01s/it] 38%|███▊      | 971/2541 [16:28:43<26:36:54, 61.03s/it] 38%|███▊      | 972/2541 [16:29:44<26:37:52, 61.10s/it] 38%|███▊      | 973/2541 [16:30:45<26:36:26, 61.09s/it] 38%|███▊      | 974/2541 [16:31:46<26:35:15, 61.08s/it] 38%|███▊      | 975/2541 [16:32:47<26:33:27, 61.05s/it] 38%|███▊      | 976/2541 [16:33:48<26:31:41, 61.02s/it] 38%|███▊      | 977/2541 [16:34:49<26:32:19, 61.09s/it] 38%|███▊      | 978/2541 [16:35:50<26:30:30, 61.06s/it] 39%|███▊      | 979/2541 [16:36:51<26:28:04, 61.00s/it] 39%|███▊      | 980/2541 [16:37:52<26:26:21, 60.97s/it]                                                        {'loss': 1.4206, 'learning_rate': 3.378726434499368e-05, 'epoch': 0.39}
 39%|███▊      | 980/2541 [16:37:52<26:26:21, 60.97s/it] 39%|███▊      | 981/2541 [16:38:53<26:26:23, 61.02s/it] 39%|███▊      | 982/2541 [16:39:54<26:26:17, 61.05s/it] 39%|███▊      | 983/2541 [16:40:56<26:27:41, 61.14s/it] 39%|███▊      | 984/2541 [16:41:57<26:27:43, 61.18s/it] 39%|███▉      | 985/2541 [16:42:58<26:27:13, 61.20s/it] 39%|███▉      | 986/2541 [16:43:59<26:24:32, 61.14s/it] 39%|███▉      | 987/2541 [16:45:00<26:22:19, 61.09s/it] 39%|███▉      | 988/2541 [16:46:01<26:21:09, 61.09s/it] 39%|███▉      | 989/2541 [16:47:02<26:20:55, 61.12s/it] 39%|███▉      | 990/2541 [16:48:03<26:20:06, 61.13s/it]                                                        {'loss': 1.4243, 'learning_rate': 3.349723253956542e-05, 'epoch': 0.39}
 39%|███▉      | 990/2541 [16:48:03<26:20:06, 61.13s/it] 39%|███▉      | 991/2541 [16:49:04<26:17:49, 61.08s/it] 39%|███▉      | 992/2541 [16:50:05<26:16:43, 61.07s/it] 39%|███▉      | 993/2541 [16:51:06<26:14:52, 61.04s/it] 39%|███▉      | 994/2541 [16:52:07<26:12:51, 61.00s/it] 39%|███▉      | 995/2541 [16:53:09<26:14:54, 61.12s/it] 39%|███▉      | 996/2541 [16:54:10<26:13:07, 61.09s/it] 39%|███▉      | 997/2541 [16:55:11<26:10:54, 61.05s/it] 39%|███▉      | 998/2541 [16:56:12<26:09:13, 61.02s/it] 39%|███▉      | 999/2541 [16:57:13<26:09:42, 61.08s/it] 39%|███▉      | 1000/2541 [16:58:14<26:11:59, 61.21s/it]                                                         {'loss': 1.412, 'learning_rate': 3.3205901874004654e-05, 'epoch': 0.39}
 39%|███▉      | 1000/2541 [16:58:14<26:11:59, 61.21s/it] 39%|███▉      | 1001/2541 [16:59:15<26:09:53, 61.16s/it] 39%|███▉      | 1002/2541 [17:00:17<26:13:14, 61.34s/it] 39%|███▉      | 1003/2541 [17:01:18<26:09:18, 61.22s/it] 40%|███▉      | 1004/2541 [17:02:19<26:06:28, 61.15s/it] 40%|███▉      | 1005/2541 [17:03:20<26:05:12, 61.14s/it] 40%|███▉      | 1006/2541 [17:04:22<26:07:54, 61.29s/it] 40%|███▉      | 1007/2541 [17:05:23<26:05:39, 61.24s/it] 40%|███▉      | 1008/2541 [17:06:24<26:04:20, 61.23s/it] 40%|███▉      | 1009/2541 [17:07:25<26:02:48, 61.21s/it] 40%|███▉      | 1010/2541 [17:08:26<25:59:10, 61.10s/it]                                                         {'loss': 1.4105, 'learning_rate': 3.291331688019693e-05, 'epoch': 0.4}
 40%|███▉      | 1010/2541 [17:08:26<25:59:10, 61.10s/it] 40%|███▉      | 1011/2541 [17:09:27<25:57:38, 61.08s/it] 40%|███▉      | 1012/2541 [17:10:29<26:01:30, 61.28s/it] 40%|███▉      | 1013/2541 [17:11:30<26:01:14, 61.31s/it] 40%|███▉      | 1014/2541 [17:12:32<25:59:57, 61.30s/it] 40%|███▉      | 1015/2541 [17:13:33<25:56:53, 61.21s/it] 40%|███▉      | 1016/2541 [17:14:34<25:57:20, 61.27s/it] 40%|████      | 1017/2541 [17:15:35<25:54:30, 61.20s/it] 40%|████      | 1018/2541 [17:16:37<25:55:31, 61.28s/it] 40%|████      | 1019/2541 [17:17:38<25:52:49, 61.22s/it] 40%|████      | 1020/2541 [17:18:39<25:50:23, 61.16s/it]                                                         {'loss': 1.409, 'learning_rate': 3.261952228176044e-05, 'epoch': 0.4}
 40%|████      | 1020/2541 [17:18:39<25:50:23, 61.16s/it] 40%|████      | 1021/2541 [17:19:40<25:48:10, 61.11s/it] 40%|████      | 1022/2541 [17:20:41<25:46:06, 61.07s/it] 40%|████      | 1023/2541 [17:21:42<25:46:10, 61.11s/it] 40%|████      | 1024/2541 [17:22:43<25:44:44, 61.10s/it] 40%|████      | 1025/2541 [17:23:44<25:43:01, 61.07s/it] 40%|████      | 1026/2541 [17:24:45<25:41:51, 61.06s/it] 40%|████      | 1027/2541 [17:25:46<25:43:01, 61.15s/it] 40%|████      | 1028/2541 [17:26:47<25:41:50, 61.14s/it] 40%|████      | 1029/2541 [17:27:49<25:42:00, 61.19s/it] 41%|████      | 1030/2541 [17:28:50<25:41:19, 61.20s/it]                                                         {'loss': 1.4137, 'learning_rate': 3.232456298720973e-05, 'epoch': 0.41}
 41%|████      | 1030/2541 [17:28:50<25:41:19, 61.20s/it] 41%|████      | 1031/2541 [17:29:51<25:38:51, 61.15s/it] 41%|████      | 1032/2541 [17:30:52<25:36:38, 61.10s/it] 41%|████      | 1033/2541 [17:31:53<25:35:31, 61.10s/it] 41%|████      | 1034/2541 [17:32:54<25:34:26, 61.09s/it] 41%|████      | 1035/2541 [17:33:55<25:34:29, 61.14s/it] 41%|████      | 1036/2541 [17:34:57<25:35:52, 61.23s/it] 41%|████      | 1037/2541 [17:35:58<25:33:35, 61.18s/it] 41%|████      | 1038/2541 [17:36:59<25:31:55, 61.15s/it] 41%|████      | 1039/2541 [17:38:00<25:28:17, 61.05s/it] 41%|████      | 1040/2541 [17:39:01<25:26:51, 61.03s/it]                                                         {'loss': 1.4054, 'learning_rate': 3.202848408309111e-05, 'epoch': 0.41}
 41%|████      | 1040/2541 [17:39:01<25:26:51, 61.03s/it] 41%|████      | 1041/2541 [17:40:02<25:28:49, 61.15s/it] 41%|████      | 1042/2541 [17:41:03<25:26:59, 61.12s/it] 41%|████      | 1043/2541 [17:42:04<25:25:00, 61.08s/it] 41%|████      | 1044/2541 [17:43:06<25:25:28, 61.14s/it] 41%|████      | 1045/2541 [17:44:07<25:24:39, 61.15s/it] 41%|████      | 1046/2541 [17:45:08<25:23:20, 61.14s/it] 41%|████      | 1047/2541 [17:46:09<25:23:11, 61.17s/it] 41%|████      | 1048/2541 [17:47:10<25:21:29, 61.14s/it] 41%|████▏     | 1049/2541 [17:48:11<25:18:50, 61.08s/it] 41%|████▏     | 1050/2541 [17:49:12<25:17:01, 61.05s/it]                                                         {'loss': 1.406, 'learning_rate': 3.1731330827090865e-05, 'epoch': 0.41}
 41%|████▏     | 1050/2541 [17:49:12<25:17:01, 61.05s/it] 41%|████▏     | 1051/2541 [17:50:13<25:17:17, 61.10s/it] 41%|████▏     | 1052/2541 [17:51:14<25:15:07, 61.05s/it] 41%|████▏     | 1053/2541 [17:52:16<25:16:16, 61.14s/it] 41%|████▏     | 1054/2541 [17:53:17<25:15:14, 61.14s/it] 42%|████▏     | 1055/2541 [17:54:18<25:13:12, 61.10s/it] 42%|████▏     | 1056/2541 [17:55:19<25:10:52, 61.05s/it] 42%|████▏     | 1057/2541 [17:56:20<25:08:51, 61.00s/it] 42%|████▏     | 1058/2541 [17:57:21<25:09:26, 61.07s/it] 42%|████▏     | 1059/2541 [17:58:22<25:10:19, 61.15s/it] 42%|████▏     | 1060/2541 [17:59:23<25:07:24, 61.07s/it]                                                         {'loss': 1.418, 'learning_rate': 3.143314864111733e-05, 'epoch': 0.42}
 42%|████▏     | 1060/2541 [17:59:23<25:07:24, 61.07s/it] 42%|████▏     | 1061/2541 [18:00:24<25:06:16, 61.07s/it] 42%|████▏     | 1062/2541 [18:01:25<25:05:09, 61.06s/it] 42%|████▏     | 1063/2541 [18:02:26<25:05:33, 61.12s/it] 42%|████▏     | 1064/2541 [18:03:28<25:06:22, 61.19s/it] 42%|████▏     | 1065/2541 [18:04:29<25:06:50, 61.25s/it] 42%|████▏     | 1066/2541 [18:05:30<25:03:18, 61.15s/it] 42%|████▏     | 1067/2541 [18:06:31<25:00:32, 61.08s/it] 42%|████▏     | 1068/2541 [18:07:32<24:59:08, 61.06s/it] 42%|████▏     | 1069/2541 [18:08:33<25:01:24, 61.20s/it] 42%|████▏     | 1070/2541 [18:09:34<24:58:01, 61.10s/it]                                                         {'loss': 1.4061, 'learning_rate': 3.113398310435782e-05, 'epoch': 0.42}
 42%|████▏     | 1070/2541 [18:09:34<24:58:01, 61.10s/it] 42%|████▏     | 1071/2541 [18:10:35<24:55:50, 61.05s/it] 42%|████▏     | 1072/2541 [18:11:36<24:54:53, 61.06s/it] 42%|████▏     | 1073/2541 [18:12:37<24:53:31, 61.04s/it] 42%|████▏     | 1074/2541 [18:13:38<24:51:14, 60.99s/it] 42%|████▏     | 1075/2541 [18:14:39<24:49:18, 60.95s/it] 42%|████▏     | 1076/2541 [18:15:41<24:52:53, 61.14s/it] 42%|████▏     | 1077/2541 [18:16:42<24:50:46, 61.10s/it] 42%|████▏     | 1078/2541 [18:17:43<24:48:14, 61.03s/it] 42%|████▏     | 1079/2541 [18:18:44<24:46:52, 61.02s/it] 43%|████▎     | 1080/2541 [18:19:45<24:45:45, 61.02s/it]                                                         {'loss': 1.4178, 'learning_rate': 3.083387994631154e-05, 'epoch': 0.42}
 43%|████▎     | 1080/2541 [18:19:45<24:45:45, 61.02s/it] 43%|████▎     | 1081/2541 [18:20:45<24:43:24, 60.96s/it] 43%|████▎     | 1082/2541 [18:21:47<24:47:36, 61.18s/it] 43%|████▎     | 1083/2541 [18:22:48<24:47:01, 61.19s/it] 43%|████▎     | 1084/2541 [18:23:49<24:43:43, 61.10s/it] 43%|████▎     | 1085/2541 [18:24:50<24:41:30, 61.05s/it] 43%|████▎     | 1086/2541 [18:25:51<24:39:48, 61.02s/it] 43%|████▎     | 1087/2541 [18:26:52<24:40:44, 61.10s/it] 43%|████▎     | 1088/2541 [18:27:54<24:40:23, 61.13s/it] 43%|████▎     | 1089/2541 [18:28:55<24:37:51, 61.07s/it] 43%|████▎     | 1090/2541 [18:29:56<24:37:09, 61.08s/it]                                                         {'loss': 1.4056, 'learning_rate': 3.053288503979955e-05, 'epoch': 0.43}
 43%|████▎     | 1090/2541 [18:29:56<24:37:09, 61.08s/it] 43%|████▎     | 1091/2541 [18:30:57<24:37:00, 61.12s/it] 43%|████▎     | 1092/2541 [18:31:58<24:36:56, 61.16s/it] 43%|████▎     | 1093/2541 [18:32:59<24:36:51, 61.20s/it] 43%|████▎     | 1094/2541 [18:34:01<24:35:34, 61.18s/it] 43%|████▎     | 1095/2541 [18:35:02<24:33:09, 61.13s/it] 43%|████▎     | 1096/2541 [18:36:02<24:30:39, 61.07s/it] 43%|████▎     | 1097/2541 [18:37:04<24:29:58, 61.08s/it] 43%|████▎     | 1098/2541 [18:38:05<24:30:07, 61.13s/it] 43%|████▎     | 1099/2541 [18:39:06<24:32:17, 61.26s/it] 43%|████▎     | 1100/2541 [18:40:08<24:31:21, 61.26s/it]                                                         {'loss': 1.4151, 'learning_rate': 3.0231044393952712e-05, 'epoch': 0.43}
 43%|████▎     | 1100/2541 [18:40:08<24:31:21, 61.26s/it] 43%|████▎     | 1101/2541 [18:41:09<24:29:25, 61.23s/it] 43%|████▎     | 1102/2541 [18:42:10<24:29:15, 61.26s/it] 43%|████▎     | 1103/2541 [18:43:11<24:25:59, 61.17s/it] 43%|████▎     | 1104/2541 [18:44:12<24:24:36, 61.15s/it] 43%|████▎     | 1105/2541 [18:45:14<24:25:59, 61.25s/it] 44%|████▎     | 1106/2541 [18:46:15<24:23:01, 61.17s/it] 44%|████▎     | 1107/2541 [18:47:16<24:19:55, 61.09s/it] 44%|████▎     | 1108/2541 [18:48:17<24:19:15, 61.10s/it] 44%|████▎     | 1109/2541 [18:49:18<24:17:10, 61.05s/it] 44%|████▎     | 1110/2541 [18:50:19<24:17:23, 61.11s/it]                                                         {'loss': 1.4039, 'learning_rate': 2.992840414717899e-05, 'epoch': 0.44}
 44%|████▎     | 1110/2541 [18:50:19<24:17:23, 61.11s/it] 44%|████▎     | 1111/2541 [18:51:20<24:16:59, 61.13s/it] 44%|████▍     | 1112/2541 [18:52:21<24:18:13, 61.23s/it] 44%|████▍     | 1113/2541 [18:53:22<24:14:56, 61.13s/it] 44%|████▍     | 1114/2541 [18:54:23<24:12:31, 61.07s/it] 44%|████▍     | 1115/2541 [18:55:25<24:13:40, 61.16s/it] 44%|████▍     | 1116/2541 [18:56:26<24:13:04, 61.18s/it] 44%|████▍     | 1117/2541 [18:57:27<24:09:18, 61.07s/it] 44%|████▍     | 1118/2541 [18:58:28<24:10:05, 61.14s/it] 44%|████▍     | 1119/2541 [18:59:29<24:07:51, 61.09s/it] 44%|████▍     | 1120/2541 [19:00:30<24:05:07, 61.02s/it]                                                         {'loss': 1.4161, 'learning_rate': 2.9625010560110788e-05, 'epoch': 0.44}
 44%|████▍     | 1120/2541 [19:00:30<24:05:07, 61.02s/it] 44%|████▍     | 1121/2541 [19:01:31<24:04:01, 61.02s/it] 44%|████▍     | 1122/2541 [19:02:32<24:05:19, 61.11s/it] 44%|████▍     | 1123/2541 [19:03:33<24:03:22, 61.07s/it] 44%|████▍     | 1124/2541 [19:04:34<24:01:19, 61.03s/it] 44%|████▍     | 1125/2541 [19:05:35<24:02:18, 61.11s/it] 44%|████▍     | 1126/2541 [19:06:37<24:01:41, 61.13s/it] 44%|████▍     | 1127/2541 [19:07:38<23:59:39, 61.09s/it] 44%|████▍     | 1128/2541 [19:08:39<24:04:03, 61.32s/it] 44%|████▍     | 1129/2541 [19:09:41<24:03:32, 61.34s/it] 44%|████▍     | 1130/2541 [19:10:42<24:00:42, 61.26s/it]                                                         {'loss': 1.3943, 'learning_rate': 2.9320910008533735e-05, 'epoch': 0.44}
 44%|████▍     | 1130/2541 [19:10:42<24:00:42, 61.26s/it] 45%|████▍     | 1131/2541 [19:11:43<23:57:42, 61.18s/it] 45%|████▍     | 1132/2541 [19:12:44<23:57:35, 61.22s/it] 45%|████▍     | 1133/2541 [19:13:45<23:56:59, 61.24s/it] 45%|████▍     | 1134/2541 [19:14:47<23:55:12, 61.20s/it] 45%|████▍     | 1135/2541 [19:15:48<23:54:37, 61.22s/it] 45%|████▍     | 1136/2541 [19:16:49<23:53:39, 61.22s/it] 45%|████▍     | 1137/2541 [19:17:50<23:50:48, 61.15s/it] 45%|████▍     | 1138/2541 [19:18:51<23:50:08, 61.16s/it] 45%|████▍     | 1139/2541 [19:19:53<23:52:58, 61.33s/it] 45%|████▍     | 1140/2541 [19:20:54<23:49:15, 61.21s/it]                                                         {'loss': 1.3998, 'learning_rate': 2.9016148976297832e-05, 'epoch': 0.45}
 45%|████▍     | 1140/2541 [19:20:54<23:49:15, 61.21s/it] 45%|████▍     | 1141/2541 [19:21:55<23:45:52, 61.11s/it] 45%|████▍     | 1142/2541 [19:22:56<23:45:09, 61.12s/it] 45%|████▍     | 1143/2541 [19:23:57<23:45:44, 61.19s/it] 45%|████▌     | 1144/2541 [19:24:58<23:42:16, 61.09s/it] 45%|████▌     | 1145/2541 [19:25:59<23:43:03, 61.16s/it] 45%|████▌     | 1146/2541 [19:27:01<23:43:33, 61.23s/it] 45%|████▌     | 1147/2541 [19:28:02<23:41:08, 61.17s/it] 45%|████▌     | 1148/2541 [19:29:03<23:38:08, 61.08s/it] 45%|████▌     | 1149/2541 [19:30:04<23:36:35, 61.06s/it] 45%|████▌     | 1150/2541 [19:31:05<23:39:48, 61.24s/it]                                                         {'loss': 1.4045, 'learning_rate': 2.871077404821204e-05, 'epoch': 0.45}
 45%|████▌     | 1150/2541 [19:31:05<23:39:48, 61.24s/it] 45%|████▌     | 1151/2541 [19:32:07<23:39:36, 61.28s/it] 45%|████▌     | 1152/2541 [19:33:08<23:35:40, 61.15s/it] 45%|████▌     | 1153/2541 [19:34:09<23:35:16, 61.18s/it] 45%|████▌     | 1154/2541 [19:35:10<23:33:20, 61.14s/it] 45%|████▌     | 1155/2541 [19:36:11<23:30:47, 61.07s/it] 45%|████▌     | 1156/2541 [19:37:12<23:28:39, 61.02s/it] 46%|████▌     | 1157/2541 [19:38:13<23:32:25, 61.23s/it] 46%|████▌     | 1158/2541 [19:39:15<23:31:28, 61.24s/it] 46%|████▌     | 1159/2541 [19:40:16<23:28:38, 61.16s/it] 46%|████▌     | 1160/2541 [19:41:17<23:29:15, 61.23s/it]                                                         {'loss': 1.4058, 'learning_rate': 2.840483190292353e-05, 'epoch': 0.46}
 46%|████▌     | 1160/2541 [19:41:17<23:29:15, 61.23s/it] 46%|████▌     | 1161/2541 [19:42:18<23:28:46, 61.25s/it] 46%|████▌     | 1162/2541 [19:43:20<23:27:59, 61.26s/it] 46%|████▌     | 1163/2541 [19:44:21<23:24:38, 61.16s/it] 46%|████▌     | 1164/2541 [19:45:22<23:25:04, 61.22s/it] 46%|████▌     | 1165/2541 [19:46:23<23:21:59, 61.13s/it] 46%|████▌     | 1166/2541 [19:47:24<23:19:30, 61.07s/it] 46%|████▌     | 1167/2541 [19:48:25<23:18:29, 61.07s/it] 46%|████▌     | 1168/2541 [19:49:27<23:21:20, 61.24s/it] 46%|████▌     | 1169/2541 [19:50:28<23:19:33, 61.20s/it] 46%|████▌     | 1170/2541 [19:51:28<23:15:58, 61.09s/it]                                                         {'loss': 1.3915, 'learning_rate': 2.809836930578249e-05, 'epoch': 0.46}
 46%|████▌     | 1170/2541 [19:51:28<23:15:58, 61.09s/it] 46%|████▌     | 1171/2541 [19:52:30<23:16:00, 61.14s/it] 46%|████▌     | 1172/2541 [19:53:31<23:16:25, 61.20s/it] 46%|████▌     | 1173/2541 [19:54:32<23:12:57, 61.09s/it] 46%|████▌     | 1174/2541 [19:55:33<23:12:53, 61.14s/it] 46%|████▌     | 1175/2541 [19:56:35<23:15:04, 61.28s/it] 46%|████▋     | 1176/2541 [19:57:36<23:12:43, 61.22s/it] 46%|████▋     | 1177/2541 [19:58:37<23:09:06, 61.10s/it] 46%|████▋     | 1178/2541 [19:59:38<23:06:29, 61.03s/it] 46%|████▋     | 1179/2541 [20:00:39<23:08:41, 61.18s/it] 46%|████▋     | 1180/2541 [20:01:40<23:07:18, 61.16s/it]                                                         {'loss': 1.3989, 'learning_rate': 2.779143310169375e-05, 'epoch': 0.46}
 46%|████▋     | 1180/2541 [20:01:40<23:07:18, 61.16s/it] 46%|████▋     | 1181/2541 [20:02:41<23:04:06, 61.06s/it] 47%|████▋     | 1182/2541 [20:03:42<23:05:08, 61.15s/it] 47%|████▋     | 1183/2541 [20:04:44<23:05:58, 61.24s/it] 47%|████▋     | 1184/2541 [20:05:45<23:02:28, 61.13s/it] 47%|████▋     | 1185/2541 [20:06:46<23:02:15, 61.16s/it] 47%|████▋     | 1186/2541 [20:07:47<23:02:04, 61.20s/it] 47%|████▋     | 1187/2541 [20:08:48<23:00:23, 61.17s/it] 47%|████▋     | 1188/2541 [20:09:49<22:57:06, 61.07s/it] 47%|████▋     | 1189/2541 [20:10:50<22:57:18, 61.12s/it] 47%|████▋     | 1190/2541 [20:11:52<22:57:35, 61.18s/it]                                                         {'loss': 1.4061, 'learning_rate': 2.7484070207956168e-05, 'epoch': 0.47}
 47%|████▋     | 1190/2541 [20:11:52<22:57:35, 61.18s/it] 47%|████▋     | 1191/2541 [20:12:53<22:56:52, 61.19s/it] 47%|████▋     | 1192/2541 [20:13:54<22:55:27, 61.18s/it] 47%|████▋     | 1193/2541 [20:14:55<22:55:08, 61.21s/it] 47%|████▋     | 1194/2541 [20:15:56<22:52:46, 61.15s/it] 47%|████▋     | 1195/2541 [20:16:57<22:51:18, 61.13s/it] 47%|████▋     | 1196/2541 [20:17:59<22:50:32, 61.14s/it] 47%|████▋     | 1197/2541 [20:19:00<22:52:20, 61.26s/it] 47%|████▋     | 1198/2541 [20:20:01<22:51:03, 61.25s/it] 47%|████▋     | 1199/2541 [20:21:02<22:47:58, 61.16s/it] 47%|████▋     | 1200/2541 [20:22:04<22:48:23, 61.23s/it]                                                         {'loss': 1.4059, 'learning_rate': 2.7176327607091075e-05, 'epoch': 0.47}
 47%|████▋     | 1200/2541 [20:22:04<22:48:23, 61.23s/it] 47%|████▋     | 1201/2541 [20:23:05<22:45:21, 61.14s/it] 47%|████▋     | 1202/2541 [20:24:06<22:42:46, 61.07s/it] 47%|████▋     | 1203/2541 [20:25:07<22:45:25, 61.23s/it] 47%|████▋     | 1204/2541 [20:26:09<22:45:27, 61.28s/it] 47%|████▋     | 1205/2541 [20:27:09<22:42:00, 61.17s/it] 47%|████▋     | 1206/2541 [20:28:10<22:39:15, 61.09s/it] 48%|████▊     | 1207/2541 [20:29:12<22:38:45, 61.11s/it] 48%|████▊     | 1208/2541 [20:30:13<22:38:11, 61.13s/it] 48%|████▊     | 1209/2541 [20:31:14<22:36:15, 61.09s/it] 48%|████▊     | 1210/2541 [20:32:15<22:35:41, 61.11s/it]                                                         {'loss': 1.3948, 'learning_rate': 2.686825233966061e-05, 'epoch': 0.48}
 48%|████▊     | 1210/2541 [20:32:15<22:35:41, 61.11s/it] 48%|████▊     | 1211/2541 [20:33:16<22:35:27, 61.15s/it] 48%|████▊     | 1212/2541 [20:34:17<22:33:56, 61.13s/it] 48%|████▊     | 1213/2541 [20:35:18<22:32:53, 61.12s/it] 48%|████▊     | 1214/2541 [20:36:20<22:36:26, 61.33s/it] 48%|████▊     | 1215/2541 [20:37:21<22:35:01, 61.31s/it] 48%|████▊     | 1216/2541 [20:38:22<22:32:13, 61.23s/it] 48%|████▊     | 1217/2541 [20:39:24<22:31:04, 61.23s/it] 48%|████▊     | 1218/2541 [20:40:25<22:30:18, 61.24s/it] 48%|████▊     | 1219/2541 [20:41:26<22:26:45, 61.12s/it] 48%|████▊     | 1220/2541 [20:42:27<22:25:44, 61.12s/it]                                                         {'loss': 1.3985, 'learning_rate': 2.655989149707728e-05, 'epoch': 0.48}
 48%|████▊     | 1220/2541 [20:42:27<22:25:44, 61.12s/it] 48%|████▊     | 1221/2541 [20:43:29<22:28:35, 61.30s/it] 48%|████▊     | 1222/2541 [20:44:30<22:25:38, 61.21s/it] 48%|████▊     | 1223/2541 [20:45:31<22:23:40, 61.17s/it] 48%|████▊     | 1224/2541 [20:46:32<22:21:30, 61.12s/it] 48%|████▊     | 1225/2541 [20:47:33<22:24:01, 61.28s/it] 48%|████▊     | 1226/2541 [20:48:35<22:22:29, 61.25s/it] 48%|████▊     | 1227/2541 [20:49:35<22:19:24, 61.16s/it] 48%|████▊     | 1228/2541 [20:50:37<22:18:51, 61.18s/it] 48%|████▊     | 1229/2541 [20:51:38<22:17:19, 61.16s/it] 48%|████▊     | 1230/2541 [20:52:39<22:15:07, 61.10s/it]                                                         {'loss': 1.3991, 'learning_rate': 2.625129221440569e-05, 'epoch': 0.48}
 48%|████▊     | 1230/2541 [20:52:39<22:15:07, 61.10s/it] 48%|████▊     | 1231/2541 [20:53:40<22:15:35, 61.17s/it] 48%|████▊     | 1232/2541 [20:54:42<22:17:50, 61.32s/it] 49%|████▊     | 1233/2541 [20:55:43<22:15:44, 61.27s/it] 49%|████▊     | 1234/2541 [20:56:44<22:12:31, 61.17s/it] 49%|████▊     | 1235/2541 [20:57:45<22:14:21, 61.30s/it] 49%|████▊     | 1236/2541 [20:58:47<22:12:08, 61.25s/it] 49%|████▊     | 1237/2541 [20:59:48<22:08:56, 61.15s/it] 49%|████▊     | 1238/2541 [21:00:49<22:10:41, 61.28s/it] 49%|████▉     | 1239/2541 [21:01:51<22:11:35, 61.36s/it] 49%|████▉     | 1240/2541 [21:02:52<22:07:26, 61.22s/it]                                                         {'loss': 1.3931, 'learning_rate': 2.5942501663157687e-05, 'epoch': 0.49}
 49%|████▉     | 1240/2541 [21:02:52<22:07:26, 61.22s/it] 49%|████▉     | 1241/2541 [21:03:52<22:04:30, 61.13s/it] 49%|████▉     | 1242/2541 [21:04:54<22:04:14, 61.17s/it] 49%|████▉     | 1243/2541 [21:05:55<22:02:08, 61.12s/it] 49%|████▉     | 1244/2541 [21:06:56<22:01:48, 61.15s/it] 49%|████▉     | 1245/2541 [21:07:57<22:00:30, 61.13s/it] 49%|████▉     | 1246/2541 [21:08:59<22:03:12, 61.31s/it] 49%|████▉     | 1247/2541 [21:10:00<22:00:54, 61.25s/it] 49%|████▉     | 1248/2541 [21:11:01<21:57:50, 61.15s/it] 49%|████▉     | 1249/2541 [21:12:03<22:03:09, 61.45s/it] 49%|████▉     | 1250/2541 [21:13:04<21:59:23, 61.32s/it]                                                         {'loss': 1.3866, 'learning_rate': 2.5633567044081784e-05, 'epoch': 0.49}
 49%|████▉     | 1250/2541 [21:13:04<21:59:23, 61.32s/it] 49%|████▉     | 1251/2541 [21:14:05<21:57:03, 61.26s/it] 49%|████▉     | 1252/2541 [21:15:06<21:55:50, 61.25s/it] 49%|████▉     | 1253/2541 [21:16:08<21:56:00, 61.30s/it] 49%|████▉     | 1254/2541 [21:17:09<21:54:19, 61.27s/it] 49%|████▉     | 1255/2541 [21:18:10<21:51:44, 61.20s/it] 49%|████▉     | 1256/2541 [21:19:12<21:52:51, 61.30s/it] 49%|████▉     | 1257/2541 [21:20:13<21:50:35, 61.24s/it] 50%|████▉     | 1258/2541 [21:21:13<21:47:16, 61.14s/it] 50%|████▉     | 1259/2541 [21:22:14<21:45:13, 61.09s/it] 50%|████▉     | 1260/2541 [21:23:16<21:46:26, 61.19s/it]                                                         {'loss': 1.3878, 'learning_rate': 2.5324535579948274e-05, 'epoch': 0.5}
 50%|████▉     | 1260/2541 [21:23:16<21:46:26, 61.19s/it] 50%|████▉     | 1261/2541 [21:24:17<21:44:49, 61.16s/it] 50%|████▉     | 1262/2541 [21:25:18<21:42:18, 61.09s/it] 50%|████▉     | 1263/2541 [21:26:19<21:42:12, 61.14s/it] 50%|████▉     | 1264/2541 [21:27:20<21:39:17, 61.05s/it] 50%|████▉     | 1265/2541 [21:28:21<21:38:23, 61.05s/it] 50%|████▉     | 1266/2541 [21:29:22<21:37:15, 61.05s/it] 50%|████▉     | 1267/2541 [21:30:24<21:39:26, 61.20s/it] 50%|████▉     | 1268/2541 [21:31:25<21:36:34, 61.11s/it] 50%|████▉     | 1269/2541 [21:32:25<21:33:54, 61.03s/it] 50%|████▉     | 1270/2541 [21:33:27<21:34:37, 61.12s/it]                                                         {'loss': 1.3941, 'learning_rate': 2.5015454508330864e-05, 'epoch': 0.5}
 50%|████▉     | 1270/2541 [21:33:27<21:34:37, 61.12s/it] 50%|█████     | 1271/2541 [21:34:28<21:32:06, 61.04s/it] 50%|█████     | 1272/2541 [21:35:29<21:31:56, 61.09s/it] 50%|█████     | 1273/2541 [21:36:30<21:31:31, 61.11s/it] 50%|█████     | 1274/2541 [21:37:31<21:32:48, 61.22s/it] 50%|█████     | 1275/2541 [21:38:32<21:29:40, 61.12s/it] 50%|█████     | 1276/2541 [21:39:33<21:27:37, 61.07s/it] 50%|█████     | 1277/2541 [21:40:34<21:27:24, 61.11s/it] 50%|█████     | 1278/2541 [21:41:36<21:27:07, 61.15s/it] 50%|█████     | 1279/2541 [21:42:36<21:23:24, 61.02s/it] 50%|█████     | 1280/2541 [21:43:37<21:21:25, 60.97s/it]                                                         {'loss': 1.3936, 'learning_rate': 2.4706371074386113e-05, 'epoch': 0.5}
 50%|█████     | 1280/2541 [21:43:37<21:21:25, 60.97s/it] 50%|█████     | 1281/2541 [21:44:39<21:22:33, 61.07s/it] 50%|█████     | 1282/2541 [21:45:39<21:19:54, 61.00s/it] 50%|█████     | 1283/2541 [21:46:40<21:19:12, 61.01s/it] 51%|█████     | 1284/2541 [21:47:42<21:21:47, 61.18s/it] 51%|█████     | 1285/2541 [21:48:43<21:19:49, 61.14s/it] 51%|█████     | 1286/2541 [21:49:44<21:17:39, 61.08s/it] 51%|█████     | 1287/2541 [21:50:45<21:15:45, 61.04s/it] 51%|█████     | 1288/2541 [21:51:46<21:16:05, 61.11s/it] 51%|█████     | 1289/2541 [21:52:47<21:13:30, 61.03s/it] 51%|█████     | 1290/2541 [21:53:48<21:13:24, 61.08s/it]                                                         {'loss': 1.3825, 'learning_rate': 2.4397332523631684e-05, 'epoch': 0.51}
 51%|█████     | 1290/2541 [21:53:48<21:13:24, 61.08s/it] 51%|█████     | 1291/2541 [21:54:49<21:13:16, 61.12s/it] 51%|█████     | 1292/2541 [21:55:51<21:12:05, 61.11s/it] 51%|█████     | 1293/2541 [21:56:51<21:09:24, 61.03s/it] 51%|█████     | 1294/2541 [21:57:53<21:09:59, 61.11s/it] 51%|█████     | 1295/2541 [21:58:54<21:11:20, 61.22s/it] 51%|█████     | 1296/2541 [21:59:55<21:09:19, 61.17s/it] 51%|█████     | 1297/2541 [22:00:56<21:05:42, 61.05s/it] 51%|█████     | 1298/2541 [22:01:57<21:06:43, 61.14s/it] 51%|█████     | 1299/2541 [22:02:58<21:03:55, 61.06s/it] 51%|█████     | 1300/2541 [22:03:59<21:02:02, 61.02s/it]                                                         {'loss': 1.3945, 'learning_rate': 2.4088386094724517e-05, 'epoch': 0.51}
 51%|█████     | 1300/2541 [22:03:59<21:02:02, 61.02s/it] 51%|█████     | 1301/2541 [22:05:00<21:02:33, 61.09s/it] 51%|█████     | 1302/2541 [22:06:02<21:04:15, 61.22s/it] 51%|█████▏    | 1303/2541 [22:07:03<21:02:00, 61.16s/it] 51%|█████▏    | 1304/2541 [22:08:04<20:59:01, 61.07s/it] 51%|█████▏    | 1305/2541 [22:09:05<20:59:19, 61.13s/it] 51%|█████▏    | 1306/2541 [22:10:06<20:57:31, 61.09s/it] 51%|█████▏    | 1307/2541 [22:11:07<20:57:26, 61.14s/it] 51%|█████▏    | 1308/2541 [22:12:09<20:57:11, 61.18s/it] 52%|█████▏    | 1309/2541 [22:13:10<20:59:27, 61.34s/it] 52%|█████▏    | 1310/2541 [22:14:11<20:55:37, 61.20s/it]                                                         {'loss': 1.386, 'learning_rate': 2.377957901224012e-05, 'epoch': 0.52}
 52%|█████▏    | 1310/2541 [22:14:11<20:55:37, 61.20s/it] 52%|█████▏    | 1311/2541 [22:15:12<20:52:32, 61.10s/it] 52%|█████▏    | 1312/2541 [22:16:13<20:52:51, 61.17s/it] 52%|█████▏    | 1313/2541 [22:17:15<20:52:33, 61.20s/it] 52%|█████▏    | 1314/2541 [22:18:15<20:49:07, 61.08s/it] 52%|█████▏    | 1315/2541 [22:19:16<20:47:02, 61.03s/it] 52%|█████▏    | 1316/2541 [22:20:18<20:48:34, 61.15s/it] 52%|█████▏    | 1317/2541 [22:21:19<20:46:05, 61.08s/it] 52%|█████▏    | 1318/2541 [22:22:20<20:45:04, 61.08s/it] 52%|█████▏    | 1319/2541 [22:23:21<20:46:28, 61.20s/it] 52%|█████▏    | 1320/2541 [22:24:22<20:43:54, 61.13s/it]                                                         {'loss': 1.389, 'learning_rate': 2.3470958479453938e-05, 'epoch': 0.52}
 52%|█████▏    | 1320/2541 [22:24:22<20:43:54, 61.13s/it] 52%|█████▏    | 1321/2541 [22:25:23<20:41:12, 61.04s/it] 52%|█████▏    | 1322/2541 [22:26:24<20:41:07, 61.09s/it] 52%|█████▏    | 1323/2541 [22:27:26<20:42:43, 61.22s/it] 52%|█████▏    | 1324/2541 [22:28:27<20:40:08, 61.14s/it] 52%|█████▏    | 1325/2541 [22:29:28<20:37:48, 61.08s/it] 52%|█████▏    | 1326/2541 [22:30:29<20:39:07, 61.19s/it] 52%|█████▏    | 1327/2541 [22:31:30<20:37:04, 61.14s/it] 52%|█████▏    | 1328/2541 [22:32:31<20:34:00, 61.04s/it] 52%|█████▏    | 1329/2541 [22:33:32<20:33:53, 61.08s/it] 52%|█████▏    | 1330/2541 [22:34:34<20:35:43, 61.23s/it]                                                         {'loss': 1.3841, 'learning_rate': 2.3162571671126005e-05, 'epoch': 0.52}
 52%|█████▏    | 1330/2541 [22:34:34<20:35:43, 61.23s/it] 52%|█████▏    | 1331/2541 [22:35:35<20:32:42, 61.13s/it] 52%|█████▏    | 1332/2541 [22:36:36<20:30:18, 61.06s/it] 52%|█████▏    | 1333/2541 [22:37:37<20:32:51, 61.23s/it] 52%|█████▏    | 1334/2541 [22:38:38<20:30:40, 61.18s/it] 53%|█████▎    | 1335/2541 [22:39:39<20:28:08, 61.10s/it] 53%|█████▎    | 1336/2541 [22:40:41<20:29:00, 61.20s/it] 53%|█████▎    | 1337/2541 [22:41:42<20:28:55, 61.24s/it] 53%|█████▎    | 1338/2541 [22:42:43<20:25:29, 61.12s/it] 53%|█████▎    | 1339/2541 [22:43:44<20:22:09, 61.01s/it] 53%|█████▎    | 1340/2541 [22:44:45<20:23:01, 61.10s/it]                                                         {'loss': 1.3842, 'learning_rate': 2.2854465726289987e-05, 'epoch': 0.53}
 53%|█████▎    | 1340/2541 [22:44:45<20:23:01, 61.10s/it] 53%|█████▎    | 1341/2541 [22:45:46<20:21:51, 61.09s/it] 53%|█████▎    | 1342/2541 [22:46:47<20:22:17, 61.17s/it] 53%|█████▎    | 1343/2541 [22:47:48<20:21:47, 61.19s/it] 53%|█████▎    | 1344/2541 [22:48:50<20:23:12, 61.31s/it] 53%|█████▎    | 1345/2541 [22:49:51<20:21:02, 61.26s/it] 53%|█████▎    | 1346/2541 [22:50:52<20:18:05, 61.16s/it] 53%|█████▎    | 1347/2541 [22:51:54<20:19:36, 61.29s/it] 53%|█████▎    | 1348/2541 [22:52:55<20:15:47, 61.15s/it] 53%|█████▎    | 1349/2541 [22:53:56<20:14:35, 61.14s/it] 53%|█████▎    | 1350/2541 [22:54:57<20:13:03, 61.11s/it]                                                         {'loss': 1.3814, 'learning_rate': 2.2546687741047645e-05, 'epoch': 0.53}
 53%|█████▎    | 1350/2541 [22:54:57<20:13:03, 61.11s/it] 53%|█████▎    | 1351/2541 [22:55:58<20:15:14, 61.27s/it] 53%|█████▎    | 1352/2541 [22:56:59<20:12:50, 61.20s/it] 53%|█████▎    | 1353/2541 [22:58:01<20:11:28, 61.19s/it] 53%|█████▎    | 1354/2541 [22:59:02<20:12:18, 61.28s/it] 53%|█████▎    | 1355/2541 [23:00:03<20:10:27, 61.24s/it] 53%|█████▎    | 1356/2541 [23:01:04<20:07:03, 61.12s/it] 53%|█████▎    | 1357/2541 [23:02:05<20:06:36, 61.15s/it] 53%|█████▎    | 1358/2541 [23:03:06<20:06:09, 61.17s/it] 53%|█████▎    | 1359/2541 [23:04:08<20:04:47, 61.16s/it] 54%|█████▎    | 1360/2541 [23:05:09<20:03:42, 61.15s/it]                                                         {'loss': 1.3815, 'learning_rate': 2.2239284761369868e-05, 'epoch': 0.54}
 54%|█████▎    | 1360/2541 [23:05:09<20:03:42, 61.15s/it] 54%|█████▎    | 1361/2541 [23:06:10<20:04:10, 61.23s/it] 54%|█████▎    | 1362/2541 [23:07:11<20:03:11, 61.23s/it] 54%|█████▎    | 1363/2541 [23:08:13<20:02:32, 61.25s/it] 54%|█████▎    | 1364/2541 [23:09:14<20:01:22, 61.24s/it] 54%|█████▎    | 1365/2541 [23:10:16<20:03:05, 61.38s/it] 54%|█████▍    | 1366/2541 [23:11:17<19:59:18, 61.24s/it] 54%|█████▍    | 1367/2541 [23:12:17<19:56:36, 61.16s/it] 54%|█████▍    | 1368/2541 [23:13:19<19:56:39, 61.21s/it] 54%|█████▍    | 1369/2541 [23:14:20<19:54:50, 61.17s/it] 54%|█████▍    | 1370/2541 [23:15:21<19:53:40, 61.16s/it]                                                         {'loss': 1.3885, 'learning_rate': 2.1932303775905373e-05, 'epoch': 0.54}
 54%|█████▍    | 1370/2541 [23:15:21<19:53:40, 61.16s/it] 54%|█████▍    | 1371/2541 [23:16:22<19:52:38, 61.16s/it] 54%|█████▍    | 1372/2541 [23:17:24<19:53:45, 61.27s/it] 54%|█████▍    | 1373/2541 [23:18:25<19:51:04, 61.19s/it] 54%|█████▍    | 1374/2541 [23:19:26<19:48:31, 61.11s/it] 54%|█████▍    | 1375/2541 [23:20:27<19:47:41, 61.12s/it] 54%|█████▍    | 1376/2541 [23:21:28<19:48:12, 61.20s/it] 54%|█████▍    | 1377/2541 [23:22:29<19:47:47, 61.23s/it] 54%|█████▍    | 1378/2541 [23:23:30<19:45:11, 61.15s/it] 54%|█████▍    | 1379/2541 [23:24:32<19:45:08, 61.20s/it] 54%|█████▍    | 1380/2541 [23:25:33<19:42:15, 61.10s/it]                                                         {'loss': 1.3797, 'learning_rate': 2.1625791708798188e-05, 'epoch': 0.54}
 54%|█████▍    | 1380/2541 [23:25:33<19:42:15, 61.10s/it] 54%|█████▍    | 1381/2541 [23:26:33<19:40:07, 61.04s/it] 54%|█████▍    | 1382/2541 [23:27:35<19:42:02, 61.19s/it] 54%|█████▍    | 1383/2541 [23:28:36<19:42:00, 61.24s/it] 54%|█████▍    | 1384/2541 [23:29:37<19:39:06, 61.15s/it] 55%|█████▍    | 1385/2541 [23:30:39<19:38:39, 61.18s/it] 55%|█████▍    | 1386/2541 [23:31:40<19:38:10, 61.20s/it] 55%|█████▍    | 1387/2541 [23:32:41<19:35:53, 61.14s/it] 55%|█████▍    | 1388/2541 [23:33:42<19:36:10, 61.21s/it] 55%|█████▍    | 1389/2541 [23:34:44<19:36:25, 61.27s/it] 55%|█████▍    | 1390/2541 [23:35:45<19:35:04, 61.26s/it]                                                         {'loss': 1.3873, 'learning_rate': 2.1319795412514947e-05, 'epoch': 0.55}
 55%|█████▍    | 1390/2541 [23:35:45<19:35:04, 61.26s/it] 55%|█████▍    | 1391/2541 [23:36:46<19:32:08, 61.16s/it] 55%|█████▍    | 1392/2541 [23:37:47<19:30:37, 61.13s/it] 55%|█████▍    | 1393/2541 [23:38:48<19:31:49, 61.24s/it] 55%|█████▍    | 1394/2541 [23:39:49<19:29:14, 61.16s/it] 55%|█████▍    | 1395/2541 [23:40:50<19:26:41, 61.08s/it] 55%|█████▍    | 1396/2541 [23:41:52<19:27:10, 61.16s/it] 55%|█████▍    | 1397/2541 [23:42:52<19:24:48, 61.09s/it] 55%|█████▌    | 1398/2541 [23:43:53<19:22:15, 61.01s/it] 55%|█████▌    | 1399/2541 [23:44:55<19:23:00, 61.10s/it] 55%|█████▌    | 1400/2541 [23:45:56<19:23:56, 61.21s/it]                                                         {'loss': 1.3895, 'learning_rate': 2.1014361660683195e-05, 'epoch': 0.55}
 55%|█████▌    | 1400/2541 [23:45:56<19:23:56, 61.21s/it] 55%|█████▌    | 1401/2541 [23:46:57<19:20:42, 61.09s/it] 55%|█████▌    | 1402/2541 [23:47:58<19:18:11, 61.01s/it] 55%|█████▌    | 1403/2541 [23:48:59<19:17:49, 61.05s/it] 55%|█████▌    | 1404/2541 [23:50:00<19:17:10, 61.06s/it] 55%|█████▌    | 1405/2541 [23:51:01<19:16:06, 61.06s/it] 55%|█████▌    | 1406/2541 [23:52:02<19:15:31, 61.08s/it] 55%|█████▌    | 1407/2541 [23:53:04<19:16:09, 61.17s/it] 55%|█████▌    | 1408/2541 [23:54:04<19:13:38, 61.09s/it] 55%|█████▌    | 1409/2541 [23:55:06<19:12:40, 61.10s/it] 55%|█████▌    | 1410/2541 [23:56:07<19:11:43, 61.10s/it]                                                         {'loss': 1.3853, 'learning_rate': 2.0709537140941705e-05, 'epoch': 0.55}
 55%|█████▌    | 1410/2541 [23:56:07<19:11:43, 61.10s/it] 56%|█████▌    | 1411/2541 [23:57:08<19:13:25, 61.24s/it] 56%|█████▌    | 1412/2541 [23:58:09<19:10:25, 61.14s/it] 56%|█████▌    | 1413/2541 [23:59:10<19:07:55, 61.06s/it] 56%|█████▌    | 1414/2541 [24:00:11<19:08:16, 61.13s/it] 56%|█████▌    | 1415/2541 [24:01:13<19:08:16, 61.19s/it] 56%|█████▌    | 1416/2541 [24:02:14<19:07:15, 61.19s/it] 56%|█████▌    | 1417/2541 [24:03:15<19:06:17, 61.19s/it] 56%|█████▌    | 1418/2541 [24:04:16<19:05:27, 61.20s/it] 56%|█████▌    | 1419/2541 [24:05:17<19:03:13, 61.13s/it] 56%|█████▌    | 1420/2541 [24:06:18<19:00:29, 61.04s/it]                                                         {'loss': 1.3788, 'learning_rate': 2.040536844780395e-05, 'epoch': 0.56}
 56%|█████▌    | 1420/2541 [24:06:18<19:00:29, 61.04s/it] 56%|█████▌    | 1421/2541 [24:07:19<19:00:14, 61.08s/it] 56%|█████▌    | 1422/2541 [24:08:21<19:01:00, 61.18s/it] 56%|█████▌    | 1423/2541 [24:09:22<19:00:51, 61.23s/it] 56%|█████▌    | 1424/2541 [24:10:23<18:58:09, 61.14s/it] 56%|█████▌    | 1425/2541 [24:11:24<18:56:35, 61.11s/it] 56%|█████▌    | 1426/2541 [24:12:25<18:56:34, 61.16s/it] 56%|█████▌    | 1427/2541 [24:13:26<18:54:51, 61.12s/it] 56%|█████▌    | 1428/2541 [24:14:27<18:52:47, 61.07s/it] 56%|█████▌    | 1429/2541 [24:15:29<18:54:09, 61.20s/it] 56%|█████▋    | 1430/2541 [24:16:30<18:55:02, 61.30s/it]                                                         {'loss': 1.3805, 'learning_rate': 2.0101902075535834e-05, 'epoch': 0.56}
 56%|█████▋    | 1430/2541 [24:16:30<18:55:02, 61.30s/it] 56%|█████▋    | 1431/2541 [24:17:31<18:51:58, 61.19s/it] 56%|█████▋    | 1432/2541 [24:18:32<18:50:18, 61.15s/it] 56%|█████▋    | 1433/2541 [24:19:33<18:49:08, 61.15s/it] 56%|█████▋    | 1434/2541 [24:20:35<18:51:03, 61.30s/it] 56%|█████▋    | 1435/2541 [24:21:36<18:47:32, 61.17s/it] 57%|█████▋    | 1436/2541 [24:22:37<18:44:54, 61.08s/it] 57%|█████▋    | 1437/2541 [24:23:38<18:44:14, 61.10s/it] 57%|█████▋    | 1438/2541 [24:24:39<18:43:55, 61.14s/it] 57%|█████▋    | 1439/2541 [24:25:40<18:42:59, 61.14s/it] 57%|█████▋    | 1440/2541 [24:26:41<18:40:30, 61.06s/it]                                                         {'loss': 1.3729, 'learning_rate': 1.9799184411048695e-05, 'epoch': 0.57}
 57%|█████▋    | 1440/2541 [24:26:41<18:40:30, 61.06s/it] 57%|█████▋    | 1441/2541 [24:27:43<18:44:04, 61.31s/it] 57%|█████▋    | 1442/2541 [24:28:44<18:43:52, 61.36s/it] 57%|█████▋    | 1443/2541 [24:29:46<18:41:17, 61.27s/it] 57%|█████▋    | 1444/2541 [24:30:46<18:37:54, 61.14s/it] 57%|█████▋    | 1445/2541 [24:31:48<18:37:46, 61.19s/it] 57%|█████▋    | 1446/2541 [24:32:49<18:39:41, 61.35s/it] 57%|█████▋    | 1447/2541 [24:33:50<18:36:19, 61.22s/it] 57%|█████▋    | 1448/2541 [24:34:51<18:33:53, 61.15s/it] 57%|█████▋    | 1449/2541 [24:35:52<18:32:36, 61.13s/it] 57%|█████▋    | 1450/2541 [24:36:53<18:29:50, 61.04s/it]                                                         {'loss': 1.3699, 'learning_rate': 1.9497261726808777e-05, 'epoch': 0.57}
 57%|█████▋    | 1450/2541 [24:36:53<18:29:50, 61.04s/it] 57%|█████▋    | 1451/2541 [24:37:54<18:27:53, 60.98s/it] 57%|█████▋    | 1452/2541 [24:38:56<18:29:44, 61.14s/it] 57%|█████▋    | 1453/2541 [24:39:57<18:27:58, 61.10s/it] 57%|█████▋    | 1454/2541 [24:40:58<18:28:50, 61.21s/it] 57%|█████▋    | 1455/2541 [24:41:59<18:26:51, 61.15s/it] 57%|█████▋    | 1456/2541 [24:43:00<18:26:27, 61.19s/it] 57%|█████▋    | 1457/2541 [24:44:02<18:26:13, 61.23s/it] 57%|█████▋    | 1458/2541 [24:45:03<18:23:49, 61.15s/it] 57%|█████▋    | 1459/2541 [24:46:04<18:23:14, 61.18s/it] 57%|█████▋    | 1460/2541 [24:47:05<18:21:05, 61.12s/it]                                                         {'loss': 1.3811, 'learning_rate': 1.9196180173764157e-05, 'epoch': 0.57}
 57%|█████▋    | 1460/2541 [24:47:05<18:21:05, 61.12s/it] 57%|█████▋    | 1461/2541 [24:48:06<18:19:02, 61.06s/it] 58%|█████▊    | 1462/2541 [24:49:07<18:19:16, 61.13s/it] 58%|█████▊    | 1463/2541 [24:50:09<18:20:09, 61.23s/it] 58%|█████▊    | 1464/2541 [24:51:10<18:18:14, 61.18s/it] 58%|█████▊    | 1465/2541 [24:52:11<18:16:10, 61.12s/it] 58%|█████▊    | 1466/2541 [24:53:12<18:15:53, 61.17s/it] 58%|█████▊    | 1467/2541 [24:54:13<18:13:57, 61.12s/it] 58%|█████▊    | 1468/2541 [24:55:14<18:11:46, 61.05s/it] 58%|█████▊    | 1469/2541 [24:56:15<18:12:31, 61.15s/it] 58%|█████▊    | 1470/2541 [24:57:16<18:11:54, 61.17s/it]                                                         {'loss': 1.3739, 'learning_rate': 1.889598577429022e-05, 'epoch': 0.58}
 58%|█████▊    | 1470/2541 [24:57:16<18:11:54, 61.17s/it] 58%|█████▊    | 1471/2541 [24:58:17<18:09:28, 61.09s/it] 58%|█████▊    | 1472/2541 [24:59:18<18:08:04, 61.07s/it] 58%|█████▊    | 1473/2541 [25:00:19<18:06:58, 61.07s/it] 58%|█████▊    | 1474/2541 [25:01:21<18:08:51, 61.23s/it] 58%|█████▊    | 1475/2541 [25:02:22<18:08:01, 61.24s/it] 58%|█████▊    | 1476/2541 [25:03:23<18:04:58, 61.12s/it] 58%|█████▊    | 1477/2541 [25:04:24<18:04:08, 61.14s/it] 58%|█████▊    | 1478/2541 [25:05:25<18:02:02, 61.08s/it] 58%|█████▊    | 1479/2541 [25:06:26<18:01:06, 61.08s/it] 58%|█████▊    | 1480/2541 [25:07:27<18:00:31, 61.10s/it]                                                         {'loss': 1.3707, 'learning_rate': 1.85967244151549e-05, 'epoch': 0.58}
 58%|█████▊    | 1480/2541 [25:07:27<18:00:31, 61.10s/it] 58%|█████▊    | 1481/2541 [25:08:29<18:03:23, 61.32s/it] 58%|█████▊    | 1482/2541 [25:09:31<18:01:51, 61.30s/it] 58%|█████▊    | 1483/2541 [25:10:31<17:58:32, 61.16s/it] 58%|█████▊    | 1484/2541 [25:11:33<17:57:25, 61.16s/it] 58%|█████▊    | 1485/2541 [25:12:34<17:57:07, 61.20s/it] 58%|█████▊    | 1486/2541 [25:13:35<17:55:46, 61.18s/it] 59%|█████▊    | 1487/2541 [25:14:36<17:53:43, 61.12s/it] 59%|█████▊    | 1488/2541 [25:15:37<17:53:03, 61.14s/it] 59%|█████▊    | 1489/2541 [25:16:38<17:51:42, 61.12s/it] 59%|█████▊    | 1490/2541 [25:17:39<17:49:09, 61.04s/it]                                                         {'loss': 1.3756, 'learning_rate': 1.829844184050451e-05, 'epoch': 0.59}
 59%|█████▊    | 1490/2541 [25:17:39<17:49:09, 61.04s/it] 59%|█████▊    | 1491/2541 [25:18:40<17:47:16, 60.99s/it] 59%|█████▊    | 1492/2541 [25:19:41<17:48:45, 61.13s/it] 59%|█████▉    | 1493/2541 [25:20:42<17:46:20, 61.05s/it] 59%|█████▉    | 1494/2541 [25:21:43<17:44:15, 60.99s/it] 59%|█████▉    | 1495/2541 [25:22:44<17:44:29, 61.06s/it] 59%|█████▉    | 1496/2541 [25:23:45<17:43:35, 61.07s/it] 59%|█████▉    | 1497/2541 [25:24:46<17:41:22, 61.00s/it] 59%|█████▉    | 1498/2541 [25:25:48<17:42:28, 61.12s/it] 59%|█████▉    | 1499/2541 [25:26:49<17:41:45, 61.14s/it] 59%|█████▉    | 1500/2541 [25:27:50<17:39:42, 61.08s/it]                                                         {'loss': 1.3811, 'learning_rate': 1.800118364487146e-05, 'epoch': 0.59}
 59%|█████▉    | 1500/2541 [25:27:50<17:39:42, 61.08s/it] 59%|█████▉    | 1501/2541 [25:28:51<17:37:28, 61.01s/it] 59%|█████▉    | 1502/2541 [25:29:52<17:36:46, 61.03s/it] 59%|█████▉    | 1503/2541 [25:30:53<17:38:15, 61.17s/it] 59%|█████▉    | 1504/2541 [25:31:54<17:36:58, 61.16s/it] 59%|█████▉    | 1505/2541 [25:32:55<17:34:24, 61.07s/it] 59%|█████▉    | 1506/2541 [25:33:56<17:33:57, 61.10s/it] 59%|█████▉    | 1507/2541 [25:34:57<17:33:09, 61.11s/it] 59%|█████▉    | 1508/2541 [25:35:58<17:31:36, 61.08s/it] 59%|█████▉    | 1509/2541 [25:37:00<17:31:02, 61.11s/it] 59%|█████▉    | 1510/2541 [25:38:01<17:30:33, 61.14s/it]                                                         {'loss': 1.3739, 'learning_rate': 1.7704995266204824e-05, 'epoch': 0.59}
 59%|█████▉    | 1510/2541 [25:38:01<17:30:33, 61.14s/it] 59%|█████▉    | 1511/2541 [25:39:02<17:28:27, 61.08s/it] 60%|█████▉    | 1512/2541 [25:40:03<17:26:32, 61.02s/it] 60%|█████▉    | 1513/2541 [25:41:04<17:25:27, 61.02s/it] 60%|█████▉    | 1514/2541 [25:42:05<17:25:47, 61.10s/it] 60%|█████▉    | 1515/2541 [25:43:06<17:25:08, 61.12s/it] 60%|█████▉    | 1516/2541 [25:44:07<17:22:39, 61.03s/it] 60%|█████▉    | 1517/2541 [25:45:08<17:21:49, 61.04s/it] 60%|█████▉    | 1518/2541 [25:46:10<17:23:13, 61.19s/it] 60%|█████▉    | 1519/2541 [25:47:11<17:20:56, 61.11s/it] 60%|█████▉    | 1520/2541 [25:48:11<17:18:55, 61.05s/it]                                                         {'loss': 1.3775, 'learning_rate': 1.7409921978924825e-05, 'epoch': 0.6}
 60%|█████▉    | 1520/2541 [25:48:11<17:18:55, 61.05s/it] 60%|█████▉    | 1521/2541 [25:49:13<17:19:16, 61.13s/it] 60%|█████▉    | 1522/2541 [25:50:14<17:19:27, 61.20s/it] 60%|█████▉    | 1523/2541 [25:51:15<17:16:50, 61.11s/it] 60%|█████▉    | 1524/2541 [25:52:16<17:14:39, 61.04s/it] 60%|██████    | 1525/2541 [25:53:17<17:13:42, 61.05s/it] 60%|██████    | 1526/2541 [25:54:18<17:14:10, 61.13s/it] 60%|██████    | 1527/2541 [25:55:19<17:13:36, 61.16s/it] 60%|██████    | 1528/2541 [25:56:20<17:11:28, 61.09s/it] 60%|██████    | 1529/2541 [25:57:21<17:09:58, 61.07s/it] 60%|██████    | 1530/2541 [25:58:23<17:10:18, 61.15s/it]                                                         {'loss': 1.37, 'learning_rate': 1.7116008887002344e-05, 'epoch': 0.6}
 60%|██████    | 1530/2541 [25:58:23<17:10:18, 61.15s/it] 60%|██████    | 1531/2541 [25:59:24<17:08:18, 61.09s/it] 60%|██████    | 1532/2541 [26:00:25<17:07:44, 61.11s/it] 60%|██████    | 1533/2541 [26:01:26<17:06:12, 61.08s/it] 60%|██████    | 1534/2541 [26:02:27<17:05:46, 61.12s/it] 60%|██████    | 1535/2541 [26:03:28<17:04:07, 61.08s/it] 60%|██████    | 1536/2541 [26:04:29<17:02:31, 61.05s/it] 60%|██████    | 1537/2541 [26:05:30<17:02:08, 61.08s/it] 61%|██████    | 1538/2541 [26:06:32<17:03:29, 61.23s/it] 61%|██████    | 1539/2541 [26:07:33<17:02:30, 61.23s/it] 61%|██████    | 1540/2541 [26:08:34<16:59:47, 61.13s/it]                                                         {'loss': 1.3701, 'learning_rate': 1.682330091706446e-05, 'epoch': 0.61}
 61%|██████    | 1540/2541 [26:08:34<16:59:47, 61.13s/it] 61%|██████    | 1541/2541 [26:09:35<16:59:16, 61.16s/it] 61%|██████    | 1542/2541 [26:10:36<16:56:54, 61.08s/it] 61%|██████    | 1543/2541 [26:11:37<16:56:21, 61.10s/it] 61%|██████    | 1544/2541 [26:12:38<16:55:08, 61.09s/it] 61%|██████    | 1545/2541 [26:13:40<16:56:37, 61.24s/it] 61%|██████    | 1546/2541 [26:14:41<16:54:29, 61.18s/it] 61%|██████    | 1547/2541 [26:15:42<16:52:06, 61.09s/it] 61%|██████    | 1548/2541 [26:16:43<16:51:02, 61.09s/it] 61%|██████    | 1549/2541 [26:17:44<16:51:31, 61.18s/it] 61%|██████    | 1550/2541 [26:18:45<16:50:18, 61.17s/it]                                                         {'loss': 1.3662, 'learning_rate': 1.653184281152711e-05, 'epoch': 0.61}
 61%|██████    | 1550/2541 [26:18:45<16:50:18, 61.17s/it] 61%|██████    | 1551/2541 [26:19:46<16:48:13, 61.10s/it] 61%|██████    | 1552/2541 [26:20:47<16:47:19, 61.11s/it] 61%|██████    | 1553/2541 [26:21:49<16:47:51, 61.21s/it] 61%|██████    | 1554/2541 [26:22:50<16:47:18, 61.23s/it] 61%|██████    | 1555/2541 [26:23:52<16:47:07, 61.29s/it] 61%|██████    | 1556/2541 [26:24:53<16:45:39, 61.26s/it] 61%|██████▏   | 1557/2541 [26:25:54<16:43:33, 61.19s/it] 61%|██████▏   | 1558/2541 [26:26:55<16:42:48, 61.21s/it] 61%|██████▏   | 1559/2541 [26:27:56<16:41:08, 61.17s/it] 61%|██████▏   | 1560/2541 [26:28:58<16:41:56, 61.28s/it]                                                         {'loss': 1.3711, 'learning_rate': 1.6241679121755914e-05, 'epoch': 0.61}
 61%|██████▏   | 1560/2541 [26:28:58<16:41:56, 61.28s/it] 61%|██████▏   | 1561/2541 [26:29:59<16:42:19, 61.37s/it] 61%|██████▏   | 1562/2541 [26:31:01<16:40:50, 61.34s/it] 62%|██████▏   | 1563/2541 [26:32:02<16:38:49, 61.28s/it] 62%|██████▏   | 1564/2541 [26:33:03<16:37:58, 61.29s/it] 62%|██████▏   | 1565/2541 [26:34:04<16:35:21, 61.19s/it] 62%|██████▏   | 1566/2541 [26:35:05<16:32:54, 61.10s/it] 62%|██████▏   | 1567/2541 [26:36:06<16:33:32, 61.20s/it] 62%|██████▏   | 1568/2541 [26:37:08<16:34:39, 61.34s/it] 62%|██████▏   | 1569/2541 [26:38:09<16:31:25, 61.20s/it] 62%|██████▏   | 1570/2541 [26:39:10<16:30:20, 61.20s/it]                                                         {'loss': 1.3655, 'learning_rate': 1.5952854201256172e-05, 'epoch': 0.62}
 62%|██████▏   | 1570/2541 [26:39:10<16:30:20, 61.20s/it] 62%|██████▏   | 1571/2541 [26:40:11<16:28:34, 61.15s/it] 62%|██████▏   | 1572/2541 [26:41:12<16:28:37, 61.22s/it] 62%|██████▏   | 1573/2541 [26:42:14<16:28:50, 61.29s/it] 62%|██████▏   | 1574/2541 [26:43:15<16:26:27, 61.21s/it] 62%|██████▏   | 1575/2541 [26:44:16<16:25:16, 61.20s/it] 62%|██████▏   | 1576/2541 [26:45:18<16:26:11, 61.32s/it] 62%|██████▏   | 1577/2541 [26:46:19<16:23:50, 61.24s/it] 62%|██████▏   | 1578/2541 [26:47:20<16:21:41, 61.16s/it] 62%|██████▏   | 1579/2541 [26:48:21<16:20:40, 61.16s/it] 62%|██████▏   | 1580/2541 [26:49:22<16:20:35, 61.22s/it]                                                         {'loss': 1.3743, 'learning_rate': 1.566541219889316e-05, 'epoch': 0.62}
 62%|██████▏   | 1580/2541 [26:49:22<16:20:35, 61.22s/it] 62%|██████▏   | 1581/2541 [26:50:23<16:18:36, 61.16s/it] 62%|██████▏   | 1582/2541 [26:51:24<16:17:17, 61.14s/it] 62%|██████▏   | 1583/2541 [26:52:25<16:16:08, 61.14s/it] 62%|██████▏   | 1584/2541 [26:53:27<16:18:17, 61.33s/it] 62%|██████▏   | 1585/2541 [26:54:29<16:17:08, 61.33s/it] 62%|██████▏   | 1586/2541 [26:55:29<16:14:02, 61.20s/it] 62%|██████▏   | 1587/2541 [26:56:31<16:12:35, 61.17s/it] 62%|██████▏   | 1588/2541 [26:57:32<16:11:44, 61.18s/it] 63%|██████▎   | 1589/2541 [26:58:33<16:08:57, 61.07s/it] 63%|██████▎   | 1590/2541 [26:59:34<16:08:01, 61.07s/it]                                                         {'loss': 1.372, 'learning_rate': 1.537939705214364e-05, 'epoch': 0.63}
 63%|██████▎   | 1590/2541 [26:59:34<16:08:01, 61.07s/it] 63%|██████▎   | 1591/2541 [27:00:35<16:08:06, 61.14s/it] 63%|██████▎   | 1592/2541 [27:01:37<16:09:06, 61.27s/it] 63%|██████▎   | 1593/2541 [27:02:38<16:06:40, 61.18s/it] 63%|██████▎   | 1594/2541 [27:03:38<16:04:28, 61.11s/it] 63%|██████▎   | 1595/2541 [27:04:40<16:03:27, 61.11s/it] 63%|██████▎   | 1596/2541 [27:05:41<16:05:39, 61.31s/it] 63%|██████▎   | 1597/2541 [27:06:42<16:02:37, 61.18s/it] 63%|██████▎   | 1598/2541 [27:07:43<16:00:23, 61.11s/it] 63%|██████▎   | 1599/2541 [27:08:45<16:00:26, 61.17s/it] 63%|██████▎   | 1600/2541 [27:09:46<16:00:09, 61.22s/it]                                                         {'loss': 1.375, 'learning_rate': 1.5094852480379717e-05, 'epoch': 0.63}
 63%|██████▎   | 1600/2541 [27:09:46<16:00:09, 61.22s/it] 63%|██████▎   | 1601/2541 [27:10:47<15:59:39, 61.26s/it] 63%|██████▎   | 1602/2541 [27:11:48<15:57:34, 61.19s/it] 63%|██████▎   | 1603/2541 [27:12:49<15:56:47, 61.20s/it] 63%|██████▎   | 1604/2541 [27:13:51<15:56:54, 61.27s/it] 63%|██████▎   | 1605/2541 [27:14:52<15:54:17, 61.17s/it] 63%|██████▎   | 1606/2541 [27:15:53<15:52:46, 61.14s/it] 63%|██████▎   | 1607/2541 [27:16:54<15:51:33, 61.13s/it] 63%|██████▎   | 1608/2541 [27:17:56<15:53:45, 61.33s/it] 63%|██████▎   | 1609/2541 [27:18:57<15:50:21, 61.18s/it] 63%|██████▎   | 1610/2541 [27:19:58<15:48:15, 61.11s/it]                                                         {'loss': 1.3629, 'learning_rate': 1.481182197818608e-05, 'epoch': 0.63}
 63%|██████▎   | 1610/2541 [27:19:58<15:48:15, 61.11s/it] 63%|██████▎   | 1611/2541 [27:20:59<15:47:23, 61.12s/it] 63%|██████▎   | 1612/2541 [27:22:00<15:47:28, 61.19s/it] 63%|██████▎   | 1613/2541 [27:23:01<15:45:42, 61.15s/it] 64%|██████▎   | 1614/2541 [27:24:02<15:44:39, 61.14s/it] 64%|██████▎   | 1615/2541 [27:25:03<15:44:00, 61.17s/it] 64%|██████▎   | 1616/2541 [27:26:05<15:43:40, 61.21s/it] 64%|██████▎   | 1617/2541 [27:27:06<15:41:22, 61.13s/it] 64%|██████▎   | 1618/2541 [27:28:07<15:39:03, 61.04s/it] 64%|██████▎   | 1619/2541 [27:29:08<15:40:02, 61.17s/it] 64%|██████▍   | 1620/2541 [27:30:09<15:38:48, 61.16s/it]                                                         {'loss': 1.3742, 'learning_rate': 1.4530348808711508e-05, 'epoch': 0.64}
 64%|██████▍   | 1620/2541 [27:30:09<15:38:48, 61.16s/it] 64%|██████▍   | 1621/2541 [27:31:10<15:37:03, 61.11s/it] 64%|██████▍   | 1622/2541 [27:32:11<15:35:54, 61.10s/it] 64%|██████▍   | 1623/2541 [27:33:13<15:35:50, 61.17s/it] 64%|██████▍   | 1624/2541 [27:34:14<15:35:32, 61.21s/it] 64%|██████▍   | 1625/2541 [27:35:15<15:33:45, 61.16s/it] 64%|██████▍   | 1626/2541 [27:36:16<15:33:17, 61.20s/it] 64%|██████▍   | 1627/2541 [27:37:18<15:33:29, 61.28s/it] 64%|██████▍   | 1628/2541 [27:38:19<15:30:56, 61.18s/it] 64%|██████▍   | 1629/2541 [27:39:20<15:29:09, 61.13s/it] 64%|██████▍   | 1630/2541 [27:40:21<15:28:00, 61.12s/it]                                                         {'loss': 1.3699, 'learning_rate': 1.4250475997055823e-05, 'epoch': 0.64}
 64%|██████▍   | 1630/2541 [27:40:21<15:28:00, 61.12s/it] 64%|██████▍   | 1631/2541 [27:41:23<15:30:26, 61.35s/it] 64%|██████▍   | 1632/2541 [27:42:24<15:27:52, 61.25s/it] 64%|██████▍   | 1633/2541 [27:43:25<15:26:28, 61.22s/it] 64%|██████▍   | 1634/2541 [27:44:26<15:25:12, 61.20s/it] 64%|██████▍   | 1635/2541 [27:45:27<15:24:59, 61.26s/it] 64%|██████▍   | 1636/2541 [27:46:28<15:22:03, 61.13s/it] 64%|██████▍   | 1637/2541 [27:47:29<15:21:19, 61.15s/it] 64%|██████▍   | 1638/2541 [27:48:30<15:19:57, 61.13s/it] 65%|██████▍   | 1639/2541 [27:49:32<15:21:03, 61.27s/it] 65%|██████▍   | 1640/2541 [27:50:33<15:19:01, 61.20s/it]                                                         {'loss': 1.3666, 'learning_rate': 1.3972246323693186e-05, 'epoch': 0.65}
 65%|██████▍   | 1640/2541 [27:50:33<15:19:01, 61.20s/it] 65%|██████▍   | 1641/2541 [27:51:34<15:16:35, 61.11s/it] 65%|██████▍   | 1642/2541 [27:52:35<15:16:44, 61.18s/it] 65%|██████▍   | 1643/2541 [27:53:37<15:19:23, 61.43s/it] 65%|██████▍   | 1644/2541 [27:54:38<15:16:31, 61.31s/it] 65%|██████▍   | 1645/2541 [27:55:39<15:14:24, 61.23s/it] 65%|██████▍   | 1646/2541 [27:56:41<15:13:30, 61.24s/it] 65%|██████▍   | 1647/2541 [27:57:42<15:14:00, 61.34s/it] 65%|██████▍   | 1648/2541 [27:58:44<15:12:43, 61.32s/it] 65%|██████▍   | 1649/2541 [27:59:44<15:09:50, 61.20s/it] 65%|██████▍   | 1650/2541 [28:00:46<15:08:26, 61.17s/it]                                                         {'loss': 1.368, 'learning_rate': 1.3695702317932862e-05, 'epoch': 0.65}
 65%|██████▍   | 1650/2541 [28:00:46<15:08:26, 61.17s/it] 65%|██████▍   | 1651/2541 [28:01:47<15:08:13, 61.23s/it] 65%|██████▌   | 1652/2541 [28:02:48<15:05:56, 61.14s/it] 65%|██████▌   | 1653/2541 [28:03:49<15:04:16, 61.10s/it] 65%|██████▌   | 1654/2541 [28:04:50<15:05:36, 61.26s/it] 65%|██████▌   | 1655/2541 [28:05:52<15:06:05, 61.36s/it] 65%|██████▌   | 1656/2541 [28:06:53<15:03:17, 61.24s/it] 65%|██████▌   | 1657/2541 [28:07:54<15:01:17, 61.17s/it] 65%|██████▌   | 1658/2541 [28:08:55<15:00:21, 61.18s/it] 65%|██████▌   | 1659/2541 [28:09:57<15:00:48, 61.28s/it] 65%|██████▌   | 1660/2541 [28:10:58<15:00:17, 61.31s/it]                                                         {'loss': 1.3663, 'learning_rate': 1.3420886251418258e-05, 'epoch': 0.65}
 65%|██████▌   | 1660/2541 [28:10:58<15:00:17, 61.31s/it] 65%|██████▌   | 1661/2541 [28:11:59<14:57:24, 61.19s/it] 65%|██████▌   | 1662/2541 [28:13:00<14:55:26, 61.12s/it] 65%|██████▌   | 1663/2541 [28:14:02<14:56:01, 61.23s/it] 65%|██████▌   | 1664/2541 [28:15:02<14:53:45, 61.15s/it] 66%|██████▌   | 1665/2541 [28:16:04<14:52:30, 61.13s/it] 66%|██████▌   | 1666/2541 [28:17:05<14:52:26, 61.20s/it] 66%|██████▌   | 1667/2541 [28:18:07<14:53:09, 61.32s/it] 66%|██████▌   | 1668/2541 [28:19:08<14:51:19, 61.26s/it] 66%|██████▌   | 1669/2541 [28:20:09<14:48:58, 61.17s/it] 66%|██████▌   | 1670/2541 [28:21:10<14:48:21, 61.20s/it]                                                         {'loss': 1.3684, 'learning_rate': 1.3147840131665467e-05, 'epoch': 0.66}
 66%|██████▌   | 1670/2541 [28:21:10<14:48:21, 61.20s/it] 66%|██████▌   | 1671/2541 [28:22:12<14:49:57, 61.38s/it] 66%|██████▌   | 1672/2541 [28:23:12<14:46:38, 61.22s/it] 66%|██████▌   | 1673/2541 [28:24:14<14:44:45, 61.16s/it] 66%|██████▌   | 1674/2541 [28:25:15<14:43:07, 61.12s/it] 66%|██████▌   | 1675/2541 [28:26:16<14:43:40, 61.23s/it] 66%|██████▌   | 1676/2541 [28:27:17<14:41:05, 61.12s/it] 66%|██████▌   | 1677/2541 [28:28:18<14:40:55, 61.18s/it] 66%|██████▌   | 1678/2541 [28:29:19<14:39:14, 61.13s/it] 66%|██████▌   | 1679/2541 [28:30:21<14:40:19, 61.28s/it] 66%|██████▌   | 1680/2541 [28:31:22<14:37:29, 61.15s/it]                                                         {'loss': 1.3633, 'learning_rate': 1.2876605695642086e-05, 'epoch': 0.66}
 66%|██████▌   | 1680/2541 [28:31:22<14:37:29, 61.15s/it] 66%|██████▌   | 1681/2541 [28:32:23<14:35:54, 61.11s/it] 66%|██████▌   | 1682/2541 [28:33:24<14:34:23, 61.07s/it] 66%|██████▌   | 1683/2541 [28:34:25<14:36:29, 61.29s/it] 66%|██████▋   | 1684/2541 [28:35:26<14:33:47, 61.18s/it] 66%|██████▋   | 1685/2541 [28:36:27<14:31:45, 61.10s/it] 66%|██████▋   | 1686/2541 [28:37:28<14:30:35, 61.09s/it] 66%|██████▋   | 1687/2541 [28:38:30<14:31:25, 61.22s/it] 66%|██████▋   | 1688/2541 [28:39:31<14:30:27, 61.23s/it] 66%|██████▋   | 1689/2541 [28:40:32<14:28:58, 61.20s/it] 67%|██████▋   | 1690/2541 [28:41:34<14:28:04, 61.20s/it]                                                         {'loss': 1.3738, 'learning_rate': 1.2607224403387485e-05, 'epoch': 0.66}
 67%|██████▋   | 1690/2541 [28:41:34<14:28:04, 61.20s/it] 67%|██████▋   | 1691/2541 [28:42:35<14:28:19, 61.29s/it] 67%|██████▋   | 1692/2541 [28:43:36<14:25:46, 61.19s/it] 67%|██████▋   | 1693/2541 [28:44:37<14:24:33, 61.17s/it] 67%|██████▋   | 1694/2541 [28:45:38<14:23:57, 61.20s/it] 67%|██████▋   | 1695/2541 [28:46:40<14:24:29, 61.31s/it] 67%|██████▋   | 1696/2541 [28:47:41<14:22:04, 61.21s/it] 67%|██████▋   | 1697/2541 [28:48:42<14:19:43, 61.12s/it] 67%|██████▋   | 1698/2541 [28:49:43<14:18:16, 61.09s/it] 67%|██████▋   | 1699/2541 [28:50:44<14:19:03, 61.22s/it] 67%|██████▋   | 1700/2541 [28:51:46<14:18:34, 61.25s/it]                                                         {'loss': 1.3545, 'learning_rate': 1.2339737431675304e-05, 'epoch': 0.67}
 67%|██████▋   | 1700/2541 [28:51:46<14:18:34, 61.25s/it] 67%|██████▋   | 1701/2541 [28:52:47<14:16:01, 61.14s/it] 67%|██████▋   | 1702/2541 [28:53:48<14:14:59, 61.14s/it] 67%|██████▋   | 1703/2541 [28:54:49<14:15:33, 61.26s/it] 67%|██████▋   | 1704/2541 [28:55:50<14:13:40, 61.20s/it] 67%|██████▋   | 1705/2541 [28:56:51<14:11:10, 61.09s/it] 67%|██████▋   | 1706/2541 [28:57:53<14:11:43, 61.20s/it] 67%|██████▋   | 1707/2541 [28:58:54<14:12:27, 61.33s/it] 67%|██████▋   | 1708/2541 [28:59:55<14:10:07, 61.23s/it] 67%|██████▋   | 1709/2541 [29:00:56<14:08:03, 61.16s/it] 67%|██████▋   | 1710/2541 [29:01:57<14:06:57, 61.15s/it]                                                         {'loss': 1.3621, 'learning_rate': 1.2074185667719353e-05, 'epoch': 0.67}
 67%|██████▋   | 1710/2541 [29:01:57<14:06:57, 61.15s/it] 67%|██████▋   | 1711/2541 [29:02:59<14:07:32, 61.27s/it] 67%|██████▋   | 1712/2541 [29:04:00<14:07:02, 61.31s/it] 67%|██████▋   | 1713/2541 [29:05:01<14:04:39, 61.21s/it] 67%|██████▋   | 1714/2541 [29:06:03<14:04:20, 61.26s/it] 67%|██████▋   | 1715/2541 [29:07:04<14:04:30, 61.34s/it] 68%|██████▊   | 1716/2541 [29:08:05<14:02:10, 61.25s/it] 68%|██████▊   | 1717/2541 [29:09:06<14:01:11, 61.25s/it] 68%|██████▊   | 1718/2541 [29:10:07<13:59:13, 61.18s/it] 68%|██████▊   | 1719/2541 [29:11:09<13:59:41, 61.29s/it] 68%|██████▊   | 1720/2541 [29:12:10<13:57:01, 61.17s/it]                                                         {'loss': 1.3644, 'learning_rate': 1.1810609702923667e-05, 'epoch': 0.68}
 68%|██████▊   | 1720/2541 [29:12:10<13:57:01, 61.17s/it] 68%|██████▊   | 1721/2541 [29:13:11<13:54:45, 61.08s/it] 68%|██████▊   | 1722/2541 [29:14:12<13:53:48, 61.08s/it] 68%|██████▊   | 1723/2541 [29:15:13<13:54:44, 61.23s/it] 68%|██████▊   | 1724/2541 [29:16:15<13:53:50, 61.24s/it] 68%|██████▊   | 1725/2541 [29:17:16<13:52:25, 61.21s/it] 68%|██████▊   | 1726/2541 [29:18:17<13:51:00, 61.18s/it] 68%|██████▊   | 1727/2541 [29:19:19<13:51:37, 61.30s/it] 68%|██████▊   | 1728/2541 [29:20:19<13:49:11, 61.19s/it] 68%|██████▊   | 1729/2541 [29:21:21<13:47:58, 61.18s/it] 68%|██████▊   | 1730/2541 [29:22:22<13:46:58, 61.18s/it]                                                         {'loss': 1.3656, 'learning_rate': 1.1549049826677902e-05, 'epoch': 0.68}
 68%|██████▊   | 1730/2541 [29:22:22<13:46:58, 61.18s/it] 68%|██████▊   | 1731/2541 [29:23:23<13:47:14, 61.28s/it] 68%|██████▊   | 1732/2541 [29:24:24<13:44:42, 61.16s/it] 68%|██████▊   | 1733/2541 [29:25:25<13:42:59, 61.11s/it] 68%|██████▊   | 1734/2541 [29:26:26<13:41:58, 61.11s/it] 68%|██████▊   | 1735/2541 [29:27:28<13:44:02, 61.34s/it] 68%|██████▊   | 1736/2541 [29:28:29<13:41:30, 61.23s/it] 68%|██████▊   | 1737/2541 [29:29:30<13:39:54, 61.19s/it] 68%|██████▊   | 1738/2541 [29:30:31<13:38:05, 61.13s/it] 68%|██████▊   | 1739/2541 [29:31:33<13:38:14, 61.21s/it] 68%|██████▊   | 1740/2541 [29:32:34<13:37:15, 61.22s/it]                                                         {'loss': 1.361, 'learning_rate': 1.1289546020198719e-05, 'epoch': 0.68}
 68%|██████▊   | 1740/2541 [29:32:34<13:37:15, 61.22s/it] 69%|██████▊   | 1741/2541 [29:33:35<13:35:08, 61.14s/it] 69%|██████▊   | 1742/2541 [29:34:36<13:33:50, 61.11s/it] 69%|██████▊   | 1743/2541 [29:35:37<13:34:06, 61.21s/it] 69%|██████▊   | 1744/2541 [29:36:38<13:32:33, 61.17s/it] 69%|██████▊   | 1745/2541 [29:37:39<13:30:03, 61.06s/it] 69%|██████▊   | 1746/2541 [29:38:41<13:30:07, 61.14s/it] 69%|██████▉   | 1747/2541 [29:39:42<13:31:14, 61.30s/it] 69%|██████▉   | 1748/2541 [29:40:43<13:28:32, 61.18s/it] 69%|██████▉   | 1749/2541 [29:41:44<13:26:41, 61.11s/it] 69%|██████▉   | 1750/2541 [29:42:45<13:24:57, 61.06s/it]                                                         {'loss': 1.3517, 'learning_rate': 1.1032137950418514e-05, 'epoch': 0.69}
 69%|██████▉   | 1750/2541 [29:42:45<13:24:57, 61.06s/it] 69%|██████▉   | 1751/2541 [29:43:47<13:25:51, 61.20s/it] 69%|██████▉   | 1752/2541 [29:44:48<13:24:46, 61.20s/it] 69%|██████▉   | 1753/2541 [29:45:49<13:24:28, 61.25s/it] 69%|██████▉   | 1754/2541 [29:46:50<13:23:08, 61.23s/it] 69%|██████▉   | 1755/2541 [29:47:52<13:23:14, 61.32s/it] 69%|██████▉   | 1756/2541 [29:48:53<13:21:19, 61.25s/it] 69%|██████▉   | 1757/2541 [29:49:54<13:19:18, 61.17s/it] 69%|██████▉   | 1758/2541 [29:50:55<13:19:15, 61.25s/it] 69%|██████▉   | 1759/2541 [29:51:57<13:19:04, 61.31s/it] 69%|██████▉   | 1760/2541 [29:52:58<13:16:52, 61.22s/it]                                                         {'loss': 1.3643, 'learning_rate': 1.0776864963921946e-05, 'epoch': 0.69}
 69%|██████▉   | 1760/2541 [29:52:58<13:16:52, 61.22s/it] 69%|██████▉   | 1761/2541 [29:53:59<13:15:01, 61.16s/it] 69%|██████▉   | 1762/2541 [29:55:00<13:13:32, 61.12s/it] 69%|██████▉   | 1763/2541 [29:56:02<13:15:06, 61.32s/it] 69%|██████▉   | 1764/2541 [29:57:03<13:12:33, 61.20s/it] 69%|██████▉   | 1765/2541 [29:58:04<13:11:04, 61.17s/it] 70%|██████▉   | 1766/2541 [29:59:05<13:09:46, 61.14s/it] 70%|██████▉   | 1767/2541 [30:00:06<13:10:49, 61.30s/it] 70%|██████▉   | 1768/2541 [30:01:07<13:08:12, 61.18s/it] 70%|██████▉   | 1769/2541 [30:02:08<13:07:05, 61.17s/it] 70%|██████▉   | 1770/2541 [30:03:10<13:06:06, 61.18s/it]                                                         {'loss': 1.3609, 'learning_rate': 1.052376608093162e-05, 'epoch': 0.7}
 70%|██████▉   | 1770/2541 [30:03:10<13:06:06, 61.18s/it] 70%|██████▉   | 1771/2541 [30:04:11<13:06:01, 61.25s/it] 70%|██████▉   | 1772/2541 [30:05:12<13:04:28, 61.21s/it] 70%|██████▉   | 1773/2541 [30:06:13<13:03:03, 61.18s/it] 70%|██████▉   | 1774/2541 [30:07:14<13:01:59, 61.17s/it] 70%|██████▉   | 1775/2541 [30:08:16<13:03:37, 61.38s/it] 70%|██████▉   | 1776/2541 [30:09:18<13:02:42, 61.39s/it] 70%|██████▉   | 1777/2541 [30:10:19<12:59:53, 61.25s/it] 70%|██████▉   | 1778/2541 [30:11:20<12:58:07, 61.19s/it] 70%|███████   | 1779/2541 [30:12:21<12:58:08, 61.27s/it] 70%|███████   | 1780/2541 [30:13:22<12:55:58, 61.18s/it]                                                         {'loss': 1.3678, 'learning_rate': 1.0272879989343509e-05, 'epoch': 0.7}
 70%|███████   | 1780/2541 [30:13:22<12:55:58, 61.18s/it] 70%|███████   | 1781/2541 [30:14:23<12:55:40, 61.24s/it] 70%|███████   | 1782/2541 [30:15:24<12:53:46, 61.17s/it] 70%|███████   | 1783/2541 [30:16:26<12:54:13, 61.28s/it] 70%|███████   | 1784/2541 [30:17:27<12:51:56, 61.18s/it] 70%|███████   | 1785/2541 [30:18:28<12:50:05, 61.12s/it] 70%|███████   | 1786/2541 [30:19:29<12:49:57, 61.19s/it] 70%|███████   | 1787/2541 [30:20:31<12:51:33, 61.40s/it] 70%|███████   | 1788/2541 [30:21:32<12:49:02, 61.28s/it] 70%|███████   | 1789/2541 [30:22:33<12:47:12, 61.21s/it] 70%|███████   | 1790/2541 [30:23:34<12:45:55, 61.19s/it]                                                         {'loss': 1.3639, 'learning_rate': 1.0024245038813312e-05, 'epoch': 0.7}
 70%|███████   | 1790/2541 [30:23:34<12:45:55, 61.19s/it] 70%|███████   | 1791/2541 [30:24:36<12:46:01, 61.28s/it] 71%|███████   | 1792/2541 [30:25:37<12:43:21, 61.15s/it] 71%|███████   | 1793/2541 [30:26:38<12:42:45, 61.18s/it] 71%|███████   | 1794/2541 [30:27:39<12:41:48, 61.19s/it] 71%|███████   | 1795/2541 [30:28:41<12:41:57, 61.28s/it] 71%|███████   | 1796/2541 [30:29:42<12:39:45, 61.19s/it] 71%|███████   | 1797/2541 [30:30:43<12:39:02, 61.21s/it] 71%|███████   | 1798/2541 [30:31:44<12:38:06, 61.22s/it] 71%|███████   | 1799/2541 [30:32:46<12:39:33, 61.42s/it] 71%|███████   | 1800/2541 [30:33:47<12:36:55, 61.29s/it]                                                         {'loss': 1.3595, 'learning_rate': 9.777899234894387e-06, 'epoch': 0.71}
 71%|███████   | 1800/2541 [30:33:47<12:36:55, 61.29s/it] 71%|███████   | 1801/2541 [30:34:48<12:34:30, 61.18s/it] 71%|███████   | 1802/2541 [30:35:49<12:32:22, 61.09s/it] 71%|███████   | 1803/2541 [30:36:50<12:33:18, 61.24s/it] 71%|███████   | 1804/2541 [30:37:52<12:32:42, 61.28s/it] 71%|███████   | 1805/2541 [30:38:53<12:30:51, 61.21s/it] 71%|███████   | 1806/2541 [30:39:54<12:29:32, 61.19s/it] 71%|███████   | 1807/2541 [30:40:55<12:29:22, 61.26s/it] 71%|███████   | 1808/2541 [30:41:57<12:28:14, 61.25s/it] 71%|███████   | 1809/2541 [30:42:58<12:28:42, 61.37s/it] 71%|███████   | 1810/2541 [30:44:00<12:27:34, 61.36s/it]                                                         {'loss': 1.3612, 'learning_rate': 9.533880233228395e-06, 'epoch': 0.71}
 71%|███████   | 1810/2541 [30:44:00<12:27:34, 61.36s/it] 71%|███████▏  | 1811/2541 [30:45:01<12:27:21, 61.43s/it] 71%|███████▏  | 1812/2541 [30:46:02<12:24:43, 61.29s/it] 71%|███████▏  | 1813/2541 [30:47:03<12:23:21, 61.27s/it] 71%|███████▏  | 1814/2541 [30:48:05<12:22:01, 61.24s/it] 71%|███████▏  | 1815/2541 [30:49:06<12:22:51, 61.39s/it] 71%|███████▏  | 1816/2541 [30:50:08<12:21:44, 61.39s/it] 72%|███████▏  | 1817/2541 [30:51:09<12:19:54, 61.32s/it] 72%|███████▏  | 1818/2541 [30:52:10<12:18:40, 61.30s/it] 72%|███████▏  | 1819/2541 [30:53:12<12:18:27, 61.37s/it] 72%|███████▏  | 1820/2541 [30:54:13<12:16:26, 61.28s/it]                                                         {'loss': 1.353, 'learning_rate': 9.292225333789329e-06, 'epoch': 0.72}
 72%|███████▏  | 1820/2541 [30:54:13<12:16:26, 61.28s/it] 72%|███████▏  | 1821/2541 [30:55:14<12:14:13, 61.19s/it] 72%|███████▏  | 1822/2541 [30:56:15<12:14:14, 61.27s/it] 72%|███████▏  | 1823/2541 [30:57:17<12:14:32, 61.38s/it] 72%|███████▏  | 1824/2541 [30:58:18<12:11:42, 61.23s/it] 72%|███████▏  | 1825/2541 [30:59:19<12:09:30, 61.13s/it] 72%|███████▏  | 1826/2541 [31:00:19<12:07:37, 61.06s/it] 72%|███████▏  | 1827/2541 [31:01:21<12:08:42, 61.24s/it] 72%|███████▏  | 1828/2541 [31:02:22<12:07:31, 61.22s/it] 72%|███████▏  | 1829/2541 [31:03:23<12:06:10, 61.19s/it] 72%|███████▏  | 1830/2541 [31:04:24<12:04:41, 61.16s/it]                                                         {'loss': 1.3614, 'learning_rate': 9.052971475182004e-06, 'epoch': 0.72}
 72%|███████▏  | 1830/2541 [31:04:24<12:04:41, 61.16s/it] 72%|███████▏  | 1831/2541 [31:05:26<12:05:15, 61.29s/it] 72%|███████▏  | 1832/2541 [31:06:27<12:03:46, 61.25s/it] 72%|███████▏  | 1833/2541 [31:07:28<12:02:21, 61.22s/it] 72%|███████▏  | 1834/2541 [31:08:29<12:00:27, 61.14s/it] 72%|███████▏  | 1835/2541 [31:09:31<12:00:58, 61.27s/it] 72%|███████▏  | 1836/2541 [31:10:32<11:58:32, 61.15s/it] 72%|███████▏  | 1837/2541 [31:11:33<11:57:27, 61.15s/it] 72%|███████▏  | 1838/2541 [31:12:34<11:55:50, 61.10s/it] 72%|███████▏  | 1839/2541 [31:13:36<11:57:24, 61.32s/it] 72%|███████▏  | 1840/2541 [31:14:37<11:55:14, 61.22s/it]                                                         {'loss': 1.3628, 'learning_rate': 8.816155228995693e-06, 'epoch': 0.72}
 72%|███████▏  | 1840/2541 [31:14:37<11:55:14, 61.22s/it] 72%|███████▏  | 1841/2541 [31:15:38<11:53:21, 61.15s/it] 72%|███████▏  | 1842/2541 [31:16:39<11:51:52, 61.10s/it] 73%|███████▎  | 1843/2541 [31:17:40<11:52:24, 61.24s/it] 73%|███████▎  | 1844/2541 [31:18:41<11:50:29, 61.16s/it] 73%|███████▎  | 1845/2541 [31:19:43<11:50:03, 61.21s/it] 73%|███████▎  | 1846/2541 [31:20:44<11:48:34, 61.17s/it] 73%|███████▎  | 1847/2541 [31:21:45<11:49:14, 61.32s/it] 73%|███████▎  | 1848/2541 [31:22:46<11:46:48, 61.20s/it] 73%|███████▎  | 1849/2541 [31:23:47<11:45:02, 61.13s/it] 73%|███████▎  | 1850/2541 [31:24:48<11:44:24, 61.16s/it]                                                         {'loss': 1.3604, 'learning_rate': 8.581812794213959e-06, 'epoch': 0.73}
 73%|███████▎  | 1850/2541 [31:24:48<11:44:24, 61.16s/it] 73%|███████▎  | 1851/2541 [31:25:50<11:45:45, 61.37s/it] 73%|███████▎  | 1852/2541 [31:26:51<11:43:46, 61.29s/it] 73%|███████▎  | 1853/2541 [31:27:52<11:42:07, 61.23s/it] 73%|███████▎  | 1854/2541 [31:28:53<11:40:17, 61.16s/it] 73%|███████▎  | 1855/2541 [31:29:55<11:40:39, 61.28s/it] 73%|███████▎  | 1856/2541 [31:30:56<11:38:56, 61.22s/it] 73%|███████▎  | 1857/2541 [31:31:57<11:36:37, 61.11s/it] 73%|███████▎  | 1858/2541 [31:32:58<11:34:57, 61.05s/it] 73%|███████▎  | 1859/2541 [31:33:59<11:35:17, 61.17s/it] 73%|███████▎  | 1860/2541 [31:35:00<11:33:42, 61.12s/it]                                                         {'loss': 1.37, 'learning_rate': 8.349979991681333e-06, 'epoch': 0.73}
 73%|███████▎  | 1860/2541 [31:35:00<11:33:42, 61.12s/it] 73%|███████▎  | 1861/2541 [31:36:01<11:32:01, 61.06s/it] 73%|███████▎  | 1862/2541 [31:37:03<11:33:13, 61.26s/it] 73%|███████▎  | 1863/2541 [31:38:05<11:33:35, 61.38s/it] 73%|███████▎  | 1864/2541 [31:39:06<11:31:36, 61.29s/it] 73%|███████▎  | 1865/2541 [31:40:07<11:29:25, 61.19s/it] 73%|███████▎  | 1866/2541 [31:41:08<11:27:57, 61.15s/it] 73%|███████▎  | 1867/2541 [31:42:09<11:27:56, 61.24s/it] 74%|███████▎  | 1868/2541 [31:43:11<11:27:54, 61.33s/it] 74%|███████▎  | 1869/2541 [31:44:12<11:25:23, 61.20s/it] 74%|███████▎  | 1870/2541 [31:45:13<11:24:53, 61.24s/it]                                                         {'loss': 1.3573, 'learning_rate': 8.120692258627927e-06, 'epoch': 0.74}
 74%|███████▎  | 1870/2541 [31:45:13<11:24:53, 61.24s/it] 74%|███████▎  | 1871/2541 [31:46:14<11:24:43, 61.32s/it] 74%|███████▎  | 1872/2541 [31:47:15<11:22:10, 61.18s/it] 74%|███████▎  | 1873/2541 [31:48:16<11:20:51, 61.15s/it] 74%|███████▍  | 1874/2541 [31:49:18<11:20:04, 61.18s/it] 74%|███████▍  | 1875/2541 [31:50:19<11:21:11, 61.37s/it] 74%|███████▍  | 1876/2541 [31:51:21<11:19:08, 61.28s/it] 74%|███████▍  | 1877/2541 [31:52:21<11:17:00, 61.18s/it] 74%|███████▍  | 1878/2541 [31:53:23<11:16:30, 61.22s/it] 74%|███████▍  | 1879/2541 [31:54:25<11:17:37, 61.42s/it] 74%|███████▍  | 1880/2541 [31:55:26<11:15:07, 61.28s/it]                                                         {'loss': 1.3578, 'learning_rate': 7.893984643252515e-06, 'epoch': 0.74}
 74%|███████▍  | 1880/2541 [31:55:26<11:15:07, 61.28s/it] 74%|███████▍  | 1881/2541 [31:56:27<11:12:45, 61.16s/it] 74%|███████▍  | 1882/2541 [31:57:28<11:11:42, 61.16s/it] 74%|███████▍  | 1883/2541 [31:58:29<11:11:37, 61.24s/it] 74%|███████▍  | 1884/2541 [31:59:30<11:10:23, 61.22s/it] 74%|███████▍  | 1885/2541 [32:00:32<11:10:39, 61.34s/it] 74%|███████▍  | 1886/2541 [32:01:33<11:08:57, 61.28s/it] 74%|███████▍  | 1887/2541 [32:02:35<11:09:07, 61.39s/it] 74%|███████▍  | 1888/2541 [32:03:36<11:07:33, 61.34s/it] 74%|███████▍  | 1889/2541 [32:04:37<11:05:21, 61.23s/it] 74%|███████▍  | 1890/2541 [32:05:38<11:04:21, 61.23s/it]                                                         {'loss': 1.3558, 'learning_rate': 7.669891799365283e-06, 'epoch': 0.74}
 74%|███████▍  | 1890/2541 [32:05:38<11:04:21, 61.23s/it] 74%|███████▍  | 1891/2541 [32:06:40<11:04:53, 61.37s/it] 74%|███████▍  | 1892/2541 [32:07:41<11:02:16, 61.23s/it] 74%|███████▍  | 1893/2541 [32:08:42<11:00:23, 61.15s/it] 75%|███████▍  | 1894/2541 [32:09:43<10:58:56, 61.11s/it] 75%|███████▍  | 1895/2541 [32:10:44<10:58:36, 61.17s/it] 75%|███████▍  | 1896/2541 [32:11:45<10:56:58, 61.11s/it] 75%|███████▍  | 1897/2541 [32:12:46<10:56:22, 61.15s/it] 75%|███████▍  | 1898/2541 [32:13:47<10:55:37, 61.18s/it] 75%|███████▍  | 1899/2541 [32:14:49<10:55:29, 61.26s/it] 75%|███████▍  | 1900/2541 [32:15:50<10:53:58, 61.21s/it]                                                         {'loss': 1.3608, 'learning_rate': 7.448447981090667e-06, 'epoch': 0.75}
 75%|███████▍  | 1900/2541 [32:15:50<10:53:58, 61.21s/it] 75%|███████▍  | 1901/2541 [32:16:51<10:52:04, 61.13s/it] 75%|███████▍  | 1902/2541 [32:17:52<10:52:12, 61.24s/it] 75%|███████▍  | 1903/2541 [32:18:54<10:52:48, 61.39s/it] 75%|███████▍  | 1904/2541 [32:19:55<10:50:12, 61.24s/it] 75%|███████▍  | 1905/2541 [32:20:56<10:48:05, 61.14s/it] 75%|███████▌  | 1906/2541 [32:21:57<10:47:09, 61.15s/it] 75%|███████▌  | 1907/2541 [32:22:59<10:47:01, 61.23s/it] 75%|███████▌  | 1908/2541 [32:24:00<10:45:41, 61.20s/it] 75%|███████▌  | 1909/2541 [32:25:01<10:44:31, 61.19s/it] 75%|███████▌  | 1910/2541 [32:26:02<10:43:15, 61.17s/it]                                                         {'loss': 1.3653, 'learning_rate': 7.229687037631444e-06, 'epoch': 0.75}
 75%|███████▌  | 1910/2541 [32:26:02<10:43:15, 61.17s/it] 75%|███████▌  | 1911/2541 [32:27:04<10:43:34, 61.29s/it] 75%|███████▌  | 1912/2541 [32:28:05<10:41:35, 61.20s/it] 75%|███████▌  | 1913/2541 [32:29:06<10:39:46, 61.13s/it] 75%|███████▌  | 1914/2541 [32:30:07<10:38:56, 61.14s/it] 75%|███████▌  | 1915/2541 [32:31:08<10:39:47, 61.32s/it] 75%|███████▌  | 1916/2541 [32:32:09<10:37:15, 61.18s/it] 75%|███████▌  | 1917/2541 [32:33:10<10:35:51, 61.14s/it] 75%|███████▌  | 1918/2541 [32:34:11<10:34:51, 61.14s/it] 76%|███████▌  | 1919/2541 [32:35:13<10:35:15, 61.28s/it] 76%|███████▌  | 1920/2541 [32:36:14<10:33:59, 61.26s/it]                                                         {'loss': 1.3564, 'learning_rate': 7.01364240809459e-06, 'epoch': 0.76}
 76%|███████▌  | 1920/2541 [32:36:14<10:33:59, 61.26s/it] 76%|███████▌  | 1921/2541 [32:37:15<10:32:49, 61.24s/it] 76%|███████▌  | 1922/2541 [32:38:17<10:31:09, 61.18s/it] 76%|███████▌  | 1923/2541 [32:39:18<10:31:46, 61.34s/it] 76%|███████▌  | 1924/2541 [32:40:19<10:29:29, 61.21s/it] 76%|███████▌  | 1925/2541 [32:41:20<10:28:22, 61.21s/it] 76%|███████▌  | 1926/2541 [32:42:22<10:27:24, 61.21s/it] 76%|███████▌  | 1927/2541 [32:43:23<10:27:56, 61.36s/it] 76%|███████▌  | 1928/2541 [32:44:24<10:26:02, 61.28s/it] 76%|███████▌  | 1929/2541 [32:45:25<10:24:23, 61.21s/it] 76%|███████▌  | 1930/2541 [32:46:27<10:23:00, 61.18s/it]                                                         {'loss': 1.3612, 'learning_rate': 6.8003471163799436e-06, 'epoch': 0.76}
 76%|███████▌  | 1930/2541 [32:46:27<10:23:00, 61.18s/it] 76%|███████▌  | 1931/2541 [32:47:28<10:24:18, 61.41s/it] 76%|███████▌  | 1932/2541 [32:48:30<10:22:12, 61.30s/it] 76%|███████▌  | 1933/2541 [32:49:30<10:20:00, 61.18s/it] 76%|███████▌  | 1934/2541 [32:50:32<10:18:57, 61.18s/it] 76%|███████▌  | 1935/2541 [32:51:33<10:18:51, 61.27s/it] 76%|███████▌  | 1936/2541 [32:52:34<10:16:39, 61.16s/it] 76%|███████▌  | 1937/2541 [32:53:35<10:15:50, 61.18s/it] 76%|███████▋  | 1938/2541 [32:54:37<10:15:17, 61.22s/it] 76%|███████▋  | 1939/2541 [32:55:38<10:15:00, 61.30s/it] 76%|███████▋  | 1940/2541 [32:56:39<10:12:57, 61.19s/it]                                                         {'loss': 1.3605, 'learning_rate': 6.589833766132208e-06, 'epoch': 0.76}
 76%|███████▋  | 1940/2541 [32:56:39<10:12:57, 61.19s/it] 76%|███████▋  | 1941/2541 [32:57:40<10:10:58, 61.10s/it] 76%|███████▋  | 1942/2541 [32:58:41<10:09:52, 61.09s/it] 76%|███████▋  | 1943/2541 [32:59:43<10:11:03, 61.31s/it] 77%|███████▋  | 1944/2541 [33:00:44<10:09:10, 61.22s/it] 77%|███████▋  | 1945/2541 [33:01:45<10:07:24, 61.15s/it] 77%|███████▋  | 1946/2541 [33:02:46<10:06:16, 61.14s/it] 77%|███████▋  | 1947/2541 [33:03:47<10:05:53, 61.20s/it] 77%|███████▋  | 1948/2541 [33:04:48<10:05:14, 61.24s/it] 77%|███████▋  | 1949/2541 [33:05:49<10:03:21, 61.15s/it] 77%|███████▋  | 1950/2541 [33:06:50<10:02:02, 61.12s/it]                                                         {'loss': 1.3563, 'learning_rate': 6.382134535757339e-06, 'epoch': 0.77}
 77%|███████▋  | 1950/2541 [33:06:51<10:02:02, 61.12s/it] 77%|███████▋  | 1951/2541 [33:07:52<10:02:41, 61.29s/it] 77%|███████▋  | 1952/2541 [33:08:53<10:01:06, 61.23s/it] 77%|███████▋  | 1953/2541 [33:09:54<9:59:17, 61.15s/it]  77%|███████▋  | 1954/2541 [33:10:56<9:58:42, 61.20s/it] 77%|███████▋  | 1955/2541 [33:11:57<9:59:24, 61.37s/it] 77%|███████▋  | 1956/2541 [33:12:58<9:57:00, 61.23s/it] 77%|███████▋  | 1957/2541 [33:13:59<9:55:09, 61.15s/it] 77%|███████▋  | 1958/2541 [33:15:00<9:54:20, 61.17s/it] 77%|███████▋  | 1959/2541 [33:16:02<9:54:29, 61.29s/it] 77%|███████▋  | 1960/2541 [33:17:03<9:53:38, 61.31s/it]                                                        {'loss': 1.3591, 'learning_rate': 6.177281173503779e-06, 'epoch': 0.77}
 77%|███████▋  | 1960/2541 [33:17:03<9:53:38, 61.31s/it] 77%|███████▋  | 1961/2541 [33:18:05<9:52:27, 61.29s/it] 77%|███████▋  | 1962/2541 [33:19:06<9:50:50, 61.23s/it] 77%|███████▋  | 1963/2541 [33:20:07<9:50:16, 61.27s/it] 77%|███████▋  | 1964/2541 [33:21:08<9:48:30, 61.20s/it] 77%|███████▋  | 1965/2541 [33:22:09<9:46:40, 61.11s/it] 77%|███████▋  | 1966/2541 [33:23:10<9:46:28, 61.20s/it] 77%|███████▋  | 1967/2541 [33:24:12<9:46:26, 61.30s/it] 77%|███████▋  | 1968/2541 [33:25:13<9:44:07, 61.16s/it] 77%|███████▋  | 1969/2541 [33:26:14<9:42:14, 61.08s/it] 78%|███████▊  | 1970/2541 [33:27:14<9:40:36, 61.01s/it]                                                        {'loss': 1.3483, 'learning_rate': 5.97530499260959e-06, 'epoch': 0.78}
 78%|███████▊  | 1970/2541 [33:27:14<9:40:36, 61.01s/it] 78%|███████▊  | 1971/2541 [33:28:16<9:41:08, 61.17s/it] 78%|███████▊  | 1972/2541 [33:29:17<9:39:53, 61.15s/it] 78%|███████▊  | 1973/2541 [33:30:18<9:38:20, 61.09s/it] 78%|███████▊  | 1974/2541 [33:31:19<9:37:03, 61.06s/it] 78%|███████▊  | 1975/2541 [33:32:21<9:37:32, 61.22s/it] 78%|███████▊  | 1976/2541 [33:33:22<9:35:36, 61.13s/it] 78%|███████▊  | 1977/2541 [33:34:23<9:34:04, 61.07s/it] 78%|███████▊  | 1978/2541 [33:35:24<9:34:30, 61.23s/it] 78%|███████▊  | 1979/2541 [33:36:26<9:34:40, 61.35s/it] 78%|███████▊  | 1980/2541 [33:37:27<9:32:25, 61.22s/it]                                                        {'loss': 1.3592, 'learning_rate': 5.776236866515947e-06, 'epoch': 0.78}
 78%|███████▊  | 1980/2541 [33:37:27<9:32:25, 61.22s/it] 78%|███████▊  | 1981/2541 [33:38:27<9:30:10, 61.09s/it] 78%|███████▊  | 1982/2541 [33:39:28<9:28:49, 61.05s/it] 78%|███████▊  | 1983/2541 [33:40:30<9:29:20, 61.22s/it] 78%|███████▊  | 1984/2541 [33:41:31<9:28:28, 61.24s/it] 78%|███████▊  | 1985/2541 [33:42:32<9:26:13, 61.10s/it] 78%|███████▊  | 1986/2541 [33:43:33<9:24:55, 61.07s/it] 78%|███████▊  | 1987/2541 [33:44:35<9:25:35, 61.26s/it] 78%|███████▊  | 1988/2541 [33:45:36<9:23:47, 61.17s/it] 78%|███████▊  | 1989/2541 [33:46:37<9:23:41, 61.27s/it] 78%|███████▊  | 1990/2541 [33:47:39<9:22:57, 61.30s/it]                                                        {'loss': 1.3618, 'learning_rate': 5.580107224147979e-06, 'epoch': 0.78}
 78%|███████▊  | 1990/2541 [33:47:39<9:22:57, 61.30s/it] 78%|███████▊  | 1991/2541 [33:48:40<9:22:39, 61.38s/it] 78%|███████▊  | 1992/2541 [33:49:41<9:20:25, 61.25s/it] 78%|███████▊  | 1993/2541 [33:50:42<9:18:42, 61.17s/it] 78%|███████▊  | 1994/2541 [33:51:43<9:18:07, 61.22s/it] 79%|███████▊  | 1995/2541 [33:52:45<9:17:54, 61.31s/it] 79%|███████▊  | 1996/2541 [33:53:46<9:15:56, 61.21s/it] 79%|███████▊  | 1997/2541 [33:54:47<9:14:18, 61.14s/it] 79%|███████▊  | 1998/2541 [33:55:48<9:13:09, 61.12s/it] 79%|███████▊  | 1999/2541 [33:56:49<9:13:04, 61.23s/it] 79%|███████▊  | 2000/2541 [33:57:51<9:11:45, 61.19s/it]                                                        {'loss': 1.3582, 'learning_rate': 5.386946045263444e-06, 'epoch': 0.79}
 79%|███████▊  | 2000/2541 [33:57:51<9:11:45, 61.19s/it] 79%|███████▊  | 2001/2541 [33:58:52<9:10:48, 61.20s/it] 79%|███████▉  | 2002/2541 [33:59:53<9:09:17, 61.14s/it] 79%|███████▉  | 2003/2541 [34:00:54<9:09:11, 61.25s/it] 79%|███████▉  | 2004/2541 [34:01:55<9:07:23, 61.16s/it] 79%|███████▉  | 2005/2541 [34:02:56<9:05:25, 61.05s/it] 79%|███████▉  | 2006/2541 [34:03:57<9:04:15, 61.04s/it] 79%|███████▉  | 2007/2541 [34:04:59<9:05:03, 61.24s/it] 79%|███████▉  | 2008/2541 [34:06:00<9:03:15, 61.15s/it] 79%|███████▉  | 2009/2541 [34:07:01<9:01:32, 61.08s/it] 79%|███████▉  | 2010/2541 [34:08:02<9:00:35, 61.08s/it]                                                        {'loss': 1.346, 'learning_rate': 5.19678285587018e-06, 'epoch': 0.79}
 79%|███████▉  | 2010/2541 [34:08:02<9:00:35, 61.08s/it] 79%|███████▉  | 2011/2541 [34:09:03<9:00:40, 61.21s/it] 79%|███████▉  | 2012/2541 [34:10:05<8:59:50, 61.23s/it] 79%|███████▉  | 2013/2541 [34:11:06<8:58:34, 61.20s/it] 79%|███████▉  | 2014/2541 [34:12:07<8:57:23, 61.18s/it] 79%|███████▉  | 2015/2541 [34:13:08<8:57:40, 61.33s/it] 79%|███████▉  | 2016/2541 [34:14:09<8:55:38, 61.22s/it] 79%|███████▉  | 2017/2541 [34:15:11<8:54:30, 61.20s/it] 79%|███████▉  | 2018/2541 [34:16:12<8:53:41, 61.23s/it] 79%|███████▉  | 2019/2541 [34:17:13<8:53:28, 61.32s/it] 79%|███████▉  | 2020/2541 [34:18:14<8:51:35, 61.22s/it]                                                        {'loss': 1.3558, 'learning_rate': 5.009646723712777e-06, 'epoch': 0.79}
 79%|███████▉  | 2020/2541 [34:18:14<8:51:35, 61.22s/it] 80%|███████▉  | 2021/2541 [34:19:15<8:49:41, 61.12s/it] 80%|███████▉  | 2022/2541 [34:20:16<8:48:55, 61.15s/it] 80%|███████▉  | 2023/2541 [34:21:18<8:48:51, 61.26s/it] 80%|███████▉  | 2024/2541 [34:22:19<8:48:03, 61.28s/it] 80%|███████▉  | 2025/2541 [34:23:20<8:45:53, 61.15s/it] 80%|███████▉  | 2026/2541 [34:24:21<8:44:29, 61.11s/it] 80%|███████▉  | 2027/2541 [34:25:23<8:45:14, 61.31s/it] 80%|███████▉  | 2028/2541 [34:26:24<8:43:07, 61.18s/it] 80%|███████▉  | 2029/2541 [34:27:25<8:41:42, 61.14s/it] 80%|███████▉  | 2030/2541 [34:28:26<8:41:25, 61.22s/it]                                                        {'loss': 1.3472, 'learning_rate': 4.82556625382945e-06, 'epoch': 0.8}
 80%|███████▉  | 2030/2541 [34:28:26<8:41:25, 61.22s/it] 80%|███████▉  | 2031/2541 [34:29:28<8:41:25, 61.34s/it] 80%|███████▉  | 2032/2541 [34:30:29<8:39:27, 61.23s/it] 80%|████████  | 2033/2541 [34:31:30<8:37:20, 61.10s/it] 80%|████████  | 2034/2541 [34:32:31<8:36:07, 61.08s/it] 80%|████████  | 2035/2541 [34:33:33<8:37:25, 61.36s/it] 80%|████████  | 2036/2541 [34:34:34<8:35:50, 61.29s/it] 80%|████████  | 2037/2541 [34:35:35<8:34:07, 61.21s/it] 80%|████████  | 2038/2541 [34:36:36<8:32:48, 61.17s/it] 80%|████████  | 2039/2541 [34:37:38<8:32:40, 61.28s/it] 80%|████████  | 2040/2541 [34:38:38<8:30:31, 61.14s/it]                                                        {'loss': 1.3613, 'learning_rate': 4.644569584179509e-06, 'epoch': 0.8}
 80%|████████  | 2040/2541 [34:38:38<8:30:31, 61.14s/it] 80%|████████  | 2041/2541 [34:39:39<8:29:22, 61.12s/it] 80%|████████  | 2042/2541 [34:40:40<8:28:08, 61.10s/it] 80%|████████  | 2043/2541 [34:41:42<8:27:59, 61.20s/it] 80%|████████  | 2044/2541 [34:42:43<8:26:23, 61.13s/it] 80%|████████  | 2045/2541 [34:43:44<8:25:02, 61.09s/it] 81%|████████  | 2046/2541 [34:44:45<8:23:55, 61.08s/it] 81%|████████  | 2047/2541 [34:45:47<8:24:29, 61.27s/it] 81%|████████  | 2048/2541 [34:46:48<8:22:49, 61.20s/it] 81%|████████  | 2049/2541 [34:47:48<8:20:53, 61.08s/it] 81%|████████  | 2050/2541 [34:48:50<8:20:13, 61.13s/it]                                                        {'loss': 1.3552, 'learning_rate': 4.466684381342317e-06, 'epoch': 0.81}
 81%|████████  | 2050/2541 [34:48:50<8:20:13, 61.13s/it] 81%|████████  | 2051/2541 [34:49:51<8:20:10, 61.25s/it] 81%|████████  | 2052/2541 [34:50:52<8:18:35, 61.18s/it] 81%|████████  | 2053/2541 [34:51:54<8:17:48, 61.21s/it] 81%|████████  | 2054/2541 [34:52:55<8:16:43, 61.20s/it] 81%|████████  | 2055/2541 [34:53:56<8:16:33, 61.30s/it] 81%|████████  | 2056/2541 [34:54:57<8:15:09, 61.26s/it] 81%|████████  | 2057/2541 [34:55:58<8:13:18, 61.15s/it] 81%|████████  | 2058/2541 [34:57:00<8:12:49, 61.22s/it] 81%|████████  | 2059/2541 [34:58:02<8:13:41, 61.46s/it] 81%|████████  | 2060/2541 [34:59:03<8:12:07, 61.39s/it]                                                        {'loss': 1.3399, 'learning_rate': 4.291937836288221e-06, 'epoch': 0.81}
 81%|████████  | 2060/2541 [34:59:03<8:12:07, 61.39s/it] 81%|████████  | 2061/2541 [35:00:04<8:10:47, 61.35s/it] 81%|████████  | 2062/2541 [35:01:06<8:09:57, 61.37s/it] 81%|████████  | 2063/2541 [35:02:07<8:10:00, 61.51s/it] 81%|████████  | 2064/2541 [35:03:08<8:07:51, 61.36s/it] 81%|████████▏ | 2065/2541 [35:04:09<8:05:41, 61.22s/it] 81%|████████▏ | 2066/2541 [35:05:10<8:04:29, 61.20s/it] 81%|████████▏ | 2067/2541 [35:06:12<8:04:22, 61.31s/it] 81%|████████▏ | 2068/2541 [35:07:13<8:03:24, 61.32s/it] 81%|████████▏ | 2069/2541 [35:08:14<8:01:38, 61.22s/it] 81%|████████▏ | 2070/2541 [35:09:16<8:01:40, 61.36s/it]                                                        {'loss': 1.3498, 'learning_rate': 4.1203566602222745e-06, 'epoch': 0.81}
 81%|████████▏ | 2070/2541 [35:09:16<8:01:40, 61.36s/it] 82%|████████▏ | 2071/2541 [35:10:18<8:01:20, 61.45s/it] 82%|████████▏ | 2072/2541 [35:11:19<7:59:11, 61.30s/it] 82%|████████▏ | 2073/2541 [35:12:20<7:57:11, 61.18s/it] 82%|████████▏ | 2074/2541 [35:13:21<7:56:00, 61.16s/it] 82%|████████▏ | 2075/2541 [35:14:22<7:55:42, 61.25s/it] 82%|████████▏ | 2076/2541 [35:15:24<7:54:52, 61.27s/it] 82%|████████▏ | 2077/2541 [35:16:24<7:53:03, 61.17s/it] 82%|████████▏ | 2078/2541 [35:17:26<7:51:50, 61.15s/it] 82%|████████▏ | 2079/2541 [35:18:27<7:51:48, 61.27s/it] 82%|████████▏ | 2080/2541 [35:19:28<7:50:20, 61.22s/it]                                                        {'loss': 1.3577, 'learning_rate': 3.951967080501203e-06, 'epoch': 0.82}
 82%|████████▏ | 2080/2541 [35:19:28<7:50:20, 61.22s/it] 82%|████████▏ | 2081/2541 [35:20:29<7:48:41, 61.13s/it] 82%|████████▏ | 2082/2541 [35:21:31<7:48:38, 61.26s/it] 82%|████████▏ | 2083/2541 [35:22:32<7:48:51, 61.42s/it] 82%|████████▏ | 2084/2541 [35:23:33<7:46:46, 61.28s/it] 82%|████████▏ | 2085/2541 [35:24:34<7:44:59, 61.18s/it] 82%|████████▏ | 2086/2541 [35:25:36<7:43:50, 61.17s/it] 82%|████████▏ | 2087/2541 [35:26:37<7:44:14, 61.35s/it] 82%|████████▏ | 2088/2541 [35:27:38<7:42:19, 61.24s/it] 82%|████████▏ | 2089/2541 [35:28:39<7:40:40, 61.15s/it] 82%|████████▏ | 2090/2541 [35:29:40<7:39:18, 61.11s/it]                                                        {'loss': 1.3551, 'learning_rate': 3.786794836624416e-06, 'epoch': 0.82}
 82%|████████▏ | 2090/2541 [35:29:40<7:39:18, 61.11s/it] 82%|████████▏ | 2091/2541 [35:30:42<7:39:13, 61.23s/it] 82%|████████▏ | 2092/2541 [35:31:43<7:37:28, 61.13s/it] 82%|████████▏ | 2093/2541 [35:32:44<7:36:33, 61.15s/it] 82%|████████▏ | 2094/2541 [35:33:45<7:35:59, 61.21s/it] 82%|████████▏ | 2095/2541 [35:34:47<7:35:37, 61.29s/it] 82%|████████▏ | 2096/2541 [35:35:48<7:33:44, 61.18s/it] 83%|████████▎ | 2097/2541 [35:36:49<7:32:19, 61.12s/it] 83%|████████▎ | 2098/2541 [35:37:50<7:31:34, 61.16s/it] 83%|████████▎ | 2099/2541 [35:38:51<7:31:30, 61.29s/it] 83%|████████▎ | 2100/2541 [35:39:53<7:30:16, 61.26s/it]                                                        {'loss': 1.353, 'learning_rate': 3.6248651762994995e-06, 'epoch': 0.83}
 83%|████████▎ | 2100/2541 [35:39:53<7:30:16, 61.26s/it] 83%|████████▎ | 2101/2541 [35:40:54<7:29:10, 61.25s/it] 83%|████████▎ | 2102/2541 [35:41:55<7:27:58, 61.23s/it] 83%|████████▎ | 2103/2541 [35:42:57<7:27:46, 61.34s/it] 83%|████████▎ | 2104/2541 [35:43:58<7:25:54, 61.22s/it] 83%|████████▎ | 2105/2541 [35:44:59<7:24:44, 61.20s/it] 83%|████████▎ | 2106/2541 [35:46:00<7:23:46, 61.21s/it] 83%|████████▎ | 2107/2541 [35:47:01<7:23:10, 61.27s/it] 83%|████████▎ | 2108/2541 [35:48:02<7:21:35, 61.19s/it] 83%|████████▎ | 2109/2541 [35:49:03<7:20:12, 61.14s/it] 83%|████████▎ | 2110/2541 [35:50:05<7:19:43, 61.21s/it]                                                        {'loss': 1.3571, 'learning_rate': 3.4662028515829863e-06, 'epoch': 0.83}
 83%|████████▎ | 2110/2541 [35:50:05<7:19:43, 61.21s/it] 83%|████████▎ | 2111/2541 [35:51:06<7:19:22, 61.31s/it] 83%|████████▎ | 2112/2541 [35:52:07<7:17:44, 61.22s/it] 83%|████████▎ | 2113/2541 [35:53:08<7:16:09, 61.14s/it] 83%|████████▎ | 2114/2541 [35:54:10<7:15:23, 61.18s/it] 83%|████████▎ | 2115/2541 [35:55:11<7:15:08, 61.29s/it] 83%|████████▎ | 2116/2541 [35:56:12<7:13:35, 61.21s/it] 83%|████████▎ | 2117/2541 [35:57:13<7:12:25, 61.19s/it] 83%|████████▎ | 2118/2541 [35:58:14<7:11:09, 61.16s/it] 83%|████████▎ | 2119/2541 [35:59:16<7:11:10, 61.30s/it] 83%|████████▎ | 2120/2541 [36:00:17<7:09:18, 61.18s/it]                                                        {'loss': 1.3491, 'learning_rate': 3.3108321150967763e-06, 'epoch': 0.83}
 83%|████████▎ | 2120/2541 [36:00:17<7:09:18, 61.18s/it] 83%|████████▎ | 2121/2541 [36:01:18<7:08:11, 61.17s/it] 84%|████████▎ | 2122/2541 [36:02:19<7:07:28, 61.21s/it] 84%|████████▎ | 2123/2541 [36:03:21<7:07:48, 61.41s/it] 84%|████████▎ | 2124/2541 [36:04:22<7:05:38, 61.24s/it] 84%|████████▎ | 2125/2541 [36:05:23<7:03:52, 61.13s/it] 84%|████████▎ | 2126/2541 [36:06:24<7:02:38, 61.11s/it] 84%|████████▎ | 2127/2541 [36:07:25<7:02:21, 61.21s/it] 84%|████████▎ | 2128/2541 [36:08:27<7:01:29, 61.23s/it] 84%|████████▍ | 2129/2541 [36:09:28<6:59:48, 61.14s/it] 84%|████████▍ | 2130/2541 [36:10:29<6:58:36, 61.11s/it]                                                        {'loss': 1.3415, 'learning_rate': 3.1587767163210157e-06, 'epoch': 0.84}
 84%|████████▍ | 2130/2541 [36:10:29<6:58:36, 61.11s/it] 84%|████████▍ | 2131/2541 [36:11:30<6:58:51, 61.30s/it] 84%|████████▍ | 2132/2541 [36:12:31<6:56:58, 61.17s/it] 84%|████████▍ | 2133/2541 [36:13:32<6:55:54, 61.16s/it] 84%|████████▍ | 2134/2541 [36:14:34<6:54:55, 61.17s/it] 84%|████████▍ | 2135/2541 [36:15:35<6:54:42, 61.29s/it] 84%|████████▍ | 2136/2541 [36:16:36<6:53:03, 61.19s/it] 84%|████████▍ | 2137/2541 [36:17:37<6:51:24, 61.10s/it] 84%|████████▍ | 2138/2541 [36:18:38<6:50:21, 61.09s/it] 84%|████████▍ | 2139/2541 [36:19:40<6:50:36, 61.28s/it] 84%|████████▍ | 2140/2541 [36:20:41<6:49:19, 61.24s/it]                                                        {'loss': 1.3568, 'learning_rate': 3.0100598979637713e-06, 'epoch': 0.84}
 84%|████████▍ | 2140/2541 [36:20:41<6:49:19, 61.24s/it] 84%|████████▍ | 2141/2541 [36:21:42<6:47:39, 61.15s/it] 84%|████████▍ | 2142/2541 [36:22:43<6:46:30, 61.13s/it] 84%|████████▍ | 2143/2541 [36:23:45<6:46:38, 61.30s/it] 84%|████████▍ | 2144/2541 [36:24:46<6:45:09, 61.23s/it] 84%|████████▍ | 2145/2541 [36:25:47<6:44:12, 61.24s/it] 84%|████████▍ | 2146/2541 [36:26:49<6:43:36, 61.31s/it] 84%|████████▍ | 2147/2541 [36:27:50<6:43:23, 61.43s/it] 85%|████████▍ | 2148/2541 [36:28:51<6:41:50, 61.35s/it] 85%|████████▍ | 2149/2541 [36:29:52<6:39:56, 61.22s/it] 85%|████████▍ | 2150/2541 [36:30:53<6:38:46, 61.19s/it]                                                        {'loss': 1.3594, 'learning_rate': 2.8647043924082793e-06, 'epoch': 0.85}
 85%|████████▍ | 2150/2541 [36:30:53<6:38:46, 61.19s/it] 85%|████████▍ | 2151/2541 [36:31:55<6:38:37, 61.33s/it] 85%|████████▍ | 2152/2541 [36:32:56<6:36:49, 61.21s/it] 85%|████████▍ | 2153/2541 [36:33:57<6:35:10, 61.11s/it] 85%|████████▍ | 2154/2541 [36:34:58<6:33:58, 61.08s/it] 85%|████████▍ | 2155/2541 [36:35:59<6:33:29, 61.17s/it] 85%|████████▍ | 2156/2541 [36:37:00<6:32:19, 61.14s/it] 85%|████████▍ | 2157/2541 [36:38:01<6:31:12, 61.13s/it] 85%|████████▍ | 2158/2541 [36:39:03<6:30:12, 61.13s/it] 85%|████████▍ | 2159/2541 [36:40:04<6:29:49, 61.23s/it] 85%|████████▌ | 2160/2541 [36:41:05<6:28:09, 61.13s/it]                                                        {'loss': 1.35, 'learning_rate': 2.7227324182380775e-06, 'epoch': 0.85}
 85%|████████▌ | 2160/2541 [36:41:05<6:28:09, 61.13s/it] 85%|████████▌ | 2161/2541 [36:42:06<6:26:44, 61.06s/it] 85%|████████▌ | 2162/2541 [36:43:07<6:26:00, 61.11s/it] 85%|████████▌ | 2163/2541 [36:44:09<6:26:24, 61.33s/it] 85%|████████▌ | 2164/2541 [36:45:10<6:24:44, 61.23s/it] 85%|████████▌ | 2165/2541 [36:46:11<6:23:13, 61.15s/it] 85%|████████▌ | 2166/2541 [36:47:12<6:21:44, 61.08s/it] 85%|████████▌ | 2167/2541 [36:48:13<6:21:33, 61.21s/it] 85%|████████▌ | 2168/2541 [36:49:14<6:20:06, 61.14s/it] 85%|████████▌ | 2169/2541 [36:50:16<6:19:24, 61.19s/it] 85%|████████▌ | 2170/2541 [36:51:17<6:18:04, 61.14s/it]                                                        {'loss': 1.3613, 'learning_rate': 2.584165676840822e-06, 'epoch': 0.85}
 85%|████████▌ | 2170/2541 [36:51:17<6:18:04, 61.14s/it] 85%|████████▌ | 2171/2541 [36:52:18<6:17:45, 61.26s/it] 85%|████████▌ | 2172/2541 [36:53:19<6:16:08, 61.16s/it] 86%|████████▌ | 2173/2541 [36:54:20<6:14:25, 61.05s/it] 86%|████████▌ | 2174/2541 [36:55:21<6:14:04, 61.16s/it] 86%|████████▌ | 2175/2541 [36:56:23<6:13:42, 61.26s/it] 86%|████████▌ | 2176/2541 [36:57:24<6:11:57, 61.14s/it] 86%|████████▌ | 2177/2541 [36:58:25<6:10:39, 61.10s/it] 86%|████████▌ | 2178/2541 [36:59:26<6:09:55, 61.14s/it] 86%|████████▌ | 2179/2541 [37:00:28<6:09:58, 61.32s/it] 86%|████████▌ | 2180/2541 [37:01:29<6:08:43, 61.28s/it]                                                        {'loss': 1.3581, 'learning_rate': 2.4490253490910083e-06, 'epoch': 0.86}
 86%|████████▌ | 2180/2541 [37:01:29<6:08:43, 61.28s/it] 86%|████████▌ | 2181/2541 [37:02:30<6:08:22, 61.40s/it] 86%|████████▌ | 2182/2541 [37:03:32<6:07:01, 61.34s/it] 86%|████████▌ | 2183/2541 [37:04:33<6:06:26, 61.41s/it] 86%|████████▌ | 2184/2541 [37:05:34<6:04:36, 61.28s/it] 86%|████████▌ | 2185/2541 [37:06:35<6:02:57, 61.17s/it] 86%|████████▌ | 2186/2541 [37:07:37<6:02:28, 61.26s/it] 86%|████████▌ | 2187/2541 [37:08:38<6:02:00, 61.36s/it] 86%|████████▌ | 2188/2541 [37:09:39<6:00:16, 61.24s/it] 86%|████████▌ | 2189/2541 [37:10:40<5:58:56, 61.18s/it] 86%|████████▌ | 2190/2541 [37:11:41<5:58:00, 61.20s/it]                                                        {'loss': 1.3582, 'learning_rate': 2.317332092112384e-06, 'epoch': 0.86}
 86%|████████▌ | 2190/2541 [37:11:41<5:58:00, 61.20s/it] 86%|████████▌ | 2191/2541 [37:12:43<5:57:34, 61.30s/it] 86%|████████▋ | 2192/2541 [37:13:44<5:56:18, 61.26s/it] 86%|████████▋ | 2193/2541 [37:14:45<5:54:32, 61.13s/it] 86%|████████▋ | 2194/2541 [37:15:46<5:53:20, 61.10s/it] 86%|████████▋ | 2195/2541 [37:16:48<5:53:07, 61.23s/it] 86%|████████▋ | 2196/2541 [37:17:48<5:51:34, 61.14s/it] 86%|████████▋ | 2197/2541 [37:18:50<5:50:37, 61.15s/it] 87%|████████▋ | 2198/2541 [37:19:51<5:49:49, 61.19s/it] 87%|████████▋ | 2199/2541 [37:20:53<5:49:29, 61.31s/it] 87%|████████▋ | 2200/2541 [37:21:53<5:47:49, 61.20s/it]                                                        {'loss': 1.3545, 'learning_rate': 2.189106036120328e-06, 'epoch': 0.87}
 87%|████████▋ | 2200/2541 [37:21:53<5:47:49, 61.20s/it] 87%|████████▋ | 2201/2541 [37:22:55<5:46:51, 61.21s/it] 87%|████████▋ | 2202/2541 [37:23:56<5:46:00, 61.24s/it] 87%|████████▋ | 2203/2541 [37:24:58<5:46:31, 61.51s/it] 87%|████████▋ | 2204/2541 [37:25:59<5:44:39, 61.36s/it] 87%|████████▋ | 2205/2541 [37:27:00<5:42:52, 61.23s/it] 87%|████████▋ | 2206/2541 [37:28:01<5:41:42, 61.20s/it] 87%|████████▋ | 2207/2541 [37:29:03<5:41:11, 61.29s/it] 87%|████████▋ | 2208/2541 [37:30:04<5:39:33, 61.18s/it] 87%|████████▋ | 2209/2541 [37:31:05<5:38:52, 61.24s/it] 87%|████████▋ | 2210/2541 [37:32:06<5:37:24, 61.16s/it]                                                        {'loss': 1.3514, 'learning_rate': 2.0643667813448425e-06, 'epoch': 0.87}
 87%|████████▋ | 2210/2541 [37:32:06<5:37:24, 61.16s/it] 87%|████████▋ | 2211/2541 [37:33:08<5:36:58, 61.27s/it] 87%|████████▋ | 2212/2541 [37:34:08<5:35:10, 61.13s/it] 87%|████████▋ | 2213/2541 [37:35:09<5:33:44, 61.05s/it] 87%|████████▋ | 2214/2541 [37:36:10<5:32:52, 61.08s/it] 87%|████████▋ | 2215/2541 [37:37:12<5:33:08, 61.31s/it] 87%|████████▋ | 2216/2541 [37:38:13<5:31:45, 61.25s/it] 87%|████████▋ | 2217/2541 [37:39:14<5:30:16, 61.16s/it] 87%|████████▋ | 2218/2541 [37:40:15<5:29:16, 61.16s/it] 87%|████████▋ | 2219/2541 [37:41:17<5:29:03, 61.31s/it] 87%|████████▋ | 2220/2541 [37:42:18<5:28:04, 61.32s/it]                                                        {'loss': 1.3541, 'learning_rate': 1.9431333950344855e-06, 'epoch': 0.87}
 87%|████████▋ | 2220/2541 [37:42:18<5:28:04, 61.32s/it] 87%|████████▋ | 2221/2541 [37:43:20<5:27:02, 61.32s/it] 87%|████████▋ | 2222/2541 [37:44:21<5:25:43, 61.26s/it] 87%|████████▋ | 2223/2541 [37:45:23<5:25:15, 61.37s/it] 88%|████████▊ | 2224/2541 [37:46:23<5:23:35, 61.25s/it] 88%|████████▊ | 2225/2541 [37:47:25<5:22:11, 61.18s/it] 88%|████████▊ | 2226/2541 [37:48:26<5:21:30, 61.24s/it] 88%|████████▊ | 2227/2541 [37:49:27<5:21:02, 61.35s/it] 88%|████████▊ | 2228/2541 [37:50:28<5:19:25, 61.23s/it] 88%|████████▊ | 2229/2541 [37:51:29<5:18:02, 61.16s/it] 88%|████████▊ | 2230/2541 [37:52:31<5:16:58, 61.15s/it]                                                        {'loss': 1.3425, 'learning_rate': 1.8254244085418565e-06, 'epoch': 0.88}
 88%|████████▊ | 2230/2541 [37:52:31<5:16:58, 61.15s/it] 88%|████████▊ | 2231/2541 [37:53:32<5:16:33, 61.27s/it] 88%|████████▊ | 2232/2541 [37:54:33<5:15:27, 61.26s/it] 88%|████████▊ | 2233/2541 [37:55:34<5:13:49, 61.13s/it] 88%|████████▊ | 2234/2541 [37:56:35<5:12:50, 61.14s/it] 88%|████████▊ | 2235/2541 [37:57:37<5:12:13, 61.22s/it] 88%|████████▊ | 2236/2541 [37:58:38<5:10:53, 61.16s/it] 88%|████████▊ | 2237/2541 [37:59:39<5:09:47, 61.14s/it] 88%|████████▊ | 2238/2541 [38:00:40<5:09:23, 61.27s/it] 88%|████████▊ | 2239/2541 [38:01:42<5:09:00, 61.39s/it] 88%|████████▊ | 2240/2541 [38:02:43<5:07:28, 61.29s/it]                                                        {'loss': 1.3581, 'learning_rate': 1.7112578144909048e-06, 'epoch': 0.88}
 88%|████████▊ | 2240/2541 [38:02:43<5:07:28, 61.29s/it] 88%|████████▊ | 2241/2541 [38:03:44<5:06:13, 61.25s/it] 88%|████████▊ | 2242/2541 [38:04:46<5:05:15, 61.26s/it] 88%|████████▊ | 2243/2541 [38:05:47<5:05:00, 61.41s/it] 88%|████████▊ | 2244/2541 [38:06:49<5:03:41, 61.35s/it] 88%|████████▊ | 2245/2541 [38:07:50<5:02:10, 61.25s/it] 88%|████████▊ | 2246/2541 [38:08:51<5:01:15, 61.27s/it] 88%|████████▊ | 2247/2541 [38:09:53<5:00:44, 61.38s/it] 88%|████████▊ | 2248/2541 [38:10:54<4:59:35, 61.35s/it] 89%|████████▊ | 2249/2541 [38:11:55<4:58:06, 61.25s/it] 89%|████████▊ | 2250/2541 [38:12:56<4:57:04, 61.25s/it]                                                        {'loss': 1.349, 'learning_rate': 1.6006510640266787e-06, 'epoch': 0.89}
 89%|████████▊ | 2250/2541 [38:12:56<4:57:04, 61.25s/it] 89%|████████▊ | 2251/2541 [38:13:57<4:56:14, 61.29s/it] 89%|████████▊ | 2252/2541 [38:14:59<4:55:08, 61.28s/it] 89%|████████▊ | 2253/2541 [38:16:00<4:53:28, 61.14s/it] 89%|████████▊ | 2254/2541 [38:17:01<4:52:47, 61.21s/it] 89%|████████▊ | 2255/2541 [38:18:03<4:52:57, 61.46s/it] 89%|████████▉ | 2256/2541 [38:19:04<4:51:19, 61.33s/it] 89%|████████▉ | 2257/2541 [38:20:05<4:49:38, 61.19s/it] 89%|████████▉ | 2258/2541 [38:21:06<4:48:58, 61.27s/it] 89%|████████▉ | 2259/2541 [38:22:08<4:48:21, 61.35s/it] 89%|████████▉ | 2260/2541 [38:23:09<4:46:48, 61.24s/it]                                                        {'loss': 1.3541, 'learning_rate': 1.49362106414776e-06, 'epoch': 0.89}
 89%|████████▉ | 2260/2541 [38:23:09<4:46:48, 61.24s/it] 89%|████████▉ | 2261/2541 [38:24:10<4:45:56, 61.27s/it] 89%|████████▉ | 2262/2541 [38:25:11<4:44:56, 61.28s/it] 89%|████████▉ | 2263/2541 [38:26:13<4:44:11, 61.34s/it] 89%|████████▉ | 2264/2541 [38:27:14<4:42:48, 61.26s/it] 89%|████████▉ | 2265/2541 [38:28:15<4:41:15, 61.14s/it] 89%|████████▉ | 2266/2541 [38:29:16<4:40:35, 61.22s/it] 89%|████████▉ | 2267/2541 [38:30:18<4:40:26, 61.41s/it] 89%|████████▉ | 2268/2541 [38:31:20<4:39:25, 61.41s/it] 89%|████████▉ | 2269/2541 [38:32:21<4:37:44, 61.27s/it] 89%|████████▉ | 2270/2541 [38:33:22<4:36:51, 61.30s/it]                                                        {'loss': 1.3551, 'learning_rate': 1.3901841751219475e-06, 'epoch': 0.89}
 89%|████████▉ | 2270/2541 [38:33:22<4:36:51, 61.30s/it] 89%|████████▉ | 2271/2541 [38:34:23<4:35:46, 61.28s/it] 89%|████████▉ | 2272/2541 [38:35:24<4:34:46, 61.29s/it] 89%|████████▉ | 2273/2541 [38:36:25<4:33:27, 61.22s/it] 89%|████████▉ | 2274/2541 [38:37:27<4:32:24, 61.22s/it] 90%|████████▉ | 2275/2541 [38:38:28<4:31:21, 61.21s/it] 90%|████████▉ | 2276/2541 [38:39:29<4:30:12, 61.18s/it] 90%|████████▉ | 2277/2541 [38:40:30<4:29:05, 61.16s/it] 90%|████████▉ | 2278/2541 [38:41:32<4:28:41, 61.30s/it] 90%|████████▉ | 2279/2541 [38:42:33<4:27:25, 61.24s/it] 90%|████████▉ | 2280/2541 [38:43:34<4:25:58, 61.14s/it]                                                        {'loss': 1.353, 'learning_rate': 1.2903562079854492e-06, 'epoch': 0.9}
 90%|████████▉ | 2280/2541 [38:43:34<4:25:58, 61.14s/it] 90%|████████▉ | 2281/2541 [38:44:35<4:24:51, 61.12s/it] 90%|████████▉ | 2282/2541 [38:45:37<4:24:35, 61.30s/it] 90%|████████▉ | 2283/2541 [38:46:38<4:23:10, 61.21s/it] 90%|████████▉ | 2284/2541 [38:47:39<4:22:10, 61.21s/it] 90%|████████▉ | 2285/2541 [38:48:40<4:21:18, 61.24s/it] 90%|████████▉ | 2286/2541 [38:49:42<4:20:43, 61.35s/it] 90%|█████████ | 2287/2541 [38:50:43<4:19:29, 61.30s/it] 90%|█████████ | 2288/2541 [38:51:44<4:17:58, 61.18s/it] 90%|█████████ | 2289/2541 [38:52:45<4:16:51, 61.15s/it] 90%|█████████ | 2290/2541 [38:53:47<4:17:01, 61.44s/it]                                                        {'loss': 1.3409, 'learning_rate': 1.1941524221260715e-06, 'epoch': 0.9}
 90%|█████████ | 2290/2541 [38:53:47<4:17:01, 61.44s/it] 90%|█████████ | 2291/2541 [38:54:48<4:15:21, 61.29s/it] 90%|█████████ | 2292/2541 [38:55:49<4:14:07, 61.24s/it] 90%|█████████ | 2293/2541 [38:56:50<4:13:12, 61.26s/it] 90%|█████████ | 2294/2541 [38:57:52<4:12:35, 61.36s/it] 90%|█████████ | 2295/2541 [38:58:53<4:11:29, 61.34s/it] 90%|█████████ | 2296/2541 [38:59:54<4:09:59, 61.22s/it] 90%|█████████ | 2297/2541 [39:00:55<4:08:45, 61.17s/it] 90%|█████████ | 2298/2541 [39:01:57<4:08:23, 61.33s/it] 90%|█████████ | 2299/2541 [39:02:58<4:06:52, 61.21s/it] 91%|█████████ | 2300/2541 [39:03:59<4:05:34, 61.14s/it]                                                        {'loss': 1.3506, 'learning_rate': 1.1015875229507172e-06, 'epoch': 0.9}
 91%|█████████ | 2300/2541 [39:03:59<4:05:34, 61.14s/it] 91%|█████████ | 2301/2541 [39:05:00<4:05:01, 61.25s/it] 91%|█████████ | 2302/2541 [39:06:02<4:04:49, 61.46s/it] 91%|█████████ | 2303/2541 [39:07:03<4:03:12, 61.31s/it] 91%|█████████ | 2304/2541 [39:08:04<4:01:45, 61.21s/it] 91%|█████████ | 2305/2541 [39:09:05<4:00:29, 61.14s/it] 91%|█████████ | 2306/2541 [39:10:07<4:00:07, 61.31s/it] 91%|█████████ | 2307/2541 [39:11:08<3:58:49, 61.24s/it] 91%|█████████ | 2308/2541 [39:12:09<3:57:54, 61.26s/it] 91%|█████████ | 2309/2541 [39:13:10<3:56:44, 61.23s/it] 91%|█████████ | 2310/2541 [39:14:12<3:56:20, 61.39s/it]                                                        {'loss': 1.3564, 'learning_rate': 1.0126756596375686e-06, 'epoch': 0.91}
 91%|█████████ | 2310/2541 [39:14:12<3:56:20, 61.39s/it] 91%|█████████ | 2311/2541 [39:15:13<3:54:47, 61.25s/it] 91%|█████████ | 2312/2541 [39:16:14<3:53:27, 61.17s/it] 91%|█████████ | 2313/2541 [39:17:16<3:52:59, 61.31s/it] 91%|█████████ | 2314/2541 [39:18:17<3:52:28, 61.45s/it] 91%|█████████ | 2315/2541 [39:19:19<3:51:13, 61.39s/it] 91%|█████████ | 2316/2541 [39:20:20<3:49:40, 61.25s/it] 91%|█████████ | 2317/2541 [39:21:21<3:48:26, 61.19s/it] 91%|█████████ | 2318/2541 [39:22:22<3:48:01, 61.35s/it] 91%|█████████▏| 2319/2541 [39:23:23<3:46:32, 61.23s/it] 91%|█████████▏| 2320/2541 [39:24:24<3:45:03, 61.10s/it]                                                        {'loss': 1.3576, 'learning_rate': 9.274304229732688e-07, 'epoch': 0.91}
 91%|█████████▏| 2320/2541 [39:24:24<3:45:03, 61.10s/it] 91%|█████████▏| 2321/2541 [39:25:25<3:44:00, 61.09s/it] 91%|█████████▏| 2322/2541 [39:26:27<3:43:19, 61.19s/it] 91%|█████████▏| 2323/2541 [39:27:28<3:42:05, 61.12s/it] 91%|█████████▏| 2324/2541 [39:28:29<3:41:09, 61.15s/it] 91%|█████████▏| 2325/2541 [39:29:30<3:40:19, 61.20s/it] 92%|█████████▏| 2326/2541 [39:30:32<3:39:41, 61.31s/it] 92%|█████████▏| 2327/2541 [39:31:33<3:38:12, 61.18s/it] 92%|█████████▏| 2328/2541 [39:32:34<3:36:57, 61.11s/it] 92%|█████████▏| 2329/2541 [39:33:35<3:35:56, 61.12s/it] 92%|█████████▏| 2330/2541 [39:34:37<3:35:42, 61.34s/it]                                                        {'loss': 1.3482, 'learning_rate': 8.45864843275504e-07, 'epoch': 0.92}
 92%|█████████▏| 2330/2541 [39:34:37<3:35:42, 61.34s/it] 92%|█████████▏| 2331/2541 [39:35:38<3:34:25, 61.26s/it] 92%|█████████▏| 2332/2541 [39:36:39<3:33:05, 61.18s/it] 92%|█████████▏| 2333/2541 [39:37:40<3:31:59, 61.15s/it] 92%|█████████▏| 2334/2541 [39:38:41<3:31:27, 61.29s/it] 92%|█████████▏| 2335/2541 [39:39:42<3:30:02, 61.18s/it] 92%|█████████▏| 2336/2541 [39:40:44<3:29:15, 61.25s/it] 92%|█████████▏| 2337/2541 [39:41:45<3:28:04, 61.20s/it] 92%|█████████▏| 2338/2541 [39:42:46<3:27:31, 61.34s/it] 92%|█████████▏| 2339/2541 [39:43:47<3:26:09, 61.24s/it] 92%|█████████▏| 2340/2541 [39:44:48<3:24:46, 61.13s/it]                                                        {'loss': 1.3397, 'learning_rate': 7.679913884012069e-07, 'epoch': 0.92}
 92%|█████████▏| 2340/2541 [39:44:48<3:24:46, 61.13s/it] 92%|█████████▏| 2341/2541 [39:45:50<3:23:57, 61.19s/it] 92%|█████████▏| 2342/2541 [39:46:51<3:23:34, 61.38s/it] 92%|█████████▏| 2343/2541 [39:47:52<3:22:04, 61.24s/it] 92%|█████████▏| 2344/2541 [39:48:53<3:20:43, 61.13s/it] 92%|█████████▏| 2345/2541 [39:49:54<3:19:44, 61.15s/it] 92%|█████████▏| 2346/2541 [39:50:56<3:18:58, 61.22s/it] 92%|█████████▏| 2347/2541 [39:51:57<3:17:43, 61.15s/it] 92%|█████████▏| 2348/2541 [39:52:58<3:16:51, 61.20s/it] 92%|█████████▏| 2349/2541 [39:53:59<3:15:49, 61.19s/it] 92%|█████████▏| 2350/2541 [39:55:01<3:15:01, 61.27s/it]                                                        {'loss': 1.3558, 'learning_rate': 6.938219618407738e-07, 'epoch': 0.92}
 92%|█████████▏| 2350/2541 [39:55:01<3:15:01, 61.27s/it] 93%|█████████▎| 2351/2541 [39:56:02<3:13:44, 61.18s/it] 93%|█████████▎| 2352/2541 [39:57:03<3:12:29, 61.11s/it] 93%|█████████▎| 2353/2541 [39:58:04<3:11:34, 61.14s/it] 93%|█████████▎| 2354/2541 [39:59:06<3:11:25, 61.42s/it] 93%|█████████▎| 2355/2541 [40:00:07<3:09:56, 61.27s/it] 93%|█████████▎| 2356/2541 [40:01:08<3:08:44, 61.21s/it] 93%|█████████▎| 2357/2541 [40:02:09<3:07:48, 61.24s/it] 93%|█████████▎| 2358/2541 [40:03:11<3:07:13, 61.38s/it] 93%|█████████▎| 2359/2541 [40:04:12<3:06:09, 61.37s/it] 93%|█████████▎| 2360/2541 [40:05:13<3:04:45, 61.24s/it]                                                        {'loss': 1.358, 'learning_rate': 6.233679008985249e-07, 'epoch': 0.93}
 93%|█████████▎| 2360/2541 [40:05:13<3:04:45, 61.24s/it] 93%|█████████▎| 2361/2541 [40:06:14<3:03:35, 61.20s/it] 93%|█████████▎| 2362/2541 [40:07:16<3:02:52, 61.30s/it] 93%|█████████▎| 2363/2541 [40:08:17<3:01:28, 61.17s/it] 93%|█████████▎| 2364/2541 [40:09:18<3:00:25, 61.16s/it] 93%|█████████▎| 2365/2541 [40:10:19<2:59:21, 61.15s/it] 93%|█████████▎| 2366/2541 [40:11:21<2:58:46, 61.29s/it] 93%|█████████▎| 2367/2541 [40:12:22<2:57:31, 61.21s/it] 93%|█████████▎| 2368/2541 [40:13:23<2:56:19, 61.16s/it] 93%|█████████▎| 2369/2541 [40:14:24<2:55:22, 61.18s/it] 93%|█████████▎| 2370/2541 [40:15:25<2:54:39, 61.28s/it]                                                        {'loss': 1.3535, 'learning_rate': 5.566399749597328e-07, 'epoch': 0.93}
 93%|█████████▎| 2370/2541 [40:15:25<2:54:39, 61.28s/it] 93%|█████████▎| 2371/2541 [40:16:27<2:53:33, 61.26s/it] 93%|█████████▎| 2372/2541 [40:17:28<2:52:17, 61.17s/it] 93%|█████████▎| 2373/2541 [40:18:29<2:51:13, 61.15s/it] 93%|█████████▎| 2374/2541 [40:19:30<2:50:31, 61.27s/it] 93%|█████████▎| 2375/2541 [40:20:31<2:49:11, 61.15s/it] 94%|█████████▎| 2376/2541 [40:21:32<2:47:55, 61.07s/it] 94%|█████████▎| 2377/2541 [40:22:34<2:47:21, 61.23s/it] 94%|█████████▎| 2378/2541 [40:23:35<2:46:30, 61.29s/it] 94%|█████████▎| 2379/2541 [40:24:36<2:45:17, 61.22s/it] 94%|█████████▎| 2380/2541 [40:25:37<2:44:02, 61.14s/it]                                                        {'loss': 1.349, 'learning_rate': 4.936483838444305e-07, 'epoch': 0.94}
 94%|█████████▎| 2380/2541 [40:25:37<2:44:02, 61.14s/it] 94%|█████████▎| 2381/2541 [40:26:38<2:43:04, 61.15s/it] 94%|█████████▎| 2382/2541 [40:27:40<2:42:31, 61.33s/it] 94%|█████████▍| 2383/2541 [40:28:41<2:41:18, 61.26s/it] 94%|█████████▍| 2384/2541 [40:29:42<2:40:04, 61.17s/it] 94%|█████████▍| 2385/2541 [40:30:43<2:39:00, 61.16s/it] 94%|█████████▍| 2386/2541 [40:31:45<2:38:23, 61.31s/it] 94%|█████████▍| 2387/2541 [40:32:46<2:37:29, 61.36s/it] 94%|█████████▍| 2388/2541 [40:33:47<2:36:17, 61.29s/it] 94%|█████████▍| 2389/2541 [40:34:49<2:35:07, 61.23s/it] 94%|█████████▍| 2390/2541 [40:35:50<2:34:13, 61.28s/it]                                                        {'loss': 1.3398, 'learning_rate': 4.344027562483227e-07, 'epoch': 0.94}
 94%|█████████▍| 2390/2541 [40:35:50<2:34:13, 61.28s/it] 94%|█████████▍| 2391/2541 [40:36:51<2:32:51, 61.14s/it] 94%|█████████▍| 2392/2541 [40:37:52<2:31:37, 61.05s/it] 94%|█████████▍| 2393/2541 [40:38:53<2:30:41, 61.09s/it] 94%|█████████▍| 2394/2541 [40:39:55<2:30:15, 61.33s/it] 94%|█████████▍| 2395/2541 [40:40:56<2:28:59, 61.23s/it] 94%|█████████▍| 2396/2541 [40:41:57<2:27:44, 61.13s/it] 94%|█████████▍| 2397/2541 [40:42:58<2:26:47, 61.16s/it] 94%|█████████▍| 2398/2541 [40:43:59<2:25:59, 61.25s/it] 94%|█████████▍| 2399/2541 [40:45:00<2:24:45, 61.17s/it] 94%|█████████▍| 2400/2541 [40:46:02<2:23:51, 61.22s/it]                                                        {'loss': 1.3509, 'learning_rate': 3.789121482709407e-07, 'epoch': 0.94}
 94%|█████████▍| 2400/2541 [40:46:02<2:23:51, 61.22s/it] 94%|█████████▍| 2401/2541 [40:47:04<2:23:25, 61.46s/it] 95%|█████████▍| 2402/2541 [40:48:05<2:22:38, 61.57s/it] 95%|█████████▍| 2403/2541 [40:49:06<2:21:14, 61.41s/it] 95%|█████████▍| 2404/2541 [40:50:08<2:20:03, 61.34s/it] 95%|█████████▍| 2405/2541 [40:51:09<2:19:07, 61.38s/it] 95%|█████████▍| 2406/2541 [40:52:11<2:18:23, 61.51s/it] 95%|█████████▍| 2407/2541 [40:53:12<2:16:56, 61.31s/it] 95%|█████████▍| 2408/2541 [40:54:13<2:15:43, 61.23s/it] 95%|█████████▍| 2409/2541 [40:55:14<2:14:35, 61.18s/it] 95%|█████████▍| 2410/2541 [40:56:15<2:13:50, 61.30s/it]                                                        {'loss': 1.3408, 'learning_rate': 3.2718504203139154e-07, 'epoch': 0.95}
 95%|█████████▍| 2410/2541 [40:56:15<2:13:50, 61.30s/it] 95%|█████████▍| 2411/2541 [40:57:17<2:12:46, 61.28s/it] 95%|█████████▍| 2412/2541 [40:58:18<2:11:33, 61.19s/it] 95%|█████████▍| 2413/2541 [40:59:19<2:10:37, 61.23s/it] 95%|█████████▌| 2414/2541 [41:00:20<2:09:30, 61.18s/it] 95%|█████████▌| 2415/2541 [41:01:21<2:08:28, 61.18s/it] 95%|█████████▌| 2416/2541 [41:02:22<2:07:20, 61.12s/it] 95%|█████████▌| 2417/2541 [41:03:24<2:06:40, 61.29s/it] 95%|█████████▌| 2418/2541 [41:04:25<2:05:25, 61.18s/it] 95%|█████████▌| 2419/2541 [41:05:26<2:04:13, 61.10s/it] 95%|█████████▌| 2420/2541 [41:06:27<2:03:20, 61.16s/it]                                                        {'loss': 1.3593, 'learning_rate': 2.7922934437178695e-07, 'epoch': 0.95}
 95%|█████████▌| 2420/2541 [41:06:27<2:03:20, 61.16s/it] 95%|█████████▌| 2421/2541 [41:07:28<2:02:22, 61.19s/it] 95%|█████████▌| 2422/2541 [41:08:29<2:01:13, 61.12s/it] 95%|█████████▌| 2423/2541 [41:09:31<2:00:24, 61.22s/it] 95%|█████████▌| 2424/2541 [41:10:32<1:59:17, 61.18s/it] 95%|█████████▌| 2425/2541 [41:11:33<1:58:19, 61.20s/it] 95%|█████████▌| 2426/2541 [41:12:34<1:57:11, 61.15s/it] 96%|█████████▌| 2427/2541 [41:13:35<1:56:05, 61.10s/it] 96%|█████████▌| 2428/2541 [41:14:36<1:55:11, 61.16s/it] 96%|█████████▌| 2429/2541 [41:15:38<1:54:11, 61.17s/it] 96%|█████████▌| 2430/2541 [41:16:38<1:53:01, 61.09s/it]                                                        {'loss': 1.3469, 'learning_rate': 2.350523856486292e-07, 'epoch': 0.96}
 96%|█████████▌| 2430/2541 [41:16:38<1:53:01, 61.09s/it] 96%|█████████▌| 2431/2541 [41:17:40<1:51:59, 61.09s/it] 96%|█████████▌| 2432/2541 [41:18:41<1:50:56, 61.07s/it] 96%|█████████▌| 2433/2541 [41:19:42<1:49:59, 61.11s/it] 96%|█████████▌| 2434/2541 [41:20:43<1:49:01, 61.13s/it] 96%|█████████▌| 2435/2541 [41:21:44<1:48:00, 61.14s/it] 96%|█████████▌| 2436/2541 [41:22:45<1:47:03, 61.17s/it] 96%|█████████▌| 2437/2541 [41:23:46<1:45:58, 61.14s/it] 96%|█████████▌| 2438/2541 [41:24:47<1:44:54, 61.11s/it] 96%|█████████▌| 2439/2541 [41:25:49<1:43:56, 61.14s/it] 96%|█████████▌| 2440/2541 [41:26:50<1:43:15, 61.34s/it]                                                        {'loss': 1.357, 'learning_rate': 1.9466091861231884e-07, 'epoch': 0.96}
 96%|█████████▌| 2440/2541 [41:26:50<1:43:15, 61.34s/it] 96%|█████████▌| 2441/2541 [41:27:52<1:42:05, 61.25s/it] 96%|█████████▌| 2442/2541 [41:28:53<1:40:58, 61.20s/it] 96%|█████████▌| 2443/2541 [41:29:54<1:39:55, 61.18s/it] 96%|█████████▌| 2444/2541 [41:30:55<1:38:56, 61.20s/it] 96%|█████████▌| 2445/2541 [41:31:56<1:37:51, 61.16s/it] 96%|█████████▋| 2446/2541 [41:32:57<1:36:54, 61.21s/it] 96%|█████████▋| 2447/2541 [41:33:58<1:35:48, 61.16s/it] 96%|█████████▋| 2448/2541 [41:34:59<1:34:41, 61.09s/it] 96%|█████████▋| 2449/2541 [41:36:00<1:33:38, 61.07s/it] 96%|█████████▋| 2450/2541 [41:37:01<1:32:32, 61.02s/it]                                                        {'loss': 1.3467, 'learning_rate': 1.5806111737495798e-07, 'epoch': 0.96}
 96%|█████████▋| 2450/2541 [41:37:01<1:32:32, 61.02s/it] 96%|█████████▋| 2451/2541 [41:38:02<1:31:34, 61.05s/it] 96%|█████████▋| 2452/2541 [41:39:04<1:30:38, 61.11s/it] 97%|█████████▋| 2453/2541 [41:40:05<1:29:35, 61.08s/it] 97%|█████████▋| 2454/2541 [41:41:06<1:28:33, 61.08s/it] 97%|█████████▋| 2455/2541 [41:42:07<1:27:32, 61.07s/it] 97%|█████████▋| 2456/2541 [41:43:08<1:26:30, 61.06s/it] 97%|█████████▋| 2457/2541 [41:44:09<1:25:33, 61.12s/it] 97%|█████████▋| 2458/2541 [41:45:10<1:24:31, 61.10s/it] 97%|█████████▋| 2459/2541 [41:46:11<1:23:31, 61.12s/it] 97%|█████████▋| 2460/2541 [41:47:12<1:22:28, 61.10s/it]                                                        {'loss': 1.3539, 'learning_rate': 1.2525857646658312e-07, 'epoch': 0.97}
 97%|█████████▋| 2460/2541 [41:47:12<1:22:28, 61.10s/it] 97%|█████████▋| 2461/2541 [41:48:13<1:21:24, 61.06s/it] 97%|█████████▋| 2462/2541 [41:49:15<1:20:28, 61.12s/it] 97%|█████████▋| 2463/2541 [41:50:16<1:19:30, 61.16s/it] 97%|█████████▋| 2464/2541 [41:51:17<1:18:29, 61.17s/it] 97%|█████████▋| 2465/2541 [41:52:18<1:17:29, 61.18s/it] 97%|█████████▋| 2466/2541 [41:53:19<1:16:27, 61.16s/it] 97%|█████████▋| 2467/2541 [41:54:20<1:15:23, 61.13s/it] 97%|█████████▋| 2468/2541 [41:55:21<1:14:19, 61.09s/it] 97%|█████████▋| 2469/2541 [41:56:23<1:13:27, 61.21s/it] 97%|█████████▋| 2470/2541 [41:57:24<1:12:27, 61.23s/it]                                                        {'loss': 1.3484, 'learning_rate': 9.625830998001295e-08, 'epoch': 0.97}
 97%|█████████▋| 2470/2541 [41:57:24<1:12:27, 61.23s/it] 97%|█████████▋| 2471/2541 [41:58:25<1:11:21, 61.16s/it] 97%|█████████▋| 2472/2541 [41:59:26<1:10:16, 61.12s/it] 97%|█████████▋| 2473/2541 [42:00:27<1:09:11, 61.05s/it] 97%|█████████▋| 2474/2541 [42:01:28<1:08:09, 61.03s/it] 97%|█████████▋| 2475/2541 [42:02:29<1:07:15, 61.14s/it] 97%|█████████▋| 2476/2541 [42:03:30<1:06:12, 61.11s/it] 97%|█████████▋| 2477/2541 [42:04:32<1:05:10, 61.10s/it] 98%|█████████▊| 2478/2541 [42:05:33<1:04:06, 61.06s/it] 98%|█████████▊| 2479/2541 [42:06:34<1:03:04, 61.04s/it] 98%|█████████▊| 2480/2541 [42:07:35<1:02:07, 61.10s/it]                                                        {'loss': 1.3514, 'learning_rate': 7.106475080440044e-08, 'epoch': 0.98}
 98%|█████████▊| 2480/2541 [42:07:35<1:02:07, 61.10s/it] 98%|█████████▊| 2481/2541 [42:08:36<1:01:03, 61.06s/it] 98%|█████████▊| 2482/2541 [42:09:37<1:00:01, 61.04s/it] 98%|█████████▊| 2483/2541 [42:10:38<59:01, 61.06s/it]   98%|█████████▊| 2484/2541 [42:11:39<57:59, 61.05s/it] 98%|█████████▊| 2485/2541 [42:12:40<57:06, 61.18s/it] 98%|█████████▊| 2486/2541 [42:13:41<56:04, 61.17s/it] 98%|█████████▊| 2487/2541 [42:14:43<55:04, 61.20s/it] 98%|█████████▊| 2488/2541 [42:15:44<54:00, 61.15s/it] 98%|█████████▊| 2489/2541 [42:16:45<52:58, 61.13s/it] 98%|█████████▊| 2490/2541 [42:17:46<51:55, 61.10s/it]                                                      {'loss': 1.3543, 'learning_rate': 4.968174994764152e-08, 'epoch': 0.98}
 98%|█████████▊| 2490/2541 [42:17:46<51:55, 61.10s/it] 98%|█████████▊| 2491/2541 [42:18:47<50:54, 61.09s/it] 98%|█████████▊| 2492/2541 [42:19:48<49:59, 61.21s/it] 98%|█████████▊| 2493/2541 [42:20:50<48:59, 61.24s/it] 98%|█████████▊| 2494/2541 [42:21:51<47:57, 61.23s/it] 98%|█████████▊| 2495/2541 [42:22:52<46:56, 61.23s/it] 98%|█████████▊| 2496/2541 [42:23:53<45:55, 61.23s/it] 98%|█████████▊| 2497/2541 [42:24:55<45:03, 61.44s/it] 98%|█████████▊| 2498/2541 [42:25:57<44:02, 61.44s/it] 98%|█████████▊| 2499/2541 [42:26:58<42:56, 61.35s/it] 98%|█████████▊| 2500/2541 [42:27:59<41:51, 61.25s/it]                                                      {'loss': 1.3482, 'learning_rate': 3.2112575947723656e-08, 'epoch': 0.98}
 98%|█████████▊| 2500/2541 [42:27:59<41:51, 61.25s/it] 98%|█████████▊| 2501/2541 [42:29:00<40:50, 61.27s/it] 98%|█████████▊| 2502/2541 [42:30:01<39:46, 61.19s/it] 99%|█████████▊| 2503/2541 [42:31:03<38:46, 61.23s/it] 99%|█████████▊| 2504/2541 [42:32:04<37:43, 61.17s/it] 99%|█████████▊| 2505/2541 [42:33:05<36:40, 61.12s/it] 99%|█████████▊| 2506/2541 [42:34:06<35:38, 61.09s/it] 99%|█████████▊| 2507/2541 [42:35:07<34:38, 61.13s/it] 99%|█████████▊| 2508/2541 [42:36:08<33:36, 61.11s/it] 99%|█████████▊| 2509/2541 [42:37:09<32:36, 61.14s/it] 99%|█████████▉| 2510/2541 [42:38:11<31:37, 61.20s/it]                                                      {'loss': 1.3456, 'learning_rate': 1.835991437309781e-08, 'epoch': 0.99}
 99%|█████████▉| 2510/2541 [42:38:11<31:37, 61.20s/it] 99%|█████████▉| 2511/2541 [42:39:12<30:37, 61.26s/it] 99%|█████████▉| 2512/2541 [42:40:13<29:33, 61.16s/it] 99%|█████████▉| 2513/2541 [42:41:14<28:33, 61.20s/it] 99%|█████████▉| 2514/2541 [42:42:15<27:30, 61.13s/it] 99%|█████████▉| 2515/2541 [42:43:17<26:31, 61.22s/it] 99%|█████████▉| 2516/2541 [42:44:18<25:29, 61.19s/it] 99%|█████████▉| 2517/2541 [42:45:19<24:26, 61.10s/it] 99%|█████████▉| 2518/2541 [42:46:20<23:24, 61.07s/it] 99%|█████████▉| 2519/2541 [42:47:21<22:24, 61.10s/it] 99%|█████████▉| 2520/2541 [42:48:22<21:22, 61.08s/it]                                                      {'loss': 1.3644, 'learning_rate': 8.42586741219009e-09, 'epoch': 0.99}
 99%|█████████▉| 2520/2541 [42:48:22<21:22, 61.08s/it] 99%|█████████▉| 2521/2541 [42:49:23<20:23, 61.19s/it] 99%|█████████▉| 2522/2541 [42:50:24<19:21, 61.14s/it] 99%|█████████▉| 2523/2541 [42:51:25<18:20, 61.12s/it] 99%|█████████▉| 2524/2541 [42:52:26<17:18, 61.10s/it] 99%|█████████▉| 2525/2541 [42:53:27<16:17, 61.11s/it] 99%|█████████▉| 2526/2541 [42:54:29<15:17, 61.16s/it] 99%|█████████▉| 2527/2541 [42:55:30<14:17, 61.23s/it] 99%|█████████▉| 2528/2541 [42:56:31<13:15, 61.19s/it]100%|█████████▉| 2529/2541 [42:57:32<12:13, 61.16s/it]100%|█████████▉| 2530/2541 [42:58:34<11:12, 61.17s/it]                                                      {'loss': 1.3624, 'learning_rate': 2.3119535520421674e-09, 'epoch': 1.0}
100%|█████████▉| 2530/2541 [42:58:34<11:12, 61.17s/it]100%|█████████▉| 2531/2541 [42:59:35<10:11, 61.13s/it]100%|█████████▉| 2532/2541 [43:00:36<09:10, 61.14s/it]100%|█████████▉| 2533/2541 [43:01:37<08:09, 61.22s/it]100%|█████████▉| 2534/2541 [43:02:38<07:08, 61.17s/it]100%|█████████▉| 2535/2541 [43:03:39<06:06, 61.16s/it]100%|█████████▉| 2536/2541 [43:04:40<05:05, 61.14s/it]100%|█████████▉| 2537/2541 [43:05:41<04:04, 61.10s/it]100%|█████████▉| 2538/2541 [43:06:43<03:03, 61.10s/it]100%|█████████▉| 2539/2541 [43:07:44<02:02, 61.14s/it]100%|█████████▉| 2540/2541 [43:08:45<01:01, 61.14s/it]                                                      {'loss': 1.3557, 'learning_rate': 1.9107346219127132e-11, 'epoch': 1.0}
100%|█████████▉| 2540/2541 [43:08:45<01:01, 61.14s/it]100%|██████████| 2541/2541 [43:09:46<00:00, 61.09s/it][INFO|trainer.py:1988] 2024-02-28 04:54:13,782 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                      {'train_runtime': 155386.3972, 'train_samples_per_second': 3.14, 'train_steps_per_second': 0.016, 'train_loss': 1.4621030064090788, 'epoch': 1.0}
100%|██████████| 2541/2541 [43:09:46<00:00, 61.09s/it]100%|██████████| 2541/2541 [43:09:46<00:00, 61.15s/it]
[INFO|trainer.py:2979] 2024-02-28 04:54:27,995 >> Saving model checkpoint to /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1
/home/yangdezhao/zhouh_temp/peft/src/peft/utils/save_and_load.py:151: UserWarning: Could not find a config file in /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf - will assume that the vocabulary was not modified.
  warnings.warn(
[2024-02-28 04:54:34,660] [INFO] [launch.py:347:main] Process 1980319 exits successfully.
[2024-02-28 04:54:36,664] [INFO] [launch.py:347:main] Process 1980320 exits successfully.
[2024-02-28 04:54:37,665] [INFO] [launch.py:347:main] Process 1980321 exits successfully.
[INFO|tokenization_utils_base.py:2435] 2024-02-28 04:54:40,299 >> tokenizer config file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-28 04:54:40,299 >> Special tokens file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1/special_tokens_map.json
***** train metrics *****
  epoch                    =                1.0
  train_loss               =             1.4621
  train_runtime            = 1 day, 19:09:46.39
  train_samples_per_second =               3.14
  train_steps_per_second   =              0.016
Figure saved: /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-is-top1/training_loss.png
02/28/2024 04:54:41 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-02-28 04:54:41,364 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-02-28 04:54:46,676] [INFO] [launch.py:347:main] Process 1980318 exits successfully.
