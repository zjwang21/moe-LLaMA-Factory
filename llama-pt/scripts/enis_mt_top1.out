[2024-02-25 18:05:26,195] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 18:05:27,776] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-02-25 18:05:27,776] [INFO] [runner.py:568:main] cmd = /home/yangdezhao/anaconda3/envs/zhouh/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json --stage sft --model_name_or_path /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf --flash_attn --do_train --task mt --max_samples 1000000 --template alpaca --src_lang en --tgt_lang is --dataset opus100_enis --preprocessing_num_workers 20 --cutoff_len 512 --finetuning_type moe --moe_every_k_layers 1 --moe_router_type top1 --moe_num_experts 2 --output_dir /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1 --overwrite_output_dir --overwrite_cache --per_device_train_batch_size 32 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 1 --save_only_model --save_steps 500000 --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-02-25 18:05:30,397] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 18:05:31,896] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-25 18:05:31,896] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-25 18:05:31,896] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-25 18:05:31,896] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-25 18:05:31,896] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-25 18:05:35,405] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 18:05:35,416] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 18:05:35,420] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 18:05:35,478] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-25 18:05:37,992] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-25 18:05:37,992] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-25 18:05:38,012] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-25 18:05:38,020] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-25 18:05:38,060] [INFO] [comm.py:637:init_distributed] cdb=None
02/25/2024 18:05:38 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/25/2024 18:05:38 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1/runs/Feb25_18-05-37_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=500000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-02-25 18:05:38,214 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2027] 2024-02-25 18:05:38,214 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-02-25 18:05:38,214 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-02-25 18:05:38,214 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-02-25 18:05:38,214 >> loading file tokenizer.json
02/25/2024 18:05:38 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/25/2024 18:05:38 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1/runs/Feb25_18-05-38_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=500000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/25/2024 18:05:38 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/25/2024 18:05:38 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1/runs/Feb25_18-05-38_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=500000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/25/2024 18:05:38 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/25/2024 18:05:38 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1/runs/Feb25_18-05-38_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=500000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:727] 2024-02-25 18:05:38,332 >> loading configuration file /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:792] 2024-02-25 18:05:38,333 >> Model config LlamaConfig {
  "_name_or_path": "/home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

02/25/2024 18:05:38 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
[INFO|modeling_utils.py:3334] 2024-02-25 18:05:38,352 >> loading weights file /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1459] 2024-02-25 18:05:38,352 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[WARNING|logging.py:329] 2024-02-25 18:05:38,353 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[WARNING|logging.py:329] 2024-02-25 18:05:38,354 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:827] 2024-02-25 18:05:38,356 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

02/25/2024 18:05:38 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
02/25/2024 18:05:38 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
02/25/2024 18:05:38 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.97s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.95s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.59s/it]
02/25/2024 18:05:45 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/25/2024 18:05:45 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]
02/25/2024 18:05:46 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/25/2024 18:05:46 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.59s/it]
02/25/2024 18:05:46 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/25/2024 18:05:46 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.69s/it]
[INFO|modeling_utils.py:4070] 2024-02-25 18:05:46,159 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4078] 2024-02-25 18:05:46,159 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-02-25 18:05:46,165 >> loading configuration file /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:827] 2024-02-25 18:05:46,166 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

02/25/2024 18:05:46 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/25/2024 18:05:46 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
02/25/2024 18:05:48 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
02/25/2024 18:05:48 - INFO - llmtuner.data.template - Add pad token: </s>
02/25/2024 18:05:48 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
02/25/2024 18:05:48 - INFO - llmtuner.data.template - Add pad token: </s>
02/25/2024 18:05:48 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
02/25/2024 18:05:48 - INFO - llmtuner.data.template - Add pad token: </s>
02/25/2024 18:05:48 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
02/25/2024 18:05:48 - INFO - llmtuner.data.template - Add pad token: </s>
02/25/2024 18:05:48 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/opus100_enis.jsonl.
Using custom data configuration default-08dad3fbe934fe4f
Loading Dataset Infos from /home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Downloading and preparing dataset json/default to /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 10485.76it/s]
Downloading took 0.0 min
Checksum Computation took 0.0 min
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1231.45it/s]
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 361561 examples [00:00, 2577550.61 examples/s]Generating train split: 844087 examples [00:00, 3144161.52 examples/s]Generating train split: 1000000 examples [00:00, 3166509.01 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
{'en': 'What do you want?', 'is': 'Hvađ er í gangi? - Hæ, Larry.'}
Process #0 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00000_of_00020.arrow
Process #1 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00001_of_00020.arrow
Process #2 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00002_of_00020.arrow
Process #3 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00003_of_00020.arrow
Process #4 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00004_of_00020.arrow
Process #5 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00005_of_00020.arrow
Process #6 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00006_of_00020.arrow
Process #7 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00007_of_00020.arrow
Process #8 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00008_of_00020.arrow
Process #9 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00009_of_00020.arrow
Process #10 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00010_of_00020.arrow
Process #11 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00011_of_00020.arrow
Process #12 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00012_of_00020.arrow
Process #13 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00013_of_00020.arrow
Process #14 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00014_of_00020.arrow
Process #15 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00015_of_00020.arrow
Process #16 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00016_of_00020.arrow
Process #17 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00017_of_00020.arrow
Process #18 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00018_of_00020.arrow
Process #19 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00019_of_00020.arrow
02/25/2024 18:05:51 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/opus100_enis.jsonl.
02/25/2024 18:05:51 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/opus100_enis.jsonl.
02/25/2024 18:05:51 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/opus100_enis.jsonl.
{'en': 'What do you want?', 'is': 'Hvađ er í gangi? - Hæ, Larry.'}
{'en': 'What do you want?', 'is': 'Hvađ er í gangi? - Hæ, Larry.'}
{'en': 'What do you want?', 'is': 'Hvađ er í gangi? - Hæ, Larry.'}
Spawning 20 processes
Running tokenizer on dataset (num_proc=20):   0%|          | 0/1000000 [00:00<?, ? examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00000_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00002_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   0%|          | 1000/1000000 [00:00<12:37, 1319.25 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00001_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00008_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00003_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00005_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00004_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00007_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00006_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00009_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00010_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00011_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00012_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00013_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00015_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00014_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00017_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00019_of_00020.arrow
Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00018_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   1%|▏         | 13000/1000000 [00:00<00:49, 20060.19 examples/s]Caching processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-08dad3fbe934fe4f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8c25bfbc29d0703d_00016_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   2%|▏         | 21000/1000000 [00:00<00:33, 29601.27 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 29000/1000000 [00:01<00:24, 39133.38 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 41000/1000000 [00:01<00:17, 54868.79 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 50000/1000000 [00:01<00:16, 56948.48 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 62000/1000000 [00:01<00:15, 59085.74 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 76000/1000000 [00:01<00:12, 74624.90 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 85000/1000000 [00:01<00:14, 63227.70 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 101000/1000000 [00:01<00:11, 79913.75 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 111000/1000000 [00:02<00:11, 76215.53 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 121000/1000000 [00:02<00:10, 81003.00 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 130000/1000000 [00:02<00:12, 71078.88 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 141000/1000000 [00:02<00:10, 78966.27 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 150000/1000000 [00:02<00:11, 71754.31 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 162000/1000000 [00:02<00:12, 69681.17 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 176000/1000000 [00:02<00:09, 84883.02 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 186000/1000000 [00:03<00:11, 70665.34 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 200000/1000000 [00:03<00:09, 84377.58 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 210000/1000000 [00:03<00:10, 74438.38 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 221000/1000000 [00:03<00:09, 79542.99 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 230000/1000000 [00:03<00:10, 75953.64 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 240000/1000000 [00:03<00:09, 80361.88 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 249000/1000000 [00:03<00:10, 72594.63 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 260000/1000000 [00:04<00:09, 77111.58 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 269000/1000000 [00:04<00:09, 75925.25 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 280000/1000000 [00:04<00:09, 76201.31 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 288000/1000000 [00:04<00:09, 72854.29 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 300000/1000000 [00:04<00:09, 73741.06 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 310000/1000000 [00:04<00:08, 79340.19 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 320000/1000000 [00:04<00:09, 73419.72 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 330000/1000000 [00:04<00:08, 78561.39 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 339000/1000000 [00:05<00:08, 80030.64 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 348000/1000000 [00:05<00:08, 75716.64 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 359000/1000000 [00:05<00:08, 79283.18 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 368000/1000000 [00:05<00:08, 76817.22 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 378000/1000000 [00:05<00:07, 81726.02 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▊      | 387000/1000000 [00:05<00:08, 72904.83 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 398000/1000000 [00:05<00:07, 78784.52 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 407000/1000000 [00:06<00:07, 75394.86 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 418000/1000000 [00:06<00:07, 78402.14 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 426000/1000000 [00:06<00:07, 72345.11 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 438000/1000000 [00:06<00:06, 81392.46 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 447000/1000000 [00:06<00:07, 75634.64 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 458000/1000000 [00:06<00:06, 78367.60 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 466000/1000000 [00:06<00:07, 74047.69 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 478000/1000000 [00:06<00:06, 80265.75 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▊     | 487000/1000000 [00:07<00:06, 74367.34 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 498000/1000000 [00:07<00:06, 81846.02 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 507000/1000000 [00:07<00:06, 75045.72 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 517000/1000000 [00:07<00:06, 80179.95 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 526000/1000000 [00:07<00:06, 72303.29 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 537000/1000000 [00:07<00:05, 78729.36 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 546000/1000000 [00:07<00:06, 73633.73 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 557000/1000000 [00:07<00:05, 80455.30 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 566000/1000000 [00:08<00:05, 73038.49 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 576000/1000000 [00:08<00:05, 79091.12 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 585000/1000000 [00:08<00:05, 71986.45 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 596000/1000000 [00:08<00:05, 79853.39 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 605000/1000000 [00:08<00:05, 73761.28 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 616000/1000000 [00:08<00:04, 81715.56 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▎   | 625000/1000000 [00:08<00:05, 72602.49 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 636000/1000000 [00:08<00:04, 79523.35 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 645000/1000000 [00:09<00:08, 40424.13 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 652000/1000000 [00:09<00:08, 38755.12 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 661000/1000000 [00:09<00:07, 46651.10 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 672000/1000000 [00:09<00:06, 53101.15 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 681000/1000000 [00:10<00:05, 59689.22 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 692000/1000000 [00:10<00:04, 63588.33 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 700000/1000000 [00:10<00:04, 65852.51 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 712000/1000000 [00:10<00:04, 69453.19 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 720000/1000000 [00:10<00:03, 70942.50 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 732000/1000000 [00:10<00:03, 75507.19 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 740000/1000000 [00:10<00:03, 74481.54 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 752000/1000000 [00:10<00:03, 75421.32 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 760000/1000000 [00:11<00:03, 74085.23 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 771000/1000000 [00:11<00:02, 81949.65 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 780000/1000000 [00:11<00:02, 75580.76 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 791000/1000000 [00:11<00:02, 77980.71 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 799000/1000000 [00:11<00:02, 75056.64 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 811000/1000000 [00:11<00:02, 78277.24 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 819000/1000000 [00:11<00:02, 76100.11 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 831000/1000000 [00:12<00:02, 74568.25 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 840000/1000000 [00:12<00:02, 78020.28 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 851000/1000000 [00:12<00:01, 77773.10 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 859000/1000000 [00:12<00:01, 77536.24 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 871000/1000000 [00:12<00:01, 76027.28 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 881000/1000000 [00:12<00:01, 77784.99 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 891000/1000000 [00:12<00:01, 76933.29 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 899000/1000000 [00:12<00:01, 77268.38 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 910000/1000000 [00:12<00:01, 83049.46 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 919000/1000000 [00:13<00:01, 75933.61 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 929000/1000000 [00:13<00:00, 79548.74 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 938000/1000000 [00:13<00:00, 74786.22 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 949000/1000000 [00:13<00:00, 81057.63 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 958000/1000000 [00:13<00:00, 72316.38 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 970000/1000000 [00:13<00:00, 79652.97 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 979000/1000000 [00:13<00:00, 77065.88 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 988000/1000000 [00:14<00:00, 77120.59 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 996000/1000000 [00:14<00:00, 61922.82 examples/s]Running tokenizer on dataset (num_proc=20): 100%|██████████| 1000000/1000000 [00:15<00:00, 66637.07 examples/s]
Concatenating 20 shards
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
input_ids:
[1, 4103, 9632, 515, 4223, 304, 306, 2242, 392, 293, 29901, 1724, 437, 366, 864, 29973, 1149, 29871, 379, 1564, 30128, 604, 14468, 20676, 29875, 29973, 448, 379, 30078, 29892, 26977, 29889, 2]
inputs:
<s> Translate from English to Icelandic: What do you want? =>  Hvađ er í gangi? - Hæ, Larry.</s>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 379, 1564, 30128, 604, 14468, 20676, 29875, 29973, 448, 379, 30078, 29892, 26977, 29889, 2]
labels:
Hvađ er í gangi? - Hæ, Larry.</s>
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=4096, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): LlamaMLP(
                    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
    )
  )
)
Running tokenizer on dataset (num_proc=20):   0%|          | 0/1000000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 0/1000000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 0/1000000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 1000/1000000 [00:00<12:17, 1354.05 examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 1000/1000000 [00:00<15:54, 1046.96 examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 4000/1000000 [00:00<02:53, 5742.73 examples/s]Running tokenizer on dataset (num_proc=20):   1%|          | 8000/1000000 [00:01<01:43, 9628.65 examples/s]Running tokenizer on dataset (num_proc=20):   1%|▏         | 14000/1000000 [00:00<00:43, 22464.15 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 20000/1000000 [00:01<00:38, 25725.47 examples/s]Running tokenizer on dataset (num_proc=20):   0%|          | 1000/1000000 [00:01<18:48, 884.98 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 21000/1000000 [00:01<00:37, 25803.30 examples/s]Running tokenizer on dataset (num_proc=20):   1%|▏         | 13000/1000000 [00:01<01:09, 14180.10 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 26000/1000000 [00:01<00:35, 27660.31 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 27000/1000000 [00:01<00:40, 24055.98 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▎         | 35000/1000000 [00:01<00:24, 39504.76 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 33000/1000000 [00:01<00:32, 29436.06 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 21000/1000000 [00:01<00:54, 17970.41 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 41000/1000000 [00:01<00:27, 35409.61 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 41000/1000000 [00:01<00:25, 37655.93 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 26000/1000000 [00:01<00:44, 21917.03 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 39000/1000000 [00:01<00:24, 38706.43 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 46000/1000000 [00:01<00:27, 34308.85 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 47000/1000000 [00:01<00:28, 33435.75 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 54000/1000000 [00:01<00:22, 41367.25 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 53000/1000000 [00:02<00:25, 36874.07 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 62000/1000000 [00:02<00:19, 46996.20 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 62000/1000000 [00:02<00:21, 44012.17 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 47000/1000000 [00:02<00:31, 30559.87 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 59000/1000000 [00:02<00:21, 43189.54 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 68000/1000000 [00:02<00:25, 36714.57 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 68000/1000000 [00:02<00:26, 35741.01 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 75000/1000000 [00:02<00:23, 40197.42 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 73000/1000000 [00:02<00:26, 35328.76 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 80000/1000000 [00:02<00:22, 41282.44 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 79000/1000000 [00:02<00:22, 40148.16 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 67000/1000000 [00:02<00:27, 34222.37 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 75000/1000000 [00:02<00:22, 40679.15 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 85000/1000000 [00:02<00:25, 35652.70 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 84000/1000000 [00:02<00:27, 33529.67 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 91000/1000000 [00:02<00:22, 39887.92 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 82000/1000000 [00:02<00:23, 39600.21 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 91000/1000000 [00:03<00:23, 38682.50 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 96000/1000000 [00:02<00:21, 42034.46 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 97000/1000000 [00:03<00:22, 40915.39 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 101000/1000000 [00:03<00:20, 43703.61 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 88000/1000000 [00:03<00:25, 35949.85 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 102000/1000000 [00:03<00:23, 38940.45 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 95000/1000000 [00:03<00:22, 41047.54 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 106000/1000000 [00:03<00:26, 33691.32 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 107000/1000000 [00:03<00:23, 38762.58 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 101000/1000000 [00:03<00:20, 43921.04 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 112000/1000000 [00:03<00:22, 39197.31 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█▏        | 114000/1000000 [00:03<00:21, 40455.59 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 118000/1000000 [00:03<00:20, 43920.03 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 119000/1000000 [00:03<00:21, 40464.09 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 107000/1000000 [00:03<00:26, 34129.51 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 117000/1000000 [00:03<00:19, 44702.51 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 124000/1000000 [00:03<00:24, 36220.95 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 123000/1000000 [00:03<00:26, 33368.54 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 129000/1000000 [00:03<00:22, 38937.88 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 123000/1000000 [00:03<00:19, 43903.93 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 131000/1000000 [00:04<00:20, 42456.76 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 134000/1000000 [00:03<00:21, 40208.80 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▎        | 137000/1000000 [00:04<00:18, 46174.88 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 139000/1000000 [00:04<00:20, 42031.14 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 129000/1000000 [00:04<00:25, 34203.81 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 144000/1000000 [00:04<00:21, 39268.98 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 143000/1000000 [00:04<00:22, 37585.83 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 140000/1000000 [00:04<00:18, 47273.25 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 148000/1000000 [00:04<00:21, 39435.70 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 149000/1000000 [00:04<00:24, 35396.56 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 153000/1000000 [00:04<00:21, 38661.38 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 155000/1000000 [00:04<00:21, 38707.66 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 147000/1000000 [00:04<00:24, 35301.88 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 162000/1000000 [00:04<00:17, 48431.78 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 162000/1000000 [00:04<00:20, 39977.54 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 152000/1000000 [00:04<00:23, 36812.91 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▋        | 163000/1000000 [00:04<00:17, 47506.76 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 167000/1000000 [00:04<00:22, 37194.12 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 168000/1000000 [00:05<00:22, 37271.40 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 171000/1000000 [00:04<00:22, 37642.40 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 173000/1000000 [00:05<00:22, 37529.93 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 177000/1000000 [00:05<00:19, 42073.79 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 181000/1000000 [00:05<00:17, 45972.85 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 169000/1000000 [00:05<00:23, 34884.06 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 182000/1000000 [00:05<00:18, 43796.86 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 180000/1000000 [00:05<00:17, 46481.70 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 187000/1000000 [00:05<00:22, 36302.04 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 187000/1000000 [00:05<00:23, 35101.77 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 193000/1000000 [00:05<00:20, 39067.88 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 192000/1000000 [00:05<00:21, 38157.72 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 201000/1000000 [00:05<00:17, 46125.48 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 187000/1000000 [00:05<00:23, 35021.62 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 199000/1000000 [00:05<00:18, 42481.58 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 204000/1000000 [00:05<00:18, 44188.30 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 192000/1000000 [00:05<00:22, 36572.00 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 207000/1000000 [00:05<00:20, 38072.67 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 203000/1000000 [00:05<00:18, 43654.27 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 209000/1000000 [00:05<00:22, 35490.27 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██▏       | 214000/1000000 [00:06<00:18, 43529.17 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 215000/1000000 [00:06<00:19, 40082.45 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 220000/1000000 [00:06<00:17, 44559.86 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 221000/1000000 [00:06<00:17, 43827.00 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 209000/1000000 [00:06<00:22, 35844.63 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 226000/1000000 [00:06<00:17, 44575.49 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 218000/1000000 [00:06<00:17, 44726.66 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 226000/1000000 [00:06<00:21, 35991.20 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 231000/1000000 [00:06<00:20, 38365.16 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 232000/1000000 [00:06<00:19, 38717.46 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▎       | 236000/1000000 [00:06<00:19, 39481.14 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 224000/1000000 [00:06<00:21, 36514.79 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 243000/1000000 [00:06<00:16, 45820.72 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 240000/1000000 [00:06<00:17, 43950.37 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 229000/1000000 [00:06<00:20, 36846.70 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 248000/1000000 [00:06<00:16, 44652.24 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▎       | 235000/1000000 [00:06<00:18, 41055.49 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 245000/1000000 [00:06<00:20, 36056.07 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 253000/1000000 [00:07<00:20, 37322.16 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 251000/1000000 [00:06<00:19, 38640.56 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 243000/1000000 [00:06<00:18, 40351.63 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 260000/1000000 [00:07<00:17, 43389.53 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 258000/1000000 [00:07<00:16, 45205.15 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 248000/1000000 [00:07<00:20, 37507.70 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 267000/1000000 [00:07<00:15, 47714.00 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▋       | 263000/1000000 [00:07<00:17, 41965.81 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 253000/1000000 [00:07<00:19, 38800.60 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 273000/1000000 [00:07<00:18, 39662.00 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 262000/1000000 [00:07<00:16, 45325.57 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 268000/1000000 [00:07<00:20, 36302.27 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 278000/1000000 [00:07<00:18, 39739.68 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 274000/1000000 [00:07<00:18, 40137.54 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 286000/1000000 [00:07<00:15, 46279.11 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 281000/1000000 [00:07<00:16, 44891.31 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 267000/1000000 [00:07<00:20, 35109.28 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 272000/1000000 [00:07<00:19, 37804.36 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 287000/1000000 [00:07<00:18, 39429.14 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 291000/1000000 [00:07<00:19, 35842.48 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 282000/1000000 [00:07<00:14, 49374.94 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 296000/1000000 [00:08<00:18, 38079.35 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 292000/1000000 [00:07<00:18, 38663.22 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 303000/1000000 [00:08<00:15, 44499.21 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 299000/1000000 [00:08<00:15, 45625.42 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 288000/1000000 [00:08<00:19, 35936.36 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 304000/1000000 [00:08<00:15, 44474.51 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 308000/1000000 [00:08<00:17, 40273.09 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 294000/1000000 [00:08<00:17, 39889.47 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███▏      | 313000/1000000 [00:08<00:18, 37046.33 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 302000/1000000 [00:08<00:15, 45291.42 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 309000/1000000 [00:08<00:18, 36540.30 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 319000/1000000 [00:08<00:16, 40807.94 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 315000/1000000 [00:08<00:16, 40769.37 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 326000/1000000 [00:08<00:14, 45092.63 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 322000/1000000 [00:08<00:15, 43806.02 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 308000/1000000 [00:08<00:19, 35218.96 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 327000/1000000 [00:08<00:15, 42311.70 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 315000/1000000 [00:08<00:17, 39688.36 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 331000/1000000 [00:08<00:17, 37261.91 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 332000/1000000 [00:08<00:15, 42129.65 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 323000/1000000 [00:08<00:14, 45978.44 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 336000/1000000 [00:09<00:18, 35483.30 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 337000/1000000 [00:09<00:16, 41241.77 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 346000/1000000 [00:09<00:13, 48475.61 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 343000/1000000 [00:09<00:15, 43261.91 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 329000/1000000 [00:09<00:18, 35816.99 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 348000/1000000 [00:09<00:16, 40742.03 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 335000/1000000 [00:09<00:17, 38798.31 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 352000/1000000 [00:09<00:16, 38561.29 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 354000/1000000 [00:09<00:15, 41028.11 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 343000/1000000 [00:09<00:14, 44114.87 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 357000/1000000 [00:09<00:17, 36926.84 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 359000/1000000 [00:09<00:14, 42955.36 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 367000/1000000 [00:09<00:13, 47570.93 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▋      | 364000/1000000 [00:09<00:14, 43024.56 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 348000/1000000 [00:09<00:18, 34686.00 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 355000/1000000 [00:09<00:16, 40055.30 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 369000/1000000 [00:09<00:15, 39554.99 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 373000/1000000 [00:09<00:15, 40401.90 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 362000/1000000 [00:09<00:13, 45782.59 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 374000/1000000 [00:09<00:15, 41066.20 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 378000/1000000 [00:10<00:16, 38317.57 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 379000/1000000 [00:10<00:15, 40153.54 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▊      | 386000/1000000 [00:10<00:13, 46634.18 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 384000/1000000 [00:10<00:14, 41760.32 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 368000/1000000 [00:10<00:18, 34789.59 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 390000/1000000 [00:10<00:13, 45662.55 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 392000/1000000 [00:10<00:15, 39335.90 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 375000/1000000 [00:10<00:15, 40444.16 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 395000/1000000 [00:10<00:14, 41997.93 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 382000/1000000 [00:10<00:13, 45044.81 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 397000/1000000 [00:10<00:16, 36717.34 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 400000/1000000 [00:10<00:14, 40400.91 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 405000/1000000 [00:10<00:13, 45256.19 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 405000/1000000 [00:10<00:14, 41189.17 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 388000/1000000 [00:10<00:17, 34649.63 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 411000/1000000 [00:10<00:15, 37846.88 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 411000/1000000 [00:10<00:14, 42008.99 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 394000/1000000 [00:10<00:15, 38839.85 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 416000/1000000 [00:10<00:13, 43254.55 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 403000/1000000 [00:10<00:12, 47192.19 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 417000/1000000 [00:11<00:15, 37975.47 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 421000/1000000 [00:11<00:14, 39088.22 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▎     | 425000/1000000 [00:11<00:12, 44489.84 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 426000/1000000 [00:11<00:13, 41491.30 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 430000/1000000 [00:11<00:12, 45281.22 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 409000/1000000 [00:11<00:16, 36220.49 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 431000/1000000 [00:11<00:13, 41563.28 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 415000/1000000 [00:11<00:14, 40182.09 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▎     | 435000/1000000 [00:11<00:13, 41405.69 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▎     | 437000/1000000 [00:11<00:12, 45786.44 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 423000/1000000 [00:11<00:11, 48164.41 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 440000/1000000 [00:11<00:15, 36846.74 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 442000/1000000 [00:11<00:14, 38163.16 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 447000/1000000 [00:11<00:13, 41974.15 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 429000/1000000 [00:11<00:15, 35970.96 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 449000/1000000 [00:11<00:12, 43835.70 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 452000/1000000 [00:11<00:13, 39994.96 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▎     | 435000/1000000 [00:11<00:14, 40240.80 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 454000/1000000 [00:11<00:13, 41682.07 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 442000/1000000 [00:11<00:12, 44708.38 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 459000/1000000 [00:11<00:12, 43428.25 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 457000/1000000 [00:12<00:14, 36259.75 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 464000/1000000 [00:12<00:12, 42158.35 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 464000/1000000 [00:12<00:14, 38090.15 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 448000/1000000 [00:12<00:15, 36769.97 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 469000/1000000 [00:12<00:12, 41993.69 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 471000/1000000 [00:12<00:11, 44359.05 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 453000/1000000 [00:12<00:14, 38797.44 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 474000/1000000 [00:12<00:12, 40785.96 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 476000/1000000 [00:12<00:11, 45139.79 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 459000/1000000 [00:12<00:13, 40664.89 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 479000/1000000 [00:12<00:13, 38429.95 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 481000/1000000 [00:12<00:12, 41332.07 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 466000/1000000 [00:12<00:12, 43498.94 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 484000/1000000 [00:12<00:13, 39639.79 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▊     | 486000/1000000 [00:12<00:13, 39200.63 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 491000/1000000 [00:12<00:11, 44186.89 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 471000/1000000 [00:12<00:14, 37280.42 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 491000/1000000 [00:12<00:12, 40184.35 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 477000/1000000 [00:12<00:12, 41849.62 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 498000/1000000 [00:12<00:10, 47230.29 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 496000/1000000 [00:12<00:12, 40834.43 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 484000/1000000 [00:12<00:10, 47324.68 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 501000/1000000 [00:13<00:12, 39062.38 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 503000/1000000 [00:12<00:12, 41079.50 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 505000/1000000 [00:13<00:13, 37657.21 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 508000/1000000 [00:13<00:12, 40518.09 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 490000/1000000 [00:13<00:12, 40366.50 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 511000/1000000 [00:13<00:11, 42102.03 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 513000/1000000 [00:13<00:11, 41560.04 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 495000/1000000 [00:13<00:13, 38603.34 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 516000/1000000 [00:13<00:11, 41438.72 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 519000/1000000 [00:13<00:10, 44912.70 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 500000/1000000 [00:13<00:12, 40529.85 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 524000/1000000 [00:13<00:10, 44116.25 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 521000/1000000 [00:13<00:12, 39328.37 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▎    | 525000/1000000 [00:13<00:12, 36981.52 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 508000/1000000 [00:13<00:12, 38066.63 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 529000/1000000 [00:13<00:12, 38693.59 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 532000/1000000 [00:13<00:10, 43794.69 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 513000/1000000 [00:13<00:12, 38883.93 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 534000/1000000 [00:13<00:11, 40416.35 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 519000/1000000 [00:13<00:11, 42271.79 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 540000/1000000 [00:13<00:10, 41925.26 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 537000/1000000 [00:14<00:12, 37709.53 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▎    | 525000/1000000 [00:13<00:10, 44064.13 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 546000/1000000 [00:13<00:10, 44022.82 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 543000/1000000 [00:14<00:12, 37458.70 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 530000/1000000 [00:14<00:11, 40665.51 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 550000/1000000 [00:14<00:10, 44194.12 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 551000/1000000 [00:14<00:11, 37734.94 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 535000/1000000 [00:14<00:12, 37723.95 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 558000/1000000 [00:14<00:10, 42903.68 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 555000/1000000 [00:14<00:10, 42219.49 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 540000/1000000 [00:14<00:12, 37356.68 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 564000/1000000 [00:14<00:10, 42916.13 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 560000/1000000 [00:14<00:10, 40035.21 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 549000/1000000 [00:14<00:11, 40819.35 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 565000/1000000 [00:14<00:12, 36115.95 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 569000/1000000 [00:14<00:11, 37480.42 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 576000/1000000 [00:14<00:09, 42913.60 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 554000/1000000 [00:14<00:11, 39869.83 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 573000/1000000 [00:14<00:09, 43180.92 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 582000/1000000 [00:14<00:08, 46618.19 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 560000/1000000 [00:14<00:11, 39132.42 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 578000/1000000 [00:15<00:10, 38432.42 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 587000/1000000 [00:14<00:09, 45617.94 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 567000/1000000 [00:15<00:09, 43998.11 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 583000/1000000 [00:15<00:11, 35960.91 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 592000/1000000 [00:15<00:11, 37028.55 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 591000/1000000 [00:15<00:09, 44208.26 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 572000/1000000 [00:15<00:11, 36969.97 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 599000/1000000 [00:15<00:09, 44396.67 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 577000/1000000 [00:15<00:10, 38912.97 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 596000/1000000 [00:15<00:10, 37472.02 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 605000/1000000 [00:15<00:08, 44896.21 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 582000/1000000 [00:15<00:10, 40886.01 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 602000/1000000 [00:15<00:09, 42147.44 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 610000/1000000 [00:15<00:08, 45860.41 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 588000/1000000 [00:15<00:09, 41710.01 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 607000/1000000 [00:15<00:09, 39826.16 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 615000/1000000 [00:15<00:09, 40463.64 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 593000/1000000 [00:15<00:10, 38835.52 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 613000/1000000 [00:15<00:09, 42838.22 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 622000/1000000 [00:15<00:08, 45558.26 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 597000/1000000 [00:15<00:10, 38406.68 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 618000/1000000 [00:16<00:09, 41542.15 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 627000/1000000 [00:15<00:08, 41557.07 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 602000/1000000 [00:15<00:09, 40263.92 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 607000/1000000 [00:16<00:09, 41366.71 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 624000/1000000 [00:16<00:10, 36807.67 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 632000/1000000 [00:16<00:10, 36626.83 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 632000/1000000 [00:16<00:08, 44559.45 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 640000/1000000 [00:16<00:07, 45198.80 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 612000/1000000 [00:16<00:10, 37087.51 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 617000/1000000 [00:16<00:09, 38827.33 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 645000/1000000 [00:16<00:08, 41522.85 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 637000/1000000 [00:16<00:09, 39815.65 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 622000/1000000 [00:16<00:09, 39982.79 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 644000/1000000 [00:16<00:07, 46377.18 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 652000/1000000 [00:16<00:08, 43128.54 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 627000/1000000 [00:16<00:08, 42203.69 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 657000/1000000 [00:16<00:08, 39621.25 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 650000/1000000 [00:16<00:08, 38979.05 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 632000/1000000 [00:16<00:09, 37244.24 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 664000/1000000 [00:16<00:07, 45569.68 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 657000/1000000 [00:16<00:07, 45340.27 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 637000/1000000 [00:16<00:09, 39522.78 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 669000/1000000 [00:16<00:07, 42364.96 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 663000/1000000 [00:17<00:07, 43324.12 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 642000/1000000 [00:16<00:09, 39539.78 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 647000/1000000 [00:17<00:08, 41116.29 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 674000/1000000 [00:17<00:08, 38487.64 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 668000/1000000 [00:17<00:08, 38236.05 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 680000/1000000 [00:17<00:07, 42400.79 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 673000/1000000 [00:17<00:08, 40472.16 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 652000/1000000 [00:17<00:09, 37635.28 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 685000/1000000 [00:17<00:07, 42565.36 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 657000/1000000 [00:17<00:08, 40524.86 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 679000/1000000 [00:17<00:07, 42591.91 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 690000/1000000 [00:17<00:07, 43164.18 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 662000/1000000 [00:17<00:08, 40488.61 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 685000/1000000 [00:17<00:06, 45279.03 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 695000/1000000 [00:17<00:07, 39405.60 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 667000/1000000 [00:17<00:08, 40131.97 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 690000/1000000 [00:17<00:07, 39260.29 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 701000/1000000 [00:17<00:06, 43949.84 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 695000/1000000 [00:17<00:07, 41612.81 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 672000/1000000 [00:17<00:08, 37860.80 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 706000/1000000 [00:17<00:07, 41459.04 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 677000/1000000 [00:17<00:08, 40358.02 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 700000/1000000 [00:17<00:07, 42009.49 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 711000/1000000 [00:17<00:06, 43145.07 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 682000/1000000 [00:17<00:07, 40489.15 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 706000/1000000 [00:18<00:06, 45028.73 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 716000/1000000 [00:18<00:07, 40393.65 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 687000/1000000 [00:18<00:07, 39504.22 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 711000/1000000 [00:18<00:07, 38488.59 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 721000/1000000 [00:18<00:06, 42177.20 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 692000/1000000 [00:18<00:07, 38561.75 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 716000/1000000 [00:18<00:07, 39275.08 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 728000/1000000 [00:18<00:06, 43364.94 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 697000/1000000 [00:18<00:07, 39850.38 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 724000/1000000 [00:18<00:06, 45635.62 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 702000/1000000 [00:18<00:07, 41055.02 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 733000/1000000 [00:18<00:06, 38480.44 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 729000/1000000 [00:18<00:07, 37412.99 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 707000/1000000 [00:18<00:07, 39184.20 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 738000/1000000 [00:18<00:07, 37222.67 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 734000/1000000 [00:18<00:07, 37689.96 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 711000/1000000 [00:18<00:07, 36493.65 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 747000/1000000 [00:18<00:05, 47365.78 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 741000/1000000 [00:18<00:05, 44276.42 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 717000/1000000 [00:18<00:06, 41079.24 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 746000/1000000 [00:19<00:05, 44310.58 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 752000/1000000 [00:18<00:06, 39155.55 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 722000/1000000 [00:18<00:06, 40262.19 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 758000/1000000 [00:19<00:05, 41163.81 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 727000/1000000 [00:19<00:06, 39463.99 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 751000/1000000 [00:19<00:06, 38753.15 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 764000/1000000 [00:19<00:05, 45378.32 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 731000/1000000 [00:19<00:07, 36551.95 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 756000/1000000 [00:19<00:06, 38400.05 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 737000/1000000 [00:19<00:06, 42006.03 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 769000/1000000 [00:19<00:05, 39507.99 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 764000/1000000 [00:19<00:05, 46726.29 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 742000/1000000 [00:19<00:06, 41829.28 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 774000/1000000 [00:19<00:05, 38512.02 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 769000/1000000 [00:19<00:05, 41134.20 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 747000/1000000 [00:19<00:06, 39102.55 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 779000/1000000 [00:19<00:06, 35604.81 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 774000/1000000 [00:19<00:06, 34483.40 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 752000/1000000 [00:19<00:06, 38501.21 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▊  | 786000/1000000 [00:19<00:04, 42981.27 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 783000/1000000 [00:20<00:04, 44127.86 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 759000/1000000 [00:19<00:05, 44998.40 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 791000/1000000 [00:19<00:05, 39302.30 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 764000/1000000 [00:19<00:05, 43936.39 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 789000/1000000 [00:20<00:05, 42176.50 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 796000/1000000 [00:20<00:04, 40804.16 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 769000/1000000 [00:20<00:05, 40867.26 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 801000/1000000 [00:20<00:04, 40232.71 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 794000/1000000 [00:20<00:05, 38313.87 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 774000/1000000 [00:20<00:06, 37342.28 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 801000/1000000 [00:20<00:04, 44333.33 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 807000/1000000 [00:20<00:04, 40066.81 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 780000/1000000 [00:20<00:05, 42304.32 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 807000/1000000 [00:20<00:04, 46075.90 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 812000/1000000 [00:20<00:04, 41398.40 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 812000/1000000 [00:20<00:04, 42952.88 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 785000/1000000 [00:20<00:05, 37378.45 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 817000/1000000 [00:20<00:04, 37189.79 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 824000/1000000 [00:20<00:03, 44382.51 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 789000/1000000 [00:20<00:06, 32736.11 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 817000/1000000 [00:20<00:05, 35671.94 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 829000/1000000 [00:20<00:03, 44595.62 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▎ | 825000/1000000 [00:21<00:03, 44782.07 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 796000/1000000 [00:20<00:05, 38329.49 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 834000/1000000 [00:20<00:03, 42115.72 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 801000/1000000 [00:20<00:05, 39606.56 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 831000/1000000 [00:21<00:04, 41827.59 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 839000/1000000 [00:21<00:04, 37664.64 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 806000/1000000 [00:21<00:05, 38059.55 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 845000/1000000 [00:21<00:03, 42807.94 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 836000/1000000 [00:21<00:04, 36505.12 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 810000/1000000 [00:21<00:05, 34473.55 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 850000/1000000 [00:21<00:03, 43369.52 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 842000/1000000 [00:21<00:03, 40554.81 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 817000/1000000 [00:21<00:04, 41262.20 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 847000/1000000 [00:21<00:03, 41818.11 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 855000/1000000 [00:21<00:03, 40008.17 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 822000/1000000 [00:21<00:04, 42121.04 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 860000/1000000 [00:21<00:03, 37638.45 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 852000/1000000 [00:21<00:04, 33917.81 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 827000/1000000 [00:21<00:04, 36819.14 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 867000/1000000 [00:21<00:02, 44966.74 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 831000/1000000 [00:21<00:04, 37218.40 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 856000/1000000 [00:21<00:04, 32381.52 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 836000/1000000 [00:21<00:04, 40165.01 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 872000/1000000 [00:21<00:03, 39082.62 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▋ | 863000/1000000 [00:22<00:03, 37198.25 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 841000/1000000 [00:21<00:03, 42414.08 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 867000/1000000 [00:22<00:04, 32729.36 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 877000/1000000 [00:22<00:03, 32891.44 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 846000/1000000 [00:22<00:03, 39620.24 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 883000/1000000 [00:22<00:03, 35359.09 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 871000/1000000 [00:22<00:04, 30839.39 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 851000/1000000 [00:22<00:04, 37239.43 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 887000/1000000 [00:22<00:03, 33895.40 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 878000/1000000 [00:22<00:03, 36767.98 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 855000/1000000 [00:22<00:03, 36778.94 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 861000/1000000 [00:22<00:03, 41785.53 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 882000/1000000 [00:22<00:03, 34838.83 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 891000/1000000 [00:22<00:03, 27340.76 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 866000/1000000 [00:22<00:03, 40915.53 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 886000/1000000 [00:22<00:03, 30650.56 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 871000/1000000 [00:22<00:03, 38757.47 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 891000/1000000 [00:22<00:03, 34424.37 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 896000/1000000 [00:22<00:03, 26000.24 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 875000/1000000 [00:22<00:03, 37223.97 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 895000/1000000 [00:23<00:03, 33601.27 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 899000/1000000 [00:22<00:03, 26139.79 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 879000/1000000 [00:23<00:03, 37377.37 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 902000/1000000 [00:23<00:03, 26466.69 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 899000/1000000 [00:23<00:03, 32379.74 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 908000/1000000 [00:23<00:02, 31933.25 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 903000/1000000 [00:23<00:02, 32751.59 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 883000/1000000 [00:23<00:04, 25270.98 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 907000/1000000 [00:23<00:02, 32947.34 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 912000/1000000 [00:23<00:02, 31526.38 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 912000/1000000 [00:23<00:02, 36173.07 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 916000/1000000 [00:23<00:02, 32956.80 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 887000/1000000 [00:23<00:04, 24694.79 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 922000/1000000 [00:23<00:02, 37399.92 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 917000/1000000 [00:23<00:02, 35099.05 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 893000/1000000 [00:23<00:03, 27433.31 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 926000/1000000 [00:23<00:02, 36571.32 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 922000/1000000 [00:23<00:02, 38240.77 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 930000/1000000 [00:23<00:02, 34592.90 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 899000/1000000 [00:23<00:03, 31000.07 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 928000/1000000 [00:23<00:01, 40542.55 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 934000/1000000 [00:23<00:01, 33121.72 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 933000/1000000 [00:24<00:01, 39620.02 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 903000/1000000 [00:23<00:03, 30272.43 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 939000/1000000 [00:24<00:01, 36560.78 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 938000/1000000 [00:24<00:01, 41515.58 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 910000/1000000 [00:24<00:02, 37731.89 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 946000/1000000 [00:24<00:01, 42775.04 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 943000/1000000 [00:24<00:01, 38274.73 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 915000/1000000 [00:24<00:02, 34391.24 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 951000/1000000 [00:24<00:01, 45834.90 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 920000/1000000 [00:24<00:02, 37154.38 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 951000/1000000 [00:24<00:01, 33770.41 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 924000/1000000 [00:24<00:02, 37744.68 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 956000/1000000 [00:24<00:01, 39219.66 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 958000/1000000 [00:24<00:01, 37636.13 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 928000/1000000 [00:24<00:02, 35642.82 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 962000/1000000 [00:24<00:00, 40742.34 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 963000/1000000 [00:24<00:00, 38073.19 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 933000/1000000 [00:24<00:01, 34135.85 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 968000/1000000 [00:24<00:00, 43557.68 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 967000/1000000 [00:24<00:00, 37567.71 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 938000/1000000 [00:24<00:01, 35655.43 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 971000/1000000 [00:24<00:00, 37196.77 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 974000/1000000 [00:25<00:00, 44561.23 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 944000/1000000 [00:24<00:01, 39973.38 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 975000/1000000 [00:24<00:00, 37615.09 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 979000/1000000 [00:25<00:00, 37874.53 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 979000/1000000 [00:25<00:00, 37330.62 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 949000/1000000 [00:25<00:01, 37859.76 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 983000/1000000 [00:25<00:00, 37203.71 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 984000/1000000 [00:25<00:00, 39786.10 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 955000/1000000 [00:25<00:01, 42561.88 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▊| 987000/1000000 [00:25<00:00, 37407.54 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 961000/1000000 [00:25<00:00, 42858.82 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 989000/1000000 [00:25<00:00, 30071.92 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 991000/1000000 [00:25<00:00, 31794.13 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 968000/1000000 [00:25<00:00, 40761.20 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 977000/1000000 [00:25<00:00, 51690.28 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 995000/1000000 [00:25<00:00, 27352.15 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 995000/1000000 [00:25<00:00, 22928.03 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 983000/1000000 [00:25<00:00, 43415.91 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 990000/1000000 [00:26<00:00, 42247.55 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 999000/1000000 [00:26<00:00, 20577.51 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 998000/1000000 [00:26<00:00, 17332.42 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 995000/1000000 [00:26<00:00, 39478.68 examples/s]Running tokenizer on dataset (num_proc=20): 100%|██████████| 1000000/1000000 [00:26<00:00, 21014.33 examples/s]Running tokenizer on dataset (num_proc=20): 100%|██████████| 1000000/1000000 [00:26<00:00, 37272.62 examples/s]
Running tokenizer on dataset (num_proc=20): 100%|██████████| 1000000/1000000 [00:27<00:00, 36935.09 examples/s]
Running tokenizer on dataset (num_proc=20): 100%|██████████| 1000000/1000000 [00:27<00:00, 36401.14 examples/s]
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
[INFO|training_args.py:1846] 2024-02-25 18:06:50,001 >> PyTorch: setting up devices
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=4096, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): LlamaMLP(
                    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
    )
  )
)
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=4096, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): LlamaMLP(
                    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
    )
  )
)
PeftModelForCausalLM(
  (base_model): MoeModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): moe.MLP(
              (base_layer): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (moe_router_embedding): ModuleDict(
                (default): Linear(in_features=4096, out_features=2, bias=False)
              )
              (moe_experts): ModuleDict(
                (default): ModuleList(
                  (0): LlamaMLP(
                    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                    (act_fn): SiLU()
                  )
                )
              )
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
    )
  )
)
/home/yangdezhao/zhouh_temp/transformers/src/transformers/training_args.py:1759: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/yangdezhao/zhouh_temp/transformers/src/transformers/training_args.py:1759: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/yangdezhao/zhouh_temp/transformers/src/transformers/training_args.py:1759: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/yangdezhao/zhouh_temp/transformers/src/transformers/training_args.py:1759: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
[INFO|trainer.py:586] 2024-02-25 18:06:50,082 >> Using auto half precision backend
[INFO|deepspeed.py:325] 2024-02-25 18:06:50,442 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yangdezhao/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
ninja: no work to do.
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.440232038497925 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.502243995666504 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.473644495010376 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.491215944290161 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-02-25 18:06:54,727] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2024-02-25 18:07:17,499] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-02-25 18:07:17,502] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-02-25 18:07:17,503] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-02-25 18:07:17,511] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-02-25 18:07:17,511] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-02-25 18:07:17,511] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-02-25 18:07:17,512] [INFO] [stage_1_and_2.py:143:__init__] Reduce bucket size 500000000
[2024-02-25 18:07:17,512] [INFO] [stage_1_and_2.py:144:__init__] Allgather bucket size 500000000
[2024-02-25 18:07:17,512] [INFO] [stage_1_and_2.py:145:__init__] CPU Offload: True
[2024-02-25 18:07:17,512] [INFO] [stage_1_and_2.py:146:__init__] Round robin gradient partitioning: False
[2024-02-25 18:07:39,809] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
[2024-02-25 18:07:39,810] [INFO] [utils.py:792:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-02-25 18:07:39,810] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 90.49 GB, percent = 36.0%
[2024-02-25 18:07:45,394] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
[2024-02-25 18:07:45,395] [INFO] [utils.py:792:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-02-25 18:07:45,395] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 113.79 GB, percent = 45.2%
[2024-02-25 18:07:45,395] [INFO] [stage_1_and_2.py:533:__init__] optimizer state initialized
[2024-02-25 18:07:45,574] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
[2024-02-25 18:07:45,575] [INFO] [utils.py:792:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-02-25 18:07:45,575] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 113.79 GB, percent = 45.2%
[2024-02-25 18:07:45,577] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-02-25 18:07:45,577] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-02-25 18:07:45,577] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-02-25 18:07:45,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2024-02-25 18:07:45,579] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2024-02-25 18:07:45,579] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-02-25 18:07:45,579] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-02-25 18:07:45,579] [INFO] [config.py:988:print]   amp_enabled .................. False
[2024-02-25 18:07:45,579] [INFO] [config.py:988:print]   amp_params ................... False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   bfloat16_enabled ............. True
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc3be1d76d0>
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   communication_data_type ...... None
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   disable_allgather ............ False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   dump_state ................... False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   fp16_enabled ................. False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   global_rank .................. 0
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 4
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2024-02-25 18:07:45,580] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 1
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   loss_scale ................... 1.0
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   optimizer_name ............... None
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   optimizer_params ............. None
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   pld_enabled .................. False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   pld_params ................... False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   scheduler_name ............... None
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   scheduler_params ............. None
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   sparse_attention ............. None
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   steps_per_print .............. inf
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   train_batch_size ............. 512
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  32
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   world_size ................... 4
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  True
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   zero_enabled ................. True
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2024-02-25 18:07:45,581] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2
[2024-02-25 18:07:45,581] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 512, 
    "train_micro_batch_size_per_gpu": 32, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1747] 2024-02-25 18:07:45,582 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-25 18:07:45,582 >>   Num examples = 1,000,000
[INFO|trainer.py:1749] 2024-02-25 18:07:45,582 >>   Num Epochs = 1
[INFO|trainer.py:1750] 2024-02-25 18:07:45,582 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1753] 2024-02-25 18:07:45,582 >>   Total train batch size (w. parallel, distributed & accumulation) = 512
[INFO|trainer.py:1754] 2024-02-25 18:07:45,582 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1755] 2024-02-25 18:07:45,582 >>   Total optimization steps = 1,953
[INFO|trainer.py:1756] 2024-02-25 18:07:45,584 >>   Number of trainable parameters = 4,328,783,872
  0%|          | 0/1953 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/1953 [00:22<12:20:11, 22.75s/it]  0%|          | 2/1953 [00:44<11:58:35, 22.10s/it]  0%|          | 3/1953 [01:03<11:12:23, 20.69s/it]  0%|          | 4/1953 [01:21<10:41:36, 19.75s/it]  0%|          | 5/1953 [01:41<10:35:52, 19.59s/it]  0%|          | 6/1953 [02:01<10:41:14, 19.76s/it]  0%|          | 7/1953 [02:19<10:26:30, 19.32s/it]  0%|          | 8/1953 [02:38<10:19:12, 19.10s/it]  0%|          | 9/1953 [02:57<10:24:13, 19.27s/it]  1%|          | 10/1953 [03:17<10:23:35, 19.26s/it]                                                    {'loss': 3.5724, 'learning_rate': 4.9996765583774294e-05, 'epoch': 0.01}
  1%|          | 10/1953 [03:17<10:23:35, 19.26s/it]  1%|          | 11/1953 [03:36<10:21:05, 19.19s/it]  1%|          | 12/1953 [03:55<10:22:43, 19.25s/it]  1%|          | 13/1953 [04:14<10:18:17, 19.12s/it]  1%|          | 14/1953 [04:33<10:18:10, 19.13s/it]  1%|          | 15/1953 [04:53<10:23:27, 19.30s/it]  1%|          | 16/1953 [05:12<10:26:25, 19.40s/it]  1%|          | 17/1953 [05:32<10:28:54, 19.49s/it]  1%|          | 18/1953 [05:53<10:39:57, 19.84s/it]  1%|          | 19/1953 [06:11<10:24:53, 19.39s/it]  1%|          | 20/1953 [06:32<10:36:25, 19.75s/it]                                                    {'loss': 2.4151, 'learning_rate': 4.9987063172013025e-05, 'epoch': 0.01}
  1%|          | 20/1953 [06:32<10:36:25, 19.75s/it]  1%|          | 21/1953 [06:50<10:21:19, 19.30s/it]  1%|          | 22/1953 [07:09<10:17:34, 19.19s/it]  1%|          | 23/1953 [07:27<10:09:18, 18.94s/it]  1%|          | 24/1953 [07:45<10:02:59, 18.76s/it]  1%|▏         | 25/1953 [08:04<10:02:28, 18.75s/it]  1%|▏         | 26/1953 [08:23<10:04:42, 18.83s/it]  1%|▏         | 27/1953 [08:42<10:01:36, 18.74s/it]  1%|▏         | 28/1953 [09:00<9:58:06, 18.64s/it]   1%|▏         | 29/1953 [09:19<9:57:37, 18.64s/it]  2%|▏         | 30/1953 [09:38<10:03:03, 18.82s/it]                                                    {'loss': 2.1493, 'learning_rate': 4.997089527524725e-05, 'epoch': 0.02}
  2%|▏         | 30/1953 [09:38<10:03:03, 18.82s/it]  2%|▏         | 31/1953 [09:57<10:05:02, 18.89s/it]  2%|▏         | 32/1953 [10:15<9:56:00, 18.62s/it]   2%|▏         | 33/1953 [10:33<9:50:20, 18.45s/it]  2%|▏         | 34/1953 [10:51<9:49:13, 18.42s/it]  2%|▏         | 35/1953 [11:11<9:58:48, 18.73s/it]  2%|▏         | 36/1953 [11:30<10:02:21, 18.85s/it]  2%|▏         | 37/1953 [11:49<10:01:38, 18.84s/it]  2%|▏         | 38/1953 [12:06<9:49:07, 18.46s/it]   2%|▏         | 39/1953 [12:26<9:58:07, 18.75s/it]  2%|▏         | 40/1953 [12:44<9:55:15, 18.67s/it]                                                   {'loss': 2.015, 'learning_rate': 4.994826607697358e-05, 'epoch': 0.02}
  2%|▏         | 40/1953 [12:44<9:55:15, 18.67s/it]  2%|▏         | 41/1953 [13:03<9:58:17, 18.77s/it]  2%|▏         | 42/1953 [13:22<9:55:33, 18.70s/it]  2%|▏         | 43/1953 [13:40<9:45:26, 18.39s/it]  2%|▏         | 44/1953 [14:02<10:24:44, 19.64s/it]  2%|▏         | 45/1953 [14:22<10:30:12, 19.82s/it]  2%|▏         | 46/1953 [14:41<10:20:50, 19.53s/it]  2%|▏         | 47/1953 [15:00<10:10:05, 19.21s/it]  2%|▏         | 48/1953 [15:19<10:06:58, 19.12s/it]  3%|▎         | 49/1953 [15:37<9:59:59, 18.91s/it]   3%|▎         | 50/1953 [15:56<10:00:43, 18.94s/it]                                                    {'loss': 1.8885, 'learning_rate': 4.9919181432571686e-05, 'epoch': 0.03}
  3%|▎         | 50/1953 [15:56<10:00:43, 18.94s/it]  3%|▎         | 51/1953 [16:15<9:58:10, 18.87s/it]   3%|▎         | 52/1953 [16:34<9:59:58, 18.94s/it]  3%|▎         | 53/1953 [16:52<9:57:20, 18.86s/it]  3%|▎         | 54/1953 [17:12<10:00:13, 18.96s/it]  3%|▎         | 55/1953 [17:30<9:55:37, 18.83s/it]   3%|▎         | 56/1953 [17:49<9:58:28, 18.93s/it]  3%|▎         | 57/1953 [18:09<10:01:02, 19.02s/it]  3%|▎         | 58/1953 [18:27<9:56:12, 18.88s/it]   3%|▎         | 59/1953 [18:50<10:37:15, 20.19s/it]  3%|▎         | 60/1953 [19:12<10:52:24, 20.68s/it]                                                    {'loss': 1.862, 'learning_rate': 4.988364886778925e-05, 'epoch': 0.03}
  3%|▎         | 60/1953 [19:12<10:52:24, 20.68s/it]  3%|▎         | 61/1953 [19:31<10:33:11, 20.08s/it]  3%|▎         | 62/1953 [19:49<10:17:54, 19.61s/it]  3%|▎         | 63/1953 [20:08<10:09:09, 19.34s/it]  3%|▎         | 64/1953 [20:26<9:55:48, 18.92s/it]   3%|▎         | 65/1953 [20:44<9:50:01, 18.75s/it]  3%|▎         | 66/1953 [21:03<9:50:42, 18.78s/it]  3%|▎         | 67/1953 [21:22<9:53:55, 18.89s/it]  3%|▎         | 68/1953 [21:40<9:37:03, 18.37s/it]  4%|▎         | 69/1953 [21:58<9:36:09, 18.35s/it]  4%|▎         | 70/1953 [22:22<10:31:55, 20.14s/it]                                                    {'loss': 1.7574, 'learning_rate': 4.984167757679458e-05, 'epoch': 0.04}
  4%|▎         | 70/1953 [22:22<10:31:55, 20.14s/it]  4%|▎         | 71/1953 [22:40<10:12:20, 19.52s/it]  4%|▎         | 72/1953 [22:59<10:06:31, 19.35s/it]  4%|▎         | 73/1953 [23:18<9:57:16, 19.06s/it]   4%|▍         | 74/1953 [23:36<9:50:54, 18.87s/it]  4%|▍         | 75/1953 [23:55<9:52:50, 18.94s/it]  4%|▍         | 76/1953 [24:13<9:39:41, 18.53s/it]  4%|▍         | 77/1953 [24:32<9:50:17, 18.88s/it]  4%|▍         | 78/1953 [24:50<9:41:05, 18.60s/it]  4%|▍         | 79/1953 [25:08<9:33:58, 18.38s/it]  4%|▍         | 80/1953 [25:27<9:35:58, 18.45s/it]                                                   {'loss': 1.754, 'learning_rate': 4.979327841979764e-05, 'epoch': 0.04}
  4%|▍         | 80/1953 [25:27<9:35:58, 18.45s/it]  4%|▍         | 81/1953 [25:46<9:40:35, 18.61s/it]  4%|▍         | 82/1953 [26:04<9:37:55, 18.53s/it]  4%|▍         | 83/1953 [26:23<9:38:24, 18.56s/it]  4%|▍         | 84/1953 [26:42<9:49:00, 18.91s/it]  4%|▍         | 85/1953 [27:03<10:08:08, 19.53s/it]  4%|▍         | 86/1953 [27:21<9:53:25, 19.07s/it]   4%|▍         | 87/1953 [27:40<9:46:54, 18.87s/it]  5%|▍         | 88/1953 [28:01<10:08:49, 19.59s/it]  5%|▍         | 89/1953 [28:19<9:55:49, 19.18s/it]   5%|▍         | 90/1953 [28:37<9:40:42, 18.70s/it]                                                   {'loss': 1.6691, 'learning_rate': 4.9738463920239955e-05, 'epoch': 0.05}
  5%|▍         | 90/1953 [28:37<9:40:42, 18.70s/it]  5%|▍         | 91/1953 [28:55<9:38:46, 18.65s/it]  5%|▍         | 92/1953 [29:14<9:40:30, 18.72s/it]  5%|▍         | 93/1953 [29:36<10:04:35, 19.50s/it]  5%|▍         | 94/1953 [29:54<9:50:23, 19.06s/it]   5%|▍         | 95/1953 [30:12<9:39:43, 18.72s/it]  5%|▍         | 96/1953 [30:30<9:36:43, 18.63s/it]  5%|▍         | 97/1953 [30:48<9:28:01, 18.36s/it]  5%|▌         | 98/1953 [31:13<10:30:38, 20.40s/it]  5%|▌         | 99/1953 [31:32<10:14:07, 19.87s/it]  5%|▌         | 100/1953 [31:51<10:08:50, 19.71s/it]                                                     {'loss': 1.6228, 'learning_rate': 4.967724826155404e-05, 'epoch': 0.05}
  5%|▌         | 100/1953 [31:51<10:08:50, 19.71s/it]  5%|▌         | 101/1953 [32:12<10:21:59, 20.15s/it]  5%|▌         | 102/1953 [32:33<10:25:33, 20.28s/it]  5%|▌         | 103/1953 [32:51<10:05:34, 19.64s/it]  5%|▌         | 104/1953 [33:09<9:52:45, 19.24s/it]   5%|▌         | 105/1953 [33:28<9:45:18, 19.00s/it]  5%|▌         | 106/1953 [33:46<9:37:48, 18.77s/it]  5%|▌         | 107/1953 [34:07<9:59:15, 19.48s/it]  6%|▌         | 108/1953 [34:25<9:48:57, 19.15s/it]  6%|▌         | 109/1953 [34:43<9:37:45, 18.80s/it]  6%|▌         | 110/1953 [35:01<9:28:59, 18.52s/it]                                                    {'loss': 1.5795, 'learning_rate': 4.960964728349348e-05, 'epoch': 0.06}
  6%|▌         | 110/1953 [35:01<9:28:59, 18.52s/it]  6%|▌         | 111/1953 [35:19<9:24:03, 18.37s/it]  6%|▌         | 112/1953 [35:38<9:25:48, 18.44s/it]  6%|▌         | 113/1953 [35:56<9:24:47, 18.42s/it]  6%|▌         | 114/1953 [36:15<9:28:45, 18.56s/it]  6%|▌         | 115/1953 [36:33<9:23:31, 18.40s/it]  6%|▌         | 116/1953 [36:51<9:18:03, 18.23s/it]  6%|▌         | 117/1953 [37:09<9:14:38, 18.13s/it]  6%|▌         | 118/1953 [37:28<9:23:26, 18.42s/it]  6%|▌         | 119/1953 [37:49<9:46:25, 19.19s/it]  6%|▌         | 120/1953 [38:07<9:36:05, 18.86s/it]                                                    {'loss': 1.5679, 'learning_rate': 4.9535678478034307e-05, 'epoch': 0.06}
  6%|▌         | 120/1953 [38:07<9:36:05, 18.86s/it]  6%|▌         | 121/1953 [38:28<9:52:23, 19.40s/it]  6%|▌         | 122/1953 [38:46<9:44:39, 19.16s/it]  6%|▋         | 123/1953 [39:04<9:35:07, 18.86s/it]  6%|▋         | 124/1953 [39:23<9:35:03, 18.86s/it]  6%|▋         | 125/1953 [39:43<9:41:24, 19.08s/it]  6%|▋         | 126/1953 [40:02<9:39:01, 19.02s/it]  7%|▋         | 127/1953 [40:20<9:34:49, 18.89s/it]  7%|▋         | 128/1953 [40:38<9:26:52, 18.64s/it]  7%|▋         | 129/1953 [40:57<9:25:32, 18.60s/it]  7%|▋         | 130/1953 [41:15<9:18:09, 18.37s/it]                                                    {'loss': 1.5289, 'learning_rate': 4.945536098484888e-05, 'epoch': 0.07}
  7%|▋         | 130/1953 [41:15<9:18:09, 18.37s/it]  7%|▋         | 131/1953 [41:33<9:14:19, 18.25s/it]  7%|▋         | 132/1953 [41:51<9:16:57, 18.35s/it]  7%|▋         | 133/1953 [42:10<9:19:26, 18.44s/it]  7%|▋         | 134/1953 [42:31<9:41:45, 19.19s/it]  7%|▋         | 135/1953 [42:50<9:39:11, 19.12s/it]  7%|▋         | 136/1953 [43:08<9:29:22, 18.80s/it]  7%|▋         | 137/1953 [43:26<9:20:30, 18.52s/it]  7%|▋         | 138/1953 [43:45<9:24:48, 18.67s/it]  7%|▋         | 139/1953 [44:04<9:30:11, 18.86s/it]  7%|▋         | 140/1953 [44:22<9:23:24, 18.65s/it]                                                    {'loss': 1.5168, 'learning_rate': 4.936871558635346e-05, 'epoch': 0.07}
  7%|▋         | 140/1953 [44:22<9:23:24, 18.65s/it]  7%|▋         | 141/1953 [44:41<9:20:51, 18.57s/it]  7%|▋         | 142/1953 [44:59<9:16:52, 18.45s/it]  7%|▋         | 143/1953 [45:17<9:14:08, 18.37s/it]  7%|▋         | 144/1953 [45:38<9:39:50, 19.23s/it]  7%|▋         | 145/1953 [45:57<9:33:06, 19.02s/it]  7%|▋         | 146/1953 [46:15<9:27:46, 18.85s/it]  8%|▊         | 147/1953 [46:33<9:22:14, 18.68s/it]  8%|▊         | 148/1953 [46:53<9:26:15, 18.82s/it]  8%|▊         | 149/1953 [47:11<9:25:01, 18.79s/it]  8%|▊         | 150/1953 [47:29<9:15:13, 18.48s/it]                                                    {'loss': 1.5034, 'learning_rate': 4.927576470233065e-05, 'epoch': 0.08}
  8%|▊         | 150/1953 [47:29<9:15:13, 18.48s/it]  8%|▊         | 151/1953 [47:48<9:23:36, 18.77s/it]  8%|▊         | 152/1953 [48:07<9:18:44, 18.61s/it]  8%|▊         | 153/1953 [48:25<9:16:58, 18.57s/it]  8%|▊         | 154/1953 [48:43<9:05:43, 18.20s/it]  8%|▊         | 155/1953 [49:01<9:08:11, 18.29s/it]  8%|▊         | 156/1953 [49:19<9:07:47, 18.29s/it]  8%|▊         | 157/1953 [49:39<9:17:33, 18.63s/it]  8%|▊         | 158/1953 [49:57<9:17:07, 18.62s/it]  8%|▊         | 159/1953 [50:16<9:16:58, 18.63s/it]  8%|▊         | 160/1953 [50:35<9:15:59, 18.61s/it]                                                    {'loss': 1.4815, 'learning_rate': 4.917653238412827e-05, 'epoch': 0.08}
  8%|▊         | 160/1953 [50:35<9:15:59, 18.61s/it]  8%|▊         | 161/1953 [50:53<9:13:47, 18.54s/it]  8%|▊         | 162/1953 [51:12<9:14:41, 18.58s/it]  8%|▊         | 163/1953 [51:29<9:06:07, 18.31s/it]  8%|▊         | 164/1953 [51:49<9:15:39, 18.64s/it]  8%|▊         | 165/1953 [52:07<9:10:19, 18.47s/it]  8%|▊         | 166/1953 [52:25<9:11:30, 18.52s/it]  9%|▊         | 167/1953 [52:43<9:02:20, 18.22s/it]  9%|▊         | 168/1953 [53:02<9:06:27, 18.37s/it]  9%|▊         | 169/1953 [53:20<9:04:36, 18.32s/it]  9%|▊         | 170/1953 [53:38<9:07:17, 18.42s/it]                                                    {'loss': 1.4439, 'learning_rate': 4.9071044308435927e-05, 'epoch': 0.09}
  9%|▊         | 170/1953 [53:38<9:07:17, 18.42s/it]  9%|▉         | 171/1953 [53:57<9:03:52, 18.31s/it]  9%|▉         | 172/1953 [54:14<8:56:37, 18.08s/it]  9%|▉         | 173/1953 [54:32<8:57:36, 18.12s/it]  9%|▉         | 174/1953 [54:56<9:43:52, 19.69s/it]  9%|▉         | 175/1953 [55:13<9:25:36, 19.09s/it]  9%|▉         | 176/1953 [55:32<9:19:23, 18.89s/it]  9%|▉         | 177/1953 [55:56<10:05:53, 20.47s/it]  9%|▉         | 178/1953 [56:14<9:41:28, 19.66s/it]   9%|▉         | 179/1953 [56:32<9:27:08, 19.18s/it]  9%|▉         | 180/1953 [56:50<9:20:43, 18.98s/it]                                                    {'loss': 1.4093, 'learning_rate': 4.89593277706411e-05, 'epoch': 0.09}
  9%|▉         | 180/1953 [56:50<9:20:43, 18.98s/it]  9%|▉         | 181/1953 [57:09<9:19:42, 18.95s/it]  9%|▉         | 182/1953 [57:28<9:20:30, 18.99s/it]  9%|▉         | 183/1953 [57:46<9:10:22, 18.66s/it]  9%|▉         | 184/1953 [58:04<9:04:04, 18.45s/it]  9%|▉         | 185/1953 [58:22<9:02:22, 18.41s/it] 10%|▉         | 186/1953 [58:42<9:15:52, 18.88s/it] 10%|▉         | 187/1953 [59:01<9:09:28, 18.67s/it] 10%|▉         | 188/1953 [59:19<9:03:47, 18.49s/it] 10%|▉         | 189/1953 [59:37<9:02:47, 18.46s/it] 10%|▉         | 190/1953 [59:55<8:59:58, 18.38s/it]                                                    {'loss': 1.4388, 'learning_rate': 4.884141167776639e-05, 'epoch': 0.1}
 10%|▉         | 190/1953 [59:55<8:59:58, 18.38s/it] 10%|▉         | 191/1953 [1:00:13<8:58:41, 18.34s/it] 10%|▉         | 192/1953 [1:00:32<8:56:02, 18.26s/it] 10%|▉         | 193/1953 [1:00:50<8:57:22, 18.32s/it] 10%|▉         | 194/1953 [1:01:09<9:01:51, 18.48s/it] 10%|▉         | 195/1953 [1:01:27<8:55:44, 18.28s/it] 10%|█         | 196/1953 [1:01:45<8:56:41, 18.33s/it] 10%|█         | 197/1953 [1:02:04<8:58:10, 18.39s/it] 10%|█         | 198/1953 [1:02:22<8:57:08, 18.36s/it] 10%|█         | 199/1953 [1:02:40<8:51:23, 18.18s/it] 10%|█         | 200/1953 [1:02:58<8:50:40, 18.16s/it]                                                      {'loss': 1.3979, 'learning_rate': 4.871732654098974e-05, 'epoch': 0.1}
 10%|█         | 200/1953 [1:02:58<8:50:40, 18.16s/it] 10%|█         | 201/1953 [1:03:16<8:49:07, 18.12s/it] 10%|█         | 202/1953 [1:03:34<8:53:40, 18.29s/it] 10%|█         | 203/1953 [1:03:54<9:02:11, 18.59s/it] 10%|█         | 204/1953 [1:04:12<8:56:08, 18.39s/it] 10%|█         | 205/1953 [1:04:31<9:05:46, 18.73s/it] 11%|█         | 206/1953 [1:04:50<9:06:02, 18.75s/it] 11%|█         | 207/1953 [1:05:10<9:12:47, 19.00s/it] 11%|█         | 208/1953 [1:05:28<9:03:20, 18.68s/it] 11%|█         | 209/1953 [1:05:46<8:57:37, 18.50s/it] 11%|█         | 210/1953 [1:06:05<9:01:16, 18.63s/it]                                                      {'loss': 1.3775, 'learning_rate': 4.858710446774951e-05, 'epoch': 0.11}
 11%|█         | 210/1953 [1:06:05<9:01:16, 18.63s/it] 11%|█         | 211/1953 [1:06:22<8:53:03, 18.36s/it] 11%|█         | 212/1953 [1:06:40<8:50:10, 18.27s/it] 11%|█         | 213/1953 [1:07:04<9:35:51, 19.86s/it] 11%|█         | 214/1953 [1:07:27<9:59:16, 20.68s/it] 11%|█         | 215/1953 [1:07:45<9:37:58, 19.95s/it] 11%|█         | 216/1953 [1:08:03<9:24:49, 19.51s/it] 11%|█         | 217/1953 [1:08:22<9:21:48, 19.42s/it] 11%|█         | 218/1953 [1:08:41<9:12:10, 19.10s/it] 11%|█         | 219/1953 [1:08:59<9:01:48, 18.75s/it] 11%|█▏        | 220/1953 [1:09:17<8:53:11, 18.46s/it]                                                      {'loss': 1.369, 'learning_rate': 4.845077915343664e-05, 'epoch': 0.11}
 11%|█▏        | 220/1953 [1:09:17<8:53:11, 18.46s/it] 11%|█▏        | 221/1953 [1:09:34<8:48:11, 18.30s/it] 11%|█▏        | 222/1953 [1:09:53<8:47:14, 18.28s/it] 11%|█▏        | 223/1953 [1:10:11<8:50:09, 18.39s/it] 11%|█▏        | 224/1953 [1:10:29<8:46:32, 18.27s/it] 12%|█▏        | 225/1953 [1:10:48<8:46:39, 18.29s/it] 12%|█▏        | 226/1953 [1:11:06<8:44:50, 18.23s/it] 12%|█▏        | 227/1953 [1:11:25<8:54:42, 18.59s/it] 12%|█▏        | 228/1953 [1:11:44<8:54:19, 18.59s/it] 12%|█▏        | 229/1953 [1:12:03<9:01:01, 18.83s/it] 12%|█▏        | 230/1953 [1:12:22<9:02:50, 18.90s/it]                                                      {'loss': 1.3455, 'learning_rate': 4.830838587267582e-05, 'epoch': 0.12}
 12%|█▏        | 230/1953 [1:12:22<9:02:50, 18.90s/it] 12%|█▏        | 231/1953 [1:12:40<8:56:35, 18.70s/it] 12%|█▏        | 232/1953 [1:12:59<8:57:35, 18.74s/it] 12%|█▏        | 233/1953 [1:13:18<8:59:35, 18.82s/it] 12%|█▏        | 234/1953 [1:13:37<8:57:19, 18.75s/it] 12%|█▏        | 235/1953 [1:13:56<8:56:25, 18.73s/it] 12%|█▏        | 236/1953 [1:14:15<8:59:41, 18.86s/it] 12%|█▏        | 237/1953 [1:14:33<8:56:28, 18.76s/it] 12%|█▏        | 238/1953 [1:14:53<9:03:09, 19.00s/it] 12%|█▏        | 239/1953 [1:15:11<8:55:04, 18.73s/it] 12%|█▏        | 240/1953 [1:15:29<8:48:28, 18.51s/it]                                                      {'loss': 1.3457, 'learning_rate': 4.8159961470198065e-05, 'epoch': 0.12}
 12%|█▏        | 240/1953 [1:15:29<8:48:28, 18.51s/it] 12%|█▏        | 241/1953 [1:15:47<8:45:58, 18.43s/it] 12%|█▏        | 242/1953 [1:16:06<8:51:48, 18.65s/it] 12%|█▏        | 243/1953 [1:16:25<8:49:19, 18.57s/it] 12%|█▏        | 244/1953 [1:16:44<8:57:29, 18.87s/it] 13%|█▎        | 245/1953 [1:17:02<8:47:02, 18.51s/it] 13%|█▎        | 246/1953 [1:17:21<8:50:22, 18.64s/it] 13%|█▎        | 247/1953 [1:17:40<8:55:07, 18.82s/it] 13%|█▎        | 248/1953 [1:18:05<9:43:01, 20.52s/it] 13%|█▎        | 249/1953 [1:18:23<9:24:04, 19.86s/it] 13%|█▎        | 250/1953 [1:18:42<9:18:07, 19.66s/it]                                                      {'loss': 1.3291, 'learning_rate': 4.800554435130703e-05, 'epoch': 0.13}
 13%|█▎        | 250/1953 [1:18:42<9:18:07, 19.66s/it] 13%|█▎        | 251/1953 [1:19:00<9:02:43, 19.13s/it] 13%|█▎        | 252/1953 [1:19:18<8:51:39, 18.75s/it] 13%|█▎        | 253/1953 [1:19:37<8:53:17, 18.82s/it] 13%|█▎        | 254/1953 [1:19:55<8:46:47, 18.60s/it] 13%|█▎        | 255/1953 [1:20:14<8:50:43, 18.75s/it] 13%|█▎        | 256/1953 [1:20:32<8:46:36, 18.62s/it] 13%|█▎        | 257/1953 [1:20:51<8:42:34, 18.49s/it] 13%|█▎        | 258/1953 [1:21:08<8:36:40, 18.29s/it] 13%|█▎        | 259/1953 [1:21:28<8:45:54, 18.63s/it] 13%|█▎        | 260/1953 [1:21:46<8:40:07, 18.43s/it]                                                      {'loss': 1.3276, 'learning_rate': 4.78451744719415e-05, 'epoch': 0.13}
 13%|█▎        | 260/1953 [1:21:46<8:40:07, 18.43s/it] 13%|█▎        | 261/1953 [1:22:04<8:39:20, 18.42s/it] 13%|█▎        | 262/1953 [1:22:23<8:42:11, 18.53s/it] 13%|█▎        | 263/1953 [1:22:43<8:55:30, 19.01s/it] 14%|█▎        | 264/1953 [1:23:01<8:49:19, 18.80s/it] 14%|█▎        | 265/1953 [1:23:19<8:42:10, 18.56s/it] 14%|█▎        | 266/1953 [1:23:37<8:36:03, 18.35s/it] 14%|█▎        | 267/1953 [1:23:56<8:35:06, 18.33s/it] 14%|█▎        | 268/1953 [1:24:14<8:33:06, 18.27s/it] 14%|█▍        | 269/1953 [1:24:32<8:31:53, 18.24s/it] 14%|█▍        | 270/1953 [1:24:50<8:28:02, 18.11s/it]                                                      {'loss': 1.3205, 'learning_rate': 4.767889332833667e-05, 'epoch': 0.14}
 14%|█▍        | 270/1953 [1:24:50<8:28:02, 18.11s/it] 14%|█▍        | 271/1953 [1:25:08<8:27:01, 18.09s/it] 14%|█▍        | 272/1953 [1:25:26<8:31:54, 18.27s/it] 14%|█▍        | 273/1953 [1:25:45<8:32:22, 18.30s/it] 14%|█▍        | 274/1953 [1:26:03<8:32:23, 18.31s/it] 14%|█▍        | 275/1953 [1:26:21<8:29:37, 18.22s/it] 14%|█▍        | 276/1953 [1:26:39<8:26:08, 18.11s/it] 14%|█▍        | 277/1953 [1:26:59<8:39:13, 18.59s/it] 14%|█▍        | 278/1953 [1:27:17<8:35:24, 18.46s/it] 14%|█▍        | 279/1953 [1:27:35<8:33:43, 18.41s/it] 14%|█▍        | 280/1953 [1:27:55<8:43:40, 18.78s/it]                                                      {'loss': 1.2892, 'learning_rate': 4.750674394628687e-05, 'epoch': 0.14}
 14%|█▍        | 280/1953 [1:27:55<8:43:40, 18.78s/it] 14%|█▍        | 281/1953 [1:28:13<8:39:15, 18.63s/it] 14%|█▍        | 282/1953 [1:28:31<8:34:40, 18.48s/it] 14%|█▍        | 283/1953 [1:28:49<8:32:02, 18.40s/it] 15%|█▍        | 284/1953 [1:29:08<8:32:45, 18.43s/it] 15%|█▍        | 285/1953 [1:29:26<8:31:14, 18.39s/it] 15%|█▍        | 286/1953 [1:29:45<8:35:30, 18.55s/it] 15%|█▍        | 287/1953 [1:30:05<8:43:45, 18.86s/it] 15%|█▍        | 288/1953 [1:30:24<8:43:05, 18.85s/it] 15%|█▍        | 289/1953 [1:30:41<8:34:54, 18.57s/it] 15%|█▍        | 290/1953 [1:31:00<8:34:39, 18.57s/it]                                                      {'loss': 1.2999, 'learning_rate': 4.732877087001243e-05, 'epoch': 0.15}
 15%|█▍        | 290/1953 [1:31:00<8:34:39, 18.57s/it] 15%|█▍        | 291/1953 [1:31:18<8:31:49, 18.48s/it] 15%|█▍        | 292/1953 [1:31:36<8:27:53, 18.35s/it] 15%|█▌        | 293/1953 [1:31:54<8:22:51, 18.18s/it] 15%|█▌        | 294/1953 [1:32:16<8:56:27, 19.40s/it] 15%|█▌        | 295/1953 [1:32:35<8:50:44, 19.21s/it] 15%|█▌        | 296/1953 [1:32:54<8:46:40, 19.07s/it] 15%|█▌        | 297/1953 [1:33:12<8:38:41, 18.79s/it] 15%|█▌        | 298/1953 [1:33:30<8:32:56, 18.60s/it] 15%|█▌        | 299/1953 [1:33:49<8:34:18, 18.66s/it] 15%|█▌        | 300/1953 [1:34:08<8:37:56, 18.80s/it]                                                      {'loss': 1.3086, 'learning_rate': 4.714502015063383e-05, 'epoch': 0.15}
 15%|█▌        | 300/1953 [1:34:08<8:37:56, 18.80s/it] 15%|█▌        | 301/1953 [1:34:26<8:33:02, 18.63s/it] 15%|█▌        | 302/1953 [1:34:46<8:38:41, 18.85s/it] 16%|█▌        | 303/1953 [1:35:04<8:33:44, 18.68s/it] 16%|█▌        | 304/1953 [1:35:21<8:23:46, 18.33s/it] 16%|█▌        | 305/1953 [1:35:42<8:39:00, 18.90s/it] 16%|█▌        | 306/1953 [1:36:01<8:41:34, 19.00s/it] 16%|█▌        | 307/1953 [1:36:19<8:34:35, 18.76s/it] 16%|█▌        | 308/1953 [1:36:38<8:36:11, 18.83s/it] 16%|█▌        | 309/1953 [1:36:58<8:44:41, 19.15s/it] 16%|█▌        | 310/1953 [1:37:18<8:48:08, 19.29s/it]                                                      {'loss': 1.2655, 'learning_rate': 4.6955539334255716e-05, 'epoch': 0.16}
 16%|█▌        | 310/1953 [1:37:18<8:48:08, 19.29s/it] 16%|█▌        | 311/1953 [1:37:36<8:40:15, 19.01s/it] 16%|█▌        | 312/1953 [1:37:56<8:49:28, 19.36s/it] 16%|█▌        | 313/1953 [1:38:14<8:39:07, 18.99s/it] 16%|█▌        | 314/1953 [1:38:33<8:32:36, 18.77s/it] 16%|█▌        | 315/1953 [1:38:51<8:32:09, 18.76s/it] 16%|█▌        | 316/1953 [1:39:11<8:42:50, 19.16s/it] 16%|█▌        | 317/1953 [1:39:32<8:56:05, 19.66s/it] 16%|█▋        | 318/1953 [1:39:50<8:42:23, 19.17s/it] 16%|█▋        | 319/1953 [1:40:08<8:33:55, 18.87s/it] 16%|█▋        | 320/1953 [1:40:26<8:26:28, 18.61s/it]                                                      {'loss': 1.2612, 'learning_rate': 4.676037744966425e-05, 'epoch': 0.16}
 16%|█▋        | 320/1953 [1:40:26<8:26:28, 18.61s/it] 16%|█▋        | 321/1953 [1:40:45<8:22:20, 18.47s/it] 16%|█▋        | 322/1953 [1:41:03<8:20:14, 18.40s/it] 17%|█▋        | 323/1953 [1:41:25<8:49:46, 19.50s/it] 17%|█▋        | 324/1953 [1:41:43<8:39:24, 19.13s/it] 17%|█▋        | 325/1953 [1:42:01<8:31:21, 18.85s/it] 17%|█▋        | 326/1953 [1:42:20<8:25:56, 18.66s/it] 17%|█▋        | 327/1953 [1:42:38<8:24:10, 18.60s/it] 17%|█▋        | 328/1953 [1:42:57<8:30:41, 18.86s/it] 17%|█▋        | 329/1953 [1:43:16<8:24:46, 18.65s/it] 17%|█▋        | 330/1953 [1:43:34<8:23:10, 18.60s/it]                                                      {'loss': 1.2556, 'learning_rate': 4.655958499564072e-05, 'epoch': 0.17}
 17%|█▋        | 330/1953 [1:43:34<8:23:10, 18.60s/it] 17%|█▋        | 331/1953 [1:43:52<8:14:47, 18.30s/it] 17%|█▋        | 332/1953 [1:44:10<8:14:06, 18.29s/it] 17%|█▋        | 333/1953 [1:44:28<8:13:04, 18.26s/it] 17%|█▋        | 334/1953 [1:44:47<8:13:34, 18.29s/it] 17%|█▋        | 335/1953 [1:45:05<8:16:04, 18.40s/it] 17%|█▋        | 336/1953 [1:45:27<8:43:31, 19.43s/it] 17%|█▋        | 337/1953 [1:45:45<8:32:56, 19.04s/it] 17%|█▋        | 338/1953 [1:46:05<8:35:39, 19.16s/it] 17%|█▋        | 339/1953 [1:46:23<8:25:50, 18.80s/it] 17%|█▋        | 340/1953 [1:46:41<8:20:26, 18.62s/it]                                                      {'loss': 1.2585, 'learning_rate': 4.635321392789484e-05, 'epoch': 0.17}
 17%|█▋        | 340/1953 [1:46:41<8:20:26, 18.62s/it] 17%|█▋        | 341/1953 [1:46:58<8:13:04, 18.35s/it] 18%|█▊        | 342/1953 [1:47:18<8:20:04, 18.62s/it] 18%|█▊        | 343/1953 [1:47:37<8:21:08, 18.68s/it] 18%|█▊        | 344/1953 [1:47:56<8:24:40, 18.82s/it] 18%|█▊        | 345/1953 [1:48:14<8:16:56, 18.54s/it] 18%|█▊        | 346/1953 [1:48:32<8:14:22, 18.46s/it] 18%|█▊        | 347/1953 [1:48:51<8:16:42, 18.56s/it] 18%|█▊        | 348/1953 [1:49:08<8:10:24, 18.33s/it] 18%|█▊        | 349/1953 [1:49:28<8:23:39, 18.84s/it] 18%|█▊        | 350/1953 [1:49:47<8:23:29, 18.85s/it]                                                      {'loss': 1.2349, 'learning_rate': 4.6141317645621e-05, 'epoch': 0.18}
 18%|█▊        | 350/1953 [1:49:47<8:23:29, 18.85s/it] 18%|█▊        | 351/1953 [1:50:07<8:27:56, 19.02s/it] 18%|█▊        | 352/1953 [1:50:26<8:26:34, 18.98s/it] 18%|█▊        | 353/1953 [1:50:44<8:17:25, 18.65s/it] 18%|█▊        | 354/1953 [1:51:02<8:16:16, 18.62s/it] 18%|█▊        | 355/1953 [1:51:21<8:16:55, 18.66s/it] 18%|█▊        | 356/1953 [1:51:39<8:11:33, 18.47s/it] 18%|█▊        | 357/1953 [1:51:57<8:08:39, 18.37s/it] 18%|█▊        | 358/1953 [1:52:16<8:09:54, 18.43s/it] 18%|█▊        | 359/1953 [1:52:34<8:06:50, 18.33s/it] 18%|█▊        | 360/1953 [1:52:52<8:04:13, 18.24s/it]                                                      {'loss': 1.2345, 'learning_rate': 4.5923950977681084e-05, 'epoch': 0.18}
 18%|█▊        | 360/1953 [1:52:52<8:04:13, 18.24s/it] 18%|█▊        | 361/1953 [1:53:12<8:17:14, 18.74s/it] 19%|█▊        | 362/1953 [1:53:29<8:09:53, 18.47s/it] 19%|█▊        | 363/1953 [1:53:48<8:07:48, 18.41s/it] 19%|█▊        | 364/1953 [1:54:06<8:09:01, 18.47s/it] 19%|█▊        | 365/1953 [1:54:27<8:24:06, 19.05s/it] 19%|█▊        | 366/1953 [1:54:46<8:29:37, 19.27s/it] 19%|█▉        | 367/1953 [1:55:05<8:22:12, 19.00s/it] 19%|█▉        | 368/1953 [1:55:23<8:16:12, 18.78s/it] 19%|█▉        | 369/1953 [1:55:42<8:16:11, 18.80s/it] 19%|█▉        | 370/1953 [1:56:01<8:16:56, 18.84s/it]                                                      {'loss': 1.2428, 'learning_rate': 4.570117016841732e-05, 'epoch': 0.19}
 19%|█▉        | 370/1953 [1:56:01<8:16:56, 18.84s/it] 19%|█▉        | 371/1953 [1:56:22<8:31:50, 19.41s/it] 19%|█▉        | 372/1953 [1:56:40<8:22:36, 19.07s/it] 19%|█▉        | 373/1953 [1:56:58<8:14:48, 18.79s/it] 19%|█▉        | 374/1953 [1:57:17<8:13:56, 18.77s/it] 19%|█▉        | 375/1953 [1:57:35<8:10:12, 18.64s/it] 19%|█▉        | 376/1953 [1:57:53<8:03:19, 18.39s/it] 19%|█▉        | 377/1953 [1:58:11<8:00:15, 18.28s/it] 19%|█▉        | 378/1953 [1:58:29<8:01:12, 18.33s/it] 19%|█▉        | 379/1953 [1:58:48<7:59:56, 18.30s/it] 19%|█▉        | 380/1953 [1:59:06<8:03:20, 18.44s/it]                                                      {'loss': 1.2255, 'learning_rate': 4.547303286309885e-05, 'epoch': 0.19}
 19%|█▉        | 380/1953 [1:59:06<8:03:20, 18.44s/it] 20%|█▉        | 381/1953 [1:59:24<7:58:28, 18.26s/it] 20%|█▉        | 382/1953 [1:59:43<7:58:32, 18.28s/it] 20%|█▉        | 383/1953 [2:00:02<8:08:34, 18.67s/it] 20%|█▉        | 384/1953 [2:00:20<8:04:01, 18.51s/it] 20%|█▉        | 385/1953 [2:00:40<8:11:05, 18.79s/it] 20%|█▉        | 386/1953 [2:01:00<8:19:21, 19.12s/it] 20%|█▉        | 387/1953 [2:01:18<8:10:40, 18.80s/it] 20%|█▉        | 388/1953 [2:01:37<8:14:43, 18.97s/it] 20%|█▉        | 389/1953 [2:01:55<8:09:42, 18.79s/it] 20%|█▉        | 390/1953 [2:02:14<8:11:20, 18.86s/it]                                                      {'loss': 1.2269, 'learning_rate': 4.523959809300582e-05, 'epoch': 0.2}
 20%|█▉        | 390/1953 [2:02:14<8:11:20, 18.86s/it] 20%|██        | 391/1953 [2:02:33<8:12:30, 18.92s/it] 20%|██        | 392/1953 [2:02:53<8:15:15, 19.04s/it] 20%|██        | 393/1953 [2:03:15<8:39:29, 19.98s/it] 20%|██        | 394/1953 [2:03:34<8:28:58, 19.59s/it] 20%|██        | 395/1953 [2:03:52<8:17:58, 19.18s/it] 20%|██        | 396/1953 [2:04:10<8:10:44, 18.91s/it] 20%|██        | 397/1953 [2:04:29<8:10:17, 18.91s/it] 20%|██        | 398/1953 [2:04:48<8:11:20, 18.96s/it] 20%|██        | 399/1953 [2:05:07<8:10:04, 18.92s/it] 20%|██        | 400/1953 [2:05:26<8:11:09, 18.98s/it]                                                      {'loss': 1.2276, 'learning_rate': 4.500092626015488e-05, 'epoch': 0.2}
 20%|██        | 400/1953 [2:05:26<8:11:09, 18.98s/it] 21%|██        | 401/1953 [2:05:45<8:07:44, 18.86s/it] 21%|██        | 402/1953 [2:06:03<8:00:18, 18.58s/it] 21%|██        | 403/1953 [2:06:22<8:03:56, 18.73s/it] 21%|██        | 404/1953 [2:06:40<8:04:16, 18.76s/it] 21%|██        | 405/1953 [2:06:59<8:00:13, 18.61s/it] 21%|██        | 406/1953 [2:07:17<7:53:37, 18.37s/it] 21%|██        | 407/1953 [2:07:36<8:03:39, 18.77s/it] 21%|██        | 408/1953 [2:07:56<8:13:48, 19.18s/it] 21%|██        | 409/1953 [2:08:14<8:03:12, 18.78s/it] 21%|██        | 410/1953 [2:08:33<8:03:46, 18.81s/it]                                                      {'loss': 1.2158, 'learning_rate': 4.475707912166994e-05, 'epoch': 0.21}
 21%|██        | 410/1953 [2:08:33<8:03:46, 18.81s/it] 21%|██        | 411/1953 [2:08:51<7:57:55, 18.60s/it] 21%|██        | 412/1953 [2:09:10<7:56:42, 18.56s/it] 21%|██        | 413/1953 [2:09:28<7:54:35, 18.49s/it] 21%|██        | 414/1953 [2:09:47<7:56:41, 18.58s/it] 21%|██        | 415/1953 [2:10:05<7:53:10, 18.46s/it] 21%|██▏       | 416/1953 [2:10:27<8:22:19, 19.61s/it] 21%|██▏       | 417/1953 [2:10:45<8:07:33, 19.05s/it] 21%|██▏       | 418/1953 [2:11:04<8:05:33, 18.98s/it] 21%|██▏       | 419/1953 [2:11:24<8:15:27, 19.38s/it] 22%|██▏       | 420/1953 [2:11:45<8:24:17, 19.74s/it]                                                      {'loss': 1.1942, 'learning_rate': 4.45081197738023e-05, 'epoch': 0.22}
 22%|██▏       | 420/1953 [2:11:45<8:24:17, 19.74s/it] 22%|██▏       | 421/1953 [2:12:03<8:12:07, 19.27s/it] 22%|██▏       | 422/1953 [2:12:21<8:03:10, 18.94s/it] 22%|██▏       | 423/1953 [2:12:40<8:04:51, 19.01s/it] 22%|██▏       | 424/1953 [2:12:59<7:59:24, 18.81s/it] 22%|██▏       | 425/1953 [2:13:17<7:53:29, 18.59s/it] 22%|██▏       | 426/1953 [2:13:40<8:25:56, 19.88s/it] 22%|██▏       | 427/1953 [2:13:58<8:11:29, 19.32s/it] 22%|██▏       | 428/1953 [2:14:16<8:06:45, 19.15s/it] 22%|██▏       | 429/1953 [2:14:35<7:59:04, 18.86s/it] 22%|██▏       | 430/1953 [2:14:53<7:54:37, 18.70s/it]                                                      {'loss': 1.1879, 'learning_rate': 4.4254112635604294e-05, 'epoch': 0.22}
 22%|██▏       | 430/1953 [2:14:53<7:54:37, 18.70s/it] 22%|██▏       | 431/1953 [2:15:11<7:47:21, 18.42s/it] 22%|██▏       | 432/1953 [2:15:29<7:45:51, 18.38s/it] 22%|██▏       | 433/1953 [2:15:47<7:45:40, 18.38s/it] 22%|██▏       | 434/1953 [2:16:06<7:47:45, 18.48s/it] 22%|██▏       | 435/1953 [2:16:24<7:44:24, 18.36s/it] 22%|██▏       | 436/1953 [2:16:42<7:37:40, 18.10s/it] 22%|██▏       | 437/1953 [2:16:59<7:33:00, 17.93s/it] 22%|██▏       | 438/1953 [2:17:17<7:34:18, 17.99s/it] 22%|██▏       | 439/1953 [2:17:36<7:40:38, 18.26s/it] 23%|██▎       | 440/1953 [2:18:00<8:23:19, 19.96s/it]                                                      {'loss': 1.1759, 'learning_rate': 4.399512343226068e-05, 'epoch': 0.23}
 23%|██▎       | 440/1953 [2:18:00<8:23:19, 19.96s/it] 23%|██▎       | 441/1953 [2:18:18<8:08:55, 19.40s/it] 23%|██▎       | 442/1953 [2:18:36<7:59:42, 19.05s/it] 23%|██▎       | 443/1953 [2:18:55<7:52:54, 18.79s/it] 23%|██▎       | 444/1953 [2:19:14<8:00:42, 19.11s/it] 23%|██▎       | 445/1953 [2:19:33<7:53:37, 18.84s/it] 23%|██▎       | 446/1953 [2:19:51<7:47:44, 18.62s/it] 23%|██▎       | 447/1953 [2:20:09<7:40:51, 18.36s/it] 23%|██▎       | 448/1953 [2:20:27<7:38:19, 18.27s/it] 23%|██▎       | 449/1953 [2:20:45<7:36:26, 18.21s/it] 23%|██▎       | 450/1953 [2:21:03<7:34:22, 18.14s/it]                                                      {'loss': 1.1769, 'learning_rate': 4.373121917808196e-05, 'epoch': 0.23}
 23%|██▎       | 450/1953 [2:21:03<7:34:22, 18.14s/it] 23%|██▎       | 451/1953 [2:21:20<7:32:08, 18.06s/it] 23%|██▎       | 452/1953 [2:21:40<7:39:57, 18.39s/it] 23%|██▎       | 453/1953 [2:21:58<7:39:54, 18.40s/it] 23%|██▎       | 454/1953 [2:22:17<7:40:47, 18.44s/it] 23%|██▎       | 455/1953 [2:22:35<7:42:46, 18.54s/it] 23%|██▎       | 456/1953 [2:22:53<7:39:11, 18.40s/it] 23%|██▎       | 457/1953 [2:23:11<7:36:03, 18.29s/it] 23%|██▎       | 458/1953 [2:23:32<7:52:06, 18.95s/it] 24%|██▎       | 459/1953 [2:23:50<7:43:06, 18.60s/it] 24%|██▎       | 460/1953 [2:24:08<7:36:55, 18.36s/it]                                                      {'loss': 1.1564, 'learning_rate': 4.346246815916429e-05, 'epoch': 0.24}
 24%|██▎       | 460/1953 [2:24:08<7:36:55, 18.36s/it] 24%|██▎       | 461/1953 [2:24:26<7:39:11, 18.47s/it] 24%|██▎       | 462/1953 [2:24:45<7:40:18, 18.52s/it] 24%|██▎       | 463/1953 [2:25:05<7:54:25, 19.10s/it] 24%|██▍       | 464/1953 [2:25:24<7:50:31, 18.96s/it] 24%|██▍       | 465/1953 [2:25:44<7:55:20, 19.17s/it] 24%|██▍       | 466/1953 [2:26:02<7:45:33, 18.79s/it] 24%|██▍       | 467/1953 [2:26:19<7:37:24, 18.47s/it] 24%|██▍       | 468/1953 [2:26:38<7:36:28, 18.44s/it] 24%|██▍       | 469/1953 [2:26:56<7:31:53, 18.27s/it] 24%|██▍       | 470/1953 [2:27:13<7:28:34, 18.15s/it]                                                      {'loss': 1.1696, 'learning_rate': 4.318893991572018e-05, 'epoch': 0.24}
 24%|██▍       | 470/1953 [2:27:13<7:28:34, 18.15s/it] 24%|██▍       | 471/1953 [2:27:31<7:27:38, 18.12s/it] 24%|██▍       | 472/1953 [2:27:50<7:29:29, 18.21s/it] 24%|██▍       | 473/1953 [2:28:08<7:28:05, 18.17s/it] 24%|██▍       | 474/1953 [2:28:26<7:29:01, 18.22s/it] 24%|██▍       | 475/1953 [2:28:45<7:29:15, 18.24s/it] 24%|██▍       | 476/1953 [2:29:02<7:26:35, 18.14s/it] 24%|██▍       | 477/1953 [2:29:21<7:31:55, 18.37s/it] 24%|██▍       | 478/1953 [2:29:40<7:35:19, 18.52s/it] 25%|██▍       | 479/1953 [2:29:58<7:31:00, 18.36s/it] 25%|██▍       | 480/1953 [2:30:17<7:30:46, 18.36s/it]                                                      {'loss': 1.1538, 'learning_rate': 4.291070522408471e-05, 'epoch': 0.25}
 25%|██▍       | 480/1953 [2:30:17<7:30:46, 18.36s/it] 25%|██▍       | 481/1953 [2:30:35<7:30:37, 18.37s/it] 25%|██▍       | 482/1953 [2:30:53<7:30:22, 18.37s/it] 25%|██▍       | 483/1953 [2:31:11<7:24:57, 18.16s/it] 25%|██▍       | 484/1953 [2:31:29<7:26:48, 18.25s/it] 25%|██▍       | 485/1953 [2:31:47<7:22:47, 18.10s/it] 25%|██▍       | 486/1953 [2:32:06<7:24:33, 18.18s/it] 25%|██▍       | 487/1953 [2:32:24<7:24:18, 18.18s/it] 25%|██▍       | 488/1953 [2:32:42<7:24:04, 18.19s/it] 25%|██▌       | 489/1953 [2:33:01<7:26:29, 18.30s/it] 25%|██▌       | 490/1953 [2:33:19<7:27:41, 18.36s/it]                                                      {'loss': 1.1656, 'learning_rate': 4.262783607840199e-05, 'epoch': 0.25}
 25%|██▌       | 490/1953 [2:33:19<7:27:41, 18.36s/it] 25%|██▌       | 491/1953 [2:33:37<7:23:04, 18.18s/it] 25%|██▌       | 492/1953 [2:33:55<7:23:27, 18.21s/it] 25%|██▌       | 493/1953 [2:34:13<7:24:02, 18.25s/it] 25%|██▌       | 494/1953 [2:34:32<7:25:07, 18.31s/it] 25%|██▌       | 495/1953 [2:34:50<7:26:00, 18.35s/it] 25%|██▌       | 496/1953 [2:35:09<7:28:53, 18.49s/it] 25%|██▌       | 497/1953 [2:35:27<7:24:46, 18.33s/it] 25%|██▌       | 498/1953 [2:35:47<7:35:56, 18.80s/it] 26%|██▌       | 499/1953 [2:36:05<7:28:53, 18.52s/it] 26%|██▌       | 500/1953 [2:36:23<7:26:36, 18.44s/it]                                                      {'loss': 1.1455, 'learning_rate': 4.234040567199637e-05, 'epoch': 0.26}
 26%|██▌       | 500/1953 [2:36:23<7:26:36, 18.44s/it] 26%|██▌       | 501/1953 [2:36:41<7:20:30, 18.20s/it] 26%|██▌       | 502/1953 [2:36:59<7:22:45, 18.31s/it] 26%|██▌       | 503/1953 [2:37:18<7:23:40, 18.36s/it] 26%|██▌       | 504/1953 [2:37:36<7:25:39, 18.45s/it] 26%|██▌       | 505/1953 [2:37:54<7:21:51, 18.31s/it] 26%|██▌       | 506/1953 [2:38:12<7:15:40, 18.07s/it] 26%|██▌       | 507/1953 [2:38:31<7:21:35, 18.32s/it] 26%|██▌       | 508/1953 [2:38:49<7:21:15, 18.32s/it] 26%|██▌       | 509/1953 [2:39:10<7:36:28, 18.97s/it] 26%|██▌       | 510/1953 [2:39:28<7:33:34, 18.86s/it]                                                      {'loss': 1.1574, 'learning_rate': 4.2048488378433493e-05, 'epoch': 0.26}
 26%|██▌       | 510/1953 [2:39:28<7:33:34, 18.86s/it] 26%|██▌       | 511/1953 [2:39:46<7:26:23, 18.57s/it] 26%|██▌       | 512/1953 [2:40:04<7:22:14, 18.41s/it] 26%|██▋       | 513/1953 [2:40:23<7:22:01, 18.42s/it] 26%|██▋       | 514/1953 [2:40:42<7:31:48, 18.84s/it] 26%|██▋       | 515/1953 [2:41:02<7:37:46, 19.10s/it] 26%|██▋       | 516/1953 [2:41:20<7:28:31, 18.73s/it] 26%|██▋       | 517/1953 [2:41:39<7:28:53, 18.76s/it] 27%|██▋       | 518/1953 [2:41:56<7:20:19, 18.41s/it] 27%|██▋       | 519/1953 [2:42:18<7:45:32, 19.48s/it] 27%|██▋       | 520/1953 [2:42:37<7:36:06, 19.10s/it]                                                      {'loss': 1.1477, 'learning_rate': 4.17521597322758e-05, 'epoch': 0.27}
 27%|██▋       | 520/1953 [2:42:37<7:36:06, 19.10s/it] 27%|██▋       | 521/1953 [2:42:55<7:30:38, 18.88s/it] 27%|██▋       | 522/1953 [2:43:15<7:37:38, 19.19s/it] 27%|██▋       | 523/1953 [2:43:34<7:38:43, 19.25s/it] 27%|██▋       | 524/1953 [2:43:53<7:32:36, 19.00s/it] 27%|██▋       | 525/1953 [2:44:11<7:27:10, 18.79s/it] 27%|██▋       | 526/1953 [2:44:31<7:33:16, 19.06s/it] 27%|██▋       | 527/1953 [2:44:49<7:29:34, 18.92s/it] 27%|██▋       | 528/1953 [2:45:08<7:26:38, 18.81s/it] 27%|██▋       | 529/1953 [2:45:27<7:28:22, 18.89s/it] 27%|██▋       | 530/1953 [2:45:45<7:24:08, 18.73s/it]                                                      {'loss': 1.1293, 'learning_rate': 4.145149640953782e-05, 'epoch': 0.27}
 27%|██▋       | 530/1953 [2:45:45<7:24:08, 18.73s/it] 27%|██▋       | 531/1953 [2:46:03<7:14:19, 18.33s/it] 27%|██▋       | 532/1953 [2:46:23<7:29:51, 19.00s/it] 27%|██▋       | 533/1953 [2:46:42<7:25:15, 18.81s/it] 27%|██▋       | 534/1953 [2:47:00<7:22:39, 18.72s/it] 27%|██▋       | 535/1953 [2:47:18<7:15:18, 18.42s/it] 27%|██▋       | 536/1953 [2:47:40<7:43:09, 19.61s/it] 27%|██▋       | 537/1953 [2:47:58<7:31:33, 19.13s/it] 28%|██▊       | 538/1953 [2:48:17<7:25:40, 18.90s/it] 28%|██▊       | 539/1953 [2:48:35<7:22:34, 18.78s/it] 28%|██▊       | 540/1953 [2:48:54<7:20:52, 18.72s/it]                                                      {'loss': 1.1269, 'learning_rate': 4.114657620784589e-05, 'epoch': 0.28}
 28%|██▊       | 540/1953 [2:48:54<7:20:52, 18.72s/it] 28%|██▊       | 541/1953 [2:49:14<7:29:03, 19.08s/it] 28%|██▊       | 542/1953 [2:49:32<7:24:31, 18.90s/it] 28%|██▊       | 543/1953 [2:49:50<7:16:46, 18.59s/it] 28%|██▊       | 544/1953 [2:50:10<7:25:27, 18.97s/it] 28%|██▊       | 545/1953 [2:50:28<7:22:26, 18.85s/it] 28%|██▊       | 546/1953 [2:50:46<7:13:05, 18.47s/it] 28%|██▊       | 547/1953 [2:51:05<7:13:16, 18.49s/it] 28%|██▊       | 548/1953 [2:51:23<7:10:02, 18.36s/it] 28%|██▊       | 549/1953 [2:51:45<7:36:21, 19.50s/it] 28%|██▊       | 550/1953 [2:52:04<7:36:42, 19.53s/it]                                                      {'loss': 1.131, 'learning_rate': 4.0837478026307864e-05, 'epoch': 0.28}
 28%|██▊       | 550/1953 [2:52:04<7:36:42, 19.53s/it] 28%|██▊       | 551/1953 [2:52:24<7:34:24, 19.45s/it] 28%|██▊       | 552/1953 [2:52:41<7:22:56, 18.97s/it] 28%|██▊       | 553/1953 [2:52:59<7:15:07, 18.65s/it] 28%|██▊       | 554/1953 [2:53:18<7:15:54, 18.70s/it] 28%|██▊       | 555/1953 [2:53:37<7:13:39, 18.61s/it] 28%|██▊       | 556/1953 [2:53:59<7:37:58, 19.67s/it] 29%|██▊       | 557/1953 [2:54:16<7:24:04, 19.09s/it] 29%|██▊       | 558/1953 [2:54:35<7:22:17, 19.02s/it] 29%|██▊       | 559/1953 [2:54:54<7:16:41, 18.80s/it] 29%|██▊       | 560/1953 [2:55:12<7:16:12, 18.79s/it]                                                      {'loss': 1.1295, 'learning_rate': 4.052428184509762e-05, 'epoch': 0.29}
 29%|██▊       | 560/1953 [2:55:12<7:16:12, 18.79s/it] 29%|██▊       | 561/1953 [2:55:31<7:12:10, 18.63s/it] 29%|██▉       | 562/1953 [2:55:49<7:07:37, 18.45s/it] 29%|██▉       | 563/1953 [2:56:08<7:11:28, 18.62s/it] 29%|██▉       | 564/1953 [2:56:26<7:10:40, 18.60s/it] 29%|██▉       | 565/1953 [2:56:45<7:08:19, 18.52s/it] 29%|██▉       | 566/1953 [2:57:03<7:06:22, 18.44s/it] 29%|██▉       | 567/1953 [2:57:21<7:04:15, 18.37s/it] 29%|██▉       | 568/1953 [2:57:41<7:14:52, 18.84s/it] 29%|██▉       | 569/1953 [2:58:00<7:14:35, 18.84s/it] 29%|██▉       | 570/1953 [2:58:18<7:10:39, 18.68s/it]                                                      {'loss': 1.1237, 'learning_rate': 4.020706870476e-05, 'epoch': 0.29}
 29%|██▉       | 570/1953 [2:58:18<7:10:39, 18.68s/it] 29%|██▉       | 571/1953 [2:58:37<7:10:39, 18.70s/it] 29%|██▉       | 572/1953 [2:58:55<7:07:47, 18.59s/it] 29%|██▉       | 573/1953 [2:59:13<7:03:21, 18.41s/it] 29%|██▉       | 574/1953 [2:59:33<7:15:16, 18.94s/it] 29%|██▉       | 575/1953 [2:59:53<7:18:15, 19.08s/it] 29%|██▉       | 576/1953 [3:00:11<7:09:46, 18.73s/it] 30%|██▉       | 577/1953 [3:00:29<7:04:28, 18.51s/it] 30%|██▉       | 578/1953 [3:00:47<7:01:04, 18.37s/it] 30%|██▉       | 579/1953 [3:01:05<7:00:16, 18.35s/it] 30%|██▉       | 580/1953 [3:01:24<7:05:22, 18.59s/it]                                                      {'loss': 1.1139, 'learning_rate': 3.988592068524125e-05, 'epoch': 0.3}
 30%|██▉       | 580/1953 [3:01:24<7:05:22, 18.59s/it] 30%|██▉       | 581/1953 [3:01:43<7:05:06, 18.59s/it] 30%|██▉       | 582/1953 [3:02:02<7:07:14, 18.70s/it] 30%|██▉       | 583/1953 [3:02:21<7:10:11, 18.84s/it] 30%|██▉       | 584/1953 [3:02:40<7:12:28, 18.95s/it] 30%|██▉       | 585/1953 [3:02:59<7:12:35, 18.97s/it] 30%|███       | 586/1953 [3:03:18<7:12:14, 18.97s/it] 30%|███       | 587/1953 [3:03:36<7:03:47, 18.61s/it] 30%|███       | 588/1953 [3:03:54<7:00:57, 18.50s/it] 30%|███       | 589/1953 [3:04:12<6:57:50, 18.38s/it] 30%|███       | 590/1953 [3:04:35<7:24:34, 19.57s/it]                                                      {'loss': 1.0867, 'learning_rate': 3.956092088465058e-05, 'epoch': 0.3}
 30%|███       | 590/1953 [3:04:35<7:24:34, 19.57s/it] 30%|███       | 591/1953 [3:04:53<7:14:05, 19.12s/it] 30%|███       | 592/1953 [3:05:11<7:07:51, 18.86s/it] 30%|███       | 593/1953 [3:05:32<7:21:39, 19.48s/it] 30%|███       | 594/1953 [3:05:51<7:16:38, 19.28s/it] 30%|███       | 595/1953 [3:06:09<7:09:54, 18.99s/it] 31%|███       | 596/1953 [3:06:27<7:01:51, 18.65s/it] 31%|███       | 597/1953 [3:06:45<7:00:46, 18.62s/it] 31%|███       | 598/1953 [3:07:03<6:54:25, 18.35s/it] 31%|███       | 599/1953 [3:07:22<6:59:48, 18.60s/it] 31%|███       | 600/1953 [3:07:41<6:58:50, 18.57s/it]                                                      {'loss': 1.1155, 'learning_rate': 3.923215339775826e-05, 'epoch': 0.31}
 31%|███       | 600/1953 [3:07:41<6:58:50, 18.57s/it] 31%|███       | 601/1953 [3:07:59<6:57:12, 18.52s/it] 31%|███       | 602/1953 [3:08:18<6:56:39, 18.50s/it] 31%|███       | 603/1953 [3:08:41<7:28:04, 19.91s/it] 31%|███       | 604/1953 [3:08:59<7:14:08, 19.31s/it] 31%|███       | 605/1953 [3:09:17<7:09:41, 19.13s/it] 31%|███       | 606/1953 [3:09:36<7:04:31, 18.91s/it] 31%|███       | 607/1953 [3:09:53<6:54:43, 18.49s/it] 31%|███       | 608/1953 [3:10:12<6:54:05, 18.47s/it] 31%|███       | 609/1953 [3:10:30<6:52:41, 18.42s/it] 31%|███       | 610/1953 [3:10:52<7:15:23, 19.45s/it]                                                      {'loss': 1.0918, 'learning_rate': 3.8899703294235825e-05, 'epoch': 0.31}
 31%|███       | 610/1953 [3:10:52<7:15:23, 19.45s/it] 31%|███▏      | 611/1953 [3:11:10<7:03:11, 18.92s/it] 31%|███▏      | 612/1953 [3:11:28<7:02:21, 18.90s/it] 31%|███▏      | 613/1953 [3:11:46<6:56:31, 18.65s/it] 31%|███▏      | 614/1953 [3:12:08<7:12:15, 19.37s/it] 31%|███▏      | 615/1953 [3:12:26<7:06:56, 19.15s/it] 32%|███▏      | 616/1953 [3:12:46<7:08:12, 19.22s/it] 32%|███▏      | 617/1953 [3:13:05<7:08:06, 19.23s/it] 32%|███▏      | 618/1953 [3:13:24<7:04:55, 19.10s/it] 32%|███▏      | 619/1953 [3:13:41<6:56:08, 18.72s/it] 32%|███▏      | 620/1953 [3:14:04<7:24:49, 20.02s/it]                                                      {'loss': 1.0803, 'learning_rate': 3.856365659664399e-05, 'epoch': 0.32}
 32%|███▏      | 620/1953 [3:14:04<7:24:49, 20.02s/it] 32%|███▏      | 621/1953 [3:14:23<7:12:47, 19.50s/it] 32%|███▏      | 622/1953 [3:14:42<7:09:24, 19.36s/it] 32%|███▏      | 623/1953 [3:15:00<7:00:54, 18.99s/it] 32%|███▏      | 624/1953 [3:15:18<6:55:54, 18.78s/it] 32%|███▏      | 625/1953 [3:15:37<6:54:27, 18.73s/it] 32%|███▏      | 626/1953 [3:15:57<7:03:32, 19.15s/it] 32%|███▏      | 627/1953 [3:16:15<6:57:27, 18.89s/it] 32%|███▏      | 628/1953 [3:16:33<6:50:24, 18.58s/it] 32%|███▏      | 629/1953 [3:16:52<6:51:51, 18.66s/it] 32%|███▏      | 630/1953 [3:17:10<6:50:32, 18.62s/it]                                                      {'loss': 1.1003, 'learning_rate': 3.822410025817406e-05, 'epoch': 0.32}
 32%|███▏      | 630/1953 [3:17:10<6:50:32, 18.62s/it] 32%|███▏      | 631/1953 [3:17:28<6:44:14, 18.35s/it] 32%|███▏      | 632/1953 [3:17:47<6:44:31, 18.37s/it] 32%|███▏      | 633/1953 [3:18:05<6:43:14, 18.33s/it] 32%|███▏      | 634/1953 [3:18:23<6:43:27, 18.35s/it] 33%|███▎      | 635/1953 [3:18:41<6:42:07, 18.31s/it] 33%|███▎      | 636/1953 [3:19:01<6:52:49, 18.81s/it] 33%|███▎      | 637/1953 [3:19:20<6:51:32, 18.76s/it] 33%|███▎      | 638/1953 [3:19:39<6:53:12, 18.85s/it] 33%|███▎      | 639/1953 [3:19:57<6:47:20, 18.60s/it] 33%|███▎      | 640/1953 [3:20:16<6:51:31, 18.81s/it]                                                      {'loss': 1.0914, 'learning_rate': 3.7881122140148505e-05, 'epoch': 0.33}
 33%|███▎      | 640/1953 [3:20:16<6:51:31, 18.81s/it] 33%|███▎      | 641/1953 [3:20:35<6:48:16, 18.67s/it] 33%|███▎      | 642/1953 [3:20:53<6:47:41, 18.66s/it] 33%|███▎      | 643/1953 [3:21:11<6:41:25, 18.39s/it] 33%|███▎      | 644/1953 [3:21:29<6:40:11, 18.34s/it] 33%|███▎      | 645/1953 [3:21:48<6:42:29, 18.46s/it] 33%|███▎      | 646/1953 [3:22:07<6:44:11, 18.55s/it] 33%|███▎      | 647/1953 [3:22:25<6:40:00, 18.38s/it] 33%|███▎      | 648/1953 [3:22:43<6:39:07, 18.35s/it] 33%|███▎      | 649/1953 [3:23:02<6:40:13, 18.42s/it] 33%|███▎      | 650/1953 [3:23:20<6:37:47, 18.32s/it]                                                      {'loss': 1.085, 'learning_rate': 3.7534810989286506e-05, 'epoch': 0.33}
 33%|███▎      | 650/1953 [3:23:20<6:37:47, 18.32s/it] 33%|███▎      | 651/1953 [3:23:38<6:38:14, 18.35s/it] 33%|███▎      | 652/1953 [3:23:57<6:38:33, 18.38s/it] 33%|███▎      | 653/1953 [3:24:15<6:36:41, 18.31s/it] 33%|███▎      | 654/1953 [3:24:34<6:42:09, 18.58s/it] 34%|███▎      | 655/1953 [3:24:52<6:38:22, 18.41s/it] 34%|███▎      | 656/1953 [3:25:11<6:43:49, 18.68s/it] 34%|███▎      | 657/1953 [3:25:29<6:37:59, 18.43s/it] 34%|███▎      | 658/1953 [3:25:48<6:41:20, 18.60s/it] 34%|███▎      | 659/1953 [3:26:06<6:38:13, 18.46s/it] 34%|███▍      | 660/1953 [3:26:25<6:38:26, 18.49s/it]                                                      {'loss': 1.0896, 'learning_rate': 3.718525641474052e-05, 'epoch': 0.34}
 34%|███▍      | 660/1953 [3:26:25<6:38:26, 18.49s/it] 34%|███▍      | 661/1953 [3:26:43<6:33:18, 18.26s/it] 34%|███▍      | 662/1953 [3:27:01<6:32:14, 18.23s/it] 34%|███▍      | 663/1953 [3:27:23<6:57:09, 19.40s/it] 34%|███▍      | 664/1953 [3:27:41<6:49:19, 19.05s/it] 34%|███▍      | 665/1953 [3:27:59<6:43:20, 18.79s/it] 34%|███▍      | 666/1953 [3:28:17<6:32:57, 18.32s/it] 34%|███▍      | 667/1953 [3:28:35<6:33:23, 18.35s/it] 34%|███▍      | 668/1953 [3:28:55<6:40:52, 18.72s/it] 34%|███▍      | 669/1953 [3:29:14<6:42:10, 18.79s/it] 34%|███▍      | 670/1953 [3:29:32<6:40:26, 18.73s/it]                                                      {'loss': 1.08, 'learning_rate': 3.6832548864909545e-05, 'epoch': 0.34}
 34%|███▍      | 670/1953 [3:29:32<6:40:26, 18.73s/it] 34%|███▍      | 671/1953 [3:29:51<6:39:53, 18.72s/it] 34%|███▍      | 672/1953 [3:30:09<6:34:39, 18.48s/it] 34%|███▍      | 673/1953 [3:30:27<6:33:28, 18.44s/it] 35%|███▍      | 674/1953 [3:30:46<6:33:49, 18.47s/it] 35%|███▍      | 675/1953 [3:31:04<6:32:50, 18.44s/it] 35%|███▍      | 676/1953 [3:31:22<6:30:42, 18.36s/it] 35%|███▍      | 677/1953 [3:31:41<6:30:55, 18.38s/it] 35%|███▍      | 678/1953 [3:31:59<6:29:20, 18.32s/it] 35%|███▍      | 679/1953 [3:32:19<6:39:41, 18.82s/it] 35%|███▍      | 680/1953 [3:32:42<7:04:13, 19.99s/it]                                                      {'loss': 1.0759, 'learning_rate': 3.647677960403536e-05, 'epoch': 0.35}
 35%|███▍      | 680/1953 [3:32:42<7:04:13, 19.99s/it] 35%|███▍      | 681/1953 [3:33:00<6:51:23, 19.41s/it] 35%|███▍      | 682/1953 [3:33:19<6:49:29, 19.33s/it] 35%|███▍      | 683/1953 [3:33:38<6:46:32, 19.21s/it] 35%|███▌      | 684/1953 [3:33:56<6:38:40, 18.85s/it] 35%|███▌      | 685/1953 [3:34:14<6:35:20, 18.71s/it] 35%|███▌      | 686/1953 [3:34:32<6:32:19, 18.58s/it] 35%|███▌      | 687/1953 [3:34:51<6:30:49, 18.52s/it] 35%|███▌      | 688/1953 [3:35:09<6:29:20, 18.47s/it] 35%|███▌      | 689/1953 [3:35:27<6:26:21, 18.34s/it] 35%|███▌      | 690/1953 [3:35:46<6:30:16, 18.54s/it]                                                      {'loss': 1.0736, 'learning_rate': 3.611804068858756e-05, 'epoch': 0.35}
 35%|███▌      | 690/1953 [3:35:46<6:30:16, 18.54s/it] 35%|███▌      | 691/1953 [3:36:04<6:27:32, 18.43s/it] 35%|███▌      | 692/1953 [3:36:22<6:25:37, 18.35s/it] 35%|███▌      | 693/1953 [3:36:40<6:22:59, 18.24s/it] 36%|███▌      | 694/1953 [3:36:59<6:27:49, 18.48s/it] 36%|███▌      | 695/1953 [3:37:17<6:23:51, 18.31s/it] 36%|███▌      | 696/1953 [3:37:36<6:26:26, 18.45s/it] 36%|███▌      | 697/1953 [3:37:55<6:27:01, 18.49s/it] 36%|███▌      | 698/1953 [3:38:13<6:24:32, 18.38s/it] 36%|███▌      | 699/1953 [3:38:31<6:24:07, 18.38s/it] 36%|███▌      | 700/1953 [3:38:49<6:22:56, 18.34s/it]                                                      {'loss': 1.0485, 'learning_rate': 3.575642494344365e-05, 'epoch': 0.36}
 36%|███▌      | 700/1953 [3:38:49<6:22:56, 18.34s/it] 36%|███▌      | 701/1953 [3:39:13<6:54:48, 19.88s/it] 36%|███▌      | 702/1953 [3:39:31<6:43:58, 19.38s/it] 36%|███▌      | 703/1953 [3:39:50<6:42:45, 19.33s/it] 36%|███▌      | 704/1953 [3:40:09<6:38:27, 19.14s/it] 36%|███▌      | 705/1953 [3:40:27<6:31:21, 18.81s/it] 36%|███▌      | 706/1953 [3:40:46<6:31:09, 18.82s/it] 36%|███▌      | 707/1953 [3:41:04<6:23:52, 18.48s/it] 36%|███▋      | 708/1953 [3:41:22<6:21:04, 18.37s/it] 36%|███▋      | 709/1953 [3:41:40<6:22:53, 18.47s/it] 36%|███▋      | 710/1953 [3:42:00<6:28:24, 18.75s/it]                                                      {'loss': 1.0573, 'learning_rate': 3.539202593787033e-05, 'epoch': 0.36}
 36%|███▋      | 710/1953 [3:42:00<6:28:24, 18.75s/it] 36%|███▋      | 711/1953 [3:42:18<6:27:15, 18.71s/it] 36%|███▋      | 712/1953 [3:42:36<6:22:45, 18.51s/it] 37%|███▋      | 713/1953 [3:42:55<6:19:41, 18.37s/it] 37%|███▋      | 714/1953 [3:43:15<6:34:12, 19.09s/it] 37%|███▋      | 715/1953 [3:43:33<6:27:01, 18.76s/it] 37%|███▋      | 716/1953 [3:43:52<6:27:47, 18.81s/it] 37%|███▋      | 717/1953 [3:44:10<6:21:28, 18.52s/it] 37%|███▋      | 718/1953 [3:44:28<6:19:53, 18.46s/it] 37%|███▋      | 719/1953 [3:44:51<6:45:35, 19.72s/it] 37%|███▋      | 720/1953 [3:45:10<6:39:15, 19.43s/it]                                                      {'loss': 1.0554, 'learning_rate': 3.50249379613121e-05, 'epoch': 0.37}
 37%|███▋      | 720/1953 [3:45:10<6:39:15, 19.43s/it] 37%|███▋      | 721/1953 [3:45:28<6:33:35, 19.17s/it] 37%|███▋      | 722/1953 [3:45:46<6:26:10, 18.82s/it] 37%|███▋      | 723/1953 [3:46:05<6:25:31, 18.81s/it] 37%|███▋      | 724/1953 [3:46:24<6:24:14, 18.76s/it] 37%|███▋      | 725/1953 [3:46:44<6:33:27, 19.22s/it] 37%|███▋      | 726/1953 [3:47:02<6:24:59, 18.83s/it] 37%|███▋      | 727/1953 [3:47:20<6:21:50, 18.69s/it] 37%|███▋      | 728/1953 [3:47:39<6:20:37, 18.64s/it] 37%|███▋      | 729/1953 [3:47:57<6:18:38, 18.56s/it] 37%|███▋      | 730/1953 [3:48:16<6:20:13, 18.65s/it]                                                      {'loss': 1.0596, 'learning_rate': 3.4655255998993555e-05, 'epoch': 0.37}
 37%|███▋      | 730/1953 [3:48:16<6:20:13, 18.65s/it] 37%|███▋      | 731/1953 [3:48:35<6:21:26, 18.73s/it] 37%|███▋      | 732/1953 [3:48:54<6:21:45, 18.76s/it] 38%|███▊      | 733/1953 [3:49:12<6:18:47, 18.63s/it] 38%|███▊      | 734/1953 [3:49:31<6:16:54, 18.55s/it] 38%|███▊      | 735/1953 [3:49:49<6:14:16, 18.44s/it] 38%|███▊      | 736/1953 [3:50:07<6:13:06, 18.39s/it] 38%|███▊      | 737/1953 [3:50:30<6:42:13, 19.85s/it] 38%|███▊      | 738/1953 [3:50:48<6:30:23, 19.28s/it] 38%|███▊      | 739/1953 [3:51:07<6:25:45, 19.07s/it] 38%|███▊      | 740/1953 [3:51:26<6:29:00, 19.24s/it]                                                      {'loss': 1.0583, 'learning_rate': 3.42830757073417e-05, 'epoch': 0.38}
 38%|███▊      | 740/1953 [3:51:26<6:29:00, 19.24s/it] 38%|███▊      | 741/1953 [3:51:45<6:22:22, 18.93s/it] 38%|███▊      | 742/1953 [3:52:03<6:19:42, 18.81s/it] 38%|███▊      | 743/1953 [3:52:21<6:13:04, 18.50s/it] 38%|███▊      | 744/1953 [3:52:39<6:10:27, 18.38s/it] 38%|███▊      | 745/1953 [3:53:03<6:46:40, 20.20s/it] 38%|███▊      | 746/1953 [3:53:25<6:52:29, 20.50s/it] 38%|███▊      | 747/1953 [3:53:43<6:39:39, 19.88s/it] 38%|███▊      | 748/1953 [3:54:01<6:27:07, 19.28s/it] 38%|███▊      | 749/1953 [3:54:19<6:20:59, 18.99s/it] 38%|███▊      | 750/1953 [3:54:37<6:14:32, 18.68s/it]                                                      {'loss': 1.053, 'learning_rate': 3.390849338923446e-05, 'epoch': 0.38}
 38%|███▊      | 750/1953 [3:54:37<6:14:32, 18.68s/it] 38%|███▊      | 751/1953 [3:54:56<6:13:15, 18.63s/it] 39%|███▊      | 752/1953 [3:55:17<6:27:32, 19.36s/it] 39%|███▊      | 753/1953 [3:55:34<6:16:15, 18.81s/it] 39%|███▊      | 754/1953 [3:56:00<6:58:47, 20.96s/it] 39%|███▊      | 755/1953 [3:56:19<6:43:14, 20.20s/it] 39%|███▊      | 756/1953 [3:56:37<6:29:19, 19.51s/it] 39%|███▉      | 757/1953 [3:56:55<6:22:28, 19.19s/it] 39%|███▉      | 758/1953 [3:57:14<6:18:09, 18.99s/it] 39%|███▉      | 759/1953 [3:57:32<6:14:34, 18.82s/it] 39%|███▉      | 760/1953 [3:57:50<6:10:20, 18.63s/it]                                                      {'loss': 1.049, 'learning_rate': 3.353160596908202e-05, 'epoch': 0.39}
 39%|███▉      | 760/1953 [3:57:50<6:10:20, 18.63s/it] 39%|███▉      | 761/1953 [3:58:09<6:08:49, 18.56s/it] 39%|███▉      | 762/1953 [3:58:26<6:03:26, 18.31s/it] 39%|███▉      | 763/1953 [3:58:45<6:06:55, 18.50s/it] 39%|███▉      | 764/1953 [3:59:04<6:04:38, 18.40s/it] 39%|███▉      | 765/1953 [3:59:22<6:07:22, 18.55s/it] 39%|███▉      | 766/1953 [3:59:42<6:11:06, 18.76s/it] 39%|███▉      | 767/1953 [4:00:03<6:23:32, 19.40s/it] 39%|███▉      | 768/1953 [4:00:21<6:18:53, 19.18s/it] 39%|███▉      | 769/1953 [4:00:39<6:10:21, 18.77s/it] 39%|███▉      | 770/1953 [4:00:58<6:11:23, 18.84s/it]                                                      {'loss': 1.0379, 'learning_rate': 3.315251096774737e-05, 'epoch': 0.39}
 39%|███▉      | 770/1953 [4:00:58<6:11:23, 18.84s/it] 39%|███▉      | 771/1953 [4:01:16<6:08:14, 18.69s/it] 40%|███▉      | 772/1953 [4:01:35<6:10:24, 18.82s/it] 40%|███▉      | 773/1953 [4:01:53<6:04:39, 18.54s/it] 40%|███▉      | 774/1953 [4:02:12<6:02:25, 18.44s/it] 40%|███▉      | 775/1953 [4:02:30<6:04:04, 18.54s/it] 40%|███▉      | 776/1953 [4:02:49<6:02:21, 18.47s/it] 40%|███▉      | 777/1953 [4:03:07<6:00:30, 18.39s/it] 40%|███▉      | 778/1953 [4:03:26<6:02:21, 18.50s/it] 40%|███▉      | 779/1953 [4:03:44<6:01:15, 18.46s/it] 40%|███▉      | 780/1953 [4:04:02<5:57:09, 18.27s/it]                                                      {'loss': 1.0303, 'learning_rate': 3.277130647731238e-05, 'epoch': 0.4}
 40%|███▉      | 780/1953 [4:04:02<5:57:09, 18.27s/it] 40%|███▉      | 781/1953 [4:04:20<5:55:50, 18.22s/it] 40%|████      | 782/1953 [4:04:38<5:55:09, 18.20s/it] 40%|████      | 783/1953 [4:04:56<5:53:37, 18.13s/it] 40%|████      | 784/1953 [4:05:14<5:50:26, 17.99s/it] 40%|████      | 785/1953 [4:05:32<5:52:21, 18.10s/it] 40%|████      | 786/1953 [4:05:51<5:54:27, 18.22s/it] 40%|████      | 787/1953 [4:06:09<5:56:49, 18.36s/it] 40%|████      | 788/1953 [4:06:28<5:56:56, 18.38s/it] 40%|████      | 789/1953 [4:06:47<6:03:57, 18.76s/it] 40%|████      | 790/1953 [4:07:05<5:57:35, 18.45s/it]                                                      {'loss': 1.0268, 'learning_rate': 3.238809113569617e-05, 'epoch': 0.4}
 40%|████      | 790/1953 [4:07:05<5:57:35, 18.45s/it] 41%|████      | 791/1953 [4:07:23<5:55:25, 18.35s/it] 41%|████      | 792/1953 [4:07:42<5:54:58, 18.35s/it] 41%|████      | 793/1953 [4:08:00<5:56:11, 18.42s/it] 41%|████      | 794/1953 [4:08:19<5:57:45, 18.52s/it] 41%|████      | 795/1953 [4:08:37<5:57:32, 18.53s/it] 41%|████      | 796/1953 [4:08:55<5:50:42, 18.19s/it] 41%|████      | 797/1953 [4:09:13<5:48:04, 18.07s/it] 41%|████      | 798/1953 [4:09:31<5:51:33, 18.26s/it] 41%|████      | 799/1953 [4:09:50<5:52:38, 18.34s/it] 41%|████      | 800/1953 [4:10:09<5:55:27, 18.50s/it]                                                      {'loss': 1.0501, 'learning_rate': 3.200296410113225e-05, 'epoch': 0.41}
 41%|████      | 800/1953 [4:10:09<5:55:27, 18.50s/it] 41%|████      | 801/1953 [4:10:27<5:54:43, 18.48s/it] 41%|████      | 802/1953 [4:10:45<5:50:38, 18.28s/it] 41%|████      | 803/1953 [4:11:03<5:51:28, 18.34s/it] 41%|████      | 804/1953 [4:11:22<5:51:26, 18.35s/it] 41%|████      | 805/1953 [4:11:42<6:04:29, 19.05s/it] 41%|████▏     | 806/1953 [4:12:00<5:57:40, 18.71s/it] 41%|████▏     | 807/1953 [4:12:18<5:51:35, 18.41s/it] 41%|████▏     | 808/1953 [4:12:36<5:48:34, 18.27s/it] 41%|████▏     | 809/1953 [4:12:57<6:04:42, 19.13s/it] 41%|████▏     | 810/1953 [4:13:16<6:00:50, 18.94s/it]                                                      {'loss': 1.0358, 'learning_rate': 3.161602502651099e-05, 'epoch': 0.41}
 41%|████▏     | 810/1953 [4:13:16<6:00:50, 18.94s/it] 42%|████▏     | 811/1953 [4:13:34<5:57:49, 18.80s/it] 42%|████▏     | 812/1953 [4:13:53<5:55:22, 18.69s/it] 42%|████▏     | 813/1953 [4:14:11<5:51:46, 18.51s/it] 42%|████▏     | 814/1953 [4:14:29<5:51:40, 18.53s/it] 42%|████▏     | 815/1953 [4:14:47<5:49:10, 18.41s/it] 42%|████▏     | 816/1953 [4:15:05<5:45:32, 18.23s/it] 42%|████▏     | 817/1953 [4:15:24<5:46:26, 18.30s/it] 42%|████▏     | 818/1953 [4:15:46<6:09:20, 19.52s/it] 42%|████▏     | 819/1953 [4:16:04<6:01:29, 19.13s/it] 42%|████▏     | 820/1953 [4:16:23<5:56:36, 18.88s/it]                                                      {'loss': 1.0378, 'learning_rate': 3.122737403359409e-05, 'epoch': 0.42}
 42%|████▏     | 820/1953 [4:16:23<5:56:36, 18.88s/it] 42%|████▏     | 821/1953 [4:16:46<6:21:29, 20.22s/it] 42%|████▏     | 822/1953 [4:17:04<6:09:17, 19.59s/it] 42%|████▏     | 823/1953 [4:17:22<6:02:31, 19.25s/it] 42%|████▏     | 824/1953 [4:17:41<5:56:16, 18.93s/it] 42%|████▏     | 825/1953 [4:18:00<5:55:42, 18.92s/it] 42%|████▏     | 826/1953 [4:18:17<5:49:28, 18.61s/it] 42%|████▏     | 827/1953 [4:18:36<5:48:11, 18.55s/it] 42%|████▏     | 828/1953 [4:18:54<5:42:44, 18.28s/it] 42%|████▏     | 829/1953 [4:19:13<5:51:58, 18.79s/it] 42%|████▏     | 830/1953 [4:19:32<5:48:48, 18.64s/it]                                                      {'loss': 1.0158, 'learning_rate': 3.083711168710778e-05, 'epoch': 0.42}
 42%|████▏     | 830/1953 [4:19:32<5:48:48, 18.64s/it] 43%|████▎     | 831/1953 [4:19:49<5:42:47, 18.33s/it] 43%|████▎     | 832/1953 [4:20:07<5:39:23, 18.17s/it] 43%|████▎     | 833/1953 [4:20:26<5:43:14, 18.39s/it] 43%|████▎     | 834/1953 [4:20:45<5:47:46, 18.65s/it] 43%|████▎     | 835/1953 [4:21:03<5:44:43, 18.50s/it] 43%|████▎     | 836/1953 [4:21:21<5:41:19, 18.33s/it] 43%|████▎     | 837/1953 [4:21:41<5:46:21, 18.62s/it] 43%|████▎     | 838/1953 [4:21:59<5:45:51, 18.61s/it] 43%|████▎     | 839/1953 [4:22:17<5:40:32, 18.34s/it] 43%|████▎     | 840/1953 [4:22:36<5:43:16, 18.51s/it]                                                      {'loss': 1.0113, 'learning_rate': 3.0445338968721287e-05, 'epoch': 0.43}
 43%|████▎     | 840/1953 [4:22:36<5:43:16, 18.51s/it] 43%|████▎     | 841/1953 [4:22:55<5:44:37, 18.60s/it] 43%|████▎     | 842/1953 [4:23:16<5:57:52, 19.33s/it] 43%|████▎     | 843/1953 [4:23:36<6:01:39, 19.55s/it] 43%|████▎     | 844/1953 [4:23:54<5:54:38, 19.19s/it] 43%|████▎     | 845/1953 [4:24:13<5:53:17, 19.13s/it] 43%|████▎     | 846/1953 [4:24:31<5:45:41, 18.74s/it] 43%|████▎     | 847/1953 [4:24:50<5:46:59, 18.82s/it] 43%|████▎     | 848/1953 [4:25:10<5:53:52, 19.22s/it] 43%|████▎     | 849/1953 [4:25:28<5:47:14, 18.87s/it] 44%|████▎     | 850/1953 [4:25:47<5:47:54, 18.93s/it]                                                      {'loss': 1.0303, 'learning_rate': 3.0052157250917613e-05, 'epoch': 0.44}
 44%|████▎     | 850/1953 [4:25:47<5:47:54, 18.93s/it] 44%|████▎     | 851/1953 [4:26:05<5:42:46, 18.66s/it] 44%|████▎     | 852/1953 [4:26:24<5:41:38, 18.62s/it] 44%|████▎     | 853/1953 [4:26:46<6:02:19, 19.76s/it] 44%|████▎     | 854/1953 [4:27:05<5:58:18, 19.56s/it] 44%|████▍     | 855/1953 [4:27:24<5:51:50, 19.23s/it] 44%|████▍     | 856/1953 [4:27:42<5:46:34, 18.96s/it] 44%|████▍     | 857/1953 [4:28:00<5:39:54, 18.61s/it] 44%|████▍     | 858/1953 [4:28:18<5:35:35, 18.39s/it] 44%|████▍     | 859/1953 [4:28:37<5:38:12, 18.55s/it] 44%|████▍     | 860/1953 [4:28:55<5:37:41, 18.54s/it]                                                      {'loss': 1.0162, 'learning_rate': 2.9657668270762957e-05, 'epoch': 0.44}
 44%|████▍     | 860/1953 [4:28:55<5:37:41, 18.54s/it] 44%|████▍     | 861/1953 [4:29:13<5:34:46, 18.39s/it] 44%|████▍     | 862/1953 [4:29:33<5:42:07, 18.82s/it] 44%|████▍     | 863/1953 [4:29:51<5:36:33, 18.53s/it] 44%|████▍     | 864/1953 [4:30:12<5:52:44, 19.44s/it] 44%|████▍     | 865/1953 [4:30:35<6:11:37, 20.49s/it] 44%|████▍     | 866/1953 [4:30:54<6:02:29, 20.01s/it] 44%|████▍     | 867/1953 [4:31:13<5:57:19, 19.74s/it] 44%|████▍     | 868/1953 [4:31:31<5:47:50, 19.24s/it] 44%|████▍     | 869/1953 [4:31:50<5:42:26, 18.95s/it] 45%|████▍     | 870/1953 [4:32:09<5:41:03, 18.90s/it]                                                      {'loss': 1.0219, 'learning_rate': 2.926197410358199e-05, 'epoch': 0.45}
 45%|████▍     | 870/1953 [4:32:09<5:41:03, 18.90s/it] 45%|████▍     | 871/1953 [4:32:27<5:39:55, 18.85s/it] 45%|████▍     | 872/1953 [4:32:46<5:38:50, 18.81s/it] 45%|████▍     | 873/1953 [4:33:04<5:33:15, 18.51s/it] 45%|████▍     | 874/1953 [4:33:23<5:36:50, 18.73s/it] 45%|████▍     | 875/1953 [4:33:43<5:42:40, 19.07s/it] 45%|████▍     | 876/1953 [4:34:01<5:36:21, 18.74s/it] 45%|████▍     | 877/1953 [4:34:21<5:41:46, 19.06s/it] 45%|████▍     | 878/1953 [4:34:39<5:37:38, 18.85s/it] 45%|████▌     | 879/1953 [4:34:57<5:32:23, 18.57s/it] 45%|████▌     | 880/1953 [4:35:15<5:28:44, 18.38s/it]                                                      {'loss': 1.0183, 'learning_rate': 2.8865177136545485e-05, 'epoch': 0.45}
 45%|████▌     | 880/1953 [4:35:15<5:28:44, 18.38s/it] 45%|████▌     | 881/1953 [4:35:33<5:27:42, 18.34s/it] 45%|████▌     | 882/1953 [4:35:51<5:26:05, 18.27s/it] 45%|████▌     | 883/1953 [4:36:10<5:30:54, 18.56s/it] 45%|████▌     | 884/1953 [4:36:29<5:28:08, 18.42s/it] 45%|████▌     | 885/1953 [4:36:47<5:30:03, 18.54s/it] 45%|████▌     | 886/1953 [4:37:06<5:29:47, 18.54s/it] 45%|████▌     | 887/1953 [4:37:24<5:26:46, 18.39s/it] 45%|████▌     | 888/1953 [4:37:42<5:24:04, 18.26s/it] 46%|████▌     | 889/1953 [4:38:01<5:29:02, 18.56s/it] 46%|████▌     | 890/1953 [4:38:19<5:24:55, 18.34s/it]                                                      {'loss': 1.0083, 'learning_rate': 2.846738004217732e-05, 'epoch': 0.46}
 46%|████▌     | 890/1953 [4:38:19<5:24:55, 18.34s/it] 46%|████▌     | 891/1953 [4:38:37<5:20:58, 18.13s/it] 46%|████▌     | 892/1953 [4:38:55<5:19:51, 18.09s/it] 46%|████▌     | 893/1953 [4:39:13<5:19:18, 18.07s/it] 46%|████▌     | 894/1953 [4:39:31<5:20:05, 18.14s/it] 46%|████▌     | 895/1953 [4:39:52<5:34:55, 18.99s/it] 46%|████▌     | 896/1953 [4:40:10<5:31:25, 18.81s/it] 46%|████▌     | 897/1953 [4:40:29<5:32:26, 18.89s/it] 46%|████▌     | 898/1953 [4:40:48<5:32:34, 18.91s/it] 46%|████▌     | 899/1953 [4:41:09<5:38:34, 19.27s/it] 46%|████▌     | 900/1953 [4:41:28<5:37:08, 19.21s/it]                                                      {'loss': 1.0183, 'learning_rate': 2.8068685751787636e-05, 'epoch': 0.46}
 46%|████▌     | 900/1953 [4:41:28<5:37:08, 19.21s/it] 46%|████▌     | 901/1953 [4:41:47<5:39:28, 19.36s/it] 46%|████▌     | 902/1953 [4:42:06<5:36:04, 19.19s/it] 46%|████▌     | 903/1953 [4:42:24<5:29:53, 18.85s/it] 46%|████▋     | 904/1953 [4:42:42<5:26:36, 18.68s/it] 46%|████▋     | 905/1953 [4:43:01<5:24:22, 18.57s/it] 46%|████▋     | 906/1953 [4:43:18<5:18:38, 18.26s/it] 46%|████▋     | 907/1953 [4:43:37<5:23:16, 18.54s/it] 46%|████▋     | 908/1953 [4:43:56<5:23:03, 18.55s/it] 47%|████▋     | 909/1953 [4:44:14<5:20:57, 18.45s/it] 47%|████▋     | 910/1953 [4:44:33<5:22:31, 18.55s/it]                                                      {'loss': 1.0162, 'learning_rate': 2.7669197428838972e-05, 'epoch': 0.47}
 47%|████▋     | 910/1953 [4:44:33<5:22:31, 18.55s/it] 47%|████▋     | 911/1953 [4:44:51<5:20:44, 18.47s/it] 47%|████▋     | 912/1953 [4:45:09<5:16:17, 18.23s/it] 47%|████▋     | 913/1953 [4:45:33<5:45:54, 19.96s/it] 47%|████▋     | 914/1953 [4:45:52<5:41:10, 19.70s/it] 47%|████▋     | 915/1953 [4:46:10<5:31:53, 19.18s/it] 47%|████▋     | 916/1953 [4:46:28<5:27:00, 18.92s/it] 47%|████▋     | 917/1953 [4:46:47<5:25:57, 18.88s/it] 47%|████▋     | 918/1953 [4:47:05<5:21:07, 18.62s/it] 47%|████▋     | 919/1953 [4:47:24<5:21:06, 18.63s/it] 47%|████▋     | 920/1953 [4:47:43<5:22:43, 18.74s/it]                                                      {'loss': 1.0282, 'learning_rate': 2.726901844225243e-05, 'epoch': 0.47}
 47%|████▋     | 920/1953 [4:47:43<5:22:43, 18.74s/it] 47%|████▋     | 921/1953 [4:48:02<5:25:08, 18.90s/it] 47%|████▋     | 922/1953 [4:48:25<5:46:05, 20.14s/it] 47%|████▋     | 923/1953 [4:48:46<5:48:34, 20.31s/it] 47%|████▋     | 924/1953 [4:49:04<5:38:39, 19.75s/it] 47%|████▋     | 925/1953 [4:49:23<5:34:02, 19.50s/it] 47%|████▋     | 926/1953 [4:49:42<5:32:29, 19.42s/it] 47%|████▋     | 927/1953 [4:50:01<5:25:16, 19.02s/it] 48%|████▊     | 928/1953 [4:50:19<5:21:54, 18.84s/it] 48%|████▊     | 929/1953 [4:50:38<5:20:10, 18.76s/it] 48%|████▊     | 930/1953 [4:50:56<5:16:51, 18.58s/it]                                                      {'loss': 1.0076, 'learning_rate': 2.686825233966061e-05, 'epoch': 0.48}
 48%|████▊     | 930/1953 [4:50:56<5:16:51, 18.58s/it] 48%|████▊     | 931/1953 [4:51:15<5:21:55, 18.90s/it] 48%|████▊     | 932/1953 [4:51:34<5:22:28, 18.95s/it] 48%|████▊     | 933/1953 [4:51:53<5:18:48, 18.75s/it] 48%|████▊     | 934/1953 [4:52:16<5:42:45, 20.18s/it] 48%|████▊     | 935/1953 [4:52:34<5:32:31, 19.60s/it] 48%|████▊     | 936/1953 [4:52:53<5:26:58, 19.29s/it] 48%|████▊     | 937/1953 [4:53:11<5:19:28, 18.87s/it] 48%|████▊     | 938/1953 [4:53:29<5:16:03, 18.68s/it] 48%|████▊     | 939/1953 [4:53:47<5:12:54, 18.52s/it] 48%|████▊     | 940/1953 [4:54:06<5:14:28, 18.63s/it]                                                      {'loss': 1.0031, 'learning_rate': 2.6467002820614296e-05, 'epoch': 0.48}
 48%|████▊     | 940/1953 [4:54:06<5:14:28, 18.63s/it] 48%|████▊     | 941/1953 [4:54:24<5:12:09, 18.51s/it] 48%|████▊     | 942/1953 [4:54:44<5:17:43, 18.86s/it] 48%|████▊     | 943/1953 [4:55:03<5:15:24, 18.74s/it] 48%|████▊     | 944/1953 [4:55:22<5:18:08, 18.92s/it] 48%|████▊     | 945/1953 [4:55:40<5:14:55, 18.75s/it] 48%|████▊     | 946/1953 [4:55:59<5:14:24, 18.73s/it] 48%|████▊     | 947/1953 [4:56:16<5:07:21, 18.33s/it] 49%|████▊     | 948/1953 [4:56:36<5:15:00, 18.81s/it] 49%|████▊     | 949/1953 [4:56:54<5:11:45, 18.63s/it] 49%|████▊     | 950/1953 [4:57:13<5:12:01, 18.67s/it]                                                      {'loss': 0.999, 'learning_rate': 2.606537370974989e-05, 'epoch': 0.49}
 49%|████▊     | 950/1953 [4:57:13<5:12:01, 18.67s/it] 49%|████▊     | 951/1953 [4:57:31<5:07:47, 18.43s/it] 49%|████▊     | 952/1953 [4:57:50<5:12:14, 18.72s/it] 49%|████▉     | 953/1953 [4:58:10<5:18:13, 19.09s/it] 49%|████▉     | 954/1953 [4:58:29<5:15:45, 18.96s/it] 49%|████▉     | 955/1953 [4:58:48<5:13:01, 18.82s/it] 49%|████▉     | 956/1953 [4:59:07<5:13:43, 18.88s/it] 49%|████▉     | 957/1953 [4:59:25<5:10:40, 18.72s/it] 49%|████▉     | 958/1953 [4:59:45<5:14:40, 18.98s/it] 49%|████▉     | 959/1953 [5:00:03<5:10:51, 18.76s/it] 49%|████▉     | 960/1953 [5:00:22<5:12:02, 18.85s/it]                                                      {'loss': 1.0121, 'learning_rate': 2.5663468929924416e-05, 'epoch': 0.49}
 49%|████▉     | 960/1953 [5:00:22<5:12:02, 18.85s/it] 49%|████▉     | 961/1953 [5:00:40<5:08:41, 18.67s/it] 49%|████▉     | 962/1953 [5:00:59<5:07:12, 18.60s/it] 49%|████▉     | 963/1953 [5:01:17<5:07:10, 18.62s/it] 49%|████▉     | 964/1953 [5:01:36<5:05:56, 18.56s/it] 49%|████▉     | 965/1953 [5:01:54<5:05:57, 18.58s/it] 49%|████▉     | 966/1953 [5:02:13<5:04:35, 18.52s/it] 50%|████▉     | 967/1953 [5:02:34<5:16:05, 19.23s/it] 50%|████▉     | 968/1953 [5:02:51<5:09:27, 18.85s/it] 50%|████▉     | 969/1953 [5:03:09<5:05:03, 18.60s/it] 50%|████▉     | 970/1953 [5:03:28<5:04:34, 18.59s/it]                                                      {'loss': 1.0005, 'learning_rate': 2.526139247532518e-05, 'epoch': 0.5}
 50%|████▉     | 970/1953 [5:03:28<5:04:34, 18.59s/it] 50%|████▉     | 971/1953 [5:03:46<5:02:50, 18.50s/it] 50%|████▉     | 972/1953 [5:04:04<4:59:46, 18.33s/it] 50%|████▉     | 973/1953 [5:04:23<5:00:48, 18.42s/it] 50%|████▉     | 974/1953 [5:04:41<4:56:30, 18.17s/it] 50%|████▉     | 975/1953 [5:04:58<4:54:52, 18.09s/it] 50%|████▉     | 976/1953 [5:05:17<4:55:33, 18.15s/it] 50%|█████     | 977/1953 [5:05:34<4:53:14, 18.03s/it] 50%|█████     | 978/1953 [5:05:53<4:56:28, 18.24s/it] 50%|█████     | 979/1953 [5:06:16<5:16:19, 19.49s/it] 50%|█████     | 980/1953 [5:06:35<5:14:01, 19.36s/it]                                                      {'loss': 1.0035, 'learning_rate': 2.485924838456086e-05, 'epoch': 0.5}
 50%|█████     | 980/1953 [5:06:35<5:14:01, 19.36s/it] 50%|█████     | 981/1953 [5:06:53<5:09:02, 19.08s/it] 50%|█████     | 982/1953 [5:07:11<5:05:21, 18.87s/it] 50%|█████     | 983/1953 [5:07:30<5:03:03, 18.75s/it] 50%|█████     | 984/1953 [5:07:49<5:03:27, 18.79s/it] 50%|█████     | 985/1953 [5:08:07<5:00:28, 18.62s/it] 50%|█████     | 986/1953 [5:08:26<5:00:01, 18.62s/it] 51%|█████     | 987/1953 [5:08:44<4:57:36, 18.48s/it] 51%|█████     | 988/1953 [5:09:01<4:53:23, 18.24s/it] 51%|█████     | 989/1953 [5:09:19<4:51:36, 18.15s/it] 51%|█████     | 990/1953 [5:09:41<5:07:20, 19.15s/it]                                                      {'loss': 1.0025, 'learning_rate': 2.4457140713741237e-05, 'epoch': 0.51}
 51%|█████     | 990/1953 [5:09:41<5:07:20, 19.15s/it] 51%|█████     | 991/1953 [5:09:59<5:01:47, 18.82s/it] 51%|█████     | 992/1953 [5:10:18<5:01:11, 18.80s/it] 51%|█████     | 993/1953 [5:10:37<5:04:13, 19.01s/it] 51%|█████     | 994/1953 [5:10:55<4:59:35, 18.74s/it] 51%|█████     | 995/1953 [5:11:14<4:57:12, 18.61s/it] 51%|█████     | 996/1953 [5:11:32<4:53:33, 18.40s/it] 51%|█████     | 997/1953 [5:11:51<4:57:27, 18.67s/it] 51%|█████     | 998/1953 [5:12:10<4:57:58, 18.72s/it] 51%|█████     | 999/1953 [5:12:28<4:56:40, 18.66s/it] 51%|█████     | 1000/1953 [5:12:46<4:52:54, 18.44s/it]                                                       {'loss': 0.9699, 'learning_rate': 2.405517350955232e-05, 'epoch': 0.51}
 51%|█████     | 1000/1953 [5:12:46<4:52:54, 18.44s/it] 51%|█████▏    | 1001/1953 [5:13:05<4:53:35, 18.50s/it] 51%|█████▏    | 1002/1953 [5:13:22<4:49:25, 18.26s/it] 51%|█████▏    | 1003/1953 [5:13:41<4:48:58, 18.25s/it] 51%|█████▏    | 1004/1953 [5:13:58<4:46:05, 18.09s/it] 51%|█████▏    | 1005/1953 [5:14:16<4:44:59, 18.04s/it] 52%|█████▏    | 1006/1953 [5:14:36<4:50:10, 18.38s/it] 52%|█████▏    | 1007/1953 [5:14:54<4:48:42, 18.31s/it] 52%|█████▏    | 1008/1953 [5:15:12<4:47:53, 18.28s/it] 52%|█████▏    | 1009/1953 [5:15:31<4:52:20, 18.58s/it] 52%|█████▏    | 1010/1953 [5:15:49<4:49:04, 18.39s/it]                                                       {'loss': 1.0095, 'learning_rate': 2.365345078233389e-05, 'epoch': 0.52}
 52%|█████▏    | 1010/1953 [5:15:49<4:49:04, 18.39s/it] 52%|█████▏    | 1011/1953 [5:16:07<4:47:06, 18.29s/it] 52%|█████▏    | 1012/1953 [5:16:26<4:49:46, 18.48s/it] 52%|█████▏    | 1013/1953 [5:16:45<4:50:05, 18.52s/it] 52%|█████▏    | 1014/1953 [5:17:03<4:47:40, 18.38s/it] 52%|█████▏    | 1015/1953 [5:17:22<4:50:33, 18.59s/it] 52%|█████▏    | 1016/1953 [5:17:40<4:50:24, 18.60s/it] 52%|█████▏    | 1017/1953 [5:17:59<4:49:23, 18.55s/it] 52%|█████▏    | 1018/1953 [5:18:17<4:48:39, 18.52s/it] 52%|█████▏    | 1019/1953 [5:18:35<4:46:36, 18.41s/it] 52%|█████▏    | 1020/1953 [5:18:54<4:45:48, 18.38s/it]                                                       {'loss': 0.9873, 'learning_rate': 2.3252076479166536e-05, 'epoch': 0.52}
 52%|█████▏    | 1020/1953 [5:18:54<4:45:48, 18.38s/it] 52%|█████▏    | 1021/1953 [5:19:12<4:43:39, 18.26s/it] 52%|█████▏    | 1022/1953 [5:19:29<4:40:32, 18.08s/it] 52%|█████▏    | 1023/1953 [5:19:49<4:45:18, 18.41s/it] 52%|█████▏    | 1024/1953 [5:20:08<4:47:37, 18.58s/it] 52%|█████▏    | 1025/1953 [5:20:27<4:49:18, 18.70s/it] 53%|█████▎    | 1026/1953 [5:20:45<4:46:24, 18.54s/it] 53%|█████▎    | 1027/1953 [5:21:03<4:45:39, 18.51s/it] 53%|█████▎    | 1028/1953 [5:21:23<4:49:22, 18.77s/it] 53%|█████▎    | 1029/1953 [5:21:41<4:45:22, 18.53s/it] 53%|█████▎    | 1030/1953 [5:21:59<4:44:45, 18.51s/it]                                                       {'loss': 0.9662, 'learning_rate': 2.285115445697495e-05, 'epoch': 0.53}
 53%|█████▎    | 1030/1953 [5:21:59<4:44:45, 18.51s/it] 53%|█████▎    | 1031/1953 [5:22:17<4:42:42, 18.40s/it] 53%|█████▎    | 1032/1953 [5:22:35<4:41:40, 18.35s/it] 53%|█████▎    | 1033/1953 [5:22:54<4:44:21, 18.55s/it] 53%|█████▎    | 1034/1953 [5:23:13<4:42:22, 18.44s/it] 53%|█████▎    | 1035/1953 [5:23:31<4:41:30, 18.40s/it] 53%|█████▎    | 1036/1953 [5:23:49<4:41:46, 18.44s/it] 53%|█████▎    | 1037/1953 [5:24:07<4:39:50, 18.33s/it] 53%|█████▎    | 1038/1953 [5:24:27<4:43:57, 18.62s/it] 53%|█████▎    | 1039/1953 [5:24:45<4:39:43, 18.36s/it] 53%|█████▎    | 1040/1953 [5:25:03<4:40:43, 18.45s/it]                                                       {'loss': 0.9754, 'learning_rate': 2.2450788455654627e-05, 'epoch': 0.53}
 53%|█████▎    | 1040/1953 [5:25:03<4:40:43, 18.45s/it] 53%|█████▎    | 1041/1953 [5:25:21<4:38:18, 18.31s/it] 53%|█████▎    | 1042/1953 [5:25:39<4:37:01, 18.25s/it] 53%|█████▎    | 1043/1953 [5:25:58<4:41:04, 18.53s/it] 53%|█████▎    | 1044/1953 [5:26:17<4:41:04, 18.55s/it] 54%|█████▎    | 1045/1953 [5:26:35<4:37:46, 18.35s/it] 54%|█████▎    | 1046/1953 [5:26:55<4:45:11, 18.87s/it] 54%|█████▎    | 1047/1953 [5:27:15<4:49:47, 19.19s/it] 54%|█████▎    | 1048/1953 [5:27:34<4:49:45, 19.21s/it] 54%|█████▎    | 1049/1953 [5:27:52<4:43:26, 18.81s/it] 54%|█████▍    | 1050/1953 [5:28:11<4:42:27, 18.77s/it]                                                       {'loss': 0.9894, 'learning_rate': 2.2051082071228854e-05, 'epoch': 0.54}
 54%|█████▍    | 1050/1953 [5:28:11<4:42:27, 18.77s/it] 54%|█████▍    | 1051/1953 [5:28:29<4:40:18, 18.65s/it] 54%|█████▍    | 1052/1953 [5:28:48<4:40:20, 18.67s/it] 54%|█████▍    | 1053/1953 [5:29:06<4:38:33, 18.57s/it] 54%|█████▍    | 1054/1953 [5:29:24<4:34:08, 18.30s/it] 54%|█████▍    | 1055/1953 [5:29:42<4:34:21, 18.33s/it] 54%|█████▍    | 1056/1953 [5:30:00<4:30:34, 18.10s/it] 54%|█████▍    | 1057/1953 [5:30:18<4:29:58, 18.08s/it] 54%|█████▍    | 1058/1953 [5:30:37<4:32:19, 18.26s/it] 54%|█████▍    | 1059/1953 [5:30:56<4:35:35, 18.50s/it] 54%|█████▍    | 1060/1953 [5:31:14<4:33:27, 18.37s/it]                                                       {'loss': 0.9716, 'learning_rate': 2.1652138729042846e-05, 'epoch': 0.54}
 54%|█████▍    | 1060/1953 [5:31:14<4:33:27, 18.37s/it] 54%|█████▍    | 1061/1953 [5:31:32<4:32:09, 18.31s/it] 54%|█████▍    | 1062/1953 [5:31:51<4:34:24, 18.48s/it] 54%|█████▍    | 1063/1953 [5:32:09<4:33:17, 18.42s/it] 54%|█████▍    | 1064/1953 [5:32:27<4:31:19, 18.31s/it] 55%|█████▍    | 1065/1953 [5:32:45<4:27:59, 18.11s/it] 55%|█████▍    | 1066/1953 [5:33:03<4:28:57, 18.19s/it] 55%|█████▍    | 1067/1953 [5:33:21<4:26:37, 18.06s/it] 55%|█████▍    | 1068/1953 [5:33:39<4:28:43, 18.22s/it] 55%|█████▍    | 1069/1953 [5:33:57<4:26:59, 18.12s/it] 55%|█████▍    | 1070/1953 [5:34:15<4:25:59, 18.07s/it]                                                       {'loss': 0.9852, 'learning_rate': 2.125406165700214e-05, 'epoch': 0.55}
 55%|█████▍    | 1070/1953 [5:34:15<4:25:59, 18.07s/it] 55%|█████▍    | 1071/1953 [5:34:33<4:23:40, 17.94s/it] 55%|█████▍    | 1072/1953 [5:34:52<4:27:05, 18.19s/it] 55%|█████▍    | 1073/1953 [5:35:10<4:29:17, 18.36s/it] 55%|█████▍    | 1074/1953 [5:35:28<4:25:41, 18.14s/it] 55%|█████▌    | 1075/1953 [5:35:46<4:25:52, 18.17s/it] 55%|█████▌    | 1076/1953 [5:36:04<4:24:58, 18.13s/it] 55%|█████▌    | 1077/1953 [5:36:23<4:28:21, 18.38s/it] 55%|█████▌    | 1078/1953 [5:36:42<4:28:13, 18.39s/it] 55%|█████▌    | 1079/1953 [5:37:00<4:25:28, 18.23s/it] 55%|█████▌    | 1080/1953 [5:37:17<4:23:32, 18.11s/it]                                                       {'loss': 0.9654, 'learning_rate': 2.0856953858861995e-05, 'epoch': 0.55}
 55%|█████▌    | 1080/1953 [5:37:17<4:23:32, 18.11s/it] 55%|█████▌    | 1081/1953 [5:37:35<4:21:45, 18.01s/it] 55%|█████▌    | 1082/1953 [5:37:54<4:24:42, 18.23s/it] 55%|█████▌    | 1083/1953 [5:38:14<4:31:43, 18.74s/it] 56%|█████▌    | 1084/1953 [5:38:31<4:26:24, 18.39s/it] 56%|█████▌    | 1085/1953 [5:38:50<4:27:24, 18.48s/it] 56%|█████▌    | 1086/1953 [5:39:09<4:26:54, 18.47s/it] 56%|█████▌    | 1087/1953 [5:39:27<4:25:41, 18.41s/it] 56%|█████▌    | 1088/1953 [5:39:46<4:27:12, 18.53s/it] 56%|█████▌    | 1089/1953 [5:40:04<4:26:37, 18.52s/it] 56%|█████▌    | 1090/1953 [5:40:23<4:27:34, 18.60s/it]                                                       {'loss': 0.9695, 'learning_rate': 2.0460918087574877e-05, 'epoch': 0.56}
 56%|█████▌    | 1090/1953 [5:40:23<4:27:34, 18.60s/it] 56%|█████▌    | 1091/1953 [5:40:42<4:29:16, 18.74s/it] 56%|█████▌    | 1092/1953 [5:41:00<4:25:33, 18.51s/it] 56%|█████▌    | 1093/1953 [5:41:17<4:21:00, 18.21s/it] 56%|█████▌    | 1094/1953 [5:41:36<4:22:33, 18.34s/it] 56%|█████▌    | 1095/1953 [5:41:54<4:20:14, 18.20s/it] 56%|█████▌    | 1096/1953 [5:42:12<4:21:18, 18.29s/it] 56%|█████▌    | 1097/1953 [5:42:30<4:19:24, 18.18s/it] 56%|█████▌    | 1098/1953 [5:42:49<4:20:38, 18.29s/it] 56%|█████▋    | 1099/1953 [5:43:07<4:17:13, 18.07s/it] 56%|█████▋    | 1100/1953 [5:43:25<4:19:37, 18.26s/it]                                                       {'loss': 0.9708, 'learning_rate': 2.0066056818702745e-05, 'epoch': 0.56}
 56%|█████▋    | 1100/1953 [5:43:25<4:19:37, 18.26s/it] 56%|█████▋    | 1101/1953 [5:43:45<4:24:41, 18.64s/it] 56%|█████▋    | 1102/1953 [5:44:03<4:22:43, 18.52s/it] 56%|█████▋    | 1103/1953 [5:44:24<4:32:34, 19.24s/it] 57%|█████▋    | 1104/1953 [5:44:42<4:29:01, 19.01s/it] 57%|█████▋    | 1105/1953 [5:45:05<4:45:21, 20.19s/it] 57%|█████▋    | 1106/1953 [5:45:23<4:35:51, 19.54s/it] 57%|█████▋    | 1107/1953 [5:45:43<4:38:06, 19.72s/it] 57%|█████▋    | 1108/1953 [5:46:07<4:55:12, 20.96s/it] 57%|█████▋    | 1109/1953 [5:46:26<4:43:39, 20.17s/it] 57%|█████▋    | 1110/1953 [5:46:44<4:37:43, 19.77s/it]                                                       {'loss': 0.9749, 'learning_rate': 1.9672472223901198e-05, 'epoch': 0.57}
 57%|█████▋    | 1110/1953 [5:46:44<4:37:43, 19.77s/it] 57%|█████▋    | 1111/1953 [5:47:03<4:32:55, 19.45s/it] 57%|█████▋    | 1112/1953 [5:47:21<4:25:22, 18.93s/it] 57%|█████▋    | 1113/1953 [5:47:39<4:21:23, 18.67s/it] 57%|█████▋    | 1114/1953 [5:47:58<4:21:24, 18.69s/it] 57%|█████▋    | 1115/1953 [5:48:17<4:23:50, 18.89s/it] 57%|█████▋    | 1116/1953 [5:48:35<4:20:52, 18.70s/it] 57%|█████▋    | 1117/1953 [5:48:53<4:15:52, 18.36s/it] 57%|█████▋    | 1118/1953 [5:49:10<4:12:06, 18.12s/it] 57%|█████▋    | 1119/1953 [5:49:28<4:10:44, 18.04s/it] 57%|█████▋    | 1120/1953 [5:49:46<4:10:59, 18.08s/it]                                                       {'loss': 0.9621, 'learning_rate': 1.928026614448221e-05, 'epoch': 0.57}
 57%|█████▋    | 1120/1953 [5:49:46<4:10:59, 18.08s/it] 57%|█████▋    | 1121/1953 [5:50:05<4:13:59, 18.32s/it] 57%|█████▋    | 1122/1953 [5:50:24<4:13:54, 18.33s/it] 58%|█████▊    | 1123/1953 [5:50:41<4:11:14, 18.16s/it] 58%|█████▊    | 1124/1953 [5:51:01<4:15:05, 18.46s/it] 58%|█████▊    | 1125/1953 [5:51:19<4:14:10, 18.42s/it] 58%|█████▊    | 1126/1953 [5:51:38<4:15:54, 18.57s/it] 58%|█████▊    | 1127/1953 [5:51:56<4:12:16, 18.33s/it] 58%|█████▊    | 1128/1953 [5:52:14<4:13:18, 18.42s/it] 58%|█████▊    | 1129/1953 [5:52:32<4:09:37, 18.18s/it] 58%|█████▊    | 1130/1953 [5:52:51<4:12:36, 18.42s/it]                                                       {'loss': 0.9609, 'learning_rate': 1.8889540065062338e-05, 'epoch': 0.58}
 58%|█████▊    | 1130/1953 [5:52:51<4:12:36, 18.42s/it] 58%|█████▊    | 1131/1953 [5:53:10<4:14:51, 18.60s/it] 58%|█████▊    | 1132/1953 [5:53:28<4:12:21, 18.44s/it] 58%|█████▊    | 1133/1953 [5:53:46<4:09:41, 18.27s/it] 58%|█████▊    | 1134/1953 [5:54:04<4:10:51, 18.38s/it] 58%|█████▊    | 1135/1953 [5:54:23<4:09:59, 18.34s/it] 58%|█████▊    | 1136/1953 [5:54:42<4:11:37, 18.48s/it] 58%|█████▊    | 1137/1953 [5:55:00<4:10:52, 18.45s/it] 58%|█████▊    | 1138/1953 [5:55:18<4:11:05, 18.48s/it] 58%|█████▊    | 1139/1953 [5:55:37<4:09:38, 18.40s/it] 58%|█████▊    | 1140/1953 [5:55:55<4:09:48, 18.44s/it]                                                       {'loss': 0.9687, 'learning_rate': 1.850039508730328e-05, 'epoch': 0.58}
 58%|█████▊    | 1140/1953 [5:55:55<4:09:48, 18.44s/it] 58%|█████▊    | 1141/1953 [5:56:13<4:08:25, 18.36s/it] 58%|█████▊    | 1142/1953 [5:56:31<4:05:47, 18.18s/it] 59%|█████▊    | 1143/1953 [5:56:50<4:07:32, 18.34s/it] 59%|█████▊    | 1144/1953 [5:57:10<4:12:41, 18.74s/it] 59%|█████▊    | 1145/1953 [5:57:28<4:12:33, 18.75s/it] 59%|█████▊    | 1146/1953 [5:57:47<4:11:43, 18.72s/it] 59%|█████▊    | 1147/1953 [5:58:05<4:10:48, 18.67s/it] 59%|█████▉    | 1148/1953 [5:58:24<4:07:53, 18.48s/it] 59%|█████▉    | 1149/1953 [5:58:42<4:08:12, 18.52s/it] 59%|█████▉    | 1150/1953 [5:59:01<4:08:53, 18.60s/it]                                                       {'loss': 0.9672, 'learning_rate': 1.811293190375144e-05, 'epoch': 0.59}
 59%|█████▉    | 1150/1953 [5:59:01<4:08:53, 18.60s/it] 59%|█████▉    | 1151/1953 [5:59:20<4:11:27, 18.81s/it] 59%|█████▉    | 1152/1953 [5:59:38<4:07:30, 18.54s/it] 59%|█████▉    | 1153/1953 [6:00:00<4:19:31, 19.46s/it] 59%|█████▉    | 1154/1953 [6:00:19<4:16:25, 19.26s/it] 59%|█████▉    | 1155/1953 [6:00:38<4:17:32, 19.36s/it] 59%|█████▉    | 1156/1953 [6:00:57<4:13:24, 19.08s/it] 59%|█████▉    | 1157/1953 [6:01:15<4:09:39, 18.82s/it] 59%|█████▉    | 1158/1953 [6:01:33<4:06:00, 18.57s/it] 59%|█████▉    | 1159/1953 [6:01:50<4:02:11, 18.30s/it] 59%|█████▉    | 1160/1953 [6:02:09<4:02:29, 18.35s/it]                                                       {'loss': 0.9657, 'learning_rate': 1.772725077178346e-05, 'epoch': 0.59}
 59%|█████▉    | 1160/1953 [6:02:09<4:02:29, 18.35s/it] 59%|█████▉    | 1161/1953 [6:02:27<4:00:39, 18.23s/it] 59%|█████▉    | 1162/1953 [6:02:49<4:17:46, 19.55s/it] 60%|█████▉    | 1163/1953 [6:03:09<4:17:36, 19.57s/it] 60%|█████▉    | 1164/1953 [6:03:28<4:13:36, 19.29s/it] 60%|█████▉    | 1165/1953 [6:03:52<4:31:58, 20.71s/it] 60%|█████▉    | 1166/1953 [6:04:09<4:19:35, 19.79s/it] 60%|█████▉    | 1167/1953 [6:04:28<4:13:06, 19.32s/it] 60%|█████▉    | 1168/1953 [6:04:46<4:10:17, 19.13s/it] 60%|█████▉    | 1169/1953 [6:05:04<4:05:12, 18.77s/it] 60%|█████▉    | 1170/1953 [6:05:23<4:03:08, 18.63s/it]                                                       {'loss': 0.9344, 'learning_rate': 1.7343451487664214e-05, 'epoch': 0.6}
 60%|█████▉    | 1170/1953 [6:05:23<4:03:08, 18.63s/it] 60%|█████▉    | 1171/1953 [6:05:41<4:02:11, 18.58s/it] 60%|██████    | 1172/1953 [6:06:00<4:03:45, 18.73s/it] 60%|██████    | 1173/1953 [6:06:18<4:02:00, 18.62s/it] 60%|██████    | 1174/1953 [6:06:36<3:59:22, 18.44s/it] 60%|██████    | 1175/1953 [6:06:55<3:59:31, 18.47s/it] 60%|██████    | 1176/1953 [6:07:13<3:58:24, 18.41s/it] 60%|██████    | 1177/1953 [6:07:31<3:56:24, 18.28s/it] 60%|██████    | 1178/1953 [6:07:50<3:59:41, 18.56s/it] 60%|██████    | 1179/1953 [6:08:14<4:17:42, 19.98s/it] 60%|██████    | 1180/1953 [6:08:35<4:23:24, 20.45s/it]                                                       {'loss': 0.965, 'learning_rate': 1.6961633360724262e-05, 'epoch': 0.6}
 60%|██████    | 1180/1953 [6:08:35<4:23:24, 20.45s/it] 60%|██████    | 1181/1953 [6:08:54<4:17:29, 20.01s/it] 61%|██████    | 1182/1953 [6:09:12<4:09:16, 19.40s/it] 61%|██████    | 1183/1953 [6:09:30<4:03:06, 18.94s/it] 61%|██████    | 1184/1953 [6:09:48<3:59:20, 18.67s/it] 61%|██████    | 1185/1953 [6:10:06<3:56:00, 18.44s/it] 61%|██████    | 1186/1953 [6:10:24<3:54:45, 18.36s/it] 61%|██████    | 1187/1953 [6:10:43<3:54:23, 18.36s/it] 61%|██████    | 1188/1953 [6:11:02<3:56:16, 18.53s/it] 61%|██████    | 1189/1953 [6:11:21<4:01:03, 18.93s/it] 61%|██████    | 1190/1953 [6:11:46<4:22:12, 20.62s/it]                                                       {'loss': 0.9655, 'learning_rate': 1.658189518766322e-05, 'epoch': 0.61}
 61%|██████    | 1190/1953 [6:11:46<4:22:12, 20.62s/it] 61%|██████    | 1191/1953 [6:12:11<4:39:04, 21.97s/it] 61%|██████    | 1192/1953 [6:12:29<4:24:07, 20.82s/it] 61%|██████    | 1193/1953 [6:12:47<4:11:02, 19.82s/it] 61%|██████    | 1194/1953 [6:13:05<4:06:48, 19.51s/it] 61%|██████    | 1195/1953 [6:13:24<4:01:42, 19.13s/it] 61%|██████    | 1196/1953 [6:13:43<4:01:11, 19.12s/it] 61%|██████▏   | 1197/1953 [6:14:01<3:57:12, 18.83s/it] 61%|██████▏   | 1198/1953 [6:14:21<4:00:08, 19.08s/it] 61%|██████▏   | 1199/1953 [6:14:39<3:56:19, 18.81s/it] 61%|██████▏   | 1200/1953 [6:14:57<3:52:11, 18.50s/it]                                                       {'loss': 0.9602, 'learning_rate': 1.620433522698575e-05, 'epoch': 0.61}
 61%|██████▏   | 1200/1953 [6:14:57<3:52:11, 18.50s/it] 61%|██████▏   | 1201/1953 [6:15:15<3:50:34, 18.40s/it] 62%|██████▏   | 1202/1953 [6:15:33<3:51:23, 18.49s/it] 62%|██████▏   | 1203/1953 [6:15:53<3:54:21, 18.75s/it] 62%|██████▏   | 1204/1953 [6:16:11<3:53:12, 18.68s/it] 62%|██████▏   | 1205/1953 [6:16:29<3:50:26, 18.48s/it] 62%|██████▏   | 1206/1953 [6:16:48<3:51:04, 18.56s/it] 62%|██████▏   | 1207/1953 [6:17:06<3:48:25, 18.37s/it] 62%|██████▏   | 1208/1953 [6:17:24<3:47:54, 18.36s/it] 62%|██████▏   | 1209/1953 [6:17:43<3:47:24, 18.34s/it] 62%|██████▏   | 1210/1953 [6:18:01<3:47:45, 18.39s/it]                                                       {'loss': 0.9586, 'learning_rate': 1.5829051173576905e-05, 'epoch': 0.62}
 62%|██████▏   | 1210/1953 [6:18:01<3:47:45, 18.39s/it] 62%|██████▏   | 1211/1953 [6:18:21<3:52:13, 18.78s/it] 62%|██████▏   | 1212/1953 [6:18:39<3:51:10, 18.72s/it] 62%|██████▏   | 1213/1953 [6:18:59<3:52:35, 18.86s/it] 62%|██████▏   | 1214/1953 [6:19:17<3:50:18, 18.70s/it] 62%|██████▏   | 1215/1953 [6:19:36<3:50:17, 18.72s/it] 62%|██████▏   | 1216/1953 [6:19:54<3:49:14, 18.66s/it] 62%|██████▏   | 1217/1953 [6:20:14<3:52:19, 18.94s/it] 62%|██████▏   | 1218/1953 [6:20:33<3:52:16, 18.96s/it] 62%|██████▏   | 1219/1953 [6:20:50<3:46:49, 18.54s/it] 62%|██████▏   | 1220/1953 [6:21:09<3:46:29, 18.54s/it]                                                       {'loss': 0.9442, 'learning_rate': 1.545614013342321e-05, 'epoch': 0.62}
 62%|██████▏   | 1220/1953 [6:21:09<3:46:29, 18.54s/it] 63%|██████▎   | 1221/1953 [6:21:27<3:45:40, 18.50s/it] 63%|██████▎   | 1222/1953 [6:21:46<3:44:24, 18.42s/it] 63%|██████▎   | 1223/1953 [6:22:03<3:41:30, 18.21s/it] 63%|██████▎   | 1224/1953 [6:22:21<3:40:32, 18.15s/it] 63%|██████▎   | 1225/1953 [6:22:43<3:54:17, 19.31s/it] 63%|██████▎   | 1226/1953 [6:23:01<3:49:17, 18.92s/it] 63%|██████▎   | 1227/1953 [6:23:20<3:47:57, 18.84s/it] 63%|██████▎   | 1228/1953 [6:23:38<3:43:31, 18.50s/it] 63%|██████▎   | 1229/1953 [6:23:56<3:43:45, 18.54s/it] 63%|██████▎   | 1230/1953 [6:24:14<3:42:02, 18.43s/it]                                                       {'loss': 0.9521, 'learning_rate': 1.5085698598486175e-05, 'epoch': 0.63}
 63%|██████▎   | 1230/1953 [6:24:14<3:42:02, 18.43s/it] 63%|██████▎   | 1231/1953 [6:24:32<3:39:38, 18.25s/it] 63%|██████▎   | 1232/1953 [6:24:50<3:37:17, 18.08s/it] 63%|██████▎   | 1233/1953 [6:25:08<3:36:40, 18.06s/it] 63%|██████▎   | 1234/1953 [6:25:26<3:35:26, 17.98s/it] 63%|██████▎   | 1235/1953 [6:25:44<3:36:21, 18.08s/it] 63%|██████▎   | 1236/1953 [6:26:02<3:36:12, 18.09s/it] 63%|██████▎   | 1237/1953 [6:26:20<3:35:46, 18.08s/it] 63%|██████▎   | 1238/1953 [6:26:38<3:35:45, 18.11s/it] 63%|██████▎   | 1239/1953 [6:26:56<3:34:15, 18.00s/it] 63%|██████▎   | 1240/1953 [6:27:14<3:33:12, 17.94s/it]                                                       {'loss': 0.939, 'learning_rate': 1.4717822421734718e-05, 'epoch': 0.63}
 63%|██████▎   | 1240/1953 [6:27:14<3:33:12, 17.94s/it] 64%|██████▎   | 1241/1953 [6:27:32<3:34:46, 18.10s/it] 64%|██████▎   | 1242/1953 [6:27:51<3:36:10, 18.24s/it] 64%|██████▎   | 1243/1953 [6:28:09<3:34:53, 18.16s/it] 64%|██████▎   | 1244/1953 [6:28:28<3:36:05, 18.29s/it] 64%|██████▎   | 1245/1953 [6:28:47<3:39:02, 18.56s/it] 64%|██████▍   | 1246/1953 [6:29:05<3:36:26, 18.37s/it] 64%|██████▍   | 1247/1953 [6:29:23<3:36:05, 18.36s/it] 64%|██████▍   | 1248/1953 [6:29:41<3:34:36, 18.26s/it] 64%|██████▍   | 1249/1953 [6:29:59<3:33:23, 18.19s/it] 64%|██████▍   | 1250/1953 [6:30:17<3:33:23, 18.21s/it]                                                       {'loss': 0.9376, 'learning_rate': 1.4352606792342829e-05, 'epoch': 0.64}
 64%|██████▍   | 1250/1953 [6:30:17<3:33:23, 18.21s/it] 64%|██████▍   | 1251/1953 [6:30:36<3:33:01, 18.21s/it] 64%|██████▍   | 1252/1953 [6:30:54<3:34:07, 18.33s/it] 64%|██████▍   | 1253/1953 [6:31:13<3:36:35, 18.56s/it] 64%|██████▍   | 1254/1953 [6:31:32<3:35:34, 18.50s/it] 64%|██████▍   | 1255/1953 [6:31:53<3:44:42, 19.32s/it] 64%|██████▍   | 1256/1953 [6:32:11<3:39:49, 18.92s/it] 64%|██████▍   | 1257/1953 [6:32:29<3:37:33, 18.75s/it] 64%|██████▍   | 1258/1953 [6:32:48<3:35:33, 18.61s/it] 64%|██████▍   | 1259/1953 [6:33:06<3:36:06, 18.68s/it] 65%|██████▍   | 1260/1953 [6:33:26<3:38:52, 18.95s/it]                                                       {'loss': 0.9624, 'learning_rate': 1.399014621105914e-05, 'epoch': 0.65}
 65%|██████▍   | 1260/1953 [6:33:26<3:38:52, 18.95s/it] 65%|██████▍   | 1261/1953 [6:33:45<3:39:36, 19.04s/it] 65%|██████▍   | 1262/1953 [6:34:04<3:38:09, 18.94s/it] 65%|██████▍   | 1263/1953 [6:34:23<3:37:29, 18.91s/it] 65%|██████▍   | 1264/1953 [6:34:41<3:34:49, 18.71s/it] 65%|██████▍   | 1265/1953 [6:35:00<3:35:10, 18.77s/it] 65%|██████▍   | 1266/1953 [6:35:18<3:30:56, 18.42s/it] 65%|██████▍   | 1267/1953 [6:35:35<3:28:22, 18.23s/it] 65%|██████▍   | 1268/1953 [6:35:56<3:37:10, 19.02s/it] 65%|██████▍   | 1269/1953 [6:36:15<3:35:39, 18.92s/it] 65%|██████▌   | 1270/1953 [6:36:33<3:33:56, 18.79s/it]                                                       {'loss': 0.9431, 'learning_rate': 1.3630534465754463e-05, 'epoch': 0.65}
 65%|██████▌   | 1270/1953 [6:36:33<3:33:56, 18.79s/it] 65%|██████▌   | 1271/1953 [6:36:52<3:31:28, 18.61s/it] 65%|██████▌   | 1272/1953 [6:37:10<3:31:24, 18.63s/it] 65%|██████▌   | 1273/1953 [6:37:29<3:30:35, 18.58s/it] 65%|██████▌   | 1274/1953 [6:37:48<3:31:06, 18.65s/it] 65%|██████▌   | 1275/1953 [6:38:06<3:29:26, 18.53s/it] 65%|██████▌   | 1276/1953 [6:38:24<3:28:44, 18.50s/it] 65%|██████▌   | 1277/1953 [6:38:43<3:29:18, 18.58s/it] 65%|██████▌   | 1278/1953 [6:39:02<3:31:22, 18.79s/it] 65%|██████▌   | 1279/1953 [6:39:20<3:28:46, 18.59s/it] 66%|██████▌   | 1280/1953 [6:39:39<3:28:03, 18.55s/it]                                                       {'loss': 0.9362, 'learning_rate': 1.3273864607153916e-05, 'epoch': 0.66}
 66%|██████▌   | 1280/1953 [6:39:39<3:28:03, 18.55s/it] 66%|██████▌   | 1281/1953 [6:39:57<3:26:56, 18.48s/it] 66%|██████▌   | 1282/1953 [6:40:15<3:24:45, 18.31s/it] 66%|██████▌   | 1283/1953 [6:40:34<3:25:42, 18.42s/it] 66%|██████▌   | 1284/1953 [6:40:51<3:22:46, 18.19s/it] 66%|██████▌   | 1285/1953 [6:41:10<3:24:06, 18.33s/it] 66%|██████▌   | 1286/1953 [6:41:29<3:25:55, 18.52s/it] 66%|██████▌   | 1287/1953 [6:41:48<3:27:29, 18.69s/it] 66%|██████▌   | 1288/1953 [6:42:06<3:24:49, 18.48s/it] 66%|██████▌   | 1289/1953 [6:42:25<3:25:01, 18.53s/it] 66%|██████▌   | 1290/1953 [6:42:43<3:23:35, 18.42s/it]                                                       {'loss': 0.9469, 'learning_rate': 1.2920228924759728e-05, 'epoch': 0.66}
 66%|██████▌   | 1290/1953 [6:42:43<3:23:35, 18.42s/it] 66%|██████▌   | 1291/1953 [6:43:01<3:23:20, 18.43s/it] 66%|██████▌   | 1292/1953 [6:43:20<3:22:43, 18.40s/it] 66%|██████▌   | 1293/1953 [6:43:39<3:25:32, 18.69s/it] 66%|██████▋   | 1294/1953 [6:43:58<3:25:35, 18.72s/it] 66%|██████▋   | 1295/1953 [6:44:16<3:24:10, 18.62s/it] 66%|██████▋   | 1296/1953 [6:44:37<3:31:13, 19.29s/it] 66%|██████▋   | 1297/1953 [6:44:55<3:24:55, 18.74s/it] 66%|██████▋   | 1298/1953 [6:45:13<3:24:28, 18.73s/it] 67%|██████▋   | 1299/1953 [6:45:33<3:28:01, 19.08s/it] 67%|██████▋   | 1300/1953 [6:45:52<3:26:07, 18.94s/it]                                                       {'loss': 0.9396, 'learning_rate': 1.2569718922971018e-05, 'epoch': 0.67}
 67%|██████▋   | 1300/1953 [6:45:52<3:26:07, 18.94s/it] 67%|██████▋   | 1301/1953 [6:46:10<3:24:09, 18.79s/it] 67%|██████▋   | 1302/1953 [6:46:28<3:21:53, 18.61s/it] 67%|██████▋   | 1303/1953 [6:46:47<3:21:36, 18.61s/it] 67%|██████▋   | 1304/1953 [6:47:05<3:20:02, 18.49s/it] 67%|██████▋   | 1305/1953 [6:47:26<3:27:27, 19.21s/it] 67%|██████▋   | 1306/1953 [6:47:45<3:25:28, 19.06s/it] 67%|██████▋   | 1307/1953 [6:48:03<3:24:09, 18.96s/it] 67%|██████▋   | 1308/1953 [6:48:21<3:20:30, 18.65s/it] 67%|██████▋   | 1309/1953 [6:48:40<3:20:17, 18.66s/it] 67%|██████▋   | 1310/1953 [6:48:59<3:19:24, 18.61s/it]                                                       {'loss': 0.9535, 'learning_rate': 1.2222425297406783e-05, 'epoch': 0.67}
 67%|██████▋   | 1310/1953 [6:48:59<3:19:24, 18.61s/it] 67%|██████▋   | 1311/1953 [6:49:17<3:18:28, 18.55s/it] 67%|██████▋   | 1312/1953 [6:49:35<3:16:00, 18.35s/it] 67%|██████▋   | 1313/1953 [6:49:54<3:17:38, 18.53s/it] 67%|██████▋   | 1314/1953 [6:50:12<3:15:19, 18.34s/it] 67%|██████▋   | 1315/1953 [6:50:31<3:17:30, 18.57s/it] 67%|██████▋   | 1316/1953 [6:50:49<3:15:04, 18.37s/it] 67%|██████▋   | 1317/1953 [6:51:08<3:16:44, 18.56s/it] 67%|██████▋   | 1318/1953 [6:51:27<3:18:04, 18.72s/it] 68%|██████▊   | 1319/1953 [6:51:45<3:14:53, 18.44s/it] 68%|██████▊   | 1320/1953 [6:52:03<3:14:06, 18.40s/it]                                                       {'loss': 0.923, 'learning_rate': 1.187843791143799e-05, 'epoch': 0.68}
 68%|██████▊   | 1320/1953 [6:52:03<3:14:06, 18.40s/it] 68%|██████▊   | 1321/1953 [6:52:21<3:12:59, 18.32s/it] 68%|██████▊   | 1322/1953 [6:52:39<3:11:27, 18.21s/it] 68%|██████▊   | 1323/1953 [6:52:57<3:11:46, 18.26s/it] 68%|██████▊   | 1324/1953 [6:53:15<3:10:54, 18.21s/it] 68%|██████▊   | 1325/1953 [6:53:34<3:11:47, 18.32s/it] 68%|██████▊   | 1326/1953 [6:53:53<3:11:47, 18.35s/it] 68%|██████▊   | 1327/1953 [6:54:11<3:11:53, 18.39s/it] 68%|██████▊   | 1328/1953 [6:54:29<3:10:50, 18.32s/it] 68%|██████▊   | 1329/1953 [6:54:48<3:12:17, 18.49s/it] 68%|██████▊   | 1330/1953 [6:55:07<3:13:27, 18.63s/it]                                                       {'loss': 0.9513, 'learning_rate': 1.1537845772935279e-05, 'epoch': 0.68}
 68%|██████▊   | 1330/1953 [6:55:07<3:13:27, 18.63s/it] 68%|██████▊   | 1331/1953 [6:55:25<3:12:08, 18.53s/it] 68%|██████▊   | 1332/1953 [6:55:44<3:11:34, 18.51s/it] 68%|██████▊   | 1333/1953 [6:56:02<3:09:41, 18.36s/it] 68%|██████▊   | 1334/1953 [6:56:20<3:08:21, 18.26s/it] 68%|██████▊   | 1335/1953 [6:56:38<3:08:33, 18.31s/it] 68%|██████▊   | 1336/1953 [6:56:57<3:08:38, 18.35s/it] 68%|██████▊   | 1337/1953 [6:57:15<3:07:44, 18.29s/it] 69%|██████▊   | 1338/1953 [6:57:32<3:05:15, 18.07s/it] 69%|██████▊   | 1339/1953 [6:57:51<3:07:27, 18.32s/it] 69%|██████▊   | 1340/1953 [6:58:09<3:06:33, 18.26s/it]                                                       {'loss': 0.9181, 'learning_rate': 1.1200737011237763e-05, 'epoch': 0.69}
 69%|██████▊   | 1340/1953 [6:58:09<3:06:33, 18.26s/it] 69%|██████▊   | 1341/1953 [6:58:28<3:07:55, 18.42s/it] 69%|██████▊   | 1342/1953 [6:58:47<3:10:07, 18.67s/it] 69%|██████▉   | 1343/1953 [6:59:06<3:08:52, 18.58s/it] 69%|██████▉   | 1344/1953 [6:59:25<3:09:25, 18.66s/it] 69%|██████▉   | 1345/1953 [6:59:43<3:09:29, 18.70s/it] 69%|██████▉   | 1346/1953 [7:00:02<3:07:40, 18.55s/it] 69%|██████▉   | 1347/1953 [7:00:21<3:08:38, 18.68s/it] 69%|██████▉   | 1348/1953 [7:00:39<3:06:14, 18.47s/it] 69%|██████▉   | 1349/1953 [7:00:58<3:07:54, 18.67s/it] 69%|██████▉   | 1350/1953 [7:01:17<3:08:39, 18.77s/it]                                                       {'loss': 0.9507, 'learning_rate': 1.086719885434935e-05, 'epoch': 0.69}
 69%|██████▉   | 1350/1953 [7:01:17<3:08:39, 18.77s/it] 69%|██████▉   | 1351/1953 [7:01:35<3:05:50, 18.52s/it] 69%|██████▉   | 1352/1953 [7:01:52<3:02:50, 18.25s/it] 69%|██████▉   | 1353/1953 [7:02:11<3:03:22, 18.34s/it] 69%|██████▉   | 1354/1953 [7:02:29<3:03:32, 18.38s/it] 69%|██████▉   | 1355/1953 [7:02:48<3:04:07, 18.47s/it] 69%|██████▉   | 1356/1953 [7:03:06<3:01:53, 18.28s/it] 69%|██████▉   | 1357/1953 [7:03:24<3:00:10, 18.14s/it] 70%|██████▉   | 1358/1953 [7:03:43<3:02:59, 18.45s/it] 70%|██████▉   | 1359/1953 [7:04:02<3:03:17, 18.51s/it] 70%|██████▉   | 1360/1953 [7:04:20<3:04:22, 18.65s/it]                                                       {'loss': 0.9257, 'learning_rate': 1.0537317606368164e-05, 'epoch': 0.7}
 70%|██████▉   | 1360/1953 [7:04:20<3:04:22, 18.65s/it] 70%|██████▉   | 1361/1953 [7:04:39<3:04:52, 18.74s/it] 70%|██████▉   | 1362/1953 [7:04:58<3:05:22, 18.82s/it] 70%|██████▉   | 1363/1953 [7:05:17<3:05:07, 18.83s/it] 70%|██████▉   | 1364/1953 [7:05:37<3:06:04, 18.96s/it] 70%|██████▉   | 1365/1953 [7:05:55<3:03:56, 18.77s/it] 70%|██████▉   | 1366/1953 [7:06:13<3:02:40, 18.67s/it] 70%|██████▉   | 1367/1953 [7:06:31<3:00:45, 18.51s/it] 70%|███████   | 1368/1953 [7:06:49<2:58:28, 18.31s/it] 70%|███████   | 1369/1953 [7:07:07<2:56:29, 18.13s/it] 70%|███████   | 1370/1953 [7:07:25<2:56:12, 18.13s/it]                                                       {'loss': 0.9389, 'learning_rate': 1.0211178625155057e-05, 'epoch': 0.7}
 70%|███████   | 1370/1953 [7:07:25<2:56:12, 18.13s/it] 70%|███████   | 1371/1953 [7:07:43<2:55:49, 18.13s/it] 70%|███████   | 1372/1953 [7:08:02<2:57:28, 18.33s/it] 70%|███████   | 1373/1953 [7:08:20<2:57:20, 18.35s/it] 70%|███████   | 1374/1953 [7:08:38<2:53:34, 17.99s/it] 70%|███████   | 1375/1953 [7:08:57<2:56:34, 18.33s/it] 70%|███████   | 1376/1953 [7:09:15<2:57:22, 18.44s/it] 71%|███████   | 1377/1953 [7:09:33<2:54:55, 18.22s/it] 71%|███████   | 1378/1953 [7:09:52<2:55:16, 18.29s/it] 71%|███████   | 1379/1953 [7:10:13<3:02:44, 19.10s/it] 71%|███████   | 1380/1953 [7:10:31<3:00:01, 18.85s/it]                                                       {'loss': 0.9208, 'learning_rate': 9.888866300247077e-06, 'epoch': 0.71}
 71%|███████   | 1380/1953 [7:10:31<3:00:01, 18.85s/it] 71%|███████   | 1381/1953 [7:10:49<2:58:03, 18.68s/it] 71%|███████   | 1382/1953 [7:11:07<2:56:18, 18.53s/it] 71%|███████   | 1383/1953 [7:11:25<2:54:57, 18.42s/it] 71%|███████   | 1384/1953 [7:11:44<2:54:05, 18.36s/it] 71%|███████   | 1385/1953 [7:12:02<2:53:12, 18.30s/it] 71%|███████   | 1386/1953 [7:12:20<2:51:33, 18.15s/it] 71%|███████   | 1387/1953 [7:12:39<2:54:10, 18.46s/it] 71%|███████   | 1388/1953 [7:12:57<2:53:43, 18.45s/it] 71%|███████   | 1389/1953 [7:13:16<2:55:21, 18.65s/it] 71%|███████   | 1390/1953 [7:13:37<2:59:23, 19.12s/it]                                                       {'loss': 0.9342, 'learning_rate': 9.570464031021273e-06, 'epoch': 0.71}
 71%|███████   | 1390/1953 [7:13:37<2:59:23, 19.12s/it] 71%|███████   | 1391/1953 [7:13:55<2:56:20, 18.83s/it] 71%|███████▏  | 1392/1953 [7:14:13<2:55:54, 18.81s/it] 71%|███████▏  | 1393/1953 [7:14:32<2:55:08, 18.76s/it] 71%|███████▏  | 1394/1953 [7:14:51<2:54:50, 18.77s/it] 71%|███████▏  | 1395/1953 [7:15:09<2:53:50, 18.69s/it] 71%|███████▏  | 1396/1953 [7:15:29<2:55:24, 18.89s/it] 72%|███████▏  | 1397/1953 [7:15:48<2:55:51, 18.98s/it] 72%|███████▏  | 1398/1953 [7:16:07<2:55:57, 19.02s/it] 72%|███████▏  | 1399/1953 [7:16:28<3:00:45, 19.58s/it] 72%|███████▏  | 1400/1953 [7:16:47<2:57:33, 19.26s/it]                                                       {'loss': 0.9242, 'learning_rate': 9.256054205114939e-06, 'epoch': 0.72}
 72%|███████▏  | 1400/1953 [7:16:47<2:57:33, 19.26s/it] 72%|███████▏  | 1401/1953 [7:17:05<2:55:17, 19.05s/it] 72%|███████▏  | 1402/1953 [7:17:24<2:53:18, 18.87s/it] 72%|███████▏  | 1403/1953 [7:17:41<2:49:44, 18.52s/it] 72%|███████▏  | 1404/1953 [7:17:59<2:47:07, 18.27s/it] 72%|███████▏  | 1405/1953 [7:18:17<2:46:26, 18.22s/it] 72%|███████▏  | 1406/1953 [7:18:35<2:46:08, 18.22s/it] 72%|███████▏  | 1407/1953 [7:18:53<2:44:29, 18.08s/it] 72%|███████▏  | 1408/1953 [7:19:12<2:47:30, 18.44s/it] 72%|███████▏  | 1409/1953 [7:19:31<2:47:08, 18.43s/it] 72%|███████▏  | 1410/1953 [7:19:49<2:45:13, 18.26s/it]                                                       {'loss': 0.9427, 'learning_rate': 8.945718177107465e-06, 'epoch': 0.72}
 72%|███████▏  | 1410/1953 [7:19:49<2:45:13, 18.26s/it] 72%|███████▏  | 1411/1953 [7:20:07<2:46:18, 18.41s/it] 72%|███████▏  | 1412/1953 [7:20:25<2:44:28, 18.24s/it] 72%|███████▏  | 1413/1953 [7:20:44<2:45:16, 18.36s/it] 72%|███████▏  | 1414/1953 [7:21:03<2:47:08, 18.61s/it] 72%|███████▏  | 1415/1953 [7:21:21<2:45:27, 18.45s/it] 73%|███████▎  | 1416/1953 [7:21:42<2:51:02, 19.11s/it] 73%|███████▎  | 1417/1953 [7:22:04<2:59:31, 20.10s/it] 73%|███████▎  | 1418/1953 [7:22:22<2:53:20, 19.44s/it] 73%|███████▎  | 1419/1953 [7:22:43<2:57:46, 19.97s/it] 73%|███████▎  | 1420/1953 [7:23:01<2:52:01, 19.37s/it]                                                       {'loss': 0.9315, 'learning_rate': 8.639536247469582e-06, 'epoch': 0.73}
 73%|███████▎  | 1420/1953 [7:23:01<2:52:01, 19.37s/it] 73%|███████▎  | 1421/1953 [7:23:20<2:50:38, 19.25s/it] 73%|███████▎  | 1422/1953 [7:23:38<2:47:49, 18.96s/it] 73%|███████▎  | 1423/1953 [7:23:56<2:44:33, 18.63s/it] 73%|███████▎  | 1424/1953 [7:24:14<2:40:38, 18.22s/it] 73%|███████▎  | 1425/1953 [7:24:32<2:39:39, 18.14s/it] 73%|███████▎  | 1426/1953 [7:24:50<2:39:26, 18.15s/it] 73%|███████▎  | 1427/1953 [7:25:08<2:40:22, 18.29s/it] 73%|███████▎  | 1428/1953 [7:25:27<2:40:49, 18.38s/it] 73%|███████▎  | 1429/1953 [7:25:44<2:38:04, 18.10s/it] 73%|███████▎  | 1430/1953 [7:26:03<2:40:02, 18.36s/it]                                                       {'loss': 0.9092, 'learning_rate': 8.33758764178541e-06, 'epoch': 0.73}
 73%|███████▎  | 1430/1953 [7:26:03<2:40:02, 18.36s/it] 73%|███████▎  | 1431/1953 [7:26:21<2:38:43, 18.24s/it] 73%|███████▎  | 1432/1953 [7:26:40<2:39:33, 18.37s/it] 73%|███████▎  | 1433/1953 [7:26:58<2:39:29, 18.40s/it] 73%|███████▎  | 1434/1953 [7:27:17<2:39:47, 18.47s/it] 73%|███████▎  | 1435/1953 [7:27:37<2:42:43, 18.85s/it] 74%|███████▎  | 1436/1953 [7:27:55<2:40:18, 18.61s/it] 74%|███████▎  | 1437/1953 [7:28:14<2:40:28, 18.66s/it] 74%|███████▎  | 1438/1953 [7:28:32<2:39:01, 18.53s/it] 74%|███████▎  | 1439/1953 [7:28:50<2:37:49, 18.42s/it] 74%|███████▎  | 1440/1953 [7:29:10<2:40:18, 18.75s/it]                                                       {'loss': 0.9332, 'learning_rate': 8.039950490252505e-06, 'epoch': 0.74}
 74%|███████▎  | 1440/1953 [7:29:10<2:40:18, 18.75s/it] 74%|███████▍  | 1441/1953 [7:29:28<2:39:28, 18.69s/it] 74%|███████▍  | 1442/1953 [7:29:47<2:40:28, 18.84s/it] 74%|███████▍  | 1443/1953 [7:30:05<2:37:45, 18.56s/it] 74%|███████▍  | 1444/1953 [7:30:23<2:35:47, 18.37s/it] 74%|███████▍  | 1445/1953 [7:30:42<2:35:57, 18.42s/it] 74%|███████▍  | 1446/1953 [7:31:00<2:35:05, 18.35s/it] 74%|███████▍  | 1447/1953 [7:31:19<2:35:38, 18.45s/it] 74%|███████▍  | 1448/1953 [7:31:40<2:43:04, 19.38s/it] 74%|███████▍  | 1449/1953 [7:31:58<2:40:03, 19.05s/it] 74%|███████▍  | 1450/1953 [7:32:17<2:38:01, 18.85s/it]                                                       {'loss': 0.9391, 'learning_rate': 7.74670180746546e-06, 'epoch': 0.74}
 74%|███████▍  | 1450/1953 [7:32:17<2:38:01, 18.85s/it] 74%|███████▍  | 1451/1953 [7:32:34<2:34:06, 18.42s/it] 74%|███████▍  | 1452/1953 [7:32:53<2:34:28, 18.50s/it] 74%|███████▍  | 1453/1953 [7:33:11<2:32:23, 18.29s/it] 74%|███████▍  | 1454/1953 [7:33:30<2:34:20, 18.56s/it] 75%|███████▍  | 1455/1953 [7:33:49<2:34:27, 18.61s/it] 75%|███████▍  | 1456/1953 [7:34:07<2:33:48, 18.57s/it] 75%|███████▍  | 1457/1953 [7:34:30<2:44:28, 19.90s/it] 75%|███████▍  | 1458/1953 [7:34:48<2:39:22, 19.32s/it] 75%|███████▍  | 1459/1953 [7:35:05<2:34:11, 18.73s/it] 75%|███████▍  | 1460/1953 [7:35:24<2:34:23, 18.79s/it]                                                       {'loss': 0.9176, 'learning_rate': 7.4579174724880875e-06, 'epoch': 0.75}
 75%|███████▍  | 1460/1953 [7:35:24<2:34:23, 18.79s/it] 75%|███████▍  | 1461/1953 [7:35:44<2:37:14, 19.18s/it] 75%|███████▍  | 1462/1953 [7:36:03<2:35:34, 19.01s/it] 75%|███████▍  | 1463/1953 [7:36:22<2:35:19, 19.02s/it] 75%|███████▍  | 1464/1953 [7:36:40<2:33:34, 18.84s/it] 75%|███████▌  | 1465/1953 [7:36:59<2:32:22, 18.73s/it] 75%|███████▌  | 1466/1953 [7:37:18<2:32:35, 18.80s/it] 75%|███████▌  | 1467/1953 [7:37:36<2:30:30, 18.58s/it] 75%|███████▌  | 1468/1953 [7:37:54<2:27:51, 18.29s/it] 75%|███████▌  | 1469/1953 [7:38:12<2:27:09, 18.24s/it] 75%|███████▌  | 1470/1953 [7:38:30<2:27:00, 18.26s/it]                                                       {'loss': 0.9451, 'learning_rate': 7.173672209219495e-06, 'epoch': 0.75}
 75%|███████▌  | 1470/1953 [7:38:30<2:27:00, 18.26s/it] 75%|███████▌  | 1471/1953 [7:38:48<2:26:15, 18.21s/it] 75%|███████▌  | 1472/1953 [7:39:06<2:25:32, 18.16s/it] 75%|███████▌  | 1473/1953 [7:39:24<2:24:07, 18.01s/it] 75%|███████▌  | 1474/1953 [7:39:42<2:23:54, 18.03s/it] 76%|███████▌  | 1475/1953 [7:40:00<2:23:18, 17.99s/it] 76%|███████▌  | 1476/1953 [7:40:17<2:21:48, 17.84s/it] 76%|███████▌  | 1477/1953 [7:40:40<2:33:41, 19.37s/it] 76%|███████▌  | 1478/1953 [7:40:58<2:29:48, 18.92s/it] 76%|███████▌  | 1479/1953 [7:41:16<2:27:10, 18.63s/it] 76%|███████▌  | 1480/1953 [7:41:40<2:39:24, 20.22s/it]                                                       {'loss': 0.9098, 'learning_rate': 6.894039567059007e-06, 'epoch': 0.76}
 76%|███████▌  | 1480/1953 [7:41:40<2:39:24, 20.22s/it] 76%|███████▌  | 1481/1953 [7:41:58<2:34:16, 19.61s/it] 76%|███████▌  | 1482/1953 [7:42:16<2:30:00, 19.11s/it] 76%|███████▌  | 1483/1953 [7:42:34<2:27:59, 18.89s/it] 76%|███████▌  | 1484/1953 [7:42:54<2:28:16, 18.97s/it] 76%|███████▌  | 1485/1953 [7:43:11<2:24:40, 18.55s/it] 76%|███████▌  | 1486/1953 [7:43:30<2:23:57, 18.50s/it] 76%|███████▌  | 1487/1953 [7:43:50<2:28:11, 19.08s/it] 76%|███████▌  | 1488/1953 [7:44:10<2:29:22, 19.28s/it] 76%|███████▌  | 1489/1953 [7:44:28<2:25:57, 18.87s/it] 76%|███████▋  | 1490/1953 [7:44:46<2:24:10, 18.68s/it]                                                       {'loss': 0.9258, 'learning_rate': 6.6190919018750215e-06, 'epoch': 0.76}
 76%|███████▋  | 1490/1953 [7:44:46<2:24:10, 18.68s/it] 76%|███████▋  | 1491/1953 [7:45:05<2:24:21, 18.75s/it] 76%|███████▋  | 1492/1953 [7:45:23<2:22:46, 18.58s/it] 76%|███████▋  | 1493/1953 [7:45:42<2:22:59, 18.65s/it] 76%|███████▋  | 1494/1953 [7:46:00<2:21:37, 18.51s/it] 77%|███████▋  | 1495/1953 [7:46:18<2:20:43, 18.43s/it] 77%|███████▋  | 1496/1953 [7:46:36<2:18:59, 18.25s/it] 77%|███████▋  | 1497/1953 [7:46:55<2:19:59, 18.42s/it] 77%|███████▋  | 1498/1953 [7:47:18<2:29:25, 19.71s/it] 77%|███████▋  | 1499/1953 [7:47:36<2:27:18, 19.47s/it] 77%|███████▋  | 1500/1953 [7:47:55<2:23:57, 19.07s/it]                                                       {'loss': 0.9305, 'learning_rate': 6.348900357282719e-06, 'epoch': 0.77}
 77%|███████▋  | 1500/1953 [7:47:55<2:23:57, 19.07s/it] 77%|███████▋  | 1501/1953 [7:48:13<2:22:24, 18.90s/it] 77%|███████▋  | 1502/1953 [7:48:32<2:21:34, 18.83s/it] 77%|███████▋  | 1503/1953 [7:48:50<2:19:08, 18.55s/it] 77%|███████▋  | 1504/1953 [7:49:08<2:17:37, 18.39s/it] 77%|███████▋  | 1505/1953 [7:49:26<2:16:48, 18.32s/it] 77%|███████▋  | 1506/1953 [7:49:44<2:15:03, 18.13s/it] 77%|███████▋  | 1507/1953 [7:50:02<2:16:11, 18.32s/it] 77%|███████▋  | 1508/1953 [7:50:21<2:16:08, 18.36s/it] 77%|███████▋  | 1509/1953 [7:50:40<2:16:55, 18.50s/it] 77%|███████▋  | 1510/1953 [7:50:59<2:17:53, 18.68s/it]                                                       {'loss': 0.9203, 'learning_rate': 6.083534846235342e-06, 'epoch': 0.77}
 77%|███████▋  | 1510/1953 [7:50:59<2:17:53, 18.68s/it] 77%|███████▋  | 1511/1953 [7:51:16<2:15:24, 18.38s/it] 77%|███████▋  | 1512/1953 [7:51:35<2:14:32, 18.31s/it] 77%|███████▋  | 1513/1953 [7:51:53<2:14:19, 18.32s/it] 78%|███████▊  | 1514/1953 [7:52:11<2:12:52, 18.16s/it] 78%|███████▊  | 1515/1953 [7:52:29<2:12:00, 18.08s/it] 78%|███████▊  | 1516/1953 [7:52:48<2:13:56, 18.39s/it] 78%|███████▊  | 1517/1953 [7:53:06<2:13:36, 18.39s/it] 78%|███████▊  | 1518/1953 [7:53:25<2:15:22, 18.67s/it] 78%|███████▊  | 1519/1953 [7:53:43<2:13:32, 18.46s/it] 78%|███████▊  | 1520/1953 [7:54:03<2:14:53, 18.69s/it]                                                       {'loss': 0.9119, 'learning_rate': 5.8230640329340835e-06, 'epoch': 0.78}
 78%|███████▊  | 1520/1953 [7:54:03<2:14:53, 18.69s/it] 78%|███████▊  | 1521/1953 [7:54:21<2:13:08, 18.49s/it] 78%|███████▊  | 1522/1953 [7:54:40<2:14:33, 18.73s/it] 78%|███████▊  | 1523/1953 [7:54:58<2:13:32, 18.63s/it] 78%|███████▊  | 1524/1953 [7:55:17<2:12:27, 18.52s/it] 78%|███████▊  | 1525/1953 [7:55:36<2:14:15, 18.82s/it] 78%|███████▊  | 1526/1953 [7:55:54<2:13:00, 18.69s/it] 78%|███████▊  | 1527/1953 [7:56:13<2:12:27, 18.66s/it] 78%|███████▊  | 1528/1953 [7:56:31<2:10:51, 18.47s/it] 78%|███████▊  | 1529/1953 [7:56:53<2:17:13, 19.42s/it] 78%|███████▊  | 1530/1953 [7:57:11<2:14:17, 19.05s/it]                                                       {'loss': 0.9117, 'learning_rate': 5.567555315060918e-06, 'epoch': 0.78}
 78%|███████▊  | 1530/1953 [7:57:11<2:14:17, 19.05s/it] 78%|███████▊  | 1531/1953 [7:57:30<2:14:37, 19.14s/it] 78%|███████▊  | 1532/1953 [7:57:49<2:13:11, 18.98s/it] 78%|███████▊  | 1533/1953 [7:58:07<2:11:20, 18.76s/it] 79%|███████▊  | 1534/1953 [7:58:25<2:09:20, 18.52s/it] 79%|███████▊  | 1535/1953 [7:58:43<2:07:36, 18.32s/it] 79%|███████▊  | 1536/1953 [7:59:01<2:07:42, 18.38s/it] 79%|███████▊  | 1537/1953 [7:59:19<2:06:43, 18.28s/it] 79%|███████▉  | 1538/1953 [7:59:38<2:06:24, 18.27s/it] 79%|███████▉  | 1539/1953 [7:59:56<2:06:23, 18.32s/it] 79%|███████▉  | 1540/1953 [8:00:15<2:07:22, 18.51s/it]                                                       {'loss': 0.9274, 'learning_rate': 5.317074806339295e-06, 'epoch': 0.79}
 79%|███████▉  | 1540/1953 [8:00:15<2:07:22, 18.51s/it] 79%|███████▉  | 1541/1953 [8:00:35<2:10:24, 18.99s/it] 79%|███████▉  | 1542/1953 [8:00:54<2:09:05, 18.84s/it] 79%|███████▉  | 1543/1953 [8:01:12<2:08:14, 18.77s/it] 79%|███████▉  | 1544/1953 [8:01:31<2:06:50, 18.61s/it] 79%|███████▉  | 1545/1953 [8:01:48<2:05:06, 18.40s/it] 79%|███████▉  | 1546/1953 [8:02:07<2:04:32, 18.36s/it] 79%|███████▉  | 1547/1953 [8:02:25<2:04:31, 18.40s/it] 79%|███████▉  | 1548/1953 [8:02:45<2:07:10, 18.84s/it] 79%|███████▉  | 1549/1953 [8:03:04<2:07:25, 18.93s/it] 79%|███████▉  | 1550/1953 [8:03:22<2:04:39, 18.56s/it]                                                       {'loss': 0.9216, 'learning_rate': 5.071687319426946e-06, 'epoch': 0.79}
 79%|███████▉  | 1550/1953 [8:03:22<2:04:39, 18.56s/it] 79%|███████▉  | 1551/1953 [8:03:40<2:04:06, 18.52s/it] 79%|███████▉  | 1552/1953 [8:03:59<2:04:01, 18.56s/it] 80%|███████▉  | 1553/1953 [8:04:18<2:04:19, 18.65s/it] 80%|███████▉  | 1554/1953 [8:04:37<2:04:25, 18.71s/it] 80%|███████▉  | 1555/1953 [8:04:56<2:04:27, 18.76s/it] 80%|███████▉  | 1556/1953 [8:05:16<2:07:21, 19.25s/it] 80%|███████▉  | 1557/1953 [8:05:36<2:08:43, 19.50s/it] 80%|███████▉  | 1558/1953 [8:05:54<2:05:57, 19.13s/it] 80%|███████▉  | 1559/1953 [8:06:17<2:13:27, 20.32s/it] 80%|███████▉  | 1560/1953 [8:06:36<2:09:57, 19.84s/it]                                                       {'loss': 0.916, 'learning_rate': 4.831456349145386e-06, 'epoch': 0.8}
 80%|███████▉  | 1560/1953 [8:06:36<2:09:57, 19.84s/it] 80%|███████▉  | 1561/1953 [8:06:57<2:11:07, 20.07s/it] 80%|███████▉  | 1562/1953 [8:07:15<2:06:59, 19.49s/it] 80%|████████  | 1563/1953 [8:07:35<2:06:57, 19.53s/it] 80%|████████  | 1564/1953 [8:07:55<2:08:09, 19.77s/it] 80%|████████  | 1565/1953 [8:08:13<2:04:23, 19.24s/it] 80%|████████  | 1566/1953 [8:08:32<2:03:50, 19.20s/it] 80%|████████  | 1567/1953 [8:08:51<2:02:17, 19.01s/it] 80%|████████  | 1568/1953 [8:09:08<1:58:52, 18.53s/it] 80%|████████  | 1569/1953 [8:09:26<1:57:48, 18.41s/it] 80%|████████  | 1570/1953 [8:09:44<1:56:19, 18.22s/it]                                                       {'loss': 0.911, 'learning_rate': 4.596444056050492e-06, 'epoch': 0.8}
 80%|████████  | 1570/1953 [8:09:44<1:56:19, 18.22s/it] 80%|████████  | 1571/1953 [8:10:03<1:58:29, 18.61s/it] 80%|████████  | 1572/1953 [8:10:22<1:58:07, 18.60s/it] 81%|████████  | 1573/1953 [8:10:41<1:59:24, 18.85s/it] 81%|████████  | 1574/1953 [8:10:59<1:57:15, 18.56s/it] 81%|████████  | 1575/1953 [8:11:18<1:56:40, 18.52s/it] 81%|████████  | 1576/1953 [8:11:35<1:54:55, 18.29s/it] 81%|████████  | 1577/1953 [8:11:54<1:54:20, 18.24s/it] 81%|████████  | 1578/1953 [8:12:13<1:55:34, 18.49s/it] 81%|████████  | 1579/1953 [8:12:31<1:54:07, 18.31s/it] 81%|████████  | 1580/1953 [8:12:48<1:53:08, 18.20s/it]                                                       {'loss': 0.8983, 'learning_rate': 4.366711250348176e-06, 'epoch': 0.81}
 81%|████████  | 1580/1953 [8:12:48<1:53:08, 18.20s/it] 81%|████████  | 1581/1953 [8:13:07<1:53:46, 18.35s/it] 81%|████████  | 1582/1953 [8:13:26<1:54:01, 18.44s/it] 81%|████████  | 1583/1953 [8:13:44<1:53:42, 18.44s/it] 81%|████████  | 1584/1953 [8:14:02<1:51:45, 18.17s/it] 81%|████████  | 1585/1953 [8:14:21<1:53:03, 18.43s/it] 81%|████████  | 1586/1953 [8:14:39<1:51:46, 18.27s/it] 81%|████████▏ | 1587/1953 [8:14:59<1:54:28, 18.77s/it] 81%|████████▏ | 1588/1953 [8:15:17<1:53:09, 18.60s/it] 81%|████████▏ | 1589/1953 [8:15:35<1:52:39, 18.57s/it] 81%|████████▏ | 1590/1953 [8:15:53<1:50:19, 18.24s/it]                                                       {'loss': 0.9303, 'learning_rate': 4.142317376159599e-06, 'epoch': 0.81}
 81%|████████▏ | 1590/1953 [8:15:53<1:50:19, 18.24s/it] 81%|████████▏ | 1591/1953 [8:16:11<1:49:43, 18.19s/it] 82%|████████▏ | 1592/1953 [8:16:30<1:50:11, 18.31s/it] 82%|████████▏ | 1593/1953 [8:16:48<1:50:23, 18.40s/it] 82%|████████▏ | 1594/1953 [8:17:07<1:51:32, 18.64s/it] 82%|████████▏ | 1595/1953 [8:17:26<1:51:28, 18.68s/it] 82%|████████▏ | 1596/1953 [8:17:44<1:49:31, 18.41s/it] 82%|████████▏ | 1597/1953 [8:18:03<1:51:01, 18.71s/it] 82%|████████▏ | 1598/1953 [8:18:22<1:50:37, 18.70s/it] 82%|████████▏ | 1599/1953 [8:18:41<1:50:51, 18.79s/it] 82%|████████▏ | 1600/1953 [8:19:00<1:50:15, 18.74s/it]                                                       {'loss': 0.9161, 'learning_rate': 3.92332049613976e-06, 'epoch': 0.82}
 82%|████████▏ | 1600/1953 [8:19:00<1:50:15, 18.74s/it] 82%|████████▏ | 1601/1953 [8:19:21<1:55:22, 19.67s/it] 82%|████████▏ | 1602/1953 [8:19:40<1:52:46, 19.28s/it] 82%|████████▏ | 1603/1953 [8:19:58<1:51:02, 19.04s/it] 82%|████████▏ | 1604/1953 [8:20:16<1:49:10, 18.77s/it] 82%|████████▏ | 1605/1953 [8:20:35<1:48:14, 18.66s/it] 82%|████████▏ | 1606/1953 [8:20:54<1:48:38, 18.79s/it] 82%|████████▏ | 1607/1953 [8:21:12<1:47:25, 18.63s/it] 82%|████████▏ | 1608/1953 [8:21:30<1:45:55, 18.42s/it] 82%|████████▏ | 1609/1953 [8:21:49<1:45:51, 18.46s/it] 82%|████████▏ | 1610/1953 [8:22:06<1:44:07, 18.22s/it]                                                       {'loss': 0.936, 'learning_rate': 3.70977727645363e-06, 'epoch': 0.82}
 82%|████████▏ | 1610/1953 [8:22:06<1:44:07, 18.22s/it] 82%|████████▏ | 1611/1953 [8:22:25<1:43:45, 18.20s/it] 83%|████████▎ | 1612/1953 [8:22:42<1:42:48, 18.09s/it] 83%|████████▎ | 1613/1953 [8:23:03<1:46:27, 18.79s/it] 83%|████████▎ | 1614/1953 [8:23:21<1:44:27, 18.49s/it] 83%|████████▎ | 1615/1953 [8:23:39<1:43:50, 18.43s/it] 83%|████████▎ | 1616/1953 [8:23:57<1:43:01, 18.34s/it] 83%|████████▎ | 1617/1953 [8:24:16<1:43:36, 18.50s/it] 83%|████████▎ | 1618/1953 [8:24:37<1:47:01, 19.17s/it] 83%|████████▎ | 1619/1953 [8:24:55<1:44:58, 18.86s/it] 83%|████████▎ | 1620/1953 [8:25:13<1:44:25, 18.81s/it]                                                       {'loss': 0.8992, 'learning_rate': 3.5017429721135807e-06, 'epoch': 0.83}
 83%|████████▎ | 1620/1953 [8:25:13<1:44:25, 18.81s/it] 83%|████████▎ | 1621/1953 [8:25:31<1:42:36, 18.54s/it] 83%|████████▎ | 1622/1953 [8:25:51<1:43:26, 18.75s/it] 83%|████████▎ | 1623/1953 [8:26:09<1:42:35, 18.65s/it] 83%|████████▎ | 1624/1953 [8:26:27<1:40:58, 18.42s/it] 83%|████████▎ | 1625/1953 [8:26:45<1:40:47, 18.44s/it] 83%|████████▎ | 1626/1953 [8:27:03<1:39:11, 18.20s/it] 83%|████████▎ | 1627/1953 [8:27:21<1:38:46, 18.18s/it] 83%|████████▎ | 1628/1953 [8:27:39<1:38:08, 18.12s/it] 83%|████████▎ | 1629/1953 [8:27:57<1:37:35, 18.07s/it] 83%|████████▎ | 1630/1953 [8:28:15<1:37:22, 18.09s/it]                                                       {'loss': 0.9276, 'learning_rate': 3.2992714126819644e-06, 'epoch': 0.83}
 83%|████████▎ | 1630/1953 [8:28:15<1:37:22, 18.09s/it] 84%|████████▎ | 1631/1953 [8:28:34<1:37:37, 18.19s/it] 84%|████████▎ | 1632/1953 [8:28:53<1:38:47, 18.47s/it] 84%|████████▎ | 1633/1953 [8:29:12<1:39:16, 18.61s/it] 84%|████████▎ | 1634/1953 [8:29:30<1:37:55, 18.42s/it] 84%|████████▎ | 1635/1953 [8:29:48<1:37:45, 18.44s/it] 84%|████████▍ | 1636/1953 [8:30:06<1:36:53, 18.34s/it] 84%|████████▍ | 1637/1953 [8:30:24<1:35:51, 18.20s/it] 84%|████████▍ | 1638/1953 [8:30:43<1:35:53, 18.26s/it] 84%|████████▍ | 1639/1953 [8:31:00<1:34:59, 18.15s/it] 84%|████████▍ | 1640/1953 [8:31:24<1:43:03, 19.75s/it]                                                       {'loss': 0.9171, 'learning_rate': 3.1024149883425586e-06, 'epoch': 0.84}
 84%|████████▍ | 1640/1953 [8:31:24<1:43:03, 19.75s/it] 84%|████████▍ | 1641/1953 [8:31:43<1:41:33, 19.53s/it] 84%|████████▍ | 1642/1953 [8:32:01<1:38:32, 19.01s/it] 84%|████████▍ | 1643/1953 [8:32:19<1:37:30, 18.87s/it] 84%|████████▍ | 1644/1953 [8:32:37<1:36:07, 18.67s/it] 84%|████████▍ | 1645/1953 [8:32:55<1:34:35, 18.43s/it] 84%|████████▍ | 1646/1953 [8:33:13<1:33:50, 18.34s/it] 84%|████████▍ | 1647/1953 [8:33:32<1:33:52, 18.41s/it] 84%|████████▍ | 1648/1953 [8:33:50<1:32:38, 18.22s/it] 84%|████████▍ | 1649/1953 [8:34:09<1:33:18, 18.42s/it] 84%|████████▍ | 1650/1953 [8:34:27<1:33:03, 18.43s/it]                                                       {'loss': 0.9305, 'learning_rate': 2.9112246363443953e-06, 'epoch': 0.84}
 84%|████████▍ | 1650/1953 [8:34:27<1:33:03, 18.43s/it] 85%|████████▍ | 1651/1953 [8:34:46<1:32:46, 18.43s/it] 85%|████████▍ | 1652/1953 [8:35:04<1:32:03, 18.35s/it] 85%|████████▍ | 1653/1953 [8:35:22<1:31:34, 18.32s/it] 85%|████████▍ | 1654/1953 [8:35:42<1:33:48, 18.82s/it] 85%|████████▍ | 1655/1953 [8:36:01<1:33:14, 18.77s/it] 85%|████████▍ | 1656/1953 [8:36:19<1:32:23, 18.67s/it] 85%|████████▍ | 1657/1953 [8:36:38<1:32:18, 18.71s/it] 85%|████████▍ | 1658/1953 [8:36:56<1:31:38, 18.64s/it] 85%|████████▍ | 1659/1953 [8:37:14<1:30:34, 18.49s/it] 85%|████████▍ | 1660/1953 [8:37:32<1:29:26, 18.32s/it]                                                       {'loss': 0.9114, 'learning_rate': 2.7257498278216135e-06, 'epoch': 0.85}
 85%|████████▍ | 1660/1953 [8:37:32<1:29:26, 18.32s/it] 85%|████████▌ | 1661/1953 [8:37:52<1:31:27, 18.79s/it] 85%|████████▌ | 1662/1953 [8:38:10<1:29:58, 18.55s/it] 85%|████████▌ | 1663/1953 [8:38:29<1:29:12, 18.46s/it] 85%|████████▌ | 1664/1953 [8:38:47<1:29:05, 18.50s/it] 85%|████████▌ | 1665/1953 [8:39:06<1:29:39, 18.68s/it] 85%|████████▌ | 1666/1953 [8:39:24<1:27:54, 18.38s/it] 85%|████████▌ | 1667/1953 [8:39:44<1:29:34, 18.79s/it] 85%|████████▌ | 1668/1953 [8:40:02<1:28:30, 18.63s/it] 85%|████████▌ | 1669/1953 [8:40:20<1:27:55, 18.57s/it] 86%|████████▌ | 1670/1953 [8:40:39<1:27:11, 18.49s/it]                                                       {'loss': 0.9021, 'learning_rate': 2.5460385549926275e-06, 'epoch': 0.85}
 86%|████████▌ | 1670/1953 [8:40:39<1:27:11, 18.49s/it] 86%|████████▌ | 1671/1953 [8:40:57<1:26:23, 18.38s/it] 86%|████████▌ | 1672/1953 [8:41:15<1:25:46, 18.31s/it] 86%|████████▌ | 1673/1953 [8:41:34<1:25:51, 18.40s/it] 86%|████████▌ | 1674/1953 [8:41:52<1:25:11, 18.32s/it] 86%|████████▌ | 1675/1953 [8:42:11<1:25:57, 18.55s/it] 86%|████████▌ | 1676/1953 [8:42:30<1:26:05, 18.65s/it] 86%|████████▌ | 1677/1953 [8:42:48<1:25:06, 18.50s/it] 86%|████████▌ | 1678/1953 [8:43:07<1:25:21, 18.62s/it] 86%|████████▌ | 1679/1953 [8:43:25<1:24:49, 18.57s/it] 86%|████████▌ | 1680/1953 [8:43:44<1:24:21, 18.54s/it]                                                       {'loss': 0.9186, 'learning_rate': 2.372137318741968e-06, 'epoch': 0.86}
 86%|████████▌ | 1680/1953 [8:43:44<1:24:21, 18.54s/it] 86%|████████▌ | 1681/1953 [8:44:02<1:24:29, 18.64s/it] 86%|████████▌ | 1682/1953 [8:44:21<1:23:58, 18.59s/it] 86%|████████▌ | 1683/1953 [8:44:48<1:34:39, 21.04s/it] 86%|████████▌ | 1684/1953 [8:45:06<1:30:22, 20.16s/it] 86%|████████▋ | 1685/1953 [8:45:23<1:26:37, 19.39s/it] 86%|████████▋ | 1686/1953 [8:45:42<1:25:13, 19.15s/it] 86%|████████▋ | 1687/1953 [8:46:01<1:24:16, 19.01s/it] 86%|████████▋ | 1688/1953 [8:46:20<1:24:44, 19.19s/it] 86%|████████▋ | 1689/1953 [8:46:38<1:22:21, 18.72s/it] 87%|████████▋ | 1690/1953 [8:46:58<1:24:02, 19.17s/it]                                                       {'loss': 0.92, 'learning_rate': 2.2040911165880723e-06, 'epoch': 0.87}
 87%|████████▋ | 1690/1953 [8:46:58<1:24:02, 19.17s/it] 87%|████████▋ | 1691/1953 [8:47:17<1:23:15, 19.07s/it] 87%|████████▋ | 1692/1953 [8:47:36<1:23:23, 19.17s/it] 87%|████████▋ | 1693/1953 [8:47:55<1:21:53, 18.90s/it] 87%|████████▋ | 1694/1953 [8:48:13<1:21:16, 18.83s/it] 87%|████████▋ | 1695/1953 [8:48:32<1:20:14, 18.66s/it] 87%|████████▋ | 1696/1953 [8:48:50<1:20:06, 18.70s/it] 87%|████████▋ | 1697/1953 [8:49:09<1:19:36, 18.66s/it] 87%|████████▋ | 1698/1953 [8:49:31<1:24:06, 19.79s/it] 87%|████████▋ | 1699/1953 [8:49:50<1:22:07, 19.40s/it] 87%|████████▋ | 1700/1953 [8:50:08<1:20:44, 19.15s/it]                                                       {'loss': 0.898, 'learning_rate': 2.041943431039953e-06, 'epoch': 0.87}
 87%|████████▋ | 1700/1953 [8:50:08<1:20:44, 19.15s/it] 87%|████████▋ | 1701/1953 [8:50:27<1:19:24, 18.91s/it] 87%|████████▋ | 1702/1953 [8:50:45<1:18:37, 18.79s/it] 87%|████████▋ | 1703/1953 [8:51:03<1:17:35, 18.62s/it] 87%|████████▋ | 1704/1953 [8:51:21<1:16:26, 18.42s/it] 87%|████████▋ | 1705/1953 [8:51:41<1:16:58, 18.62s/it] 87%|████████▋ | 1706/1953 [8:52:01<1:18:42, 19.12s/it] 87%|████████▋ | 1707/1953 [8:52:19<1:17:40, 18.95s/it] 87%|████████▋ | 1708/1953 [8:52:39<1:18:47, 19.30s/it] 88%|████████▊ | 1709/1953 [8:52:58<1:17:37, 19.09s/it] 88%|████████▊ | 1710/1953 [8:53:18<1:18:35, 19.40s/it]                                                       {'loss': 0.9153, 'learning_rate': 1.8857362183460264e-06, 'epoch': 0.88}
 88%|████████▊ | 1710/1953 [8:53:18<1:18:35, 19.40s/it] 88%|████████▊ | 1711/1953 [8:53:38<1:18:11, 19.39s/it] 88%|████████▊ | 1712/1953 [8:53:56<1:16:47, 19.12s/it] 88%|████████▊ | 1713/1953 [8:54:14<1:15:30, 18.88s/it] 88%|████████▊ | 1714/1953 [8:54:33<1:14:58, 18.82s/it] 88%|████████▊ | 1715/1953 [8:54:51<1:14:00, 18.66s/it] 88%|████████▊ | 1716/1953 [8:55:11<1:14:23, 18.83s/it] 88%|████████▊ | 1717/1953 [8:55:29<1:13:55, 18.79s/it] 88%|████████▊ | 1718/1953 [8:55:47<1:12:54, 18.61s/it] 88%|████████▊ | 1719/1953 [8:56:06<1:12:53, 18.69s/it] 88%|████████▊ | 1720/1953 [8:56:25<1:12:29, 18.67s/it]                                                       {'loss': 0.9171, 'learning_rate': 1.7355098976377575e-06, 'epoch': 0.88}
 88%|████████▊ | 1720/1953 [8:56:25<1:12:29, 18.67s/it] 88%|████████▊ | 1721/1953 [8:56:43<1:11:55, 18.60s/it] 88%|████████▊ | 1722/1953 [8:57:02<1:11:21, 18.54s/it] 88%|████████▊ | 1723/1953 [8:57:20<1:10:43, 18.45s/it] 88%|████████▊ | 1724/1953 [8:57:38<1:10:10, 18.39s/it] 88%|████████▊ | 1725/1953 [8:58:01<1:15:01, 19.74s/it] 88%|████████▊ | 1726/1953 [8:58:19<1:12:54, 19.27s/it] 88%|████████▊ | 1727/1953 [8:58:38<1:11:38, 19.02s/it] 88%|████████▊ | 1728/1953 [8:58:56<1:10:34, 18.82s/it] 89%|████████▊ | 1729/1953 [8:59:14<1:09:06, 18.51s/it] 89%|████████▊ | 1730/1953 [8:59:33<1:09:14, 18.63s/it]                                                       {'loss': 0.9024, 'learning_rate': 1.591303340471084e-06, 'epoch': 0.89}
 89%|████████▊ | 1730/1953 [8:59:33<1:09:14, 18.63s/it] 89%|████████▊ | 1731/1953 [8:59:51<1:08:57, 18.64s/it] 89%|████████▊ | 1732/1953 [9:00:09<1:07:45, 18.39s/it] 89%|████████▊ | 1733/1953 [9:00:28<1:07:16, 18.35s/it] 89%|████████▉ | 1734/1953 [9:00:47<1:07:56, 18.61s/it] 89%|████████▉ | 1735/1953 [9:01:05<1:07:23, 18.55s/it] 89%|████████▉ | 1736/1953 [9:01:23<1:06:02, 18.26s/it] 89%|████████▉ | 1737/1953 [9:01:41<1:05:43, 18.26s/it] 89%|████████▉ | 1738/1953 [9:01:59<1:05:03, 18.15s/it] 89%|████████▉ | 1739/1953 [9:02:17<1:05:09, 18.27s/it] 89%|████████▉ | 1740/1953 [9:02:36<1:05:38, 18.49s/it]                                                       {'loss': 0.9195, 'learning_rate': 1.4531538607682805e-06, 'epoch': 0.89}
 89%|████████▉ | 1740/1953 [9:02:36<1:05:38, 18.49s/it] 89%|████████▉ | 1741/1953 [9:02:54<1:04:39, 18.30s/it] 89%|████████▉ | 1742/1953 [9:03:12<1:04:10, 18.25s/it] 89%|████████▉ | 1743/1953 [9:03:31<1:04:16, 18.36s/it] 89%|████████▉ | 1744/1953 [9:03:49<1:03:37, 18.26s/it] 89%|████████▉ | 1745/1953 [9:04:07<1:03:16, 18.25s/it] 89%|████████▉ | 1746/1953 [9:04:26<1:03:02, 18.27s/it] 89%|████████▉ | 1747/1953 [9:04:49<1:07:36, 19.69s/it] 90%|████████▉ | 1748/1953 [9:05:07<1:05:52, 19.28s/it] 90%|████████▉ | 1749/1953 [9:05:27<1:05:59, 19.41s/it] 90%|████████▉ | 1750/1953 [9:05:45<1:04:12, 18.98s/it]                                                       {'loss': 0.905, 'learning_rate': 1.3210972051628328e-06, 'epoch': 0.9}
 90%|████████▉ | 1750/1953 [9:05:45<1:04:12, 18.98s/it] 90%|████████▉ | 1751/1953 [9:06:03<1:03:06, 18.75s/it] 90%|████████▉ | 1752/1953 [9:06:21<1:01:59, 18.50s/it] 90%|████████▉ | 1753/1953 [9:06:39<1:00:53, 18.27s/it] 90%|████████▉ | 1754/1953 [9:06:59<1:02:57, 18.98s/it] 90%|████████▉ | 1755/1953 [9:07:18<1:02:11, 18.84s/it] 90%|████████▉ | 1756/1953 [9:07:35<1:00:23, 18.39s/it] 90%|████████▉ | 1757/1953 [9:07:54<1:00:19, 18.47s/it] 90%|█████████ | 1758/1953 [9:08:13<1:00:44, 18.69s/it] 90%|█████████ | 1759/1953 [9:08:31<59:28, 18.39s/it]   90%|█████████ | 1760/1953 [9:08:49<59:01, 18.35s/it]                                                     {'loss': 0.9, 'learning_rate': 1.1951675437499144e-06, 'epoch': 0.9}
 90%|█████████ | 1760/1953 [9:08:49<59:01, 18.35s/it] 90%|█████████ | 1761/1953 [9:09:08<59:04, 18.46s/it] 90%|█████████ | 1762/1953 [9:09:26<58:32, 18.39s/it] 90%|█████████ | 1763/1953 [9:09:45<58:34, 18.50s/it] 90%|█████████ | 1764/1953 [9:10:03<58:18, 18.51s/it] 90%|█████████ | 1765/1953 [9:10:23<59:22, 18.95s/it] 90%|█████████ | 1766/1953 [9:10:41<57:49, 18.55s/it] 90%|█████████ | 1767/1953 [9:10:59<56:57, 18.38s/it] 91%|█████████ | 1768/1953 [9:11:17<56:43, 18.40s/it] 91%|█████████ | 1769/1953 [9:11:35<55:59, 18.26s/it] 91%|█████████ | 1770/1953 [9:11:53<55:35, 18.23s/it]                                                     {'loss': 0.9099, 'learning_rate': 1.07539746124474e-06, 'epoch': 0.91}
 91%|█████████ | 1770/1953 [9:11:53<55:35, 18.23s/it] 91%|█████████ | 1771/1953 [9:12:11<55:19, 18.24s/it] 91%|█████████ | 1772/1953 [9:12:30<55:17, 18.33s/it] 91%|█████████ | 1773/1953 [9:12:49<55:21, 18.45s/it] 91%|█████████ | 1774/1953 [9:13:07<54:53, 18.40s/it] 91%|█████████ | 1775/1953 [9:13:26<54:47, 18.47s/it] 91%|█████████ | 1776/1953 [9:13:43<53:45, 18.22s/it] 91%|█████████ | 1777/1953 [9:14:01<53:13, 18.14s/it] 91%|█████████ | 1778/1953 [9:14:20<53:19, 18.28s/it] 91%|█████████ | 1779/1953 [9:14:39<53:49, 18.56s/it] 91%|█████████ | 1780/1953 [9:14:57<53:19, 18.50s/it]                                                     {'loss': 0.8971, 'learning_rate': 9.618179485511691e-07, 'epoch': 0.91}
 91%|█████████ | 1780/1953 [9:14:57<53:19, 18.50s/it] 91%|█████████ | 1781/1953 [9:15:16<52:49, 18.43s/it] 91%|█████████ | 1782/1953 [9:15:34<52:06, 18.29s/it] 91%|█████████▏| 1783/1953 [9:15:53<52:33, 18.55s/it] 91%|█████████▏| 1784/1953 [9:16:11<52:20, 18.58s/it] 91%|█████████▏| 1785/1953 [9:16:30<51:39, 18.45s/it] 91%|█████████▏| 1786/1953 [9:16:49<52:01, 18.69s/it] 92%|█████████▏| 1787/1953 [9:17:08<51:43, 18.70s/it] 92%|█████████▏| 1788/1953 [9:17:26<50:52, 18.50s/it] 92%|█████████▏| 1789/1953 [9:17:44<50:26, 18.45s/it] 92%|█████████▏| 1790/1953 [9:18:02<49:46, 18.32s/it]                                                     {'loss': 0.9029, 'learning_rate': 8.544583947426993e-07, 'epoch': 0.92}
 92%|█████████▏| 1790/1953 [9:18:02<49:46, 18.32s/it] 92%|█████████▏| 1791/1953 [9:18:20<49:26, 18.31s/it] 92%|█████████▏| 1792/1953 [9:18:38<48:35, 18.11s/it] 92%|█████████▏| 1793/1953 [9:18:56<48:27, 18.17s/it] 92%|█████████▏| 1794/1953 [9:19:14<48:12, 18.19s/it] 92%|█████████▏| 1795/1953 [9:19:35<49:41, 18.87s/it] 92%|█████████▏| 1796/1953 [9:19:54<49:16, 18.83s/it] 92%|█████████▏| 1797/1953 [9:20:12<48:19, 18.58s/it] 92%|█████████▏| 1798/1953 [9:20:30<47:37, 18.44s/it] 92%|█████████▏| 1799/1953 [9:20:48<47:23, 18.46s/it] 92%|█████████▏| 1800/1953 [9:21:06<46:27, 18.22s/it]                                                     {'loss': 0.9087, 'learning_rate': 7.533465794579558e-07, 'epoch': 0.92}
 92%|█████████▏| 1800/1953 [9:21:06<46:27, 18.22s/it] 92%|█████████▏| 1801/1953 [9:21:24<46:13, 18.24s/it] 92%|█████████▏| 1802/1953 [9:21:43<46:11, 18.36s/it] 92%|█████████▏| 1803/1953 [9:22:03<47:32, 19.01s/it] 92%|█████████▏| 1804/1953 [9:22:22<46:45, 18.83s/it] 92%|█████████▏| 1805/1953 [9:22:40<46:15, 18.75s/it] 92%|█████████▏| 1806/1953 [9:22:59<45:37, 18.63s/it] 93%|█████████▎| 1807/1953 [9:23:17<44:51, 18.44s/it] 93%|█████████▎| 1808/1953 [9:23:35<44:40, 18.49s/it] 93%|█████████▎| 1809/1953 [9:23:54<44:34, 18.57s/it] 93%|█████████▎| 1810/1953 [9:24:12<44:02, 18.48s/it]                                                     {'loss': 0.9087, 'learning_rate': 6.585086657126177e-07, 'epoch': 0.93}
 93%|█████████▎| 1810/1953 [9:24:12<44:02, 18.48s/it] 93%|█████████▎| 1811/1953 [9:24:31<44:11, 18.68s/it] 93%|█████████▎| 1812/1953 [9:24:50<43:30, 18.52s/it] 93%|█████████▎| 1813/1953 [9:25:07<42:34, 18.24s/it] 93%|█████████▎| 1814/1953 [9:25:26<42:32, 18.36s/it] 93%|█████████▎| 1815/1953 [9:25:44<42:08, 18.32s/it] 93%|█████████▎| 1816/1953 [9:26:02<41:14, 18.06s/it] 93%|█████████▎| 1817/1953 [9:26:20<40:56, 18.06s/it] 93%|█████████▎| 1818/1953 [9:26:38<40:46, 18.12s/it] 93%|█████████▎| 1819/1953 [9:26:57<41:10, 18.44s/it] 93%|█████████▎| 1820/1953 [9:27:15<40:46, 18.40s/it]                                                     {'loss': 0.8935, 'learning_rate': 5.699691931296463e-07, 'epoch': 0.93}
 93%|█████████▎| 1820/1953 [9:27:15<40:46, 18.40s/it] 93%|█████████▎| 1821/1953 [9:27:33<40:07, 18.24s/it] 93%|█████████▎| 1822/1953 [9:27:51<39:25, 18.05s/it] 93%|█████████▎| 1823/1953 [9:28:09<39:15, 18.12s/it] 93%|█████████▎| 1824/1953 [9:28:29<39:54, 18.56s/it] 93%|█████████▎| 1825/1953 [9:28:49<40:47, 19.12s/it] 93%|█████████▎| 1826/1953 [9:29:07<39:48, 18.81s/it] 94%|█████████▎| 1827/1953 [9:29:25<38:57, 18.55s/it] 94%|█████████▎| 1828/1953 [9:29:43<38:30, 18.48s/it] 94%|█████████▎| 1829/1953 [9:30:02<38:20, 18.55s/it] 94%|█████████▎| 1830/1953 [9:30:21<38:04, 18.57s/it]                                                     {'loss': 0.9253, 'learning_rate': 4.877510715895817e-07, 'epoch': 0.94}
 94%|█████████▎| 1830/1953 [9:30:21<38:04, 18.57s/it] 94%|█████████▍| 1831/1953 [9:30:39<37:27, 18.42s/it] 94%|█████████▍| 1832/1953 [9:30:58<37:31, 18.61s/it] 94%|█████████▍| 1833/1953 [9:31:16<36:46, 18.39s/it] 94%|█████████▍| 1834/1953 [9:31:34<36:21, 18.33s/it] 94%|█████████▍| 1835/1953 [9:31:53<36:31, 18.57s/it] 94%|█████████▍| 1836/1953 [9:32:11<35:50, 18.38s/it] 94%|█████████▍| 1837/1953 [9:32:30<35:40, 18.46s/it] 94%|█████████▍| 1838/1953 [9:32:47<34:54, 18.21s/it] 94%|█████████▍| 1839/1953 [9:33:06<34:45, 18.29s/it] 94%|█████████▍| 1840/1953 [9:33:25<34:45, 18.45s/it]                                                     {'loss': 0.8935, 'learning_rate': 4.1187557530253105e-07, 'epoch': 0.94}
 94%|█████████▍| 1840/1953 [9:33:25<34:45, 18.45s/it] 94%|█████████▍| 1841/1953 [9:33:43<34:14, 18.34s/it] 94%|█████████▍| 1842/1953 [9:34:02<34:16, 18.53s/it] 94%|█████████▍| 1843/1953 [9:34:20<33:51, 18.46s/it] 94%|█████████▍| 1844/1953 [9:34:38<33:32, 18.47s/it] 94%|█████████▍| 1845/1953 [9:34:57<33:05, 18.39s/it] 95%|█████████▍| 1846/1953 [9:35:15<32:30, 18.23s/it] 95%|█████████▍| 1847/1953 [9:35:32<31:52, 18.04s/it] 95%|█████████▍| 1848/1953 [9:35:50<31:32, 18.02s/it] 95%|█████████▍| 1849/1953 [9:36:09<31:41, 18.28s/it] 95%|█████████▍| 1850/1953 [9:36:28<31:30, 18.36s/it]                                                     {'loss': 0.887, 'learning_rate': 3.423623373034035e-07, 'epoch': 0.95}
 95%|█████████▍| 1850/1953 [9:36:28<31:30, 18.36s/it] 95%|█████████▍| 1851/1953 [9:36:48<32:14, 18.96s/it] 95%|█████████▍| 1852/1953 [9:37:06<31:43, 18.84s/it] 95%|█████████▍| 1853/1953 [9:37:26<31:33, 18.94s/it] 95%|█████████▍| 1854/1953 [9:37:44<30:57, 18.76s/it] 95%|█████████▍| 1855/1953 [9:38:04<31:09, 19.08s/it] 95%|█████████▌| 1856/1953 [9:38:22<30:25, 18.82s/it] 95%|█████████▌| 1857/1953 [9:38:40<29:41, 18.56s/it] 95%|█████████▌| 1858/1953 [9:38:58<29:21, 18.54s/it] 95%|█████████▌| 1859/1953 [9:39:17<29:07, 18.59s/it] 95%|█████████▌| 1860/1953 [9:39:36<28:50, 18.61s/it]                                                     {'loss': 0.9089, 'learning_rate': 2.7922934437178695e-07, 'epoch': 0.95}
 95%|█████████▌| 1860/1953 [9:39:36<28:50, 18.61s/it] 95%|█████████▌| 1861/1953 [9:39:53<27:58, 18.24s/it] 95%|█████████▌| 1862/1953 [9:40:12<27:42, 18.27s/it] 95%|█████████▌| 1863/1953 [9:40:30<27:21, 18.24s/it] 95%|█████████▌| 1864/1953 [9:40:48<27:00, 18.20s/it] 95%|█████████▌| 1865/1953 [9:41:06<26:37, 18.15s/it] 96%|█████████▌| 1866/1953 [9:41:24<26:30, 18.28s/it] 96%|█████████▌| 1867/1953 [9:41:42<25:55, 18.09s/it] 96%|█████████▌| 1868/1953 [9:42:00<25:42, 18.14s/it] 96%|█████████▌| 1869/1953 [9:42:19<25:41, 18.35s/it] 96%|█████████▌| 1870/1953 [9:42:38<25:24, 18.36s/it]                                                     {'loss': 0.9256, 'learning_rate': 2.2249293237781854e-07, 'epoch': 0.96}
 96%|█████████▌| 1870/1953 [9:42:38<25:24, 18.36s/it] 96%|█████████▌| 1871/1953 [9:42:57<25:28, 18.64s/it] 96%|█████████▌| 1872/1953 [9:43:15<24:52, 18.42s/it] 96%|█████████▌| 1873/1953 [9:43:34<24:55, 18.70s/it] 96%|█████████▌| 1874/1953 [9:43:52<24:28, 18.59s/it] 96%|█████████▌| 1875/1953 [9:44:11<24:17, 18.68s/it] 96%|█████████▌| 1876/1953 [9:44:29<23:45, 18.51s/it] 96%|█████████▌| 1877/1953 [9:44:48<23:25, 18.49s/it] 96%|█████████▌| 1878/1953 [9:45:06<23:01, 18.42s/it] 96%|█████████▌| 1879/1953 [9:45:24<22:37, 18.34s/it] 96%|█████████▋| 1880/1953 [9:45:43<22:37, 18.59s/it]                                                     {'loss': 0.9077, 'learning_rate': 1.721677820552242e-07, 'epoch': 0.96}
 96%|█████████▋| 1880/1953 [9:45:43<22:37, 18.59s/it] 96%|█████████▋| 1881/1953 [9:46:02<22:10, 18.47s/it] 96%|█████████▋| 1882/1953 [9:46:19<21:34, 18.23s/it] 96%|█████████▋| 1883/1953 [9:46:37<21:09, 18.13s/it] 96%|█████████▋| 1884/1953 [9:46:56<21:10, 18.42s/it] 97%|█████████▋| 1885/1953 [9:47:15<20:55, 18.46s/it] 97%|█████████▋| 1886/1953 [9:47:33<20:26, 18.30s/it] 97%|█████████▋| 1887/1953 [9:47:51<20:04, 18.25s/it] 97%|█████████▋| 1888/1953 [9:48:09<19:41, 18.18s/it] 97%|█████████▋| 1889/1953 [9:48:27<19:19, 18.12s/it] 97%|█████████▋| 1890/1953 [9:48:46<19:09, 18.25s/it]                                                     {'loss': 0.8983, 'learning_rate': 1.2826691520262114e-07, 'epoch': 0.97}
 97%|█████████▋| 1890/1953 [9:48:46<19:09, 18.25s/it] 97%|█████████▋| 1891/1953 [9:49:04<18:58, 18.36s/it] 97%|█████████▋| 1892/1953 [9:49:23<18:57, 18.65s/it] 97%|█████████▋| 1893/1953 [9:49:42<18:39, 18.66s/it] 97%|█████████▋| 1894/1953 [9:50:00<18:13, 18.53s/it] 97%|█████████▋| 1895/1953 [9:50:19<17:56, 18.56s/it] 97%|█████████▋| 1896/1953 [9:50:37<17:20, 18.25s/it] 97%|█████████▋| 1897/1953 [9:50:55<17:03, 18.27s/it] 97%|█████████▋| 1898/1953 [9:51:13<16:39, 18.18s/it] 97%|█████████▋| 1899/1953 [9:51:31<16:17, 18.10s/it] 97%|█████████▋| 1900/1953 [9:51:50<16:10, 18.31s/it]                                                     {'loss': 0.9133, 'learning_rate': 9.08016913140991e-08, 'epoch': 0.97}
 97%|█████████▋| 1900/1953 [9:51:50<16:10, 18.31s/it] 97%|█████████▋| 1901/1953 [9:52:09<16:07, 18.61s/it] 97%|█████████▋| 1902/1953 [9:52:27<15:44, 18.51s/it] 97%|█████████▋| 1903/1953 [9:52:45<15:19, 18.39s/it] 97%|█████████▋| 1904/1953 [9:53:04<15:02, 18.41s/it] 98%|█████████▊| 1905/1953 [9:53:23<15:02, 18.79s/it] 98%|█████████▊| 1906/1953 [9:53:42<14:38, 18.70s/it] 98%|█████████▊| 1907/1953 [9:54:00<14:18, 18.67s/it] 98%|█████████▊| 1908/1953 [9:54:19<14:04, 18.76s/it] 98%|█████████▊| 1909/1953 [9:54:37<13:32, 18.47s/it] 98%|█████████▊| 1910/1953 [9:54:56<13:20, 18.60s/it]                                                     {'loss': 0.9286, 'learning_rate': 5.978180463989958e-08, 'epoch': 0.98}
 98%|█████████▊| 1910/1953 [9:54:56<13:20, 18.60s/it] 98%|█████████▊| 1911/1953 [9:55:15<13:05, 18.70s/it] 98%|█████████▊| 1912/1953 [9:55:33<12:41, 18.58s/it] 98%|█████████▊| 1913/1953 [9:55:52<12:25, 18.63s/it] 98%|█████████▊| 1914/1953 [9:56:10<12:00, 18.46s/it] 98%|█████████▊| 1915/1953 [9:56:28<11:36, 18.33s/it] 98%|█████████▊| 1916/1953 [9:56:47<11:23, 18.48s/it] 98%|█████████▊| 1917/1953 [9:57:06<11:08, 18.57s/it] 98%|█████████▊| 1918/1953 [9:57:23<10:39, 18.27s/it] 98%|█████████▊| 1919/1953 [9:57:41<10:15, 18.10s/it] 98%|█████████▊| 1920/1953 [9:58:00<10:04, 18.33s/it]                                                     {'loss': 0.8965, 'learning_rate': 3.521528167800547e-08, 'epoch': 0.98}
 98%|█████████▊| 1920/1953 [9:58:00<10:04, 18.33s/it] 98%|█████████▊| 1921/1953 [9:58:18<09:46, 18.33s/it] 98%|█████████▊| 1922/1953 [9:58:37<09:28, 18.34s/it] 98%|█████████▊| 1923/1953 [9:58:54<09:05, 18.17s/it] 99%|█████████▊| 1924/1953 [9:59:15<09:05, 18.80s/it] 99%|█████████▊| 1925/1953 [9:59:33<08:40, 18.58s/it] 99%|█████████▊| 1926/1953 [9:59:50<08:14, 18.33s/it] 99%|█████████▊| 1927/1953 [10:00:10<08:04, 18.64s/it] 99%|█████████▊| 1928/1953 [10:00:28<07:45, 18.61s/it] 99%|█████████▉| 1929/1953 [10:00:47<07:24, 18.52s/it] 99%|█████████▉| 1930/1953 [10:01:06<07:11, 18.77s/it]                                                      {'loss': 0.8974, 'learning_rate': 1.7108479097252548e-08, 'epoch': 0.99}
 99%|█████████▉| 1930/1953 [10:01:06<07:11, 18.77s/it] 99%|█████████▉| 1931/1953 [10:01:24<06:47, 18.52s/it] 99%|█████████▉| 1932/1953 [10:01:42<06:27, 18.44s/it] 99%|█████████▉| 1933/1953 [10:02:00<06:07, 18.36s/it] 99%|█████████▉| 1934/1953 [10:02:18<05:46, 18.26s/it] 99%|█████████▉| 1935/1953 [10:02:37<05:27, 18.22s/it] 99%|█████████▉| 1936/1953 [10:02:56<05:15, 18.57s/it] 99%|█████████▉| 1937/1953 [10:03:14<04:52, 18.30s/it] 99%|█████████▉| 1938/1953 [10:03:32<04:33, 18.21s/it] 99%|█████████▉| 1939/1953 [10:03:50<04:14, 18.15s/it] 99%|█████████▉| 1940/1953 [10:04:09<03:59, 18.42s/it]                                                      {'loss': 0.9119, 'learning_rate': 5.466082092531188e-09, 'epoch': 0.99}
 99%|█████████▉| 1940/1953 [10:04:09<03:59, 18.42s/it] 99%|█████████▉| 1941/1953 [10:04:27<03:41, 18.48s/it] 99%|█████████▉| 1942/1953 [10:04:46<03:23, 18.49s/it] 99%|█████████▉| 1943/1953 [10:05:04<03:04, 18.44s/it]100%|█████████▉| 1944/1953 [10:05:27<02:58, 19.79s/it]100%|█████████▉| 1945/1953 [10:05:46<02:36, 19.53s/it]100%|█████████▉| 1946/1953 [10:06:05<02:14, 19.25s/it]100%|█████████▉| 1947/1953 [10:06:23<01:54, 19.12s/it]100%|█████████▉| 1948/1953 [10:06:42<01:35, 19.10s/it]100%|█████████▉| 1949/1953 [10:07:03<01:17, 19.42s/it]100%|█████████▉| 1950/1953 [10:07:22<00:57, 19.30s/it]                                                      {'loss': 0.9099, 'learning_rate': 2.911031724561752e-10, 'epoch': 1.0}
100%|█████████▉| 1950/1953 [10:07:22<00:57, 19.30s/it]100%|█████████▉| 1951/1953 [10:07:39<00:37, 18.72s/it]100%|█████████▉| 1952/1953 [10:07:57<00:18, 18.45s/it]100%|██████████| 1953/1953 [10:08:15<00:00, 18.42s/it][INFO|trainer.py:1988] 2024-02-26 04:16:01,278 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                      {'train_runtime': 36495.6948, 'train_samples_per_second': 27.4, 'train_steps_per_second': 0.054, 'train_loss': 1.0990104533682343, 'epoch': 1.0}
100%|██████████| 1953/1953 [10:08:15<00:00, 18.42s/it]100%|██████████| 1953/1953 [10:08:15<00:00, 18.69s/it]
[INFO|trainer.py:2979] 2024-02-26 04:16:15,003 >> Saving model checkpoint to /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1
/home/yangdezhao/zhouh_temp/peft/src/peft/utils/save_and_load.py:151: UserWarning: Could not find a config file in /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf - will assume that the vocabulary was not modified.
  warnings.warn(
[2024-02-26 04:16:21,786] [INFO] [launch.py:347:main] Process 1975221 exits successfully.
[2024-02-26 04:16:23,789] [INFO] [launch.py:347:main] Process 1975223 exits successfully.
[2024-02-26 04:16:24,790] [INFO] [launch.py:347:main] Process 1975222 exits successfully.
[INFO|tokenization_utils_base.py:2435] 2024-02-26 04:16:28,294 >> tokenizer config file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-26 04:16:28,294 >> Special tokens file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1/special_tokens_map.json
***** train metrics *****
  epoch                    =         1.0
  train_loss               =       1.099
  train_runtime            = 10:08:15.69
  train_samples_per_second =        27.4
  train_steps_per_second   =       0.054
Figure saved: /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/moe-mt-enis-top1/training_loss.png
02/26/2024 04:16:29 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-02-26 04:16:29,304 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-02-26 04:16:34,802] [INFO] [launch.py:347:main] Process 1975220 exits successfully.
