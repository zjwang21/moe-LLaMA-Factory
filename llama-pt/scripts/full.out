[2024-02-08 23:10:22,494] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-08 23:10:23,524] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-02-08 23:10:23,525] [INFO] [runner.py:568:main] cmd = /home/yangdezhao/anaconda3/envs/zhouh/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json --stage pt --model_name_or_path /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf --flash_attn --do_train --dataset skypile_1b,is_1b --preprocessing_num_workers 20 --mix_strategy concat --cutoff_len 2048 --finetuning_type full --output_dir /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis --overwrite_output_dir --per_device_train_batch_size 16 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 3 --save_only_model --save_steps 500 --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-02-08 23:10:25,620] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-08 23:10:26,604] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-02-08 23:10:26,604] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-02-08 23:10:26,604] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-02-08 23:10:26,604] [INFO] [launch.py:163:main] dist_world_size=4
[2024-02-08 23:10:26,604] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-02-08 23:10:29,921] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-08 23:10:29,988] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-08 23:10:29,989] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-08 23:10:30,525] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-08 23:10:32,059] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-08 23:10:32,059] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-08 23:10:32,232] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-08 23:10:32,308] [INFO] [comm.py:637:init_distributed] cdb=None
02/08/2024 23:10:32 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/08/2024 23:10:32 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/runs/Feb08_23-10-32_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/08/2024 23:10:32 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/08/2024 23:10:32 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/runs/Feb08_23-10-32_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/08/2024 23:10:32 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
02/08/2024 23:10:32 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2024-02-08 23:10:32,801] [INFO] [comm.py:637:init_distributed] cdb=None
02/08/2024 23:10:32 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/08/2024 23:10:32 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/runs/Feb08_23-10-32_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2025] 2024-02-08 23:10:32,890 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2025] 2024-02-08 23:10:32,890 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2025] 2024-02-08 23:10:32,890 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2025] 2024-02-08 23:10:32,890 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2025] 2024-02-08 23:10:32,890 >> loading file tokenizer.json
02/08/2024 23:10:32 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
02/08/2024 23:10:32 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/runs/Feb08_23-10-32_ubuntu-WS-C621E-SAGE-Series,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[INFO|configuration_utils.py:727] 2024-02-08 23:10:33,004 >> loading configuration file /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:792] 2024-02-08 23:10:33,005 >> Model config LlamaConfig {
  "_name_or_path": "/home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 32000
}

02/08/2024 23:10:33 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
[INFO|modeling_utils.py:3473] 2024-02-08 23:10:33,024 >> loading weights file /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1426] 2024-02-08 23:10:33,025 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[WARNING|logging.py:329] 2024-02-08 23:10:33,025 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[WARNING|modeling_utils.py:1517] 2024-02-08 23:10:33,026 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:826] 2024-02-08 23:10:33,028 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

02/08/2024 23:10:33 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.77s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]
02/08/2024 23:10:36 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/08/2024 23:10:36 - INFO - llmtuner.model.adapter - Fine-tuning method: Full
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]
02/08/2024 23:10:37 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/08/2024 23:10:37 - INFO - llmtuner.model.adapter - Fine-tuning method: Full
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.96s/it]
[INFO|modeling_utils.py:4350] 2024-02-08 23:10:39,788 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4358] 2024-02-08 23:10:39,788 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:779] 2024-02-08 23:10:39,794 >> loading configuration file /home/yangdezhao/zhouh_temp/models/Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:826] 2024-02-08 23:10:39,794 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

02/08/2024 23:10:39 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/08/2024 23:10:39 - INFO - llmtuner.model.adapter - Fine-tuning method: Full
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.12s/it]
02/08/2024 23:10:40 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
02/08/2024 23:10:40 - INFO - llmtuner.model.adapter - Fine-tuning method: Full
02/08/2024 23:10:43 - INFO - llmtuner.model.loader - trainable params: 6738415616 || all params: 6738415616 || trainable%: 100.0000
02/08/2024 23:10:43 - INFO - llmtuner.data.template - Add pad token: </s>
02/08/2024 23:10:44 - INFO - llmtuner.model.loader - trainable params: 6738415616 || all params: 6738415616 || trainable%: 100.0000
02/08/2024 23:10:44 - INFO - llmtuner.data.template - Add pad token: </s>
02/08/2024 23:10:45 - INFO - llmtuner.model.loader - trainable params: 6738415616 || all params: 6738415616 || trainable%: 100.0000
02/08/2024 23:10:45 - INFO - llmtuner.data.template - Add pad token: </s>
02/08/2024 23:10:45 - INFO - llmtuner.model.loader - trainable params: 6738415616 || all params: 6738415616 || trainable%: 100.0000
02/08/2024 23:10:45 - INFO - llmtuner.data.template - Add pad token: </s>
02/08/2024 23:10:51 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
Using custom data configuration default-3483f37cf3cee624
Loading Dataset Infos from /home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
Found cached dataset json (/home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Loading Dataset info from /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
Process #0 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00000_of_00020.arrow
Process #1 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00001_of_00020.arrow
Process #2 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00002_of_00020.arrow
Process #3 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00003_of_00020.arrow
Process #4 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00004_of_00020.arrow
Process #5 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00005_of_00020.arrow
Process #6 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00006_of_00020.arrow
Process #7 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00007_of_00020.arrow
Process #8 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00008_of_00020.arrow
Process #9 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00009_of_00020.arrow
Process #10 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00010_of_00020.arrow
Process #11 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00011_of_00020.arrow
Process #12 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00012_of_00020.arrow
Process #13 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00013_of_00020.arrow
Process #14 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00014_of_00020.arrow
Process #15 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00015_of_00020.arrow
Process #16 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00016_of_00020.arrow
Process #17 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00017_of_00020.arrow
Process #18 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00018_of_00020.arrow
Process #19 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_00019_of_00020.arrow
Loading cached processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-207b6dfe032dc944_*_of_00020.arrow
Concatenating 20 shards
02/08/2024 23:10:57 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
Using custom data configuration default-5a96c05fd21bf34e
Loading Dataset Infos from /home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
Found cached dataset json (/home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Loading Dataset info from /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
Process #0 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00000_of_00020.arrow
Process #1 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00001_of_00020.arrow
Process #2 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00002_of_00020.arrow
Process #3 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00003_of_00020.arrow
Process #4 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00004_of_00020.arrow
Process #5 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00005_of_00020.arrow
Process #6 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00006_of_00020.arrow
Process #7 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00007_of_00020.arrow
Process #8 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00008_of_00020.arrow
Process #9 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00009_of_00020.arrow
Process #10 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00010_of_00020.arrow
Process #11 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00011_of_00020.arrow
Process #12 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00012_of_00020.arrow
Process #13 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00013_of_00020.arrow
Process #14 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00014_of_00020.arrow
Process #15 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00015_of_00020.arrow
Process #16 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00016_of_00020.arrow
Process #17 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00017_of_00020.arrow
Process #18 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00018_of_00020.arrow
Process #19 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_00019_of_00020.arrow
Loading cached processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-5a96c05fd21bf34e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-677f5d15a1d80ac3_*_of_00020.arrow
Concatenating 20 shards
Process #0 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00000_of_00020.arrow
Process #1 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00001_of_00020.arrow
Process #2 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00002_of_00020.arrow
Process #3 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00003_of_00020.arrow
Process #4 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00004_of_00020.arrow
Process #5 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00005_of_00020.arrow
Process #6 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00006_of_00020.arrow
Process #7 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00007_of_00020.arrow
Process #8 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00008_of_00020.arrow
Process #9 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00009_of_00020.arrow
Process #10 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00010_of_00020.arrow
Process #11 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00011_of_00020.arrow
Process #12 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00012_of_00020.arrow
Process #13 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00013_of_00020.arrow
Process #14 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00014_of_00020.arrow
Process #15 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00015_of_00020.arrow
Process #16 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00016_of_00020.arrow
Process #17 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00017_of_00020.arrow
Process #18 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00018_of_00020.arrow
Process #19 will write at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_00019_of_00020.arrow
Loading cached processed dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d411fb54c61397da_*_of_00020.arrow
Concatenating 20 shards
input_ids:
[29871, 30287, 31450, 235, 192, 169, 30888, 30214, 31530, 232, 179, 191, 233, 140, 155, 31885, 31600, 30346, 30533, 235, 192, 169, 30214, 29896, 29889, 29947, 232, 144, 138, 29902, 29899, 29946, 30910, 30846, 31429, 30214, 232, 187, 169, 31462, 31859, 234, 177, 180, 30210, 29896, 31859, 15633, 29911, 30214, 231, 192, 145, 231, 190, 186, 30544, 232, 151, 177, 30584, 29871, 30888, 30698, 31141, 232, 193, 132, 30383, 29899, 30666, 234, 134, 176, 30607, 30658, 233, 145, 149, 31780, 233, 167, 136, 29899, 31352, 236, 149, 168, 232, 143, 156, 31174, 30752, 29899, 235, 150, 160, 234, 140, 156, 31903, 31092, 29899, 31676, 30815, 30872, 232, 167, 138, 30893, 30494, 29899, 235, 194, 159, 31101, 232, 147, 178, 30846, 29899, 31679, 30846, 30408, 234, 173, 154, 29899, 232, 171, 180, 31616, 30689, 31021, 31542, 30858, 29899, 30486, 31613, 31382, 30607, 317, 5098, 4330, 29979, 847, 27827, 29899, 1164, 29903, 30383, 29899, 232, 131, 149, 235, 192, 169, 233, 148, 135, 31551, 31584, 29899, 232, 139, 188, 235, 192, 169, 235, 193, 136, 31931, 29899, 31679, 30846, 31631, 31649, 29899, 232, 135, 194, 234, 174, 168, 30670, 30753, 236, 151, 132, 13, 30744, 30417, 235, 192, 169, 235, 193, 137, 232, 160, 138, 30768, 31138, 21576, 30419, 30768, 30406, 233, 180, 192, 235, 192, 169, 30409, 29896, 29945, 29945, 31888, 29871, 31439, 235, 178, 132, 233, 166, 131, 31851, 30214, 31666, 231, 187, 151, 232, 136, 144, 235, 183, 188, 31302, 231, 193, 158, 29945, 29900, 29900, 29900, 8848, 31391, 29929, 29900, 30408, 30753, 235, 192, 169, 30982, 31273, 30584, 232, 136, 144, 235, 183, 188, 31545, 30287, 234, 177, 180, 233, 181, 188, 29974, 29906, 30936, 31640, 31429, 233, 181, 188, 29974, 30287, 30936, 232, 136, 144, 235, 183, 188, 30928, 235, 192, 177, 30495, 30956, 31520, 31358, 30584, 13, 236, 157, 185, 31360, 30909, 12300, 6028, 1114, 30893, 232, 158, 165, 30419, 30666, 233, 142, 194, 30257, 30878, 30257, 233, 180, 192, 235, 192, 169, 30893, 232, 158, 165, 30409, 30888, 235, 147, 168, 21576, 30419, 30768, 30406, 233, 180, 192, 235, 192, 169, 30409, 233, 154, 154, 30557, 31399, 234, 140, 143, 30214, 233, 142, 168, 30417, 30753, 232, 168, 154, 233, 180, 192, 235, 192, 169, 31520, 31358, 30584, 233, 175, 165, 235, 194, 145, 30805, 234, 145, 172, 30267, 13, 2, 306, 29950, 30705, 31041, 234, 169, 190, 30869, 233, 182, 184, 31573, 31545, 31633, 235, 183, 171, 233, 187, 172, 30898, 30573, 29899, 29906, 29900, 229, 135, 134, 30739, 29896, 29900, 29945, 229, 135, 134, 30214, 31383, 30698, 30594, 236, 138, 138, 30406, 232, 137, 186, 232, 144, 183, 233, 145, 173, 233, 153, 192, 30682, 31573, 31545, 31100, 30528, 233, 187, 172, 30898, 30210, 31633, 235, 183, 171, 30214, 236, 131, 133, 30406, 30909, 30705, 31041, 30330, 30814, 233, 181, 188, 30330, 232, 137, 185, 30659, 30330, 31679, 31074, 30330, 31420, 234, 189, 187, 30330, 31855, 31399, 30330, 31072, 235, 144, 178, 30330, 234, 145, 178, 30982, 30330, 232, 189, 162, 30716, 31548, 30687, 30503, 30733, 30494, 234, 189, 167, 234, 190, 183, 31184, 30448, 31729, 30406, 30909, 31573, 31545, 232, 147, 135, 31893, 235, 136, 147, 235, 157, 131, 30210, 31391, 30413, 232, 136, 132, 235, 177, 187, 233, 180, 164, 233, 162, 150, 30210, 30832, 231, 191, 191, 30909, 30716, 30210, 31633, 235, 183, 171, 30267, 13, 30287, 30330, 29902, 29950, 30883, 232, 144, 170, 30607, 30705, 31041, 234, 169, 190, 30869, 233, 182, 184, 231, 189, 170, 31399, 233, 169, 133, 235, 194, 179, 30383, 13, 29902, 29950, 30883, 30705, 31041, 233, 182, 184, 30392, 31166, 234, 189, 170, 31166, 232, 147, 187, 30419, 235, 192, 183, 31331, 232, 147, 187, 30752, 30409, 233, 133, 175, 235, 138, 133, 30607, 234, 169, 190, 30869, 233, 182, 184, 30214, 231, 193, 158, 31573, 31545, 30413, 232, 147, 174, 232, 158, 189, 30988, 236, 165, 154, 234, 181, 149, 232, 136, 186, 30417, 235, 136, 147, 235, 157, 131, 30952, 30330, 234, 181, 155, 30898, 30832, 231, 191, 191, 30716, 30210, 233, 185, 181, 30988, 30267, 31149, 31062, 31410, 30330, 236, 165, 160, 30495, 30952, 30815, 30503, 232, 179, 189, 232, 178, 187, 31184, 31944, 236, 138, 138, 30406, 31062, 232, 138, 137, 29096, 29906, 29947, 29945, 29947, 30214, 232, 136, 186, 30417, 30952, 30815, 235, 143, 134, 232, 158, 183, 31566, 30330, 31944, 234, 145, 138, 30528, 30330, 30015, 30457, 30705, 30024, 30716, 30606, 30528, 30503, 234, 190, 183, 31273, 30525, 231, 193, 194, 31184, 31141, 30940, 30214, 31149, 31944, 234, 145, 138, 31419, 29943, 30883, 233, 182, 184, 30606, 232, 160, 138, 31302, 30528, 29945, 242, 191, 136, 30214, 30392, 30356, 30613, 233, 145, 171, 31566, 30210, 31669, 30815, 231, 189, 170, 31399, 30267, 13, 29902, 29950, 30883, 30705, 31041, 233, 182, 184, 31573, 31545, 31633, 235, 183, 171, 233, 187, 172, 30898, 30573, 29899, 29906, 29900, 229, 135, 134, 30739, 29896, 29900, 29945, 229, 135, 134, 30214, 31383, 30698, 30594, 236, 138, 138, 30406, 232, 137, 186, 232, 144, 183, 233, 145, 173, 233, 153, 192, 30682, 31573, 31545, 31100, 30528, 233, 187, 172, 30898, 30210, 31633, 235, 183, 171, 30214, 236, 131, 133, 30406, 30909, 30705, 31041, 30330, 30814, 233, 181, 188, 30330, 232, 137, 185, 30659, 30330, 31679, 31074, 30330, 31420, 234, 189, 187, 30330, 31855, 31399, 30330, 31072, 235, 144, 178, 30330, 234, 145, 178, 30982, 30330, 232, 189, 162, 30716, 31548, 30687, 30503, 30733, 30494, 234, 189, 167, 234, 190, 183, 31184, 30448, 31729, 30406, 30909, 31573, 31545, 232, 147, 135, 31893, 235, 136, 147, 235, 157, 131, 30210, 31391, 30413, 232, 136, 132, 235, 177, 187, 233, 180, 164, 233, 162, 150, 30210, 30832, 231, 191, 191, 30909, 30716, 30210, 31633, 235, 183, 171, 30267, 13, 30654, 30606, 31534, 233, 182, 184, 31729, 30893, 232, 158, 165, 30417, 31175, 30539, 30931, 31756, 31729, 31302, 231, 193, 158, 29902, 29950, 30705, 31041, 234, 169, 190, 30869, 233, 182, 184, 31184, 231, 189, 170, 31399, 30689, 31021, 30214, 233, 175, 165, 235, 194, 145, 30805, 31679, 232, 149, 171, 235, 178, 165, 30584, 2, 29871, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 30698, 31522, 31032, 234, 153, 154, 234, 153, 193, 234, 154, 136, 30214, 31333, 233, 142, 172, 30724, 31835, 30210, 232, 143, 190, 30963, 30392, 31838, 31190, 30908, 30698, 30210, 30214, 30868, 234, 156, 159, 236, 166, 145, 30392, 30287, 31893, 31838, 31190, 236, 164, 192, 232, 158, 189, 30210, 234, 153, 193, 234, 154, 136, 30214, 31032, 234, 153, 154, 31558, 30805, 30953, 30392, 31419, 235, 193, 134, 232, 158, 179, 236, 157, 193, 30210, 30214, 30744, 30651, 233, 133, 166, 30767, 30847, 30801, 31423, 30417, 31333, 233, 142, 172, 31076, 30724, 235, 170, 135, 30210, 232, 143, 190, 30963, 30210, 31852, 30214, 234, 153, 193, 234, 154, 136, 30953, 30392, 232, 193, 139, 236, 157, 193, 31032, 234, 153, 154, 30210, 30267, 31356, 31882, 30214, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 30557, 30806, 31238, 235, 177, 172, 232, 136, 179, 30765, 30275, 31367, 30868, 234, 156, 159, 236, 166, 145, 232, 143, 190, 30963, 30210, 30805, 31999, 30257, 30613, 31633, 234, 190, 144, 30287, 30557, 30267, 13, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 13, 29896, 30330, 30868, 234, 156, 159, 236, 166, 145, 30287, 235, 139, 175, 30910, 30486, 30505, 235, 166, 187, 236, 159, 181, 30210, 234, 157, 177, 235, 133, 167, 30429, 30806, 30214, 30910, 234, 154, 136, 31120, 31117, 30868, 233, 153, 148, 30806, 234, 170, 178, 30446, 30214, 30354, 31180, 31022, 30214, 30768, 31190, 31557, 30417, 232, 138, 163, 232, 160, 154, 31391, 30767, 30392, 232, 138, 163, 30940, 30267, 30847, 30413, 234, 152, 156, 31474, 30210, 31852, 30214, 30868, 233, 153, 148, 234, 154, 136, 30910, 31599, 31238, 30437, 31305, 30494, 31022, 30354, 30210, 233, 153, 148, 232, 160, 154, 30910, 31599, 30780, 30287, 31558, 30214, 31943, 235, 138, 183, 30257, 30806, 234, 170, 178, 30210, 30868, 233, 153, 148, 234, 154, 136, 30267, 13, 29906, 30330, 30868, 234, 156, 159, 236, 166, 145, 30658, 31117, 30868, 233, 153, 148, 234, 154, 136, 30910, 234, 154, 136, 31352, 30688, 235, 170, 140, 30952, 30214, 233, 133, 166, 31548, 30636, 30956, 234, 154, 149, 233, 135, 162, 30214, 31325, 231, 187, 151, 233, 133, 166, 31548, 30210, 235, 133, 143, 235, 133, 167, 31066, 30746, 233, 185, 169, 233, 190, 148, 30214, 31352, 236, 182, 161, 232, 180, 148, 31423, 30417, 235, 147, 145, 234, 191, 172, 30210, 30746, 31133, 30544, 31424, 30267, 30810, 31238, 235, 138, 183, 30785, 232, 193, 139, 30923, 234, 154, 136, 30313, 31352, 30545, 31436, 30594, 30210, 30910, 31424, 30868, 233, 153, 148, 234, 154, 136, 30214, 31979, 31420, 30494, 30743, 30868, 233, 153, 148, 234, 154, 136, 30210, 31174, 30287, 233, 176, 168, 30748, 233, 152, 166, 30267, 13, 29941, 30330, 30868, 233, 153, 148, 30437, 236, 157, 146, 234, 160, 131, 234, 153, 193, 234, 154, 136, 30210, 30666, 30908, 236, 165, 159, 31085, 236, 131, 147, 233, 187, 147, 31462, 31947, 30214, 31267, 31149, 31221, 234, 157, 177, 235, 133, 167, 30210, 31993, 30967, 30953, 30437, 31844, 30805, 30267, 30868, 234, 156, 159, 236, 166, 145, 31120, 31117, 30392, 30417, 30287, 31959, 31141, 30210, 31141, 232, 193, 132, 30210, 30214, 30868, 234, 156, 159, 236, 166, 145, 234, 157, 177, 233, 144, 162, 30467, 30505, 31120, 31117, 30287, 235, 139, 175, 30392, 233, 184, 136, 30868, 31085, 30210, 30214, 30868, 234, 156, 159, 236, 166, 145, 30868, 233, 153, 148, 30210, 31109, 232, 158, 183, 30437, 30544, 31424, 30287, 31217, 31935, 31935, 232, 138, 187, 31558, 30210, 234, 133, 145, 234, 154, 138, 30952, 233, 157, 154, 31869, 31085, 31217, 234, 189, 188, 30214, 30287, 235, 139, 175, 30769, 30437, 31695, 234, 190, 176, 30544, 31424, 232, 138, 163, 30502, 30900, 31117, 30267, 13, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 30768, 31138, 30429, 30806, 232, 136, 179, 30765, 30275, 31367, 30868, 234, 156, 159, 236, 166, 145, 232, 143, 190, 30963, 30210, 30210, 31633, 234, 190, 144, 30214, 30257, 30613, 30783, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30417, 30743, 232, 193, 139, 30257, 30210, 30743, 31201, 30267, 31424, 30505, 30672, 31381, 31290, 31412, 30743, 31201, 31050, 30743, 30868, 234, 156, 159, 236, 166, 145, 233, 131, 145, 31882, 232, 141, 161, 30214, 31356, 31882, 30672, 31381, 30953, 31370, 31751, 31043, 30397, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 233, 131, 145, 31882, 31475, 236, 165, 135, 236, 155, 181, 30214, 30672, 31381, 31370, 31751, 30505, 30486, 31704, 30275, 234, 170, 178, 233, 161, 132, 236, 165, 135, 236, 155, 181, 234, 153, 193, 234, 154, 136, 30214, 30810, 31819, 30672, 31381, 31979, 30815, 30210, 31617, 31072, 234, 153, 193, 234, 154, 136, 30214, 232, 138, 146, 31022, 234, 153, 193, 234, 154, 136, 232, 187, 169, 30805, 30210, 232, 144, 180, 232, 177, 182, 30267, 13, 2, 29871, 30287, 30470, 30287, 30898, 233, 157, 148, 232, 132, 138, 232, 144, 182, 30998, 31026, 31020, 30214, 30573, 30446, 233, 159, 142, 31373, 232, 138, 137, 232, 167, 138, 30670, 233, 145, 149, 31704, 30846, 30210, 30613, 31143, 30682, 31424, 30505, 233, 141, 168, 235, 178, 190, 233, 157, 148, 31117, 30395, 232, 193, 135, 234, 146, 176, 30214, 30573, 30446, 233, 159, 142, 31373, 31138, 30287, 30502, 232, 136, 136, 233, 190, 164, 31704, 31074, 30210, 232, 132, 138, 31117, 29871, 30429, 31338, 30533, 30940, 30383, 31502, 30333, 30395, 31885, 31730, 30813, 30330, 31947, 30716, 232, 162, 154, 30330, 31476, 30395, 29871, 30986, 235, 164, 166, 30330, 232, 180, 178, 31649, 30330, 236, 150, 159, 236, 151, 166, 233, 188, 193, 30330, 30998, 31867, 233, 193, 182, 29871, 30257, 232, 162, 151, 31169, 235, 147, 134, 30415, 31071, 30330, 31869, 234, 166, 164, 31169, 235, 147, 134, 232, 188, 191, 234, 171, 157, 232, 158, 176, 29871, 30325, 31117, 30383, 29955, 30534, 29896, 29953, 30325, 235, 138, 182, 29947, 30534, 29906, 29955, 30325, 31117, 31016, 29871, 30783]
inputs:
一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁
所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！
隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。
</s> IH化工离心泵输送介质温度为-20℃～105℃，需要时采用冷却措施可输送更高温度的介质，适用于化工、石油、冶金、电力、造纸、食品、制药、环保、废水处理和合成纤维等行业用于输送各种腐蚀的或不允许污染的类似于水的介质。
一、IH型卧式化工离心泵产品概述：
IH型化工泵是单级单吸（轴向吸入）悬臂式离心泵，供输送不含固体颗粒具有腐蚀性、粘度类似水的液体。其标记、额定性能和尺寸等效采用标准ISO2858，具有性能范围广、效率高、“三化”水平高和维修方便等特点，其效率比F型泵平均提高5％，是国家推广的节能产品。
IH型化工泵输送介质温度为-20℃～105℃，需要时采用冷却措施可输送更高温度的介质，适用于化工、石油、冶金、电力、造纸、食品、制药、环保、废水处理和合成纤维等行业用于输送各种腐蚀的或不允许污染的类似于水的介质。
太平洋泵业集团有限公司专业提供IH化工离心泵等产品信息，欢迎来电咨询！</s> 处在发展期的白癜风应该如何治疗?要想治疗疾病，选择正确的医院是非常重要的，白癜风是一种非常顽固的疾病，治疗起来也是比较困难的，所以患者如果没有选择好正规的医院的话，疾病也是很难治疗的。那么，处在发展期的白癜风应该如何治疗?下面就让兰州中研白癜风医院的来给大家介绍一下。
处在发展期的白癜风应该如何治疗?
1、白癜风一般发生在裸露的皮肤上面，发病初期白斑面积小，数量少，通常只有几块或者是几点。如不留意的话，白斑病发展就会形成少数的斑块发展到一起，导致大面积的白斑病。
2、白癜风前期白斑病发病无自觉性，患处部位痒感，而且患处的肌肤外表润滑，无鳞屑没有萎缩的表象出现。这就致使很多病人无法及时的发现白斑病，才造成了白斑病的进一步分散。
3、白斑会随着疾病的加重颜色逐渐变深，与其他皮肤的边界也会越来。白癜风初期是有一些特的特征的，白癜风皮损区在初期一般是浅白色的，白癜风白斑的周围会出现一条微微凸起的炎症性暗红色条纹，一般都会持续出现几个星期。
处在发展期的白癜风应该如何治疗?通过上面兰州中研白癜风医院的的介绍，大家对发展期的白癜风应该有了很大的了解。现在我们已经了解得了白癜风怎么办，那么我们也应该知道白癜风应该怎么去预防，我们应该在生活中积极预防疾病，这样我们才能的控制疾病，减少疾病带来的危害。
</s> 一年一度暑假即将开始，为小朋友准备安排活动的家长可现在报读暑期田径班，为小朋友过一个充满活力的假期 上堂地点：何文田巴富街、深水埗、沙田 青衣、屯门、铜锣湾、将军澳 大埔德萃学校、红磡德萃幼稚园 日期：7月16日至8月27日期间 对
02/08/2024 23:11:05 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
02/08/2024 23:11:05 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
02/08/2024 23:11:05 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
02/08/2024 23:11:11 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
02/08/2024 23:11:11 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
02/08/2024 23:11:11 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
Loading cached shuffled indices for dataset at /home/yangdezhao/.cache/huggingface/datasets/json/default-3483f37cf3cee624/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-aad2ca9cbb685723.arrow
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 975920
})
[INFO|trainer.py:571] 2024-02-08 23:11:13,117 >> Using auto half precision backend
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 975920
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 975920
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 975920
})
[INFO|deepspeed.py:301] 2024-02-08 23:11:13,343 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yangdezhao/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.435891628265381 seconds
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yangdezhao/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.440078020095825 seconds
Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/yangdezhao/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yangdezhao/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.453876256942749 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.603614091873169 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-02-08 23:11:17,609] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
[2024-02-08 23:11:38,058] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-02-08 23:11:38,060] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-02-08 23:11:38,060] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-02-08 23:11:38,072] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-02-08 23:11:38,072] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-02-08 23:11:38,072] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-02-08 23:11:38,072] [INFO] [stage_1_and_2.py:143:__init__] Reduce bucket size 500000000
[2024-02-08 23:11:38,072] [INFO] [stage_1_and_2.py:144:__init__] Allgather bucket size 500000000
[2024-02-08 23:11:38,073] [INFO] [stage_1_and_2.py:145:__init__] CPU Offload: True
[2024-02-08 23:11:38,073] [INFO] [stage_1_and_2.py:146:__init__] Round robin gradient partitioning: False
[2024-02-08 23:12:03,217] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states
[2024-02-08 23:12:03,218] [INFO] [utils.py:792:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 12.86 GB         Max_CA 13 GB 
[2024-02-08 23:12:03,218] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 46.71 GB, percent = 18.6%
[2024-02-08 23:12:10,472] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states
[2024-02-08 23:12:10,472] [INFO] [utils.py:792:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 12.86 GB         Max_CA 13 GB 
[2024-02-08 23:12:10,473] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 106.89 GB, percent = 42.5%
[2024-02-08 23:12:10,473] [INFO] [stage_1_and_2.py:533:__init__] optimizer state initialized
[2024-02-08 23:12:10,659] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer
[2024-02-08 23:12:10,660] [INFO] [utils.py:792:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 12.86 GB         Max_CA 13 GB 
[2024-02-08 23:12:10,660] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 108.73 GB, percent = 43.2%
[2024-02-08 23:12:10,663] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-02-08 23:12:10,663] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-02-08 23:12:10,663] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-02-08 23:12:10,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
[2024-02-08 23:12:10,664] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   amp_enabled .................. False
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   amp_params ................... False
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   bfloat16_enabled ............. True
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa500949180>
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   communication_data_type ...... None
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2024-02-08 23:12:10,665] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   disable_allgather ............ False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   dump_state ................... False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   fp16_enabled ................. False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   global_rank .................. 0
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 4
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 1
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   loss_scale ................... 1.0
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   optimizer_name ............... None
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   optimizer_params ............. None
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   pld_enabled .................. False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   pld_params ................... False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   scheduler_name ............... None
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   scheduler_params ............. None
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2024-02-08 23:12:10,666] [INFO] [config.py:988:print]   sparse_attention ............. None
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   steps_per_print .............. inf
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   train_batch_size ............. 256
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  16
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   world_size ................... 4
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  True
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   zero_enabled ................. True
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2024-02-08 23:12:10,667] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2
[2024-02-08 23:12:10,667] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1721] 2024-02-08 23:12:10,667 >> ***** Running training *****
[INFO|trainer.py:1722] 2024-02-08 23:12:10,667 >>   Num examples = 975,920
[INFO|trainer.py:1723] 2024-02-08 23:12:10,667 >>   Num Epochs = 1
[INFO|trainer.py:1724] 2024-02-08 23:12:10,667 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1727] 2024-02-08 23:12:10,667 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:1728] 2024-02-08 23:12:10,667 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1729] 2024-02-08 23:12:10,667 >>   Total optimization steps = 3,812
[INFO|trainer.py:1730] 2024-02-08 23:12:10,668 >>   Number of trainable parameters = 6,738,415,616
  0%|          | 0/3812 [00:00<?, ?it/s]/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/home/yangdezhao/anaconda3/envs/zhouh/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1290: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/3812 [01:34<100:07:20, 94.58s/it]  0%|          | 2/3812 [03:04<97:05:45, 91.74s/it]   0%|          | 3/3812 [04:33<95:45:37, 90.51s/it]  0%|          | 4/3812 [06:02<95:19:10, 90.11s/it]  0%|          | 5/3812 [07:31<94:52:51, 89.72s/it]  0%|          | 6/3812 [09:01<94:45:56, 89.64s/it]  0%|          | 7/3812 [10:30<94:36:29, 89.51s/it]  0%|          | 8/3812 [12:00<94:33:20, 89.48s/it]  0%|          | 9/3812 [13:29<94:22:30, 89.34s/it]  0%|          | 10/3812 [14:58<94:22:56, 89.37s/it]                                                    {'loss': 3.0274, 'learning_rate': 4.9999151012069176e-05, 'epoch': 0.0}
  0%|          | 10/3812 [14:58<94:22:56, 89.37s/it]  0%|          | 11/3812 [16:27<94:22:46, 89.39s/it]  0%|          | 12/3812 [17:57<94:20:08, 89.37s/it]  0%|          | 13/3812 [19:26<94:10:09, 89.24s/it]  0%|          | 14/3812 [20:55<94:08:38, 89.24s/it]  0%|          | 15/3812 [22:24<93:57:44, 89.09s/it]  0%|          | 16/3812 [23:53<94:01:16, 89.17s/it]  0%|          | 17/3812 [25:22<93:51:43, 89.04s/it]  0%|          | 18/3812 [26:51<93:53:02, 89.08s/it]  0%|          | 19/3812 [28:19<93:37:57, 88.87s/it]  1%|          | 20/3812 [29:48<93:35:54, 88.86s/it]                                                    {'loss': 2.1577, 'learning_rate': 4.999660410593914e-05, 'epoch': 0.01}
  1%|          | 20/3812 [29:48<93:35:54, 88.86s/it]  1%|          | 21/3812 [31:17<93:27:26, 88.75s/it]  1%|          | 22/3812 [32:45<93:25:31, 88.74s/it]  1%|          | 23/3812 [34:14<93:17:47, 88.64s/it]  1%|          | 24/3812 [35:42<93:16:04, 88.64s/it]  1%|          | 25/3812 [37:11<93:10:11, 88.57s/it]  1%|          | 26/3812 [38:40<93:11:56, 88.62s/it]  1%|          | 27/3812 [40:08<93:09:17, 88.60s/it]  1%|          | 28/3812 [41:37<93:09:11, 88.62s/it]  1%|          | 29/3812 [43:05<93:08:17, 88.63s/it]  1%|          | 30/3812 [44:34<93:09:42, 88.68s/it]                                                    {'loss': 1.9279, 'learning_rate': 4.9992359454593307e-05, 'epoch': 0.01}
  1%|          | 30/3812 [44:34<93:09:42, 88.68s/it]  1%|          | 31/3812 [46:03<93:08:35, 88.68s/it]  1%|          | 32/3812 [47:32<93:06:26, 88.67s/it]  1%|          | 33/3812 [49:00<93:04:25, 88.67s/it]  1%|          | 34/3812 [50:29<93:05:52, 88.71s/it]  1%|          | 35/3812 [51:58<93:01:40, 88.67s/it]  1%|          | 36/3812 [53:26<93:03:02, 88.71s/it]  1%|          | 37/3812 [54:55<92:53:49, 88.59s/it]  1%|          | 38/3812 [56:24<92:55:52, 88.65s/it]  1%|          | 39/3812 [57:52<92:49:47, 88.57s/it]  1%|          | 40/3812 [59:21<92:50:49, 88.61s/it]                                                    {'loss': 1.8252, 'learning_rate': 4.998641734632428e-05, 'epoch': 0.01}
  1%|          | 40/3812 [59:21<92:50:49, 88.61s/it]  1%|          | 41/3812 [1:00:49<92:48:15, 88.60s/it]  1%|          | 42/3812 [1:02:18<92:50:32, 88.66s/it]  1%|          | 43/3812 [1:03:46<92:41:25, 88.53s/it]  1%|          | 44/3812 [1:05:15<92:42:55, 88.58s/it]  1%|          | 45/3812 [1:06:43<92:38:11, 88.53s/it]  1%|          | 46/3812 [1:08:12<92:40:57, 88.60s/it]  1%|          | 47/3812 [1:09:41<92:37:49, 88.57s/it]  1%|▏         | 48/3812 [1:11:10<92:45:30, 88.72s/it]  1%|▏         | 49/3812 [1:12:38<92:41:21, 88.67s/it]  1%|▏         | 50/3812 [1:14:07<92:44:29, 88.75s/it]                                                      {'loss': 1.7488, 'learning_rate': 4.9978778184714335e-05, 'epoch': 0.01}
  1%|▏         | 50/3812 [1:14:07<92:44:29, 88.75s/it]  1%|▏         | 51/3812 [1:15:36<92:37:06, 88.65s/it]  1%|▏         | 52/3812 [1:17:04<92:37:25, 88.68s/it]  1%|▏         | 53/3812 [1:18:33<92:36:06, 88.68s/it]  1%|▏         | 54/3812 [1:20:02<92:41:32, 88.80s/it]  1%|▏         | 55/3812 [1:21:31<92:35:53, 88.73s/it]  1%|▏         | 56/3812 [1:22:59<92:33:46, 88.72s/it]  1%|▏         | 57/3812 [1:24:28<92:30:44, 88.69s/it]  2%|▏         | 58/3812 [1:25:57<92:33:14, 88.76s/it]  2%|▏         | 59/3812 [1:27:26<92:31:28, 88.75s/it]  2%|▏         | 60/3812 [1:28:55<92:33:52, 88.81s/it]                                                      {'loss': 1.703, 'learning_rate': 4.9969442488607945e-05, 'epoch': 0.02}
  2%|▏         | 60/3812 [1:28:55<92:33:52, 88.81s/it]  2%|▏         | 61/3812 [1:30:23<92:30:03, 88.78s/it]  2%|▏         | 62/3812 [1:31:52<92:30:59, 88.82s/it]  2%|▏         | 63/3812 [1:33:21<92:22:32, 88.70s/it]  2%|▏         | 64/3812 [1:34:49<92:18:39, 88.67s/it]  2%|▏         | 65/3812 [1:36:18<92:15:19, 88.64s/it]  2%|▏         | 66/3812 [1:37:47<92:15:40, 88.67s/it]  2%|▏         | 67/3812 [1:39:15<92:09:12, 88.59s/it]  2%|▏         | 68/3812 [1:40:44<92:08:21, 88.60s/it]  2%|▏         | 69/3812 [1:42:12<92:08:49, 88.63s/it]  2%|▏         | 70/3812 [1:43:41<92:08:53, 88.65s/it]                                                      {'loss': 1.6815, 'learning_rate': 4.995841089207656e-05, 'epoch': 0.02}
  2%|▏         | 70/3812 [1:43:41<92:08:53, 88.65s/it]  2%|▏         | 71/3812 [1:45:10<92:06:47, 88.64s/it]  2%|▏         | 72/3812 [1:46:38<92:08:35, 88.69s/it]  2%|▏         | 73/3812 [1:48:07<92:01:10, 88.60s/it]  2%|▏         | 74/3812 [1:49:36<92:05:44, 88.70s/it]  2%|▏         | 75/3812 [1:51:04<91:58:36, 88.60s/it]  2%|▏         | 76/3812 [1:52:33<91:58:12, 88.62s/it]  2%|▏         | 77/3812 [1:54:01<91:54:44, 88.59s/it]  2%|▏         | 78/3812 [1:55:30<91:54:47, 88.61s/it]  2%|▏         | 79/3812 [1:56:58<91:49:38, 88.56s/it]  2%|▏         | 80/3812 [1:58:27<91:50:27, 88.59s/it]                                                      {'loss': 1.6506, 'learning_rate': 4.9945684144375595e-05, 'epoch': 0.02}
  2%|▏         | 80/3812 [1:58:27<91:50:27, 88.59s/it]  2%|▏         | 81/3812 [1:59:55<91:44:58, 88.53s/it]  2%|▏         | 82/3812 [2:01:24<91:46:49, 88.58s/it]  2%|▏         | 83/3812 [2:02:53<91:47:28, 88.62s/it]  2%|▏         | 84/3812 [2:04:22<91:54:29, 88.75s/it]  2%|▏         | 85/3812 [2:05:50<91:48:57, 88.69s/it]  2%|▏         | 86/3812 [2:07:19<91:51:44, 88.76s/it]  2%|▏         | 87/3812 [2:08:48<91:48:00, 88.72s/it]  2%|▏         | 88/3812 [2:10:17<91:50:09, 88.78s/it]  2%|▏         | 89/3812 [2:11:46<91:47:06, 88.75s/it]  2%|▏         | 90/3812 [2:13:15<91:51:06, 88.84s/it]                                                      {'loss': 1.6368, 'learning_rate': 4.9931263109893444e-05, 'epoch': 0.02}
  2%|▏         | 90/3812 [2:13:15<91:51:06, 88.84s/it]  2%|▏         | 91/3812 [2:14:43<91:43:37, 88.74s/it]  2%|▏         | 92/3812 [2:16:12<91:44:14, 88.78s/it]  2%|▏         | 93/3812 [2:17:40<91:35:17, 88.66s/it]  2%|▏         | 94/3812 [2:19:09<91:36:19, 88.70s/it]  2%|▏         | 95/3812 [2:20:38<91:36:35, 88.73s/it]  3%|▎         | 96/3812 [2:22:07<91:39:34, 88.80s/it]  3%|▎         | 97/3812 [2:23:36<91:34:28, 88.74s/it]  3%|▎         | 98/3812 [2:25:04<91:34:41, 88.77s/it]  3%|▎         | 99/3812 [2:26:33<91:27:53, 88.68s/it]  3%|▎         | 100/3812 [2:28:01<91:26:04, 88.68s/it]                                                       {'loss': 1.6227, 'learning_rate': 4.991514876809285e-05, 'epoch': 0.03}
  3%|▎         | 100/3812 [2:28:01<91:26:04, 88.68s/it]  3%|▎         | 101/3812 [2:29:30<91:23:43, 88.66s/it]  3%|▎         | 102/3812 [2:30:59<91:26:51, 88.74s/it]  3%|▎         | 103/3812 [2:32:27<91:17:25, 88.61s/it]  3%|▎         | 104/3812 [2:33:56<91:19:58, 88.67s/it]  3%|▎         | 105/3812 [2:35:25<91:13:34, 88.59s/it]  3%|▎         | 106/3812 [2:36:53<91:14:50, 88.64s/it]  3%|▎         | 107/3812 [2:38:22<91:16:23, 88.69s/it]  3%|▎         | 108/3812 [2:39:51<91:24:05, 88.84s/it]  3%|▎         | 109/3812 [2:41:20<91:23:12, 88.84s/it]  3%|▎         | 110/3812 [2:42:49<91:25:53, 88.91s/it]                                                       {'loss': 1.6, 'learning_rate': 4.9897342213444357e-05, 'epoch': 0.03}
  3%|▎         | 110/3812 [2:42:49<91:25:53, 88.91s/it]  3%|▎         | 111/3812 [2:44:18<91:16:11, 88.78s/it]  3%|▎         | 112/3812 [2:45:47<91:19:49, 88.86s/it]  3%|▎         | 113/3812 [2:47:16<91:17:39, 88.85s/it]  3%|▎         | 114/3812 [2:48:45<91:21:43, 88.94s/it]  3%|▎         | 115/3812 [2:50:13<91:12:44, 88.82s/it]  3%|▎         | 116/3812 [2:51:42<91:13:59, 88.86s/it]  3%|▎         | 117/3812 [2:53:11<91:01:53, 88.69s/it]  3%|▎         | 118/3812 [2:54:39<91:00:43, 88.70s/it]  3%|▎         | 119/3812 [2:56:08<90:58:42, 88.69s/it]  3%|▎         | 120/3812 [2:57:37<90:58:09, 88.70s/it]                                                       {'loss': 1.5838, 'learning_rate': 4.9877844655351945e-05, 'epoch': 0.03}
  3%|▎         | 120/3812 [2:57:37<90:58:09, 88.70s/it]  3%|▎         | 121/3812 [2:59:05<90:50:28, 88.60s/it]  3%|▎         | 122/3812 [3:00:34<90:51:57, 88.65s/it]  3%|▎         | 123/3812 [3:02:02<90:46:34, 88.59s/it]  3%|▎         | 124/3812 [3:03:31<90:48:32, 88.64s/it]  3%|▎         | 125/3812 [3:05:00<90:48:54, 88.67s/it]  3%|▎         | 126/3812 [3:06:29<90:50:07, 88.72s/it]  3%|▎         | 127/3812 [3:07:57<90:42:34, 88.62s/it]  3%|▎         | 128/3812 [3:09:26<90:44:56, 88.68s/it]  3%|▎         | 129/3812 [3:10:54<90:38:52, 88.61s/it]  3%|▎         | 130/3812 [3:12:23<90:45:04, 88.73s/it]                                                       {'loss': 1.5764, 'learning_rate': 4.9856657418070955e-05, 'epoch': 0.03}
  3%|▎         | 130/3812 [3:12:23<90:45:04, 88.73s/it]  3%|▎         | 131/3812 [3:13:52<90:43:03, 88.72s/it]  3%|▎         | 132/3812 [3:15:21<90:56:08, 88.96s/it]  3%|▎         | 133/3812 [3:16:50<90:43:15, 88.77s/it]  4%|▎         | 134/3812 [3:18:19<90:44:20, 88.81s/it]  4%|▎         | 135/3812 [3:19:47<90:31:40, 88.63s/it]  4%|▎         | 136/3812 [3:21:16<90:31:21, 88.65s/it]  4%|▎         | 137/3812 [3:22:44<90:29:45, 88.65s/it]  4%|▎         | 138/3812 [3:24:13<90:31:52, 88.71s/it]  4%|▎         | 139/3812 [3:25:41<90:25:04, 88.62s/it]  4%|▎         | 140/3812 [3:27:11<90:36:31, 88.83s/it]                                                       {'loss': 1.5674, 'learning_rate': 4.983378194061807e-05, 'epoch': 0.04}
  4%|▎         | 140/3812 [3:27:11<90:36:31, 88.83s/it]  4%|▎         | 141/3812 [3:28:39<90:24:13, 88.66s/it]  4%|▎         | 142/3812 [3:30:08<90:31:58, 88.81s/it]  4%|▍         | 143/3812 [3:31:37<90:26:52, 88.75s/it]  4%|▍         | 144/3812 [3:33:06<90:29:54, 88.82s/it]  4%|▍         | 145/3812 [3:34:34<90:20:50, 88.70s/it]  4%|▍         | 146/3812 [3:36:03<90:19:22, 88.70s/it]  4%|▍         | 147/3812 [3:37:31<90:12:33, 88.61s/it]  4%|▍         | 148/3812 [3:39:00<90:15:25, 88.68s/it]  4%|▍         | 149/3812 [3:40:29<90:13:46, 88.68s/it]  4%|▍         | 150/3812 [3:41:58<90:14:10, 88.71s/it]                                                       {'loss': 1.55, 'learning_rate': 4.980921977667366e-05, 'epoch': 0.04}
  4%|▍         | 150/3812 [3:41:58<90:14:10, 88.71s/it]  4%|▍         | 151/3812 [3:43:26<90:03:59, 88.57s/it]  4%|▍         | 152/3812 [3:44:54<90:04:23, 88.60s/it]  4%|▍         | 153/3812 [3:46:23<89:56:29, 88.49s/it]  4%|▍         | 154/3812 [3:47:51<89:58:03, 88.54s/it]  4%|▍         | 155/3812 [3:49:20<89:58:57, 88.58s/it]  4%|▍         | 156/3812 [3:50:49<90:01:02, 88.64s/it]  4%|▍         | 157/3812 [3:52:18<89:59:57, 88.64s/it]  4%|▍         | 158/3812 [3:53:47<90:06:39, 88.78s/it]  4%|▍         | 159/3812 [3:55:15<89:58:29, 88.67s/it]  4%|▍         | 160/3812 [3:56:44<90:00:17, 88.72s/it]                                                       {'loss': 1.5479, 'learning_rate': 4.978297259447614e-05, 'epoch': 0.04}
  4%|▍         | 160/3812 [3:56:44<90:00:17, 88.72s/it]  4%|▍         | 161/3812 [3:58:12<89:56:40, 88.69s/it]  4%|▍         | 162/3812 [3:59:42<90:03:38, 88.83s/it]  4%|▍         | 163/3812 [4:01:10<89:52:02, 88.66s/it]  4%|▍         | 164/3812 [4:02:39<89:50:06, 88.65s/it]  4%|▍         | 165/3812 [4:04:07<89:41:59, 88.54s/it]  4%|▍         | 166/3812 [4:05:36<89:45:05, 88.62s/it]  4%|▍         | 167/3812 [4:07:04<89:45:38, 88.65s/it]  4%|▍         | 168/3812 [4:08:34<89:55:26, 88.84s/it]  4%|▍         | 169/3812 [4:10:02<89:47:32, 88.73s/it]  4%|▍         | 170/3812 [4:11:31<89:52:35, 88.84s/it]                                                       {'loss': 1.5353, 'learning_rate': 4.9755042176708815e-05, 'epoch': 0.04}
  4%|▍         | 170/3812 [4:11:31<89:52:35, 88.84s/it]  4%|▍         | 171/3812 [4:12:59<89:40:29, 88.67s/it]  5%|▍         | 172/3812 [4:14:28<89:40:49, 88.69s/it]  5%|▍         | 173/3812 [4:15:57<89:40:04, 88.71s/it]  5%|▍         | 174/3812 [4:17:26<89:43:10, 88.78s/it]  5%|▍         | 175/3812 [4:18:54<89:36:02, 88.69s/it]  5%|▍         | 176/3812 [4:20:23<89:37:32, 88.74s/it]  5%|▍         | 177/3812 [4:21:52<89:28:53, 88.62s/it]  5%|▍         | 178/3812 [4:23:20<89:29:02, 88.65s/it]  5%|▍         | 179/3812 [4:24:49<89:33:43, 88.75s/it]  5%|▍         | 180/3812 [4:26:18<89:36:07, 88.81s/it]                                                       {'loss': 1.5392, 'learning_rate': 4.972543042037868e-05, 'epoch': 0.05}
  5%|▍         | 180/3812 [4:26:18<89:36:07, 88.81s/it]  5%|▍         | 181/3812 [4:27:47<89:28:06, 88.70s/it]  5%|▍         | 182/3812 [4:29:16<89:32:13, 88.80s/it]  5%|▍         | 183/3812 [4:30:44<89:30:46, 88.80s/it]  5%|▍         | 184/3812 [4:32:14<89:33:17, 88.86s/it]  5%|▍         | 185/3812 [4:33:42<89:27:32, 88.79s/it]  5%|▍         | 186/3812 [4:35:11<89:29:14, 88.85s/it]  5%|▍         | 187/3812 [4:36:40<89:19:51, 88.71s/it]  5%|▍         | 188/3812 [4:38:09<89:28:31, 88.88s/it]  5%|▍         | 189/3812 [4:39:37<89:19:00, 88.75s/it]  5%|▍         | 190/3812 [4:41:06<89:22:29, 88.83s/it]                                                       {'loss': 1.532, 'learning_rate': 4.969413933668764e-05, 'epoch': 0.05}
  5%|▍         | 190/3812 [4:41:06<89:22:29, 88.83s/it]  5%|▌         | 191/3812 [4:42:35<89:19:13, 88.80s/it]  5%|▌         | 192/3812 [4:44:04<89:24:37, 88.92s/it]  5%|▌         | 193/3812 [4:45:33<89:14:13, 88.77s/it]  5%|▌         | 194/3812 [4:47:02<89:15:44, 88.82s/it]  5%|▌         | 195/3812 [4:48:30<89:04:43, 88.66s/it]  5%|▌         | 196/3812 [4:49:59<89:08:33, 88.75s/it]  5%|▌         | 197/3812 [4:51:27<89:05:32, 88.72s/it]  5%|▌         | 198/3812 [4:52:56<89:07:20, 88.78s/it]  5%|▌         | 199/3812 [4:54:25<88:59:15, 88.67s/it]  5%|▌         | 200/3812 [4:55:54<89:08:47, 88.85s/it]                                                       {'loss': 1.5187, 'learning_rate': 4.9661171050895884e-05, 'epoch': 0.05}
  5%|▌         | 200/3812 [4:55:54<89:08:47, 88.85s/it]  5%|▌         | 201/3812 [4:57:23<89:03:04, 88.78s/it]  5%|▌         | 202/3812 [4:58:52<89:06:54, 88.87s/it]  5%|▌         | 203/3812 [5:00:21<89:10:49, 88.96s/it]  5%|▌         | 204/3812 [5:01:50<89:11:04, 88.99s/it]  5%|▌         | 205/3812 [5:03:19<89:02:24, 88.87s/it]  5%|▌         | 206/3812 [5:04:47<89:00:41, 88.86s/it]  5%|▌         | 207/3812 [5:06:16<88:52:17, 88.75s/it]  5%|▌         | 208/3812 [5:07:45<88:51:35, 88.76s/it]  5%|▌         | 209/3812 [5:09:13<88:49:55, 88.76s/it]  6%|▌         | 210/3812 [5:10:43<88:54:40, 88.86s/it]                                                       {'loss': 1.5167, 'learning_rate': 4.962652780217755e-05, 'epoch': 0.06}
  6%|▌         | 210/3812 [5:10:43<88:54:40, 88.86s/it]  6%|▌         | 211/3812 [5:12:11<88:46:42, 88.75s/it]  6%|▌         | 212/3812 [5:13:40<88:53:00, 88.88s/it]  6%|▌         | 213/3812 [5:15:09<88:42:02, 88.73s/it]  6%|▌         | 214/3812 [5:16:37<88:43:56, 88.78s/it]  6%|▌         | 215/3812 [5:18:06<88:41:51, 88.77s/it]  6%|▌         | 216/3812 [5:19:36<88:50:33, 88.94s/it]  6%|▌         | 217/3812 [5:21:04<88:45:37, 88.88s/it]  6%|▌         | 218/3812 [5:22:33<88:46:40, 88.93s/it]  6%|▌         | 219/3812 [5:24:02<88:36:50, 88.79s/it]  6%|▌         | 220/3812 [5:25:31<88:40:25, 88.87s/it]                                                       {'loss': 1.5177, 'learning_rate': 4.9590211943468646e-05, 'epoch': 0.06}
  6%|▌         | 220/3812 [5:25:31<88:40:25, 88.87s/it]  6%|▌         | 221/3812 [5:26:59<88:31:00, 88.74s/it]  6%|▌         | 222/3812 [5:28:29<88:38:22, 88.89s/it]  6%|▌         | 223/3812 [5:29:57<88:31:26, 88.80s/it]  6%|▌         | 224/3812 [5:31:26<88:39:59, 88.96s/it]  6%|▌         | 225/3812 [5:32:55<88:30:02, 88.82s/it]  6%|▌         | 226/3812 [5:34:24<88:33:12, 88.90s/it]  6%|▌         | 227/3812 [5:35:52<88:23:30, 88.76s/it]  6%|▌         | 228/3812 [5:37:22<88:31:27, 88.92s/it]  6%|▌         | 229/3812 [5:38:50<88:21:49, 88.78s/it]  6%|▌         | 230/3812 [5:40:19<88:24:42, 88.86s/it]                                                       {'loss': 1.5, 'learning_rate': 4.955222594130722e-05, 'epoch': 0.06}
  6%|▌         | 230/3812 [5:40:19<88:24:42, 88.86s/it]  6%|▌         | 231/3812 [5:41:48<88:21:31, 88.83s/it]  6%|▌         | 232/3812 [5:43:17<88:24:13, 88.90s/it]  6%|▌         | 233/3812 [5:44:45<88:13:42, 88.75s/it]  6%|▌         | 234/3812 [5:46:14<88:15:30, 88.80s/it]  6%|▌         | 235/3812 [5:47:43<88:11:00, 88.75s/it]  6%|▌         | 236/3812 [5:49:12<88:14:12, 88.83s/it]  6%|▌         | 237/3812 [5:50:40<88:02:11, 88.65s/it]  6%|▌         | 238/3812 [5:52:09<88:09:59, 88.81s/it]  6%|▋         | 239/3812 [5:53:38<88:05:44, 88.76s/it]  6%|▋         | 240/3812 [5:55:07<88:05:34, 88.78s/it]                                                       {'loss': 1.5053, 'learning_rate': 4.951257237566587e-05, 'epoch': 0.06}
  6%|▋         | 240/3812 [5:55:07<88:05:34, 88.78s/it]  6%|▋         | 241/3812 [5:56:36<88:01:14, 88.74s/it]  6%|▋         | 242/3812 [5:58:05<88:04:09, 88.81s/it]  6%|▋         | 243/3812 [5:59:33<88:04:33, 88.84s/it]  6%|▋         | 244/3812 [6:01:03<88:08:36, 88.93s/it]  6%|▋         | 245/3812 [6:02:31<87:54:55, 88.73s/it]  6%|▋         | 246/3812 [6:04:00<87:56:14, 88.78s/it]  6%|▋         | 247/3812 [6:05:28<87:54:05, 88.76s/it]  7%|▋         | 248/3812 [6:06:58<87:57:42, 88.85s/it]  7%|▋         | 249/3812 [6:08:26<87:47:46, 88.71s/it]  7%|▋         | 250/3812 [6:09:55<87:51:26, 88.79s/it]                                                       {'loss': 1.4908, 'learning_rate': 4.947125393977648e-05, 'epoch': 0.07}
  7%|▋         | 250/3812 [6:09:55<87:51:26, 88.79s/it]  7%|▋         | 251/3812 [6:11:24<87:48:32, 88.77s/it]  7%|▋         | 252/3812 [6:12:53<87:52:53, 88.87s/it]  7%|▋         | 253/3812 [6:14:21<87:49:51, 88.84s/it]  7%|▋         | 254/3812 [6:15:51<87:52:26, 88.91s/it]  7%|▋         | 255/3812 [6:17:19<87:39:57, 88.73s/it]  7%|▋         | 256/3812 [6:18:48<87:43:29, 88.81s/it]  7%|▋         | 257/3812 [6:20:16<87:35:50, 88.71s/it]  7%|▋         | 258/3812 [6:21:45<87:35:55, 88.73s/it]  7%|▋         | 259/3812 [6:23:14<87:32:03, 88.69s/it]  7%|▋         | 260/3812 [6:24:43<87:35:24, 88.77s/it]                                                       {'loss': 1.4907, 'learning_rate': 4.942827343994735e-05, 'epoch': 0.07}
  7%|▋         | 260/3812 [6:24:43<87:35:24, 88.77s/it]  7%|▋         | 261/3812 [6:26:11<87:25:25, 88.63s/it]  7%|▋         | 262/3812 [6:27:40<87:28:26, 88.71s/it]  7%|▋         | 263/3812 [6:29:08<87:23:39, 88.65s/it]  7%|▋         | 264/3812 [6:30:37<87:28:48, 88.76s/it]  7%|▋         | 265/3812 [6:32:06<87:25:00, 88.72s/it]  7%|▋         | 266/3812 [6:33:35<87:26:22, 88.77s/it]  7%|▋         | 267/3812 [6:35:03<87:20:53, 88.70s/it]  7%|▋         | 268/3812 [6:36:32<87:22:37, 88.76s/it]  7%|▋         | 269/3812 [6:38:01<87:15:32, 88.66s/it]  7%|▋         | 270/3812 [6:39:30<87:18:08, 88.73s/it]                                                       {'loss': 1.485, 'learning_rate': 4.9383633795372485e-05, 'epoch': 0.07}
  7%|▋         | 270/3812 [6:39:30<87:18:08, 88.73s/it]  7%|▋         | 271/3812 [6:40:58<87:13:29, 88.68s/it]  7%|▋         | 272/3812 [6:42:27<87:17:26, 88.77s/it]  7%|▋         | 273/3812 [6:43:56<87:08:51, 88.65s/it]  7%|▋         | 274/3812 [6:45:24<87:10:34, 88.70s/it]  7%|▋         | 275/3812 [6:46:53<87:04:12, 88.62s/it]  7%|▋         | 276/3812 [6:48:22<87:07:59, 88.71s/it]  7%|▋         | 277/3812 [6:49:50<87:04:04, 88.67s/it]  7%|▋         | 278/3812 [6:51:19<87:06:54, 88.74s/it]  7%|▋         | 279/3812 [6:52:48<87:00:48, 88.66s/it]  7%|▋         | 280/3812 [6:54:17<87:01:20, 88.70s/it]                                                       {'loss': 1.4815, 'learning_rate': 4.933733803793348e-05, 'epoch': 0.07}
  7%|▋         | 280/3812 [6:54:17<87:01:20, 88.70s/it]  7%|▋         | 281/3812 [6:55:45<86:58:10, 88.67s/it]  7%|▋         | 282/3812 [6:57:14<86:57:17, 88.68s/it]  7%|▋         | 283/3812 [6:58:43<86:55:49, 88.68s/it]  7%|▋         | 284/3812 [7:00:12<87:01:58, 88.81s/it]  7%|▋         | 285/3812 [7:01:40<86:53:15, 88.69s/it]  8%|▊         | 286/3812 [7:03:09<86:55:25, 88.75s/it]  8%|▊         | 287/3812 [7:04:37<86:47:34, 88.64s/it]  8%|▊         | 288/3812 [7:06:06<86:51:14, 88.73s/it]  8%|▊         | 289/3812 [7:07:35<86:49:28, 88.72s/it]  8%|▊         | 290/3812 [7:09:04<86:53:10, 88.81s/it]                                                       {'loss': 1.4774, 'learning_rate': 4.928938931199346e-05, 'epoch': 0.08}
  8%|▊         | 290/3812 [7:09:04<86:53:10, 88.81s/it]  8%|▊         | 291/3812 [7:10:33<86:47:32, 88.74s/it]  8%|▊         | 292/3812 [7:12:01<86:48:02, 88.77s/it]  8%|▊         | 293/3812 [7:13:30<86:41:19, 88.68s/it]  8%|▊         | 294/3812 [7:14:59<86:47:41, 88.82s/it]  8%|▊         | 295/3812 [7:16:28<86:46:11, 88.82s/it]  8%|▊         | 296/3812 [7:17:57<86:49:55, 88.91s/it]  8%|▊         | 297/3812 [7:19:25<86:40:56, 88.78s/it]  8%|▊         | 298/3812 [7:20:54<86:44:03, 88.86s/it]  8%|▊         | 299/3812 [7:22:23<86:35:24, 88.73s/it]  8%|▊         | 300/3812 [7:23:52<86:37:08, 88.79s/it]                                                       {'loss': 1.4812, 'learning_rate': 4.92397908741836e-05, 'epoch': 0.08}
  8%|▊         | 300/3812 [7:23:52<86:37:08, 88.79s/it]  8%|▊         | 301/3812 [7:25:21<86:35:15, 88.78s/it]  8%|▊         | 302/3812 [7:26:50<86:38:00, 88.85s/it]  8%|▊         | 303/3812 [7:28:18<86:29:54, 88.74s/it]  8%|▊         | 304/3812 [7:29:47<86:31:07, 88.79s/it]  8%|▊         | 305/3812 [7:31:16<86:26:59, 88.74s/it]  8%|▊         | 306/3812 [7:32:45<86:28:46, 88.80s/it]  8%|▊         | 307/3812 [7:34:13<86:26:52, 88.79s/it]  8%|▊         | 308/3812 [7:35:42<86:26:02, 88.80s/it]  8%|▊         | 309/3812 [7:37:11<86:19:43, 88.72s/it]  8%|▊         | 310/3812 [7:38:39<86:19:54, 88.75s/it]                                                       {'loss': 1.4664, 'learning_rate': 4.918854609318191e-05, 'epoch': 0.08}
  8%|▊         | 310/3812 [7:38:39<86:19:54, 88.75s/it]  8%|▊         | 311/3812 [7:40:08<86:15:24, 88.70s/it]  8%|▊         | 312/3812 [7:41:37<86:17:59, 88.77s/it]  8%|▊         | 313/3812 [7:43:06<86:17:07, 88.78s/it]  8%|▊         | 314/3812 [7:44:35<86:16:59, 88.80s/it]  8%|▊         | 315/3812 [7:46:03<86:08:17, 88.68s/it]  8%|▊         | 316/3812 [7:47:32<86:08:14, 88.70s/it]  8%|▊         | 317/3812 [7:49:00<86:03:05, 88.64s/it]  8%|▊         | 318/3812 [7:50:29<86:05:36, 88.71s/it]  8%|▊         | 319/3812 [7:51:58<86:09:15, 88.79s/it]  8%|▊         | 320/3812 [7:53:27<86:10:54, 88.85s/it]                                                       {'loss': 1.4707, 'learning_rate': 4.913565844948443e-05, 'epoch': 0.08}
  8%|▊         | 320/3812 [7:53:27<86:10:54, 88.85s/it]  8%|▊         | 321/3812 [7:54:56<86:06:00, 88.79s/it]  8%|▊         | 322/3812 [7:56:25<86:08:06, 88.85s/it]  8%|▊         | 323/3812 [7:57:53<86:01:06, 88.76s/it]  8%|▊         | 324/3812 [7:59:22<86:00:38, 88.77s/it]  9%|▊         | 325/3812 [8:00:51<85:54:46, 88.70s/it]  9%|▊         | 326/3812 [8:02:19<85:56:00, 88.74s/it]  9%|▊         | 327/3812 [8:03:48<85:51:25, 88.69s/it]  9%|▊         | 328/3812 [8:05:17<85:51:19, 88.71s/it]  9%|▊         | 329/3812 [8:06:45<85:45:17, 88.64s/it]  9%|▊         | 330/3812 [8:08:14<85:44:53, 88.65s/it]                                                       {'loss': 1.4675, 'learning_rate': 4.9081131535168865e-05, 'epoch': 0.09}
  9%|▊         | 330/3812 [8:08:14<85:44:53, 88.65s/it]  9%|▊         | 331/3812 [8:09:42<85:41:26, 88.62s/it]  9%|▊         | 332/3812 [8:11:12<85:48:28, 88.77s/it]  9%|▊         | 333/3812 [8:12:40<85:44:22, 88.72s/it]  9%|▉         | 334/3812 [8:14:09<85:49:51, 88.84s/it]  9%|▉         | 335/3812 [8:15:38<85:43:00, 88.75s/it]  9%|▉         | 336/3812 [8:17:07<85:42:34, 88.77s/it]  9%|▉         | 337/3812 [8:18:35<85:38:53, 88.73s/it]  9%|▉         | 338/3812 [8:20:04<85:43:09, 88.83s/it]  9%|▉         | 339/3812 [8:21:33<85:35:12, 88.72s/it]  9%|▉         | 340/3812 [8:23:02<85:37:11, 88.78s/it]                                                       {'loss': 1.4653, 'learning_rate': 4.902496905365058e-05, 'epoch': 0.09}
  9%|▉         | 340/3812 [8:23:02<85:37:11, 88.78s/it]  9%|▉         | 341/3812 [8:24:30<85:29:34, 88.67s/it]  9%|▉         | 342/3812 [8:25:59<85:35:54, 88.81s/it]  9%|▉         | 343/3812 [8:27:28<85:26:49, 88.67s/it]  9%|▉         | 344/3812 [8:28:56<85:26:02, 88.69s/it]  9%|▉         | 345/3812 [8:30:25<85:20:17, 88.61s/it]  9%|▉         | 346/3812 [8:31:53<85:20:04, 88.63s/it]  9%|▉         | 347/3812 [8:33:22<85:19:33, 88.65s/it]  9%|▉         | 348/3812 [8:34:51<85:24:52, 88.77s/it]  9%|▉         | 349/3812 [8:36:20<85:21:47, 88.74s/it]  9%|▉         | 350/3812 [8:37:49<85:25:08, 88.82s/it]                                                       {'loss': 1.4523, 'learning_rate': 4.89671748194311e-05, 'epoch': 0.09}
  9%|▉         | 350/3812 [8:37:49<85:25:08, 88.82s/it]  9%|▉         | 351/3812 [8:39:17<85:19:12, 88.75s/it]  9%|▉         | 352/3812 [8:40:46<85:20:34, 88.80s/it]  9%|▉         | 353/3812 [8:42:15<85:17:29, 88.77s/it]  9%|▉         | 354/3812 [8:43:44<85:24:56, 88.92s/it]  9%|▉         | 355/3812 [8:45:13<85:24:48, 88.95s/it]  9%|▉         | 356/3812 [8:46:42<85:23:29, 88.95s/it]  9%|▉         | 357/3812 [8:48:11<85:15:19, 88.83s/it]  9%|▉         | 358/3812 [8:49:40<85:14:15, 88.84s/it]  9%|▉         | 359/3812 [8:51:08<85:07:53, 88.76s/it]  9%|▉         | 360/3812 [8:52:37<85:08:23, 88.79s/it]                                                       {'loss': 1.4598, 'learning_rate': 4.8907752757839004e-05, 'epoch': 0.09}
  9%|▉         | 360/3812 [8:52:37<85:08:23, 88.79s/it]  9%|▉         | 361/3812 [8:54:06<85:04:42, 88.75s/it]  9%|▉         | 362/3812 [8:55:35<85:06:10, 88.80s/it] 10%|▉         | 363/3812 [8:57:04<85:04:18, 88.80s/it] 10%|▉         | 364/3812 [8:58:33<85:10:34, 88.93s/it] 10%|▉         | 365/3812 [9:00:01<85:01:55, 88.81s/it] 10%|▉         | 366/3812 [9:01:30<85:03:42, 88.86s/it] 10%|▉         | 367/3812 [9:02:59<85:02:00, 88.86s/it] 10%|▉         | 368/3812 [9:04:28<85:04:35, 88.93s/it] 10%|▉         | 369/3812 [9:05:57<84:57:33, 88.83s/it] 10%|▉         | 370/3812 [9:07:26<84:59:21, 88.89s/it]                                                       {'loss': 1.4515, 'learning_rate': 4.884670690476334e-05, 'epoch': 0.1}
 10%|▉         | 370/3812 [9:07:26<84:59:21, 88.89s/it] 10%|▉         | 371/3812 [9:08:55<85:01:53, 88.96s/it] 10%|▉         | 372/3812 [9:10:24<84:58:32, 88.93s/it] 10%|▉         | 373/3812 [9:11:53<84:56:00, 88.91s/it] 10%|▉         | 374/3812 [9:13:22<84:58:43, 88.98s/it] 10%|▉         | 375/3812 [9:14:50<84:49:34, 88.85s/it] 10%|▉         | 376/3812 [9:16:19<84:50:21, 88.89s/it] 10%|▉         | 377/3812 [9:17:48<84:41:13, 88.76s/it] 10%|▉         | 378/3812 [9:19:17<84:40:03, 88.76s/it] 10%|▉         | 379/3812 [9:20:45<84:37:40, 88.74s/it] 10%|▉         | 380/3812 [9:22:14<84:41:01, 88.83s/it]                                                       {'loss': 1.4603, 'learning_rate': 4.878404140637952e-05, 'epoch': 0.1}
 10%|▉         | 380/3812 [9:22:14<84:41:01, 88.83s/it] 10%|▉         | 381/3812 [9:23:43<84:34:57, 88.75s/it] 10%|█         | 382/3812 [9:25:12<84:32:33, 88.73s/it] 10%|█         | 383/3812 [9:26:40<84:26:32, 88.65s/it] 10%|█         | 384/3812 [9:28:09<84:28:05, 88.71s/it] 10%|█         | 385/3812 [9:29:38<84:25:53, 88.69s/it] 10%|█         | 386/3812 [9:31:06<84:28:17, 88.76s/it] 10%|█         | 387/3812 [9:32:35<84:25:07, 88.73s/it] 10%|█         | 388/3812 [9:34:04<84:27:42, 88.80s/it] 10%|█         | 389/3812 [9:35:33<84:25:03, 88.78s/it] 10%|█         | 390/3812 [9:37:02<84:27:19, 88.85s/it]                                                       {'loss': 1.4497, 'learning_rate': 4.871976051886767e-05, 'epoch': 0.1}
 10%|█         | 390/3812 [9:37:02<84:27:19, 88.85s/it] 10%|█         | 391/3812 [9:38:31<84:23:47, 88.81s/it] 10%|█         | 392/3812 [9:40:00<84:23:54, 88.84s/it] 10%|█         | 393/3812 [9:41:28<84:16:45, 88.74s/it] 10%|█         | 394/3812 [9:42:57<84:17:05, 88.77s/it] 10%|█         | 395/3812 [9:44:25<84:10:28, 88.68s/it] 10%|█         | 396/3812 [9:45:54<84:13:30, 88.76s/it] 10%|█         | 397/3812 [9:47:23<84:10:25, 88.73s/it] 10%|█         | 398/3812 [9:48:52<84:13:17, 88.81s/it] 10%|█         | 399/3812 [9:50:20<84:06:50, 88.72s/it] 10%|█         | 400/3812 [9:51:49<84:09:55, 88.80s/it]                                                       {'loss': 1.4436, 'learning_rate': 4.865386860812362e-05, 'epoch': 0.1}
 10%|█         | 400/3812 [9:51:49<84:09:55, 88.80s/it] 11%|█         | 401/3812 [9:53:18<84:07:58, 88.79s/it] 11%|█         | 402/3812 [9:54:47<84:09:21, 88.84s/it] 11%|█         | 403/3812 [9:56:16<84:05:15, 88.80s/it] 11%|█         | 404/3812 [9:57:45<84:04:37, 88.81s/it] 11%|█         | 405/3812 [9:59:14<84:08:24, 88.91s/it] 11%|█         | 406/3812 [10:00:43<84:04:40, 88.87s/it] 11%|█         | 407/3812 [10:02:11<84:03:16, 88.87s/it] 11%|█         | 408/3812 [10:03:40<84:03:23, 88.90s/it] 11%|█         | 409/3812 [10:05:09<83:57:18, 88.82s/it] 11%|█         | 410/3812 [10:06:38<83:56:10, 88.82s/it]                                                        {'loss': 1.4376, 'learning_rate': 4.8586370149462315e-05, 'epoch': 0.11}
 11%|█         | 410/3812 [10:06:38<83:56:10, 88.82s/it] 11%|█         | 411/3812 [10:08:06<83:49:20, 88.73s/it] 11%|█         | 412/3812 [10:09:35<83:53:44, 88.83s/it] 11%|█         | 413/3812 [10:11:04<83:51:19, 88.81s/it] 11%|█         | 414/3812 [10:12:33<83:54:43, 88.90s/it] 11%|█         | 415/3812 [10:14:02<83:48:08, 88.81s/it] 11%|█         | 416/3812 [10:15:31<83:49:26, 88.86s/it] 11%|█         | 417/3812 [10:17:00<83:43:16, 88.78s/it] 11%|█         | 418/3812 [10:18:28<83:42:49, 88.79s/it] 11%|█         | 419/3812 [10:19:57<83:40:35, 88.78s/it] 11%|█         | 420/3812 [10:21:26<83:43:23, 88.86s/it]                                                        {'loss': 1.4465, 'learning_rate': 4.8517269727313906e-05, 'epoch': 0.11}
 11%|█         | 420/3812 [10:21:26<83:43:23, 88.86s/it] 11%|█         | 421/3812 [10:22:55<83:38:58, 88.81s/it] 11%|█         | 422/3812 [10:24:24<83:39:03, 88.83s/it] 11%|█         | 423/3812 [10:25:52<83:32:26, 88.74s/it] 11%|█         | 424/3812 [10:27:21<83:29:57, 88.72s/it] 11%|█         | 425/3812 [10:28:50<83:28:48, 88.73s/it] 11%|█         | 426/3812 [10:30:19<83:31:47, 88.81s/it] 11%|█         | 427/3812 [10:31:47<83:28:07, 88.77s/it] 11%|█         | 428/3812 [10:33:16<83:29:06, 88.81s/it] 11%|█▏        | 429/3812 [10:34:45<83:27:06, 88.80s/it] 11%|█▏        | 430/3812 [10:36:14<83:28:18, 88.85s/it]                                                        {'loss': 1.4322, 'learning_rate': 4.8446572034912335e-05, 'epoch': 0.11}
 11%|█▏        | 430/3812 [10:36:14<83:28:18, 88.85s/it] 11%|█▏        | 431/3812 [10:37:43<83:26:22, 88.84s/it] 11%|█▏        | 432/3812 [10:39:12<83:26:52, 88.88s/it] 11%|█▏        | 433/3812 [10:40:41<83:22:51, 88.83s/it] 11%|█▏        | 434/3812 [10:42:09<83:19:30, 88.80s/it] 11%|█▏        | 435/3812 [10:43:38<83:11:42, 88.69s/it] 11%|█▏        | 436/3812 [10:45:06<83:09:05, 88.67s/it] 11%|█▏        | 437/3812 [10:46:35<83:07:38, 88.67s/it] 11%|█▏        | 438/3812 [10:48:04<83:11:38, 88.77s/it] 12%|█▏        | 439/3812 [10:49:33<83:07:07, 88.71s/it] 12%|█▏        | 440/3812 [10:51:01<83:06:58, 88.74s/it]                                                        {'loss': 1.4386, 'learning_rate': 4.837428187397662e-05, 'epoch': 0.12}
 12%|█▏        | 440/3812 [10:51:01<83:06:58, 88.74s/it] 12%|█▏        | 441/3812 [10:52:30<83:01:29, 88.66s/it] 12%|█▏        | 442/3812 [10:53:59<83:01:06, 88.68s/it] 12%|█▏        | 443/3812 [10:55:27<83:01:06, 88.71s/it] 12%|█▏        | 444/3812 [10:56:56<83:05:14, 88.81s/it] 12%|█▏        | 445/3812 [10:58:25<83:02:00, 88.78s/it] 12%|█▏        | 446/3812 [10:59:54<83:02:59, 88.82s/it] 12%|█▏        | 447/3812 [11:01:23<83:01:12, 88.82s/it] 12%|█▏        | 448/3812 [11:02:52<83:01:44, 88.85s/it] 12%|█▏        | 449/3812 [11:04:21<82:59:06, 88.83s/it] 12%|█▏        | 450/3812 [11:05:50<83:03:01, 88.93s/it]                                                        {'loss': 1.4303, 'learning_rate': 4.830040415438469e-05, 'epoch': 0.12}
 12%|█▏        | 450/3812 [11:05:50<83:03:01, 88.93s/it] 12%|█▏        | 451/3812 [11:07:18<82:52:53, 88.78s/it] 12%|█▏        | 452/3812 [11:08:47<82:53:38, 88.82s/it] 12%|█▏        | 453/3812 [11:10:16<82:49:25, 88.77s/it] 12%|█▏        | 454/3812 [11:11:44<82:47:31, 88.76s/it] 12%|█▏        | 455/3812 [11:13:13<82:43:59, 88.72s/it] 12%|█▏        | 456/3812 [11:14:42<82:46:46, 88.80s/it] 12%|█▏        | 457/3812 [11:16:11<82:39:46, 88.70s/it] 12%|█▏        | 458/3812 [11:17:39<82:36:54, 88.67s/it] 12%|█▏        | 459/3812 [11:19:08<82:34:20, 88.66s/it] 12%|█▏        | 460/3812 [11:20:36<82:34:25, 88.68s/it]                                                        {'loss': 1.4279, 'learning_rate': 4.8224943893839924e-05, 'epoch': 0.12}
 12%|█▏        | 460/3812 [11:20:36<82:34:25, 88.68s/it] 12%|█▏        | 461/3812 [11:22:05<82:33:11, 88.69s/it] 12%|█▏        | 462/3812 [11:23:34<82:34:17, 88.73s/it] 12%|█▏        | 463/3812 [11:25:02<82:27:25, 88.64s/it] 12%|█▏        | 464/3812 [11:26:31<82:30:07, 88.71s/it] 12%|█▏        | 465/3812 [11:28:00<82:32:08, 88.77s/it] 12%|█▏        | 466/3812 [11:29:29<82:33:58, 88.83s/it] 12%|█▏        | 467/3812 [11:30:58<82:35:57, 88.90s/it] 12%|█▏        | 468/3812 [11:32:27<82:34:00, 88.89s/it] 12%|█▏        | 469/3812 [11:33:55<82:23:20, 88.72s/it] 12%|█▏        | 470/3812 [11:35:25<82:27:35, 88.83s/it]                                                        {'loss': 1.4273, 'learning_rate': 4.8147906217530356e-05, 'epoch': 0.12}
 12%|█▏        | 470/3812 [11:35:25<82:27:35, 88.83s/it] 12%|█▏        | 471/3812 [11:36:53<82:18:17, 88.69s/it] 12%|█▏        | 472/3812 [11:38:22<82:21:30, 88.77s/it] 12%|█▏        | 473/3812 [11:39:51<82:19:01, 88.75s/it] 12%|█▏        | 474/3812 [11:41:20<82:23:21, 88.86s/it] 12%|█▏        | 475/3812 [11:42:48<82:16:01, 88.75s/it] 12%|█▏        | 476/3812 [11:44:17<82:18:47, 88.83s/it] 13%|█▎        | 477/3812 [11:45:46<82:14:14, 88.77s/it] 13%|█▎        | 478/3812 [11:47:15<82:13:50, 88.79s/it] 13%|█▎        | 479/3812 [11:48:43<82:10:50, 88.76s/it] 13%|█▎        | 480/3812 [11:50:12<82:08:56, 88.76s/it]                                                        {'loss': 1.4237, 'learning_rate': 4.806929635778059e-05, 'epoch': 0.13}
 13%|█▎        | 480/3812 [11:50:12<82:08:56, 88.76s/it] 13%|█▎        | 481/3812 [11:51:41<82:04:13, 88.70s/it] 13%|█▎        | 482/3812 [11:53:09<82:01:54, 88.68s/it] 13%|█▎        | 483/3812 [11:54:38<81:59:58, 88.67s/it] 13%|█▎        | 484/3812 [11:56:07<82:02:34, 88.75s/it] 13%|█▎        | 485/3812 [11:57:35<81:55:15, 88.64s/it] 13%|█▎        | 486/3812 [11:59:04<81:58:38, 88.73s/it] 13%|█▎        | 487/3812 [12:00:33<81:51:37, 88.63s/it] 13%|█▎        | 488/3812 [12:02:01<81:52:58, 88.68s/it] 13%|█▎        | 489/3812 [12:03:30<81:56:47, 88.78s/it] 13%|█▎        | 490/3812 [12:04:59<81:58:05, 88.83s/it]                                                        {'loss': 1.4162, 'learning_rate': 4.79891196536964e-05, 'epoch': 0.13}
 13%|█▎        | 490/3812 [12:04:59<81:58:05, 88.83s/it] 13%|█▎        | 491/3812 [12:06:28<81:51:25, 88.73s/it] 13%|█▎        | 492/3812 [12:07:57<81:52:39, 88.78s/it] 13%|█▎        | 493/3812 [12:09:25<81:44:43, 88.67s/it] 13%|█▎        | 494/3812 [12:10:54<81:44:01, 88.68s/it] 13%|█▎        | 495/3812 [12:12:22<81:40:42, 88.65s/it] 13%|█▎        | 496/3812 [12:13:51<81:42:28, 88.71s/it] 13%|█▎        | 497/3812 [12:15:20<81:39:46, 88.68s/it] 13%|█▎        | 498/3812 [12:16:49<81:39:50, 88.71s/it] 13%|█▎        | 499/3812 [12:18:17<81:38:13, 88.71s/it] 13%|█▎        | 500/3812 [12:19:46<81:40:15, 88.77s/it]                                                        {'loss': 1.4254, 'learning_rate': 4.79073815508021e-05, 'epoch': 0.13}
 13%|█▎        | 500/3812 [12:19:46<81:40:15, 88.77s/it][INFO|trainer.py:2936] 2024-02-09 11:32:05,758 >> Saving model checkpoint to /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-500
[INFO|configuration_utils.py:473] 2024-02-09 11:32:05,759 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-500/config.json
[INFO|configuration_utils.py:594] 2024-02-09 11:32:05,759 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-500/generation_config.json
[INFO|modeling_utils.py:2501] 2024-02-09 11:32:22,158 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2433] 2024-02-09 11:32:22,160 >> tokenizer config file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-09 11:32:22,160 >> Special tokens file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-500/special_tokens_map.json
 13%|█▎        | 501/3812 [12:21:41<88:46:57, 96.53s/it] 13%|█▎        | 502/3812 [12:23:09<86:31:38, 94.11s/it] 13%|█▎        | 503/3812 [12:24:38<85:01:13, 92.50s/it] 13%|█▎        | 504/3812 [12:26:07<83:54:42, 91.32s/it] 13%|█▎        | 505/3812 [12:27:36<83:14:11, 90.61s/it] 13%|█▎        | 506/3812 [12:29:04<82:42:31, 90.06s/it] 13%|█▎        | 507/3812 [12:30:33<82:23:03, 89.74s/it] 13%|█▎        | 508/3812 [12:32:02<82:04:05, 89.42s/it] 13%|█▎        | 509/3812 [12:33:31<81:54:13, 89.27s/it] 13%|█▎        | 510/3812 [12:35:00<81:45:32, 89.14s/it]                                                        {'loss': 1.4098, 'learning_rate': 4.782408760067072e-05, 'epoch': 0.13}
 13%|█▎        | 510/3812 [12:35:00<81:45:32, 89.14s/it] 13%|█▎        | 511/3812 [12:36:29<81:40:36, 89.08s/it] 13%|█▎        | 512/3812 [12:37:57<81:32:52, 88.96s/it] 13%|█▎        | 513/3812 [12:39:26<81:29:42, 88.93s/it] 13%|█▎        | 514/3812 [12:40:55<81:22:07, 88.82s/it] 14%|█▎        | 515/3812 [12:42:24<81:23:43, 88.88s/it] 14%|█▎        | 516/3812 [12:43:52<81:15:09, 88.75s/it] 14%|█▎        | 517/3812 [12:45:21<81:16:16, 88.79s/it] 14%|█▎        | 518/3812 [12:46:50<81:11:11, 88.73s/it] 14%|█▎        | 519/3812 [12:48:19<81:11:41, 88.76s/it] 14%|█▎        | 520/3812 [12:49:47<81:09:19, 88.75s/it]                                                        {'loss': 1.4234, 'learning_rate': 4.773924346054695e-05, 'epoch': 0.14}
 14%|█▎        | 520/3812 [12:49:47<81:09:19, 88.75s/it] 14%|█▎        | 521/3812 [12:51:16<81:10:21, 88.79s/it] 14%|█▎        | 522/3812 [12:52:45<81:03:11, 88.69s/it] 14%|█▎        | 523/3812 [12:54:13<81:02:35, 88.71s/it] 14%|█▎        | 524/3812 [12:55:42<80:55:36, 88.61s/it] 14%|█▍        | 525/3812 [12:57:11<80:55:08, 88.62s/it] 14%|█▍        | 526/3812 [12:58:39<80:52:11, 88.60s/it] 14%|█▍        | 527/3812 [13:00:08<81:00:30, 88.78s/it] 14%|█▍        | 528/3812 [13:01:37<80:54:09, 88.69s/it] 14%|█▍        | 529/3812 [13:03:06<80:55:03, 88.73s/it] 14%|█▍        | 530/3812 [13:04:34<80:54:43, 88.75s/it]                                                        {'loss': 1.4166, 'learning_rate': 4.765285489296284e-05, 'epoch': 0.14}
 14%|█▍        | 530/3812 [13:04:34<80:54:43, 88.75s/it] 14%|█▍        | 531/3812 [13:06:03<80:55:03, 88.78s/it] 14%|█▍        | 532/3812 [13:07:32<80:52:21, 88.76s/it] 14%|█▍        | 533/3812 [13:09:01<80:52:06, 88.79s/it] 14%|█▍        | 534/3812 [13:10:29<80:44:32, 88.67s/it] 14%|█▍        | 535/3812 [13:11:58<80:45:23, 88.72s/it] 14%|█▍        | 536/3812 [13:13:26<80:38:45, 88.62s/it] 14%|█▍        | 537/3812 [13:14:55<80:37:29, 88.63s/it] 14%|█▍        | 538/3812 [13:16:24<80:34:02, 88.59s/it] 14%|█▍        | 539/3812 [13:17:52<80:36:39, 88.66s/it] 14%|█▍        | 540/3812 [13:19:21<80:32:08, 88.61s/it]                                                        {'loss': 1.4184, 'learning_rate': 4.75649277653465e-05, 'epoch': 0.14}
 14%|█▍        | 540/3812 [13:19:21<80:32:08, 88.61s/it] 14%|█▍        | 541/3812 [13:20:50<80:33:12, 88.66s/it] 14%|█▍        | 542/3812 [13:22:18<80:31:11, 88.65s/it] 14%|█▍        | 543/3812 [13:23:47<80:33:44, 88.72s/it] 14%|█▍        | 544/3812 [13:25:16<80:31:28, 88.71s/it] 14%|█▍        | 545/3812 [13:26:45<80:29:44, 88.70s/it] 14%|█▍        | 546/3812 [13:28:13<80:24:48, 88.64s/it] 14%|█▍        | 547/3812 [13:29:42<80:23:41, 88.64s/it] 14%|█▍        | 548/3812 [13:31:10<80:23:04, 88.66s/it] 14%|█▍        | 549/3812 [13:32:39<80:23:57, 88.70s/it] 14%|█▍        | 550/3812 [13:34:08<80:25:03, 88.75s/it]                                                        {'loss': 1.4093, 'learning_rate': 4.747546804962355e-05, 'epoch': 0.14}
 14%|█▍        | 550/3812 [13:34:08<80:25:03, 88.75s/it] 14%|█▍        | 551/3812 [13:35:37<80:25:21, 88.78s/it] 14%|█▍        | 552/3812 [13:37:05<80:19:46, 88.71s/it] 15%|█▍        | 553/3812 [13:38:34<80:19:31, 88.73s/it] 15%|█▍        | 554/3812 [13:40:03<80:11:14, 88.60s/it] 15%|█▍        | 555/3812 [13:41:31<80:10:37, 88.62s/it] 15%|█▍        | 556/3812 [13:43:00<80:10:08, 88.64s/it] 15%|█▍        | 557/3812 [13:44:29<80:12:13, 88.70s/it] 15%|█▍        | 558/3812 [13:45:57<80:09:42, 88.69s/it] 15%|█▍        | 559/3812 [13:47:26<80:07:14, 88.67s/it] 15%|█▍        | 560/3812 [13:48:54<79:59:31, 88.55s/it]                                                        {'loss': 1.4125, 'learning_rate': 4.73844818218115e-05, 'epoch': 0.15}
 15%|█▍        | 560/3812 [13:48:54<79:59:31, 88.55s/it] 15%|█▍        | 561/3812 [13:50:23<80:00:24, 88.60s/it] 15%|█▍        | 562/3812 [13:51:52<80:00:15, 88.62s/it] 15%|█▍        | 563/3812 [13:53:21<80:03:48, 88.71s/it] 15%|█▍        | 564/3812 [13:54:49<79:56:39, 88.61s/it] 15%|█▍        | 565/3812 [13:56:18<79:58:29, 88.67s/it] 15%|█▍        | 566/3812 [13:57:46<79:53:35, 88.61s/it] 15%|█▍        | 567/3812 [13:59:15<79:54:21, 88.65s/it] 15%|█▍        | 568/3812 [14:00:44<79:52:41, 88.64s/it] 15%|█▍        | 569/3812 [14:02:12<79:53:59, 88.70s/it] 15%|█▍        | 570/3812 [14:03:41<79:51:22, 88.67s/it]                                                        {'loss': 1.4224, 'learning_rate': 4.729197526160708e-05, 'epoch': 0.15}
 15%|█▍        | 570/3812 [14:03:41<79:51:22, 88.67s/it] 15%|█▍        | 571/3812 [14:05:10<79:51:32, 88.70s/it] 15%|█▌        | 572/3812 [14:06:39<79:55:13, 88.80s/it] 15%|█▌        | 573/3812 [14:08:08<79:54:32, 88.82s/it] 15%|█▌        | 574/3812 [14:09:37<79:53:42, 88.83s/it] 15%|█▌        | 575/3812 [14:11:05<79:53:27, 88.85s/it] 15%|█▌        | 576/3812 [14:12:34<79:51:21, 88.84s/it] 15%|█▌        | 577/3812 [14:14:03<79:50:45, 88.85s/it] 15%|█▌        | 578/3812 [14:15:32<79:50:13, 88.87s/it] 15%|█▌        | 579/3812 [14:17:01<79:46:46, 88.84s/it] 15%|█▌        | 580/3812 [14:18:29<79:41:42, 88.77s/it]                                                        {'loss': 1.4088, 'learning_rate': 4.719795465196656e-05, 'epoch': 0.15}
 15%|█▌        | 580/3812 [14:18:29<79:41:42, 88.77s/it] 15%|█▌        | 581/3812 [14:19:58<79:40:39, 88.78s/it] 15%|█▌        | 582/3812 [14:21:27<79:34:30, 88.69s/it] 15%|█▌        | 583/3812 [14:22:55<79:33:34, 88.70s/it] 15%|█▌        | 584/3812 [14:24:24<79:25:16, 88.57s/it] 15%|█▌        | 585/3812 [14:25:52<79:25:26, 88.60s/it] 15%|█▌        | 586/3812 [14:27:21<79:22:20, 88.57s/it] 15%|█▌        | 587/3812 [14:28:50<79:22:39, 88.61s/it] 15%|█▌        | 588/3812 [14:30:18<79:19:24, 88.57s/it] 15%|█▌        | 589/3812 [14:31:47<79:20:24, 88.62s/it] 15%|█▌        | 590/3812 [14:33:16<79:21:20, 88.67s/it]                                                        {'loss': 1.3976, 'learning_rate': 4.710242637867896e-05, 'epoch': 0.15}
 15%|█▌        | 590/3812 [14:33:16<79:21:20, 88.67s/it] 16%|█▌        | 591/3812 [14:34:44<79:22:32, 88.72s/it] 16%|█▌        | 592/3812 [14:36:13<79:21:13, 88.72s/it] 16%|█▌        | 593/3812 [14:37:42<79:25:01, 88.82s/it] 16%|█▌        | 594/3812 [14:39:11<79:19:01, 88.73s/it] 16%|█▌        | 595/3812 [14:40:40<79:19:33, 88.77s/it] 16%|█▌        | 596/3812 [14:42:08<79:16:40, 88.74s/it] 16%|█▌        | 597/3812 [14:43:37<79:16:30, 88.77s/it] 16%|█▌        | 598/3812 [14:45:06<79:19:28, 88.85s/it] 16%|█▌        | 599/3812 [14:46:35<79:23:21, 88.95s/it] 16%|█▌        | 600/3812 [14:48:04<79:16:51, 88.86s/it]                                                        {'loss': 1.4057, 'learning_rate': 4.700539692993236e-05, 'epoch': 0.16}
 16%|█▌        | 600/3812 [14:48:04<79:16:51, 88.86s/it] 16%|█▌        | 601/3812 [14:49:33<79:13:41, 88.83s/it] 16%|█▌        | 602/3812 [14:51:01<79:04:27, 88.68s/it] 16%|█▌        | 603/3812 [14:52:29<78:59:23, 88.61s/it] 16%|█▌        | 604/3812 [14:53:58<78:56:57, 88.60s/it] 16%|█▌        | 605/3812 [14:55:27<78:58:28, 88.65s/it] 16%|█▌        | 606/3812 [14:56:55<78:54:37, 88.61s/it] 16%|█▌        | 607/3812 [14:58:24<78:51:44, 88.58s/it] 16%|█▌        | 608/3812 [14:59:52<78:46:18, 88.51s/it] 16%|█▌        | 609/3812 [15:01:21<78:45:46, 88.53s/it] 16%|█▌        | 610/3812 [15:02:49<78:42:15, 88.49s/it]                                                        {'loss': 1.4009, 'learning_rate': 4.690687289587323e-05, 'epoch': 0.16}
 16%|█▌        | 610/3812 [15:02:49<78:42:15, 88.49s/it] 16%|█▌        | 611/3812 [15:04:18<78:42:07, 88.51s/it] 16%|█▌        | 612/3812 [15:05:46<78:37:39, 88.46s/it] 16%|█▌        | 613/3812 [15:07:15<78:37:16, 88.48s/it] 16%|█▌        | 614/3812 [15:08:43<78:34:39, 88.46s/it] 16%|█▌        | 615/3812 [15:10:12<78:37:10, 88.53s/it] 16%|█▌        | 616/3812 [15:11:40<78:36:44, 88.55s/it] 16%|█▌        | 617/3812 [15:13:09<78:43:02, 88.70s/it] 16%|█▌        | 618/3812 [15:14:38<78:37:01, 88.61s/it] 16%|█▌        | 619/3812 [15:16:07<78:39:15, 88.68s/it] 16%|█▋        | 620/3812 [15:17:35<78:35:04, 88.63s/it]                                                        {'loss': 1.3954, 'learning_rate': 4.680686096815886e-05, 'epoch': 0.16}
 16%|█▋        | 620/3812 [15:17:35<78:35:04, 88.63s/it] 16%|█▋        | 621/3812 [15:19:04<78:37:33, 88.70s/it] 16%|█▋        | 622/3812 [15:20:33<78:34:19, 88.67s/it] 16%|█▋        | 623/3812 [15:22:02<78:38:59, 88.79s/it] 16%|█▋        | 624/3812 [15:23:30<78:32:40, 88.70s/it] 16%|█▋        | 625/3812 [15:24:59<78:37:08, 88.81s/it] 16%|█▋        | 626/3812 [15:26:28<78:30:56, 88.72s/it] 16%|█▋        | 627/3812 [15:27:57<78:32:48, 88.78s/it] 16%|█▋        | 628/3812 [15:29:25<78:31:07, 88.78s/it] 17%|█▋        | 629/3812 [15:30:54<78:32:26, 88.83s/it] 17%|█▋        | 630/3812 [15:32:23<78:25:53, 88.73s/it]                                                        {'loss': 1.4033, 'learning_rate': 4.670536793950279e-05, 'epoch': 0.17}
 17%|█▋        | 630/3812 [15:32:23<78:25:53, 88.73s/it] 17%|█▋        | 631/3812 [15:33:52<78:28:16, 88.81s/it] 17%|█▋        | 632/3812 [15:35:20<78:20:48, 88.69s/it] 17%|█▋        | 633/3812 [15:36:49<78:21:16, 88.73s/it] 17%|█▋        | 634/3812 [15:38:18<78:19:11, 88.72s/it] 17%|█▋        | 635/3812 [15:39:47<78:21:54, 88.80s/it] 17%|█▋        | 636/3812 [15:41:15<78:13:03, 88.66s/it] 17%|█▋        | 637/3812 [15:42:44<78:12:19, 88.67s/it] 17%|█▋        | 638/3812 [15:44:12<78:07:59, 88.62s/it] 17%|█▋        | 639/3812 [15:45:41<78:09:14, 88.67s/it] 17%|█▋        | 640/3812 [15:47:10<78:10:29, 88.72s/it]                                                        {'loss': 1.3948, 'learning_rate': 4.660240070321354e-05, 'epoch': 0.17}
 17%|█▋        | 640/3812 [15:47:10<78:10:29, 88.72s/it] 17%|█▋        | 641/3812 [15:48:39<78:12:11, 88.78s/it] 17%|█▋        | 642/3812 [15:50:07<78:06:42, 88.71s/it] 17%|█▋        | 643/3812 [15:51:36<78:07:58, 88.76s/it] 17%|█▋        | 644/3812 [15:53:05<78:00:54, 88.65s/it] 17%|█▋        | 645/3812 [15:54:34<78:05:13, 88.76s/it] 17%|█▋        | 646/3812 [15:56:02<78:02:31, 88.74s/it] 17%|█▋        | 647/3812 [15:57:31<78:05:22, 88.82s/it] 17%|█▋        | 648/3812 [15:59:00<77:57:16, 88.70s/it] 17%|█▋        | 649/3812 [16:00:29<78:02:29, 88.82s/it] 17%|█▋        | 650/3812 [16:01:57<77:53:30, 88.68s/it]                                                        {'loss': 1.3923, 'learning_rate': 4.6497966252726376e-05, 'epoch': 0.17}
 17%|█▋        | 650/3812 [16:01:57<77:53:30, 88.68s/it] 17%|█▋        | 651/3812 [16:03:26<77:54:55, 88.74s/it] 17%|█▋        | 652/3812 [16:04:55<77:54:12, 88.75s/it] 17%|█▋        | 653/3812 [16:06:24<77:58:05, 88.85s/it] 17%|█▋        | 654/3812 [16:07:52<77:51:27, 88.75s/it] 17%|█▋        | 655/3812 [16:09:21<77:51:06, 88.78s/it] 17%|█▋        | 656/3812 [16:10:50<77:44:10, 88.67s/it] 17%|█▋        | 657/3812 [16:12:18<77:42:52, 88.68s/it] 17%|█▋        | 658/3812 [16:13:47<77:39:50, 88.65s/it] 17%|█▋        | 659/3812 [16:15:16<77:41:29, 88.71s/it] 17%|█▋        | 660/3812 [16:16:44<77:36:09, 88.63s/it]                                                        {'loss': 1.4011, 'learning_rate': 4.639207168112835e-05, 'epoch': 0.17}
 17%|█▋        | 660/3812 [16:16:44<77:36:09, 88.63s/it] 17%|█▋        | 661/3812 [16:18:13<77:38:24, 88.70s/it] 17%|█▋        | 662/3812 [16:19:42<77:32:32, 88.62s/it] 17%|█▋        | 663/3812 [16:21:10<77:35:02, 88.70s/it] 17%|█▋        | 664/3812 [16:22:39<77:34:46, 88.72s/it] 17%|█▋        | 665/3812 [16:24:08<77:37:58, 88.81s/it] 17%|█▋        | 666/3812 [16:25:37<77:29:39, 88.68s/it] 17%|█▋        | 667/3812 [16:27:06<77:35:35, 88.82s/it] 18%|█▊        | 668/3812 [16:28:34<77:26:39, 88.68s/it] 18%|█▊        | 669/3812 [16:30:03<77:26:43, 88.71s/it] 18%|█▊        | 670/3812 [16:31:32<77:23:22, 88.67s/it]                                                        {'loss': 1.3969, 'learning_rate': 4.628472418067651e-05, 'epoch': 0.18}
 18%|█▊        | 670/3812 [16:31:32<77:23:22, 88.67s/it] 18%|█▊        | 671/3812 [16:33:00<77:25:18, 88.74s/it] 18%|█▊        | 672/3812 [16:34:29<77:23:40, 88.73s/it] 18%|█▊        | 673/3812 [16:35:58<77:26:52, 88.82s/it] 18%|█▊        | 674/3812 [16:37:27<77:19:39, 88.71s/it] 18%|█▊        | 675/3812 [16:38:56<77:21:58, 88.79s/it] 18%|█▊        | 676/3812 [16:40:24<77:19:40, 88.77s/it] 18%|█▊        | 677/3812 [16:41:53<77:22:16, 88.85s/it] 18%|█▊        | 678/3812 [16:43:22<77:13:23, 88.71s/it] 18%|█▊        | 679/3812 [16:44:51<77:20:28, 88.87s/it] 18%|█▊        | 680/3812 [16:46:19<77:12:30, 88.75s/it]                                                        {'loss': 1.3912, 'learning_rate': 4.617593104230945e-05, 'epoch': 0.18}
 18%|█▊        | 680/3812 [16:46:19<77:12:30, 88.75s/it] 18%|█▊        | 681/3812 [16:47:49<77:18:33, 88.89s/it] 18%|█▊        | 682/3812 [16:49:17<77:16:04, 88.87s/it] 18%|█▊        | 683/3812 [16:50:47<77:17:32, 88.93s/it] 18%|█▊        | 684/3812 [16:52:15<77:10:53, 88.83s/it] 18%|█▊        | 685/3812 [16:53:44<77:11:46, 88.87s/it] 18%|█▊        | 686/3812 [16:55:13<77:05:14, 88.78s/it] 18%|█▊        | 687/3812 [16:56:42<77:07:22, 88.85s/it] 18%|█▊        | 688/3812 [16:58:10<77:04:45, 88.82s/it] 18%|█▊        | 689/3812 [16:59:39<77:06:35, 88.89s/it] 18%|█▊        | 690/3812 [17:01:08<76:59:02, 88.77s/it]                                                        {'loss': 1.3869, 'learning_rate': 4.606569965515207e-05, 'epoch': 0.18}
 18%|█▊        | 690/3812 [17:01:08<76:59:02, 88.77s/it] 18%|█▊        | 691/3812 [17:02:37<77:02:25, 88.86s/it] 18%|█▊        | 692/3812 [17:04:06<76:55:18, 88.76s/it] 18%|█▊        | 693/3812 [17:05:34<76:55:54, 88.80s/it] 18%|█▊        | 694/3812 [17:07:03<76:55:02, 88.81s/it] 18%|█▊        | 695/3812 [17:08:32<76:56:57, 88.87s/it] 18%|█▊        | 696/3812 [17:10:01<76:47:10, 88.71s/it] 18%|█▊        | 697/3812 [17:11:29<76:47:04, 88.74s/it] 18%|█▊        | 698/3812 [17:12:58<76:41:37, 88.66s/it] 18%|█▊        | 699/3812 [17:14:27<76:43:29, 88.73s/it] 18%|█▊        | 700/3812 [17:15:56<76:44:34, 88.78s/it]                                                        {'loss': 1.3838, 'learning_rate': 4.595403750601377e-05, 'epoch': 0.18}
 18%|█▊        | 700/3812 [17:15:56<76:44:34, 88.78s/it] 18%|█▊        | 701/3812 [17:17:25<76:47:32, 88.86s/it] 18%|█▊        | 702/3812 [17:18:53<76:41:32, 88.78s/it] 18%|█▊        | 703/3812 [17:20:22<76:45:59, 88.89s/it] 18%|█▊        | 704/3812 [17:21:51<76:36:34, 88.74s/it] 18%|█▊        | 705/3812 [17:23:20<76:39:50, 88.83s/it] 19%|█▊        | 706/3812 [17:24:49<76:37:40, 88.82s/it] 19%|█▊        | 707/3812 [17:26:18<76:38:35, 88.86s/it] 19%|█▊        | 708/3812 [17:27:46<76:29:30, 88.71s/it] 19%|█▊        | 709/3812 [17:29:15<76:28:49, 88.73s/it] 19%|█▊        | 710/3812 [17:30:43<76:24:11, 88.67s/it]                                                        {'loss': 1.3966, 'learning_rate': 4.584095217887989e-05, 'epoch': 0.19}
 19%|█▊        | 710/3812 [17:30:43<76:24:11, 88.67s/it] 19%|█▊        | 711/3812 [17:32:12<76:22:25, 88.66s/it] 19%|█▊        | 712/3812 [17:33:40<76:18:17, 88.61s/it] 19%|█▊        | 713/3812 [17:35:09<76:20:00, 88.67s/it] 19%|█▊        | 714/3812 [17:36:38<76:14:14, 88.59s/it] 19%|█▉        | 715/3812 [17:38:06<76:13:34, 88.61s/it] 19%|█▉        | 716/3812 [17:39:35<76:12:14, 88.61s/it] 19%|█▉        | 717/3812 [17:41:04<76:11:02, 88.61s/it] 19%|█▉        | 718/3812 [17:42:33<76:14:45, 88.72s/it] 19%|█▉        | 719/3812 [17:44:01<76:15:55, 88.77s/it] 19%|█▉        | 720/3812 [17:45:30<76:12:16, 88.72s/it]                                                        {'loss': 1.3876, 'learning_rate': 4.572645135439667e-05, 'epoch': 0.19}
 19%|█▉        | 720/3812 [17:45:30<76:12:16, 88.72s/it] 19%|█▉        | 721/3812 [17:46:59<76:13:45, 88.78s/it] 19%|█▉        | 722/3812 [17:48:28<76:11:30, 88.77s/it] 19%|█▉        | 723/3812 [17:49:57<76:12:16, 88.81s/it] 19%|█▉        | 724/3812 [17:51:25<76:07:38, 88.75s/it] 19%|█▉        | 725/3812 [17:52:54<76:10:09, 88.83s/it] 19%|█▉        | 726/3812 [17:54:23<76:03:04, 88.72s/it] 19%|█▉        | 727/3812 [17:55:51<76:01:53, 88.72s/it] 19%|█▉        | 728/3812 [17:57:20<75:58:05, 88.68s/it] 19%|█▉        | 729/3812 [17:58:49<75:56:17, 88.67s/it] 19%|█▉        | 730/3812 [18:00:17<75:52:45, 88.63s/it]                                                        {'loss': 1.3859, 'learning_rate': 4.561054280934956e-05, 'epoch': 0.19}
 19%|█▉        | 730/3812 [18:00:17<75:52:45, 88.63s/it] 19%|█▉        | 731/3812 [18:01:46<75:55:46, 88.72s/it] 19%|█▉        | 732/3812 [18:03:15<75:52:05, 88.68s/it] 19%|█▉        | 733/3812 [18:04:44<75:56:58, 88.80s/it] 19%|█▉        | 734/3812 [18:06:12<75:50:07, 88.70s/it] 19%|█▉        | 735/3812 [18:07:41<75:48:45, 88.70s/it] 19%|█▉        | 736/3812 [18:09:10<75:46:05, 88.68s/it] 19%|█▉        | 737/3812 [18:10:39<75:50:46, 88.80s/it] 19%|█▉        | 738/3812 [18:12:07<75:45:24, 88.72s/it] 19%|█▉        | 739/3812 [18:13:36<75:44:02, 88.72s/it] 19%|█▉        | 740/3812 [18:15:04<75:39:30, 88.66s/it]                                                        {'loss': 1.385, 'learning_rate': 4.549323441613501e-05, 'epoch': 0.19}
 19%|█▉        | 740/3812 [18:15:04<75:39:30, 88.66s/it] 19%|█▉        | 741/3812 [18:16:33<75:37:12, 88.65s/it] 19%|█▉        | 742/3812 [18:18:02<75:34:55, 88.63s/it] 19%|█▉        | 743/3812 [18:19:31<75:37:27, 88.71s/it] 20%|█▉        | 744/3812 [18:20:59<75:34:19, 88.68s/it] 20%|█▉        | 745/3812 [18:22:28<75:36:19, 88.74s/it] 20%|█▉        | 746/3812 [18:23:56<75:30:32, 88.66s/it] 20%|█▉        | 747/3812 [18:25:25<75:31:25, 88.71s/it] 20%|█▉        | 748/3812 [18:26:54<75:29:05, 88.69s/it] 20%|█▉        | 749/3812 [18:28:23<75:29:32, 88.73s/it] 20%|█▉        | 750/3812 [18:29:51<75:24:27, 88.66s/it]                                                        {'loss': 1.3782, 'learning_rate': 4.5374534142225834e-05, 'epoch': 0.2}
 20%|█▉        | 750/3812 [18:29:51<75:24:27, 88.66s/it] 20%|█▉        | 751/3812 [18:31:20<75:27:08, 88.74s/it] 20%|█▉        | 752/3812 [18:32:49<75:24:12, 88.71s/it] 20%|█▉        | 753/3812 [18:34:18<75:27:35, 88.81s/it] 20%|█▉        | 754/3812 [18:35:47<75:25:53, 88.80s/it] 20%|█▉        | 755/3812 [18:37:16<75:29:30, 88.90s/it] 20%|█▉        | 756/3812 [18:38:44<75:24:30, 88.83s/it] 20%|█▉        | 757/3812 [18:40:13<75:22:00, 88.81s/it] 20%|█▉        | 758/3812 [18:41:42<75:17:23, 88.75s/it] 20%|█▉        | 759/3812 [18:43:11<75:16:48, 88.77s/it] 20%|█▉        | 760/3812 [18:44:39<75:15:42, 88.78s/it]                                                        {'loss': 1.3752, 'learning_rate': 4.525445004963002e-05, 'epoch': 0.2}
 20%|█▉        | 760/3812 [18:44:39<75:15:42, 88.78s/it] 20%|█▉        | 761/3812 [18:46:08<75:17:53, 88.85s/it] 20%|█▉        | 762/3812 [18:47:37<75:10:15, 88.73s/it] 20%|██        | 763/3812 [18:49:06<75:10:30, 88.76s/it] 20%|██        | 764/3812 [18:50:34<75:04:31, 88.67s/it] 20%|██        | 765/3812 [18:52:03<75:04:54, 88.71s/it] 20%|██        | 766/3812 [18:53:32<75:02:31, 88.69s/it] 20%|██        | 767/3812 [18:55:00<75:02:52, 88.73s/it] 20%|██        | 768/3812 [18:56:29<74:57:24, 88.65s/it] 20%|██        | 769/3812 [18:57:58<74:58:21, 88.70s/it] 20%|██        | 770/3812 [18:59:26<74:57:00, 88.70s/it]                                                        {'loss': 1.3837, 'learning_rate': 4.51329902943432e-05, 'epoch': 0.2}
 20%|██        | 770/3812 [18:59:26<74:57:00, 88.70s/it] 20%|██        | 771/3812 [19:00:55<75:00:36, 88.80s/it] 20%|██        | 772/3812 [19:02:24<74:56:32, 88.75s/it] 20%|██        | 773/3812 [19:03:53<74:56:19, 88.77s/it] 20%|██        | 774/3812 [19:05:21<74:51:43, 88.71s/it] 20%|██        | 775/3812 [19:06:50<74:50:34, 88.72s/it] 20%|██        | 776/3812 [19:08:19<74:50:17, 88.74s/it] 20%|██        | 777/3812 [19:09:48<74:49:40, 88.76s/it] 20%|██        | 778/3812 [19:11:17<74:48:17, 88.76s/it] 20%|██        | 779/3812 [19:12:45<74:47:21, 88.77s/it] 20%|██        | 780/3812 [19:14:14<74:43:16, 88.72s/it]                                                        {'loss': 1.3788, 'learning_rate': 4.501016312579467e-05, 'epoch': 0.2}
 20%|██        | 780/3812 [19:14:14<74:43:16, 88.72s/it] 20%|██        | 781/3812 [19:15:43<74:42:20, 88.73s/it] 21%|██        | 782/3812 [19:17:11<74:36:19, 88.64s/it] 21%|██        | 783/3812 [19:18:40<74:35:32, 88.65s/it] 21%|██        | 784/3812 [19:20:08<74:33:13, 88.64s/it] 21%|██        | 785/3812 [19:21:37<74:35:30, 88.71s/it] 21%|██        | 786/3812 [19:23:06<74:31:57, 88.67s/it] 21%|██        | 787/3812 [19:24:35<74:32:51, 88.72s/it] 21%|██        | 788/3812 [19:26:03<74:30:21, 88.70s/it] 21%|██        | 789/3812 [19:27:32<74:31:44, 88.75s/it] 21%|██        | 790/3812 [19:29:01<74:29:10, 88.73s/it]                                                        {'loss': 1.3809, 'learning_rate': 4.488597688628713e-05, 'epoch': 0.21}
 21%|██        | 790/3812 [19:29:01<74:29:10, 88.73s/it] 21%|██        | 791/3812 [19:30:30<74:33:35, 88.85s/it] 21%|██        | 792/3812 [19:31:59<74:33:00, 88.87s/it] 21%|██        | 793/3812 [19:33:28<74:34:20, 88.92s/it] 21%|██        | 794/3812 [19:34:57<74:27:50, 88.82s/it] 21%|██        | 795/3812 [19:36:26<74:30:03, 88.90s/it] 21%|██        | 796/3812 [19:37:55<74:28:55, 88.90s/it] 21%|██        | 797/3812 [19:39:24<74:30:39, 88.97s/it] 21%|██        | 798/3812 [19:40:52<74:23:40, 88.86s/it] 21%|██        | 799/3812 [19:42:21<74:22:40, 88.87s/it] 21%|██        | 800/3812 [19:43:50<74:14:36, 88.74s/it]                                                        {'loss': 1.3774, 'learning_rate': 4.476044001043007e-05, 'epoch': 0.21}
 21%|██        | 800/3812 [19:43:50<74:14:36, 88.74s/it] 21%|██        | 801/3812 [19:45:18<74:12:59, 88.73s/it] 21%|██        | 802/3812 [19:46:47<74:09:28, 88.69s/it] 21%|██        | 803/3812 [19:48:16<74:08:05, 88.70s/it] 21%|██        | 804/3812 [19:49:44<74:04:06, 88.65s/it] 21%|██        | 805/3812 [19:51:13<74:04:22, 88.68s/it] 21%|██        | 806/3812 [19:52:42<74:01:01, 88.64s/it] 21%|██        | 807/3812 [19:54:10<73:59:10, 88.64s/it] 21%|██        | 808/3812 [19:55:39<74:01:46, 88.72s/it] 21%|██        | 809/3812 [19:57:08<74:00:37, 88.72s/it] 21%|██        | 810/3812 [19:58:36<73:51:04, 88.56s/it]                                                        {'loss': 1.3705, 'learning_rate': 4.463356102456686e-05, 'epoch': 0.21}
 21%|██        | 810/3812 [19:58:36<73:51:04, 88.56s/it] 21%|██▏       | 811/3812 [20:00:04<73:48:14, 88.54s/it] 21%|██▏       | 812/3812 [20:01:33<73:44:48, 88.50s/it] 21%|██▏       | 813/3812 [20:03:01<73:42:31, 88.48s/it] 21%|██▏       | 814/3812 [20:04:30<73:40:38, 88.47s/it] 21%|██▏       | 815/3812 [20:05:58<73:43:22, 88.56s/it] 21%|██▏       | 816/3812 [20:07:27<73:42:03, 88.56s/it] 21%|██▏       | 817/3812 [20:08:56<73:42:55, 88.61s/it] 21%|██▏       | 818/3812 [20:10:24<73:40:08, 88.58s/it] 21%|██▏       | 819/3812 [20:11:53<73:39:18, 88.59s/it] 22%|██▏       | 820/3812 [20:13:22<73:39:39, 88.63s/it]                                                        {'loss': 1.3662, 'learning_rate': 4.4505348546195745e-05, 'epoch': 0.22}
 22%|██▏       | 820/3812 [20:13:22<73:39:39, 88.63s/it] 22%|██▏       | 821/3812 [20:14:51<73:44:30, 88.76s/it] 22%|██▏       | 822/3812 [20:16:19<73:38:47, 88.67s/it] 22%|██▏       | 823/3812 [20:17:48<73:39:07, 88.71s/it] 22%|██▏       | 824/3812 [20:19:16<73:34:55, 88.65s/it] 22%|██▏       | 825/3812 [20:20:45<73:33:09, 88.65s/it] 22%|██▏       | 826/3812 [20:22:14<73:31:28, 88.64s/it] 22%|██▏       | 827/3812 [20:23:43<73:31:47, 88.68s/it] 22%|██▏       | 828/3812 [20:25:11<73:25:54, 88.59s/it] 22%|██▏       | 829/3812 [20:26:40<73:26:38, 88.64s/it] 22%|██▏       | 830/3812 [20:28:08<73:22:08, 88.57s/it]                                                        {'loss': 1.3721, 'learning_rate': 4.437581128338445e-05, 'epoch': 0.22}
 22%|██▏       | 830/3812 [20:28:08<73:22:08, 88.57s/it] 22%|██▏       | 831/3812 [20:29:37<73:23:25, 88.63s/it] 22%|██▏       | 832/3812 [20:31:06<73:23:50, 88.67s/it] 22%|██▏       | 833/3812 [20:32:34<73:25:32, 88.73s/it] 22%|██▏       | 834/3812 [20:34:03<73:23:04, 88.71s/it] 22%|██▏       | 835/3812 [20:35:32<73:24:24, 88.77s/it] 22%|██▏       | 836/3812 [20:37:01<73:20:17, 88.72s/it] 22%|██▏       | 837/3812 [20:38:29<73:20:46, 88.76s/it] 22%|██▏       | 838/3812 [20:39:58<73:19:40, 88.76s/it] 22%|██▏       | 839/3812 [20:41:27<73:24:53, 88.90s/it] 22%|██▏       | 840/3812 [20:42:56<73:17:17, 88.77s/it]                                                        {'loss': 1.3766, 'learning_rate': 4.424495803417879e-05, 'epoch': 0.22}
 22%|██▏       | 840/3812 [20:42:56<73:17:17, 88.77s/it] 22%|██▏       | 841/3812 [20:44:25<73:20:45, 88.87s/it] 22%|██▏       | 842/3812 [20:45:54<73:16:13, 88.81s/it] 22%|██▏       | 843/3812 [20:47:23<73:15:38, 88.83s/it] 22%|██▏       | 844/3812 [20:48:52<73:16:14, 88.87s/it] 22%|██▏       | 845/3812 [20:50:21<73:19:06, 88.96s/it] 22%|██▏       | 846/3812 [20:51:49<73:11:18, 88.83s/it] 22%|██▏       | 847/3812 [20:53:18<73:11:54, 88.88s/it] 22%|██▏       | 848/3812 [20:54:47<73:06:50, 88.80s/it] 22%|██▏       | 849/3812 [20:56:16<73:06:07, 88.82s/it] 22%|██▏       | 850/3812 [20:57:45<73:05:00, 88.83s/it]                                                        {'loss': 1.3642, 'learning_rate': 4.411279768600511e-05, 'epoch': 0.22}
 22%|██▏       | 850/3812 [20:57:45<73:05:00, 88.83s/it] 22%|██▏       | 851/3812 [20:59:13<73:02:45, 88.81s/it] 22%|██▏       | 852/3812 [21:00:42<72:54:31, 88.67s/it] 22%|██▏       | 853/3812 [21:02:11<72:58:49, 88.79s/it] 22%|██▏       | 854/3812 [21:03:39<72:51:49, 88.68s/it] 22%|██▏       | 855/3812 [21:05:08<72:56:51, 88.81s/it] 22%|██▏       | 856/3812 [21:06:37<72:58:51, 88.88s/it] 22%|██▏       | 857/3812 [21:08:06<73:00:04, 88.94s/it] 23%|██▎       | 858/3812 [21:09:35<72:53:18, 88.83s/it] 23%|██▎       | 859/3812 [21:11:04<72:54:26, 88.88s/it] 23%|██▎       | 860/3812 [21:12:32<72:46:49, 88.76s/it]                                                        {'loss': 1.3738, 'learning_rate': 4.3979339215066654e-05, 'epoch': 0.23}
 23%|██▎       | 860/3812 [21:12:32<72:46:49, 88.76s/it] 23%|██▎       | 861/3812 [21:14:01<72:46:51, 88.79s/it] 23%|██▎       | 862/3812 [21:15:30<72:43:26, 88.75s/it] 23%|██▎       | 863/3812 [21:16:59<72:44:19, 88.80s/it] 23%|██▎       | 864/3812 [21:18:27<72:37:58, 88.70s/it] 23%|██▎       | 865/3812 [21:19:56<72:37:17, 88.71s/it] 23%|██▎       | 866/3812 [21:21:25<72:32:30, 88.65s/it] 23%|██▎       | 867/3812 [21:22:54<72:35:11, 88.73s/it] 23%|██▎       | 868/3812 [21:24:22<72:31:39, 88.69s/it] 23%|██▎       | 869/3812 [21:25:51<72:36:21, 88.81s/it] 23%|██▎       | 870/3812 [21:27:20<72:29:47, 88.71s/it]                                                        {'loss': 1.3606, 'learning_rate': 4.3844591685733905e-05, 'epoch': 0.23}
 23%|██▎       | 870/3812 [21:27:20<72:29:47, 88.71s/it] 23%|██▎       | 871/3812 [21:28:49<72:30:47, 88.76s/it] 23%|██▎       | 872/3812 [21:30:17<72:25:50, 88.69s/it] 23%|██▎       | 873/3812 [21:31:46<72:24:11, 88.69s/it] 23%|██▎       | 874/3812 [21:33:15<72:24:47, 88.73s/it] 23%|██▎       | 875/3812 [21:34:43<72:24:40, 88.76s/it] 23%|██▎       | 876/3812 [21:36:12<72:18:12, 88.66s/it] 23%|██▎       | 877/3812 [21:37:41<72:18:38, 88.69s/it] 23%|██▎       | 878/3812 [21:39:09<72:13:31, 88.62s/it] 23%|██▎       | 879/3812 [21:40:38<72:13:32, 88.65s/it] 23%|██▎       | 880/3812 [21:42:07<72:16:47, 88.75s/it]                                                        {'loss': 1.3675, 'learning_rate': 4.370856424992895e-05, 'epoch': 0.23}
 23%|██▎       | 880/3812 [21:42:07<72:16:47, 88.75s/it] 23%|██▎       | 881/3812 [21:43:36<72:19:20, 88.83s/it] 23%|██▎       | 882/3812 [21:45:04<72:12:31, 88.72s/it] 23%|██▎       | 883/3812 [21:46:33<72:13:07, 88.76s/it] 23%|██▎       | 884/3812 [21:48:02<72:09:40, 88.72s/it] 23%|██▎       | 885/3812 [21:49:31<72:11:23, 88.79s/it] 23%|██▎       | 886/3812 [21:50:59<72:09:33, 88.78s/it] 23%|██▎       | 887/3812 [21:52:28<72:09:36, 88.81s/it] 23%|██▎       | 888/3812 [21:53:57<72:01:05, 88.67s/it] 23%|██▎       | 889/3812 [21:55:26<72:02:18, 88.72s/it] 23%|██▎       | 890/3812 [21:56:54<71:57:58, 88.66s/it]                                                        {'loss': 1.3741, 'learning_rate': 4.3571266146503904e-05, 'epoch': 0.23}
 23%|██▎       | 890/3812 [21:56:54<71:57:58, 88.66s/it] 23%|██▎       | 891/3812 [21:58:23<72:01:19, 88.76s/it] 23%|██▎       | 892/3812 [21:59:52<71:59:40, 88.76s/it] 23%|██▎       | 893/3812 [22:01:21<72:02:54, 88.86s/it] 23%|██▎       | 894/3812 [22:02:49<71:57:06, 88.77s/it] 23%|██▎       | 895/3812 [22:04:18<71:56:57, 88.80s/it] 24%|██▎       | 896/3812 [22:05:47<71:52:01, 88.72s/it] 24%|██▎       | 897/3812 [22:07:16<71:52:25, 88.76s/it] 24%|██▎       | 898/3812 [22:08:44<71:45:29, 88.65s/it] 24%|██▎       | 899/3812 [22:10:13<71:50:23, 88.78s/it] 24%|██▎       | 900/3812 [22:11:41<71:41:34, 88.63s/it]                                                        {'loss': 1.3598, 'learning_rate': 4.3432706700613366e-05, 'epoch': 0.24}
 24%|██▎       | 900/3812 [22:11:41<71:41:34, 88.63s/it] 24%|██▎       | 901/3812 [22:13:11<71:46:47, 88.77s/it] 24%|██▎       | 902/3812 [22:14:39<71:43:00, 88.72s/it] 24%|██▎       | 903/3812 [22:16:08<71:43:04, 88.75s/it] 24%|██▎       | 904/3812 [22:17:37<71:40:34, 88.73s/it] 24%|██▎       | 905/3812 [22:19:06<71:41:44, 88.79s/it] 24%|██▍       | 906/3812 [22:20:34<71:36:49, 88.72s/it] 24%|██▍       | 907/3812 [22:22:03<71:40:29, 88.82s/it] 24%|██▍       | 908/3812 [22:23:32<71:35:23, 88.75s/it] 24%|██▍       | 909/3812 [22:25:01<71:37:34, 88.82s/it] 24%|██▍       | 910/3812 [22:26:30<71:35:53, 88.82s/it]                                                        {'loss': 1.3765, 'learning_rate': 4.329289532308114e-05, 'epoch': 0.24}
 24%|██▍       | 910/3812 [22:26:30<71:35:53, 88.82s/it] 24%|██▍       | 911/3812 [22:27:59<71:37:08, 88.88s/it] 24%|██▍       | 912/3812 [22:29:27<71:28:39, 88.73s/it] 24%|██▍       | 913/3812 [22:30:56<71:31:16, 88.82s/it] 24%|██▍       | 914/3812 [22:32:25<71:25:01, 88.72s/it] 24%|██▍       | 915/3812 [22:33:53<71:25:55, 88.77s/it] 24%|██▍       | 916/3812 [22:35:22<71:22:12, 88.72s/it] 24%|██▍       | 917/3812 [22:36:51<71:23:58, 88.79s/it] 24%|██▍       | 918/3812 [22:38:19<71:17:33, 88.68s/it] 24%|██▍       | 919/3812 [22:39:48<71:20:34, 88.78s/it] 24%|██▍       | 920/3812 [22:41:17<71:16:07, 88.72s/it]                                                        {'loss': 1.371, 'learning_rate': 4.3151841509760977e-05, 'epoch': 0.24}
 24%|██▍       | 920/3812 [22:41:17<71:16:07, 88.72s/it] 24%|██▍       | 921/3812 [22:42:46<71:14:40, 88.72s/it] 24%|██▍       | 922/3812 [22:44:14<71:10:30, 88.66s/it] 24%|██▍       | 923/3812 [22:45:43<71:12:37, 88.74s/it] 24%|██▍       | 924/3812 [22:47:12<71:08:07, 88.67s/it] 24%|██▍       | 925/3812 [22:48:40<71:09:05, 88.72s/it] 24%|██▍       | 926/3812 [22:50:09<71:08:12, 88.74s/it] 24%|██▍       | 927/3812 [22:51:38<71:10:17, 88.81s/it] 24%|██▍       | 928/3812 [22:53:07<71:09:15, 88.82s/it] 24%|██▍       | 929/3812 [22:54:36<71:10:32, 88.88s/it] 24%|██▍       | 930/3812 [22:56:05<71:04:30, 88.78s/it]                                                        {'loss': 1.3729, 'learning_rate': 4.300955484089169e-05, 'epoch': 0.24}
 24%|██▍       | 930/3812 [22:56:05<71:04:30, 88.78s/it] 24%|██▍       | 931/3812 [22:57:34<71:05:48, 88.84s/it] 24%|██▍       | 932/3812 [22:59:02<71:01:55, 88.79s/it] 24%|██▍       | 933/3812 [23:00:31<71:01:49, 88.82s/it] 25%|██▍       | 934/3812 [23:02:00<70:59:18, 88.80s/it] 25%|██▍       | 935/3812 [23:03:29<71:03:36, 88.92s/it] 25%|██▍       | 936/3812 [23:04:58<70:56:58, 88.81s/it] 25%|██▍       | 937/3812 [23:06:27<70:55:53, 88.82s/it] 25%|██▍       | 938/3812 [23:07:55<70:48:44, 88.70s/it] 25%|██▍       | 939/3812 [23:09:24<70:48:08, 88.72s/it] 25%|██▍       | 940/3812 [23:10:53<70:51:22, 88.82s/it]                                                        {'loss': 1.3567, 'learning_rate': 4.286604498044645e-05, 'epoch': 0.25}
 25%|██▍       | 940/3812 [23:10:53<70:51:22, 88.82s/it] 25%|██▍       | 941/3812 [23:12:22<70:53:43, 88.90s/it] 25%|██▍       | 942/3812 [23:13:51<70:50:24, 88.86s/it] 25%|██▍       | 943/3812 [23:15:20<70:50:26, 88.89s/it] 25%|██▍       | 944/3812 [23:16:48<70:45:40, 88.82s/it] 25%|██▍       | 945/3812 [23:18:17<70:46:57, 88.88s/it] 25%|██▍       | 946/3812 [23:19:46<70:42:33, 88.82s/it] 25%|██▍       | 947/3812 [23:21:15<70:41:27, 88.83s/it] 25%|██▍       | 948/3812 [23:22:43<70:37:23, 88.77s/it] 25%|██▍       | 949/3812 [23:24:12<70:38:02, 88.82s/it] 25%|██▍       | 950/3812 [23:25:41<70:37:14, 88.83s/it]                                                        {'loss': 1.357, 'learning_rate': 4.272132167547641e-05, 'epoch': 0.25}
 25%|██▍       | 950/3812 [23:25:41<70:37:14, 88.83s/it] 25%|██▍       | 951/3812 [23:27:10<70:39:04, 88.90s/it] 25%|██▍       | 952/3812 [23:28:39<70:28:56, 88.72s/it] 25%|██▌       | 953/3812 [23:30:07<70:28:55, 88.75s/it] 25%|██▌       | 954/3812 [23:31:36<70:24:00, 88.68s/it] 25%|██▌       | 955/3812 [23:33:04<70:21:29, 88.66s/it] 25%|██▌       | 956/3812 [23:34:33<70:19:30, 88.65s/it] 25%|██▌       | 957/3812 [23:36:02<70:22:19, 88.74s/it] 25%|██▌       | 958/3812 [23:37:31<70:18:27, 88.69s/it] 25%|██▌       | 959/3812 [23:38:59<70:17:40, 88.70s/it] 25%|██▌       | 960/3812 [23:40:28<70:15:32, 88.69s/it]                                                        {'loss': 1.3677, 'learning_rate': 4.257539475544871e-05, 'epoch': 0.25}
 25%|██▌       | 960/3812 [23:40:28<70:15:32, 88.69s/it] 25%|██▌       | 961/3812 [23:41:57<70:15:05, 88.71s/it] 25%|██▌       | 962/3812 [23:43:26<70:14:25, 88.72s/it] 25%|██▌       | 963/3812 [23:44:55<70:16:42, 88.80s/it] 25%|██▌       | 964/3812 [23:46:23<70:12:45, 88.75s/it] 25%|██▌       | 965/3812 [23:47:52<70:18:05, 88.90s/it] 25%|██▌       | 966/3812 [23:49:21<70:11:55, 88.80s/it] 25%|██▌       | 967/3812 [23:50:50<70:11:10, 88.81s/it] 25%|██▌       | 968/3812 [23:52:19<70:09:43, 88.81s/it] 25%|██▌       | 969/3812 [23:53:48<70:09:42, 88.84s/it] 25%|██▌       | 970/3812 [23:55:16<70:05:45, 88.79s/it]                                                        {'loss': 1.355, 'learning_rate': 4.242827413157886e-05, 'epoch': 0.25}
 25%|██▌       | 970/3812 [23:55:16<70:05:45, 88.79s/it] 25%|██▌       | 971/3812 [23:56:45<70:05:21, 88.81s/it] 25%|██▌       | 972/3812 [23:58:14<69:59:08, 88.71s/it] 26%|██▌       | 973/3812 [23:59:42<69:59:31, 88.75s/it] 26%|██▌       | 974/3812 [24:01:11<69:59:39, 88.79s/it] 26%|██▌       | 975/3812 [24:02:40<70:01:24, 88.86s/it] 26%|██▌       | 976/3812 [24:04:09<69:56:32, 88.78s/it] 26%|██▌       | 977/3812 [24:05:38<69:57:05, 88.83s/it] 26%|██▌       | 978/3812 [24:07:06<69:50:56, 88.73s/it] 26%|██▌       | 979/3812 [24:08:35<69:48:15, 88.70s/it] 26%|██▌       | 980/3812 [24:10:06<70:16:28, 89.33s/it]                                                        {'loss': 1.3597, 'learning_rate': 4.2279969796157584e-05, 'epoch': 0.26}
 26%|██▌       | 980/3812 [24:10:06<70:16:28, 89.33s/it] 26%|██▌       | 981/3812 [24:11:35<70:12:14, 89.27s/it] 26%|██▌       | 982/3812 [24:13:04<70:02:59, 89.11s/it] 26%|██▌       | 983/3812 [24:14:33<70:01:56, 89.12s/it] 26%|██▌       | 984/3812 [24:16:01<69:51:37, 88.93s/it] 26%|██▌       | 985/3812 [24:17:30<69:48:23, 88.89s/it] 26%|██▌       | 986/3812 [24:18:58<69:40:16, 88.75s/it] 26%|██▌       | 987/3812 [24:20:27<69:37:35, 88.73s/it] 26%|██▌       | 988/3812 [24:21:56<69:34:32, 88.69s/it] 26%|██▌       | 989/3812 [24:23:25<69:37:05, 88.78s/it] 26%|██▌       | 990/3812 [24:24:53<69:32:14, 88.71s/it]                                                        {'loss': 1.3582, 'learning_rate': 4.213049182187215e-05, 'epoch': 0.26}
 26%|██▌       | 990/3812 [24:24:53<69:32:14, 88.71s/it] 26%|██▌       | 991/3812 [24:26:22<69:30:01, 88.69s/it] 26%|██▌       | 992/3812 [24:27:50<69:25:00, 88.62s/it] 26%|██▌       | 993/3812 [24:29:19<69:23:57, 88.63s/it] 26%|██▌       | 994/3812 [24:30:48<69:22:56, 88.64s/it] 26%|██▌       | 995/3812 [24:32:17<69:25:42, 88.73s/it] 26%|██▌       | 996/3812 [24:33:45<69:19:20, 88.62s/it] 26%|██▌       | 997/3812 [24:35:14<69:18:57, 88.65s/it] 26%|██▌       | 998/3812 [24:36:42<69:16:12, 88.62s/it] 26%|██▌       | 999/3812 [24:38:11<69:15:40, 88.64s/it] 26%|██▌       | 1000/3812 [24:39:39<69:12:14, 88.60s/it]                                                         {'loss': 1.3558, 'learning_rate': 4.1979850361122256e-05, 'epoch': 0.26}
 26%|██▌       | 1000/3812 [24:39:39<69:12:14, 88.60s/it][INFO|trainer.py:2936] 2024-02-09 23:51:58,840 >> Saving model checkpoint to /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1000
[INFO|configuration_utils.py:473] 2024-02-09 23:51:58,841 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1000/config.json
[INFO|configuration_utils.py:594] 2024-02-09 23:51:58,842 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1000/generation_config.json
[INFO|modeling_utils.py:2501] 2024-02-09 23:52:15,615 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1000/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2433] 2024-02-09 23:52:15,616 >> tokenizer config file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-09 23:52:15,616 >> Special tokens file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1000/special_tokens_map.json
 26%|██▋       | 1001/3812 [24:41:34<75:21:13, 96.50s/it] 26%|██▋       | 1002/3812 [24:43:03<73:25:27, 94.07s/it] 26%|██▋       | 1003/3812 [24:44:32<72:09:08, 92.47s/it] 26%|██▋       | 1004/3812 [24:46:00<71:12:36, 91.30s/it] 26%|██▋       | 1005/3812 [24:47:29<70:32:34, 90.47s/it] 26%|██▋       | 1006/3812 [24:48:57<70:03:46, 89.89s/it] 26%|██▋       | 1007/3812 [24:50:26<69:44:39, 89.51s/it] 26%|██▋       | 1008/3812 [24:51:54<69:27:57, 89.19s/it] 26%|██▋       | 1009/3812 [24:53:23<69:20:06, 89.05s/it] 26%|██▋       | 1010/3812 [24:54:52<69:14:14, 88.96s/it]                                                         {'loss': 1.3558, 'learning_rate': 4.182805564533045e-05, 'epoch': 0.26}
 26%|██▋       | 1010/3812 [24:54:52<69:14:14, 88.96s/it] 27%|██▋       | 1011/3812 [24:56:20<69:10:07, 88.90s/it] 27%|██▋       | 1012/3812 [24:57:49<69:03:49, 88.80s/it] 27%|██▋       | 1013/3812 [24:59:18<69:07:30, 88.91s/it] 27%|██▋       | 1014/3812 [25:00:47<69:03:37, 88.86s/it] 27%|██▋       | 1015/3812 [25:02:16<69:01:04, 88.83s/it] 27%|██▋       | 1016/3812 [25:03:44<68:57:50, 88.79s/it] 27%|██▋       | 1017/3812 [25:05:13<68:57:13, 88.81s/it] 27%|██▋       | 1018/3812 [25:06:42<68:51:47, 88.73s/it] 27%|██▋       | 1019/3812 [25:08:11<68:51:41, 88.76s/it] 27%|██▋       | 1020/3812 [25:09:39<68:46:56, 88.69s/it]                                                         {'loss': 1.3614, 'learning_rate': 4.167511798424728e-05, 'epoch': 0.27}
 27%|██▋       | 1020/3812 [25:09:39<68:46:56, 88.69s/it] 27%|██▋       | 1021/3812 [25:11:08<68:48:54, 88.76s/it] 27%|██▋       | 1022/3812 [25:12:37<68:46:07, 88.73s/it] 27%|██▋       | 1023/3812 [25:14:06<68:48:24, 88.81s/it] 27%|██▋       | 1024/3812 [25:15:34<68:42:24, 88.72s/it] 27%|██▋       | 1025/3812 [25:17:03<68:42:23, 88.75s/it] 27%|██▋       | 1026/3812 [25:18:32<68:38:00, 88.69s/it] 27%|██▋       | 1027/3812 [25:20:00<68:37:39, 88.71s/it] 27%|██▋       | 1028/3812 [25:21:29<68:36:33, 88.72s/it] 27%|██▋       | 1029/3812 [25:22:58<68:38:36, 88.80s/it] 27%|██▋       | 1030/3812 [25:24:27<68:34:49, 88.75s/it]                                                         {'loss': 1.3581, 'learning_rate': 4.152104776525101e-05, 'epoch': 0.27}
 27%|██▋       | 1030/3812 [25:24:27<68:34:49, 88.75s/it] 27%|██▋       | 1031/3812 [25:25:55<68:33:09, 88.74s/it] 27%|██▋       | 1032/3812 [25:27:24<68:34:11, 88.80s/it] 27%|██▋       | 1033/3812 [25:28:53<68:32:49, 88.80s/it] 27%|██▋       | 1034/3812 [25:30:22<68:29:50, 88.77s/it] 27%|██▋       | 1035/3812 [25:31:51<68:31:34, 88.84s/it] 27%|██▋       | 1036/3812 [25:33:19<68:25:49, 88.74s/it] 27%|██▋       | 1037/3812 [25:34:48<68:24:57, 88.76s/it] 27%|██▋       | 1038/3812 [25:36:17<68:20:56, 88.70s/it] 27%|██▋       | 1039/3812 [25:37:45<68:17:27, 88.66s/it] 27%|██▋       | 1040/3812 [25:39:14<68:17:00, 88.68s/it]                                                         {'loss': 1.3543, 'learning_rate': 4.136585545264215e-05, 'epoch': 0.27}
 27%|██▋       | 1040/3812 [25:39:14<68:17:00, 88.68s/it] 27%|██▋       | 1041/3812 [25:40:43<68:19:12, 88.76s/it] 27%|██▋       | 1042/3812 [25:42:12<68:14:38, 88.69s/it] 27%|██▋       | 1043/3812 [25:43:41<68:17:19, 88.78s/it] 27%|██▋       | 1044/3812 [25:45:09<68:11:24, 88.69s/it] 27%|██▋       | 1045/3812 [25:46:38<68:10:32, 88.70s/it] 27%|██▋       | 1046/3812 [25:48:06<68:08:14, 88.68s/it] 27%|██▋       | 1047/3812 [25:49:35<68:10:21, 88.76s/it] 27%|██▋       | 1048/3812 [25:51:04<68:08:01, 88.74s/it] 28%|██▊       | 1049/3812 [25:52:33<68:08:03, 88.77s/it] 28%|██▊       | 1050/3812 [25:54:01<68:04:49, 88.74s/it]                                                         {'loss': 1.3498, 'learning_rate': 4.120955158693274e-05, 'epoch': 0.28}
 28%|██▊       | 1050/3812 [25:54:01<68:04:49, 88.74s/it] 28%|██▊       | 1051/3812 [25:55:30<68:05:34, 88.78s/it] 28%|██▊       | 1052/3812 [25:56:59<67:59:47, 88.69s/it] 28%|██▊       | 1053/3812 [25:58:28<67:58:19, 88.69s/it] 28%|██▊       | 1054/3812 [25:59:56<67:55:21, 88.66s/it] 28%|██▊       | 1055/3812 [26:01:25<67:54:40, 88.68s/it] 28%|██▊       | 1056/3812 [26:02:53<67:50:57, 88.63s/it] 28%|██▊       | 1057/3812 [26:04:22<67:52:07, 88.69s/it] 28%|██▊       | 1058/3812 [26:05:51<67:51:55, 88.71s/it] 28%|██▊       | 1059/3812 [26:07:20<67:56:18, 88.84s/it] 28%|██▊       | 1060/3812 [26:08:49<67:49:11, 88.72s/it]                                                         {'loss': 1.345, 'learning_rate': 4.105214678413042e-05, 'epoch': 0.28}
 28%|██▊       | 1060/3812 [26:08:49<67:49:11, 88.72s/it] 28%|██▊       | 1061/3812 [26:10:18<67:51:45, 88.81s/it] 28%|██▊       | 1062/3812 [26:11:46<67:48:04, 88.76s/it] 28%|██▊       | 1063/3812 [26:13:15<67:49:08, 88.81s/it] 28%|██▊       | 1064/3812 [26:14:44<67:45:01, 88.76s/it] 28%|██▊       | 1065/3812 [26:16:13<67:45:31, 88.80s/it] 28%|██▊       | 1066/3812 [26:17:41<67:37:14, 88.65s/it] 28%|██▊       | 1067/3812 [26:19:10<67:37:28, 88.69s/it] 28%|██▊       | 1068/3812 [26:20:38<67:31:53, 88.60s/it] 28%|██▊       | 1069/3812 [26:22:07<67:32:10, 88.64s/it] 28%|██▊       | 1070/3812 [26:23:35<67:30:21, 88.63s/it]                                                         {'loss': 1.3552, 'learning_rate': 4.089365173501741e-05, 'epoch': 0.28}
 28%|██▊       | 1070/3812 [26:23:35<67:30:21, 88.63s/it] 28%|██▊       | 1071/3812 [26:25:05<67:34:47, 88.76s/it] 28%|██▊       | 1072/3812 [26:26:33<67:29:33, 88.68s/it] 28%|██▊       | 1073/3812 [26:28:02<67:31:02, 88.74s/it] 28%|██▊       | 1074/3812 [26:29:31<67:28:30, 88.72s/it] 28%|██▊       | 1075/3812 [26:30:59<67:26:07, 88.70s/it] 28%|██▊       | 1076/3812 [26:32:28<67:24:36, 88.70s/it] 28%|██▊       | 1077/3812 [26:33:57<67:24:02, 88.72s/it] 28%|██▊       | 1078/3812 [26:35:25<67:20:31, 88.67s/it] 28%|██▊       | 1079/3812 [26:36:54<67:21:59, 88.74s/it] 28%|██▊       | 1080/3812 [26:38:23<67:17:33, 88.67s/it]                                                         {'loss': 1.3385, 'learning_rate': 4.073407720442443e-05, 'epoch': 0.28}
 28%|██▊       | 1080/3812 [26:38:23<67:17:33, 88.67s/it] 28%|██▊       | 1081/3812 [26:39:51<67:17:21, 88.70s/it] 28%|██▊       | 1082/3812 [26:41:20<67:15:53, 88.70s/it] 28%|██▊       | 1083/3812 [26:42:49<67:16:33, 88.75s/it] 28%|██▊       | 1084/3812 [26:44:17<67:10:16, 88.64s/it] 28%|██▊       | 1085/3812 [26:45:46<67:10:24, 88.68s/it] 28%|██▊       | 1086/3812 [26:47:15<67:05:27, 88.60s/it] 29%|██▊       | 1087/3812 [26:48:43<67:04:23, 88.61s/it] 29%|██▊       | 1088/3812 [26:50:12<67:02:17, 88.60s/it] 29%|██▊       | 1089/3812 [26:51:41<67:06:12, 88.72s/it] 29%|██▊       | 1090/3812 [26:53:09<67:02:40, 88.67s/it]                                                         {'loss': 1.3488, 'learning_rate': 4.0573434030499515e-05, 'epoch': 0.29}
 29%|██▊       | 1090/3812 [26:53:09<67:02:40, 88.67s/it] 29%|██▊       | 1091/3812 [26:54:38<67:03:48, 88.73s/it] 29%|██▊       | 1092/3812 [26:56:07<67:03:19, 88.75s/it] 29%|██▊       | 1093/3812 [26:57:36<67:04:32, 88.81s/it] 29%|██▊       | 1094/3812 [26:59:05<67:01:25, 88.77s/it] 29%|██▊       | 1095/3812 [27:00:34<67:03:04, 88.84s/it] 29%|██▉       | 1096/3812 [27:02:02<66:57:30, 88.75s/it] 29%|██▉       | 1097/3812 [27:03:31<67:00:17, 88.85s/it] 29%|██▉       | 1098/3812 [27:05:00<66:53:45, 88.73s/it] 29%|██▉       | 1099/3812 [27:06:29<66:53:37, 88.76s/it] 29%|██▉       | 1100/3812 [27:07:57<66:49:52, 88.71s/it]                                                         {'loss': 1.3428, 'learning_rate': 4.041173312397192e-05, 'epoch': 0.29}
 29%|██▉       | 1100/3812 [27:07:57<66:49:52, 88.71s/it] 29%|██▉       | 1101/3812 [27:09:26<66:52:05, 88.80s/it] 29%|██▉       | 1102/3812 [27:10:55<66:45:46, 88.69s/it] 29%|██▉       | 1103/3812 [27:12:23<66:47:06, 88.75s/it] 29%|██▉       | 1104/3812 [27:13:52<66:40:35, 88.64s/it] 29%|██▉       | 1105/3812 [27:15:21<66:40:46, 88.68s/it] 29%|██▉       | 1106/3812 [27:16:49<66:37:59, 88.65s/it] 29%|██▉       | 1107/3812 [27:18:18<66:43:04, 88.79s/it] 29%|██▉       | 1108/3812 [27:19:47<66:36:02, 88.67s/it] 29%|██▉       | 1109/3812 [27:21:16<66:38:40, 88.76s/it] 29%|██▉       | 1110/3812 [27:22:44<66:33:01, 88.67s/it]                                                         {'loss': 1.3456, 'learning_rate': 4.0248985467411096e-05, 'epoch': 0.29}
 29%|██▉       | 1110/3812 [27:22:44<66:33:01, 88.67s/it] 29%|██▉       | 1111/3812 [27:24:13<66:33:08, 88.70s/it] 29%|██▉       | 1112/3812 [27:25:41<66:30:08, 88.67s/it] 29%|██▉       | 1113/3812 [27:27:11<66:34:23, 88.80s/it] 29%|██▉       | 1114/3812 [27:28:39<66:32:31, 88.79s/it] 29%|██▉       | 1115/3812 [27:30:08<66:33:11, 88.84s/it] 29%|██▉       | 1116/3812 [27:31:37<66:28:10, 88.76s/it] 29%|██▉       | 1117/3812 [27:33:06<66:34:49, 88.94s/it] 29%|██▉       | 1118/3812 [27:34:35<66:31:02, 88.89s/it] 29%|██▉       | 1119/3812 [27:36:04<66:34:49, 89.00s/it] 29%|██▉       | 1120/3812 [27:37:33<66:26:03, 88.84s/it]                                                         {'loss': 1.3472, 'learning_rate': 4.008520211448074e-05, 'epoch': 0.29}
 29%|██▉       | 1120/3812 [27:37:33<66:26:03, 88.84s/it] 29%|██▉       | 1121/3812 [27:39:02<66:28:30, 88.93s/it] 29%|██▉       | 1122/3812 [27:40:30<66:21:13, 88.80s/it] 29%|██▉       | 1123/3812 [27:41:59<66:22:46, 88.87s/it] 29%|██▉       | 1124/3812 [27:43:28<66:19:13, 88.82s/it] 30%|██▉       | 1125/3812 [27:44:57<66:18:37, 88.84s/it] 30%|██▉       | 1126/3812 [27:46:25<66:10:37, 88.70s/it] 30%|██▉       | 1127/3812 [27:47:54<66:10:18, 88.72s/it] 30%|██▉       | 1128/3812 [27:49:23<66:11:39, 88.79s/it] 30%|██▉       | 1129/3812 [27:50:52<66:10:53, 88.80s/it] 30%|██▉       | 1130/3812 [27:52:21<66:12:58, 88.88s/it]                                                         {'loss': 1.3352, 'learning_rate': 3.992039418918804e-05, 'epoch': 0.3}
 30%|██▉       | 1130/3812 [27:52:21<66:12:58, 88.88s/it] 30%|██▉       | 1131/3812 [27:53:50<66:14:12, 88.94s/it] 30%|██▉       | 1132/3812 [27:55:18<66:05:55, 88.79s/it] 30%|██▉       | 1133/3812 [27:56:48<66:09:02, 88.89s/it] 30%|██▉       | 1134/3812 [27:58:16<66:02:10, 88.77s/it] 30%|██▉       | 1135/3812 [27:59:45<66:06:17, 88.90s/it] 30%|██▉       | 1136/3812 [28:01:14<66:00:47, 88.81s/it] 30%|██▉       | 1137/3812 [28:02:43<66:04:25, 88.92s/it] 30%|██▉       | 1138/3812 [28:04:12<65:58:20, 88.82s/it] 30%|██▉       | 1139/3812 [28:05:41<65:58:10, 88.85s/it] 30%|██▉       | 1140/3812 [28:07:10<65:59:07, 88.90s/it]                                                         {'loss': 1.3388, 'learning_rate': 3.975457288512816e-05, 'epoch': 0.3}
 30%|██▉       | 1140/3812 [28:07:10<65:59:07, 88.90s/it] 30%|██▉       | 1141/3812 [28:08:38<65:56:57, 88.89s/it] 30%|██▉       | 1142/3812 [28:10:07<65:56:17, 88.91s/it] 30%|██▉       | 1143/3812 [28:11:36<65:57:03, 88.96s/it] 30%|███       | 1144/3812 [28:13:05<65:50:13, 88.84s/it] 30%|███       | 1145/3812 [28:14:34<65:52:37, 88.92s/it] 30%|███       | 1146/3812 [28:16:03<65:46:56, 88.83s/it] 30%|███       | 1147/3812 [28:17:32<65:52:28, 88.99s/it] 30%|███       | 1148/3812 [28:19:01<65:46:53, 88.89s/it] 30%|███       | 1149/3812 [28:20:30<65:46:43, 88.92s/it] 30%|███       | 1150/3812 [28:21:58<65:38:37, 88.77s/it]                                                         {'loss': 1.3349, 'learning_rate': 3.9587749464723964e-05, 'epoch': 0.3}
 30%|███       | 1150/3812 [28:21:58<65:38:37, 88.77s/it] 30%|███       | 1151/3812 [28:23:27<65:40:14, 88.84s/it] 30%|███       | 1152/3812 [28:24:56<65:37:13, 88.81s/it] 30%|███       | 1153/3812 [28:26:25<65:36:32, 88.83s/it] 30%|███       | 1154/3812 [28:27:54<65:34:00, 88.80s/it] 30%|███       | 1155/3812 [28:29:23<65:35:21, 88.87s/it] 30%|███       | 1156/3812 [28:30:51<65:30:35, 88.79s/it] 30%|███       | 1157/3812 [28:32:20<65:30:43, 88.83s/it] 30%|███       | 1158/3812 [28:33:49<65:25:04, 88.74s/it] 30%|███       | 1159/3812 [28:35:18<65:26:24, 88.80s/it] 30%|███       | 1160/3812 [28:36:47<65:27:33, 88.86s/it]                                                         {'loss': 1.3428, 'learning_rate': 3.941993525846111e-05, 'epoch': 0.3}
 30%|███       | 1160/3812 [28:36:47<65:27:33, 88.86s/it] 30%|███       | 1161/3812 [28:38:15<65:26:37, 88.87s/it] 30%|███       | 1162/3812 [28:39:44<65:19:47, 88.75s/it] 31%|███       | 1163/3812 [28:41:13<65:21:28, 88.82s/it] 31%|███       | 1164/3812 [28:42:41<65:13:05, 88.67s/it] 31%|███       | 1165/3812 [28:44:10<65:12:22, 88.68s/it] 31%|███       | 1166/3812 [28:45:39<65:10:57, 88.68s/it] 31%|███       | 1167/3812 [28:47:07<65:10:52, 88.72s/it] 31%|███       | 1168/3812 [28:48:36<65:04:58, 88.62s/it] 31%|███       | 1169/3812 [28:50:04<65:03:27, 88.61s/it] 31%|███       | 1170/3812 [28:51:33<64:58:30, 88.54s/it]                                                         {'loss': 1.3381, 'learning_rate': 3.925114166411842e-05, 'epoch': 0.31}
 31%|███       | 1170/3812 [28:51:33<64:58:30, 88.54s/it] 31%|███       | 1171/3812 [28:53:02<65:02:22, 88.66s/it] 31%|███       | 1172/3812 [28:54:30<64:58:36, 88.60s/it] 31%|███       | 1173/3812 [28:55:59<65:01:13, 88.70s/it] 31%|███       | 1174/3812 [28:57:28<64:55:48, 88.61s/it] 31%|███       | 1175/3812 [28:58:56<64:57:19, 88.68s/it] 31%|███       | 1176/3812 [29:00:25<65:00:01, 88.77s/it] 31%|███       | 1177/3812 [29:01:54<64:59:21, 88.79s/it] 31%|███       | 1178/3812 [29:03:23<64:58:06, 88.80s/it] 31%|███       | 1179/3812 [29:04:52<64:57:36, 88.82s/it] 31%|███       | 1180/3812 [29:06:21<64:53:53, 88.77s/it]                                                         {'loss': 1.3481, 'learning_rate': 3.908138014599386e-05, 'epoch': 0.31}
 31%|███       | 1180/3812 [29:06:21<64:53:53, 88.77s/it] 31%|███       | 1181/3812 [29:07:49<64:52:59, 88.78s/it] 31%|███       | 1182/3812 [29:09:18<64:48:20, 88.71s/it] 31%|███       | 1183/3812 [29:10:47<64:50:28, 88.79s/it] 31%|███       | 1184/3812 [29:12:16<64:48:51, 88.79s/it] 31%|███       | 1185/3812 [29:13:45<64:51:05, 88.87s/it] 31%|███       | 1186/3812 [29:15:13<64:45:45, 88.78s/it] 31%|███       | 1187/3812 [29:16:42<64:46:15, 88.83s/it] 31%|███       | 1188/3812 [29:18:11<64:46:48, 88.88s/it] 31%|███       | 1189/3812 [29:19:40<64:45:27, 88.88s/it] 31%|███       | 1190/3812 [29:21:09<64:42:02, 88.83s/it]                                                         {'loss': 1.3396, 'learning_rate': 3.891066223412585e-05, 'epoch': 0.31}
 31%|███       | 1190/3812 [29:21:09<64:42:02, 88.83s/it] 31%|███       | 1191/3812 [29:22:38<64:42:25, 88.88s/it] 31%|███▏      | 1192/3812 [29:24:06<64:38:26, 88.82s/it] 31%|███▏      | 1193/3812 [29:25:35<64:36:40, 88.81s/it] 31%|███▏      | 1194/3812 [29:27:04<64:31:49, 88.74s/it] 31%|███▏      | 1195/3812 [29:28:32<64:29:04, 88.71s/it] 31%|███▏      | 1196/3812 [29:30:01<64:25:59, 88.67s/it] 31%|███▏      | 1197/3812 [29:31:30<64:25:35, 88.69s/it] 31%|███▏      | 1198/3812 [29:32:58<64:23:04, 88.67s/it] 31%|███▏      | 1199/3812 [29:34:27<64:23:04, 88.70s/it] 31%|███▏      | 1200/3812 [29:35:56<64:21:21, 88.70s/it]                                                         {'loss': 1.337, 'learning_rate': 3.873899952351011e-05, 'epoch': 0.31}
 31%|███▏      | 1200/3812 [29:35:56<64:21:21, 88.70s/it] 32%|███▏      | 1201/3812 [29:37:25<64:19:54, 88.70s/it] 32%|███▏      | 1202/3812 [29:38:54<64:22:19, 88.79s/it] 32%|███▏      | 1203/3812 [29:40:23<64:23:13, 88.84s/it] 32%|███▏      | 1204/3812 [29:41:51<64:19:10, 88.78s/it] 32%|███▏      | 1205/3812 [29:43:20<64:19:46, 88.83s/it] 32%|███▏      | 1206/3812 [29:44:49<64:15:31, 88.77s/it] 32%|███▏      | 1207/3812 [29:46:18<64:16:29, 88.82s/it] 32%|███▏      | 1208/3812 [29:47:46<64:14:25, 88.81s/it] 32%|███▏      | 1209/3812 [29:49:16<64:16:50, 88.90s/it] 32%|███▏      | 1210/3812 [29:50:44<64:08:30, 88.74s/it]                                                         {'loss': 1.3371, 'learning_rate': 3.85664036733122e-05, 'epoch': 0.32}
 32%|███▏      | 1210/3812 [29:50:44<64:08:30, 88.74s/it] 32%|███▏      | 1211/3812 [29:52:13<64:07:32, 88.76s/it] 32%|███▏      | 1212/3812 [29:53:41<64:02:49, 88.68s/it] 32%|███▏      | 1213/3812 [29:55:10<64:03:42, 88.73s/it] 32%|███▏      | 1214/3812 [29:56:39<64:01:39, 88.72s/it] 32%|███▏      | 1215/3812 [29:58:08<64:06:16, 88.86s/it] 32%|███▏      | 1216/3812 [29:59:36<63:59:51, 88.75s/it] 32%|███▏      | 1217/3812 [30:01:06<64:01:52, 88.83s/it] 32%|███▏      | 1218/3812 [30:02:34<63:57:43, 88.77s/it] 32%|███▏      | 1219/3812 [30:04:03<63:56:19, 88.77s/it] 32%|███▏      | 1220/3812 [30:05:32<63:53:04, 88.73s/it]                                                         {'loss': 1.3365, 'learning_rate': 3.839288640607563e-05, 'epoch': 0.32}
 32%|███▏      | 1220/3812 [30:05:32<63:53:04, 88.73s/it] 32%|███▏      | 1221/3812 [30:07:01<63:54:37, 88.80s/it] 32%|███▏      | 1222/3812 [30:08:29<63:47:13, 88.66s/it] 32%|███▏      | 1223/3812 [30:09:58<63:46:12, 88.67s/it] 32%|███▏      | 1224/3812 [30:11:26<63:40:50, 88.58s/it] 32%|███▏      | 1225/3812 [30:12:55<63:41:50, 88.64s/it] 32%|███▏      | 1226/3812 [30:14:23<63:40:16, 88.64s/it] 32%|███▏      | 1227/3812 [30:15:52<63:42:53, 88.73s/it] 32%|███▏      | 1228/3812 [30:17:21<63:40:05, 88.70s/it] 32%|███▏      | 1229/3812 [30:18:50<63:41:39, 88.77s/it] 32%|███▏      | 1230/3812 [30:20:18<63:37:40, 88.71s/it]                                                         {'loss': 1.3353, 'learning_rate': 3.821845950692564e-05, 'epoch': 0.32}
 32%|███▏      | 1230/3812 [30:20:18<63:37:40, 88.71s/it] 32%|███▏      | 1231/3812 [30:21:47<63:38:10, 88.76s/it] 32%|███▏      | 1232/3812 [30:23:16<63:35:47, 88.74s/it] 32%|███▏      | 1233/3812 [30:24:45<63:39:56, 88.87s/it] 32%|███▏      | 1234/3812 [30:26:14<63:35:29, 88.80s/it] 32%|███▏      | 1235/3812 [30:27:43<63:34:05, 88.80s/it] 32%|███▏      | 1236/3812 [30:29:11<63:29:37, 88.73s/it] 32%|███▏      | 1237/3812 [30:30:40<63:29:40, 88.77s/it] 32%|███▏      | 1238/3812 [30:32:09<63:26:47, 88.74s/it] 33%|███▎      | 1239/3812 [30:33:38<63:30:36, 88.86s/it] 33%|███▎      | 1240/3812 [30:35:06<63:23:30, 88.73s/it]                                                         {'loss': 1.3279, 'learning_rate': 3.804313482276882e-05, 'epoch': 0.33}
 33%|███▎      | 1240/3812 [30:35:06<63:23:30, 88.73s/it] 33%|███▎      | 1241/3812 [30:36:35<63:22:42, 88.74s/it] 33%|███▎      | 1242/3812 [30:38:04<63:19:27, 88.70s/it] 33%|███▎      | 1243/3812 [30:39:32<63:17:12, 88.69s/it] 33%|███▎      | 1244/3812 [30:41:01<63:17:15, 88.72s/it] 33%|███▎      | 1245/3812 [30:42:30<63:20:40, 88.84s/it] 33%|███▎      | 1246/3812 [30:43:59<63:14:05, 88.72s/it] 33%|███▎      | 1247/3812 [30:45:27<63:12:24, 88.71s/it] 33%|███▎      | 1248/3812 [30:46:56<63:07:50, 88.64s/it] 33%|███▎      | 1249/3812 [30:48:24<63:05:21, 88.62s/it] 33%|███▎      | 1250/3812 [30:49:53<63:05:24, 88.65s/it]                                                         {'loss': 1.3211, 'learning_rate': 3.786692426148842e-05, 'epoch': 0.33}
 33%|███▎      | 1250/3812 [30:49:53<63:05:24, 88.65s/it] 33%|███▎      | 1251/3812 [30:51:22<63:05:54, 88.70s/it] 33%|███▎      | 1252/3812 [30:52:50<62:59:22, 88.58s/it] 33%|███▎      | 1253/3812 [30:54:19<63:01:03, 88.65s/it] 33%|███▎      | 1254/3812 [30:55:48<62:59:32, 88.65s/it] 33%|███▎      | 1255/3812 [30:57:16<62:59:36, 88.69s/it] 33%|███▎      | 1256/3812 [30:58:45<62:58:15, 88.69s/it] 33%|███▎      | 1257/3812 [31:00:14<63:02:31, 88.83s/it] 33%|███▎      | 1258/3812 [31:01:43<62:58:00, 88.75s/it] 33%|███▎      | 1259/3812 [31:03:12<62:57:07, 88.77s/it] 33%|███▎      | 1260/3812 [31:04:40<62:53:04, 88.71s/it]                                                         {'loss': 1.3425, 'learning_rate': 3.768983979113564e-05, 'epoch': 0.33}
 33%|███▎      | 1260/3812 [31:04:40<62:53:04, 88.71s/it] 33%|███▎      | 1261/3812 [31:06:09<62:51:49, 88.71s/it] 33%|███▎      | 1262/3812 [31:07:38<62:50:03, 88.71s/it] 33%|███▎      | 1263/3812 [31:09:07<62:55:36, 88.87s/it] 33%|███▎      | 1264/3812 [31:10:35<62:48:07, 88.73s/it] 33%|███▎      | 1265/3812 [31:12:04<62:46:26, 88.73s/it] 33%|███▎      | 1266/3812 [31:13:33<62:41:53, 88.65s/it] 33%|███▎      | 1267/3812 [31:15:01<62:42:20, 88.70s/it] 33%|███▎      | 1268/3812 [31:16:30<62:44:39, 88.79s/it] 33%|███▎      | 1269/3812 [31:17:59<62:45:21, 88.84s/it] 33%|███▎      | 1270/3812 [31:19:28<62:40:44, 88.77s/it]                                                         {'loss': 1.3334, 'learning_rate': 3.751189343911671e-05, 'epoch': 0.33}
 33%|███▎      | 1270/3812 [31:19:28<62:40:44, 88.77s/it] 33%|███▎      | 1271/3812 [31:20:57<62:40:58, 88.81s/it] 33%|███▎      | 1272/3812 [31:22:25<62:36:47, 88.74s/it] 33%|███▎      | 1273/3812 [31:23:54<62:37:05, 88.79s/it] 33%|███▎      | 1274/3812 [31:25:23<62:34:37, 88.76s/it] 33%|███▎      | 1275/3812 [31:26:52<62:36:17, 88.84s/it] 33%|███▎      | 1276/3812 [31:28:21<62:30:45, 88.74s/it] 33%|███▎      | 1277/3812 [31:29:49<62:31:28, 88.79s/it] 34%|███▎      | 1278/3812 [31:31:18<62:30:38, 88.81s/it] 34%|███▎      | 1279/3812 [31:32:47<62:32:19, 88.88s/it] 34%|███▎      | 1280/3812 [31:34:16<62:31:38, 88.90s/it]                                                         {'loss': 1.3281, 'learning_rate': 3.733309729137606e-05, 'epoch': 0.34}
 34%|███▎      | 1280/3812 [31:34:16<62:31:38, 88.90s/it] 34%|███▎      | 1281/3812 [31:35:45<62:34:01, 88.99s/it] 34%|███▎      | 1282/3812 [31:37:14<62:27:34, 88.88s/it] 34%|███▎      | 1283/3812 [31:38:43<62:27:13, 88.90s/it] 34%|███▎      | 1284/3812 [31:40:12<62:20:21, 88.77s/it] 34%|███▎      | 1285/3812 [31:41:41<62:24:25, 88.91s/it] 34%|███▎      | 1286/3812 [31:43:09<62:20:10, 88.84s/it] 34%|███▍      | 1287/3812 [31:44:38<62:19:43, 88.86s/it] 34%|███▍      | 1288/3812 [31:46:07<62:11:31, 88.71s/it] 34%|███▍      | 1289/3812 [31:47:35<62:11:06, 88.73s/it] 34%|███▍      | 1290/3812 [31:49:04<62:07:03, 88.67s/it]                                                         {'loss': 1.3374, 'learning_rate': 3.71534634915754e-05, 'epoch': 0.34}
 34%|███▍      | 1290/3812 [31:49:04<62:07:03, 88.67s/it] 34%|███▍      | 1291/3812 [31:50:33<62:05:23, 88.66s/it] 34%|███▍      | 1292/3812 [31:52:01<62:04:45, 88.68s/it] 34%|███▍      | 1293/3812 [31:53:30<62:07:03, 88.77s/it] 34%|███▍      | 1294/3812 [31:54:59<62:03:16, 88.72s/it] 34%|███▍      | 1295/3812 [31:56:28<62:02:50, 88.74s/it] 34%|███▍      | 1296/3812 [31:57:56<61:57:05, 88.64s/it] 34%|███▍      | 1297/3812 [31:59:25<61:59:27, 88.73s/it] 34%|███▍      | 1298/3812 [32:00:54<61:56:12, 88.69s/it] 34%|███▍      | 1299/3812 [32:02:23<61:57:05, 88.75s/it] 34%|███▍      | 1300/3812 [32:03:51<61:53:59, 88.71s/it]                                                         {'loss': 1.3283, 'learning_rate': 3.6973004240268984e-05, 'epoch': 0.34}
 34%|███▍      | 1300/3812 [32:03:51<61:53:59, 88.71s/it] 34%|███▍      | 1301/3812 [32:05:20<61:53:09, 88.73s/it] 34%|███▍      | 1302/3812 [32:06:49<61:51:05, 88.71s/it] 34%|███▍      | 1303/3812 [32:08:17<61:51:19, 88.75s/it] 34%|███▍      | 1304/3812 [32:09:46<61:48:24, 88.72s/it] 34%|███▍      | 1305/3812 [32:11:15<61:51:35, 88.83s/it] 34%|███▍      | 1306/3812 [32:12:44<61:45:47, 88.73s/it] 34%|███▍      | 1307/3812 [32:14:13<61:46:19, 88.77s/it] 34%|███▍      | 1308/3812 [32:15:41<61:40:47, 88.68s/it] 34%|███▍      | 1309/3812 [32:17:10<61:39:43, 88.69s/it] 34%|███▍      | 1310/3812 [32:18:38<61:36:18, 88.64s/it]                                                         {'loss': 1.3225, 'learning_rate': 3.67917317940749e-05, 'epoch': 0.34}
 34%|███▍      | 1310/3812 [32:18:38<61:36:18, 88.64s/it] 34%|███▍      | 1311/3812 [32:20:07<61:38:34, 88.73s/it] 34%|███▍      | 1312/3812 [32:21:36<61:33:11, 88.64s/it] 34%|███▍      | 1313/3812 [32:23:04<61:33:29, 88.68s/it] 34%|███▍      | 1314/3812 [32:24:33<61:28:30, 88.60s/it] 34%|███▍      | 1315/3812 [32:26:01<61:28:00, 88.62s/it] 35%|███▍      | 1316/3812 [32:27:30<61:31:07, 88.73s/it] 35%|███▍      | 1317/3812 [32:28:59<61:32:35, 88.80s/it] 35%|███▍      | 1318/3812 [32:30:28<61:26:38, 88.69s/it] 35%|███▍      | 1319/3812 [32:31:57<61:26:32, 88.73s/it] 35%|███▍      | 1320/3812 [32:33:25<61:21:45, 88.65s/it]                                                         {'loss': 1.3256, 'learning_rate': 3.660965846484269e-05, 'epoch': 0.35}
 35%|███▍      | 1320/3812 [32:33:25<61:21:45, 88.65s/it] 35%|███▍      | 1321/3812 [32:34:54<61:20:31, 88.65s/it] 35%|███▍      | 1322/3812 [32:36:23<61:20:01, 88.68s/it] 35%|███▍      | 1323/3812 [32:37:52<61:26:05, 88.86s/it] 35%|███▍      | 1324/3812 [32:39:20<61:21:21, 88.78s/it] 35%|███▍      | 1325/3812 [32:40:49<61:20:46, 88.80s/it] 35%|███▍      | 1326/3812 [32:42:18<61:15:15, 88.70s/it] 35%|███▍      | 1327/3812 [32:43:46<61:14:16, 88.71s/it] 35%|███▍      | 1328/3812 [32:45:15<61:11:24, 88.68s/it] 35%|███▍      | 1329/3812 [32:46:44<61:11:45, 88.73s/it] 35%|███▍      | 1330/3812 [32:48:12<61:08:16, 88.68s/it]                                                         {'loss': 1.3356, 'learning_rate': 3.6426796618817054e-05, 'epoch': 0.35}
 35%|███▍      | 1330/3812 [32:48:12<61:08:16, 88.68s/it] 35%|███▍      | 1331/3812 [32:49:41<61:09:47, 88.75s/it] 35%|███▍      | 1332/3812 [32:51:10<61:06:27, 88.70s/it] 35%|███▍      | 1333/3812 [32:52:39<61:06:51, 88.75s/it] 35%|███▍      | 1334/3812 [32:54:08<61:05:03, 88.74s/it] 35%|███▌      | 1335/3812 [32:55:37<61:06:47, 88.82s/it] 35%|███▌      | 1336/3812 [32:57:05<61:03:12, 88.77s/it] 35%|███▌      | 1337/3812 [32:58:34<61:04:16, 88.83s/it] 35%|███▌      | 1338/3812 [33:00:03<60:57:36, 88.71s/it] 35%|███▌      | 1339/3812 [33:01:32<60:58:27, 88.76s/it] 35%|███▌      | 1340/3812 [33:03:00<60:59:29, 88.82s/it]                                                         {'loss': 1.3323, 'learning_rate': 3.624315867579804e-05, 'epoch': 0.35}
 35%|███▌      | 1340/3812 [33:03:00<60:59:29, 88.82s/it] 35%|███▌      | 1341/3812 [33:04:29<60:59:14, 88.85s/it] 35%|███▌      | 1342/3812 [33:05:58<60:52:40, 88.73s/it] 35%|███▌      | 1343/3812 [33:07:27<60:50:58, 88.72s/it] 35%|███▌      | 1344/3812 [33:08:55<60:50:17, 88.74s/it] 35%|███▌      | 1345/3812 [33:10:24<60:51:40, 88.81s/it] 35%|███▌      | 1346/3812 [33:11:53<60:48:52, 88.78s/it] 35%|███▌      | 1347/3812 [33:13:22<60:51:24, 88.88s/it] 35%|███▌      | 1348/3812 [33:14:51<60:45:38, 88.77s/it] 35%|███▌      | 1349/3812 [33:16:20<60:48:51, 88.89s/it] 35%|███▌      | 1350/3812 [33:17:48<60:42:34, 88.77s/it]                                                         {'loss': 1.3214, 'learning_rate': 3.6058757108297415e-05, 'epoch': 0.35}
 35%|███▌      | 1350/3812 [33:17:48<60:42:34, 88.77s/it] 35%|███▌      | 1351/3812 [33:19:17<60:44:11, 88.85s/it] 35%|███▌      | 1352/3812 [33:20:46<60:41:34, 88.82s/it] 35%|███▌      | 1353/3812 [33:22:15<60:41:43, 88.86s/it] 36%|███▌      | 1354/3812 [33:23:44<60:35:28, 88.74s/it] 36%|███▌      | 1355/3812 [33:25:13<60:39:15, 88.87s/it] 36%|███▌      | 1356/3812 [33:26:41<60:35:04, 88.80s/it] 36%|███▌      | 1357/3812 [33:28:10<60:35:26, 88.85s/it] 36%|███▌      | 1358/3812 [33:29:39<60:31:57, 88.80s/it] 36%|███▌      | 1359/3812 [33:31:08<60:31:44, 88.83s/it] 36%|███▌      | 1360/3812 [33:32:36<60:26:34, 88.74s/it]                                                         {'loss': 1.3347, 'learning_rate': 3.58736044406916e-05, 'epoch': 0.36}
 36%|███▌      | 1360/3812 [33:32:36<60:26:34, 88.74s/it] 36%|███▌      | 1361/3812 [33:34:05<60:26:46, 88.78s/it] 36%|███▌      | 1362/3812 [33:35:34<60:27:47, 88.84s/it] 36%|███▌      | 1363/3812 [33:37:03<60:27:31, 88.87s/it] 36%|███▌      | 1364/3812 [33:38:32<60:27:00, 88.90s/it] 36%|███▌      | 1365/3812 [33:40:01<60:27:11, 88.94s/it] 36%|███▌      | 1366/3812 [33:41:30<60:23:51, 88.89s/it] 36%|███▌      | 1367/3812 [33:42:59<60:21:52, 88.88s/it] 36%|███▌      | 1368/3812 [33:44:27<60:16:37, 88.79s/it] 36%|███▌      | 1369/3812 [33:45:56<60:17:13, 88.84s/it] 36%|███▌      | 1370/3812 [33:47:25<60:16:16, 88.85s/it]                                                         {'loss': 1.3197, 'learning_rate': 3.5687713248371e-05, 'epoch': 0.36}
 36%|███▌      | 1370/3812 [33:47:25<60:16:16, 88.85s/it] 36%|███▌      | 1371/3812 [33:48:54<60:16:34, 88.90s/it] 36%|███▌      | 1372/3812 [33:50:23<60:10:53, 88.79s/it] 36%|███▌      | 1373/3812 [33:51:52<60:09:25, 88.79s/it] 36%|███▌      | 1374/3812 [33:53:20<60:04:49, 88.72s/it] 36%|███▌      | 1375/3812 [33:54:49<60:03:58, 88.73s/it] 36%|███▌      | 1376/3812 [33:56:18<60:03:29, 88.76s/it] 36%|███▌      | 1377/3812 [33:57:47<60:05:18, 88.84s/it] 36%|███▌      | 1378/3812 [33:59:15<60:00:05, 88.75s/it] 36%|███▌      | 1379/3812 [34:00:44<60:00:12, 88.78s/it] 36%|███▌      | 1380/3812 [34:02:13<59:55:37, 88.71s/it]                                                         {'loss': 1.3228, 'learning_rate': 3.550109615688593e-05, 'epoch': 0.36}
 36%|███▌      | 1380/3812 [34:02:13<59:55:37, 88.71s/it] 36%|███▌      | 1381/3812 [34:03:42<59:56:50, 88.77s/it] 36%|███▋      | 1382/3812 [34:05:10<59:55:36, 88.78s/it] 36%|███▋      | 1383/3812 [34:06:39<59:57:56, 88.87s/it] 36%|███▋      | 1384/3812 [34:08:08<59:52:45, 88.78s/it] 36%|███▋      | 1385/3812 [34:09:37<59:52:52, 88.82s/it] 36%|███▋      | 1386/3812 [34:11:06<59:48:24, 88.75s/it] 36%|███▋      | 1387/3812 [34:12:34<59:48:18, 88.78s/it] 36%|███▋      | 1388/3812 [34:14:03<59:47:02, 88.79s/it] 36%|███▋      | 1389/3812 [34:15:32<59:49:06, 88.88s/it] 36%|███▋      | 1390/3812 [34:17:01<59:43:15, 88.77s/it]                                                         {'loss': 1.3242, 'learning_rate': 3.531376584108905e-05, 'epoch': 0.36}
 36%|███▋      | 1390/3812 [34:17:01<59:43:15, 88.77s/it] 36%|███▋      | 1391/3812 [34:18:30<59:44:16, 88.83s/it] 37%|███▋      | 1392/3812 [34:19:58<59:40:29, 88.77s/it] 37%|███▋      | 1393/3812 [34:21:27<59:40:58, 88.82s/it] 37%|███▋      | 1394/3812 [34:22:57<59:45:30, 88.97s/it] 37%|███▋      | 1395/3812 [34:24:26<59:46:18, 89.03s/it] 37%|███▋      | 1396/3812 [34:25:54<59:38:22, 88.87s/it] 37%|███▋      | 1397/3812 [34:27:23<59:38:30, 88.91s/it] 37%|███▋      | 1398/3812 [34:28:52<59:32:06, 88.78s/it] 37%|███▋      | 1399/3812 [34:30:21<59:32:12, 88.82s/it] 37%|███▋      | 1400/3812 [34:31:49<59:29:06, 88.78s/it]                                                         {'loss': 1.3278, 'learning_rate': 3.512573502427453e-05, 'epoch': 0.37}
 37%|███▋      | 1400/3812 [34:31:49<59:29:06, 88.78s/it] 37%|███▋      | 1401/3812 [34:33:18<59:30:33, 88.86s/it] 37%|███▋      | 1402/3812 [34:34:47<59:29:50, 88.88s/it] 37%|███▋      | 1403/3812 [34:36:16<59:29:18, 88.90s/it] 37%|███▋      | 1404/3812 [34:37:45<59:23:42, 88.80s/it] 37%|███▋      | 1405/3812 [34:39:14<59:22:47, 88.81s/it] 37%|███▋      | 1406/3812 [34:40:42<59:19:51, 88.77s/it] 37%|███▋      | 1407/3812 [34:42:11<59:20:28, 88.83s/it] 37%|███▋      | 1408/3812 [34:43:40<59:16:07, 88.76s/it] 37%|███▋      | 1409/3812 [34:45:09<59:14:42, 88.76s/it] 37%|███▋      | 1410/3812 [34:46:37<59:12:16, 88.73s/it]                                                         {'loss': 1.325, 'learning_rate': 3.493701647731391e-05, 'epoch': 0.37}
 37%|███▋      | 1410/3812 [34:46:37<59:12:16, 88.73s/it] 37%|███▋      | 1411/3812 [34:48:07<59:15:35, 88.85s/it] 37%|███▋      | 1412/3812 [34:49:35<59:11:49, 88.80s/it] 37%|███▋      | 1413/3812 [34:51:04<59:13:49, 88.88s/it] 37%|███▋      | 1414/3812 [34:52:33<59:06:51, 88.75s/it] 37%|███▋      | 1415/3812 [34:54:02<59:06:53, 88.78s/it] 37%|███▋      | 1416/3812 [34:55:30<59:03:33, 88.74s/it] 37%|███▋      | 1417/3812 [34:56:59<59:02:46, 88.75s/it] 37%|███▋      | 1418/3812 [34:58:27<58:58:09, 88.68s/it] 37%|███▋      | 1419/3812 [34:59:57<59:00:42, 88.78s/it] 37%|███▋      | 1420/3812 [35:01:25<58:54:56, 88.67s/it]                                                         {'loss': 1.3152, 'learning_rate': 3.4747623017788677e-05, 'epoch': 0.37}
 37%|███▋      | 1420/3812 [35:01:25<58:54:56, 88.67s/it] 37%|███▋      | 1421/3812 [35:02:54<58:57:29, 88.77s/it] 37%|███▋      | 1422/3812 [35:04:23<58:53:51, 88.72s/it] 37%|███▋      | 1423/3812 [35:05:51<58:51:49, 88.70s/it] 37%|███▋      | 1424/3812 [35:07:20<58:53:16, 88.78s/it] 37%|███▋      | 1425/3812 [35:08:49<58:57:04, 88.91s/it] 37%|███▋      | 1426/3812 [35:10:18<58:50:50, 88.79s/it] 37%|███▋      | 1427/3812 [35:11:47<58:52:43, 88.87s/it] 37%|███▋      | 1428/3812 [35:13:15<58:43:55, 88.69s/it] 37%|███▋      | 1429/3812 [35:14:44<58:44:25, 88.74s/it] 38%|███▊      | 1430/3812 [35:16:13<58:43:13, 88.75s/it]                                                         {'loss': 1.3244, 'learning_rate': 3.4557567509119736e-05, 'epoch': 0.38}
 38%|███▊      | 1430/3812 [35:16:13<58:43:13, 88.75s/it] 38%|███▊      | 1431/3812 [35:17:42<58:44:43, 88.82s/it] 38%|███▊      | 1432/3812 [35:19:10<58:39:42, 88.73s/it] 38%|███▊      | 1433/3812 [35:20:39<58:37:47, 88.72s/it] 38%|███▊      | 1434/3812 [35:22:07<58:32:53, 88.63s/it] 38%|███▊      | 1435/3812 [35:23:36<58:31:54, 88.65s/it] 38%|███▊      | 1436/3812 [35:25:05<58:32:42, 88.70s/it] 38%|███▊      | 1437/3812 [35:26:34<58:34:41, 88.79s/it] 38%|███▊      | 1438/3812 [35:28:03<58:36:02, 88.86s/it] 38%|███▊      | 1439/3812 [35:29:32<58:34:42, 88.87s/it] 38%|███▊      | 1440/3812 [35:31:01<58:31:20, 88.82s/it]                                                         {'loss': 1.3156, 'learning_rate': 3.4366862859693726e-05, 'epoch': 0.38}
 38%|███▊      | 1440/3812 [35:31:01<58:31:20, 88.82s/it] 38%|███▊      | 1441/3812 [35:32:29<58:30:43, 88.84s/it] 38%|███▊      | 1442/3812 [35:33:58<58:28:59, 88.84s/it] 38%|███▊      | 1443/3812 [35:35:27<58:28:56, 88.87s/it] 38%|███▊      | 1444/3812 [35:36:56<58:21:59, 88.73s/it] 38%|███▊      | 1445/3812 [35:38:25<58:22:37, 88.79s/it] 38%|███▊      | 1446/3812 [35:39:53<58:16:32, 88.67s/it] 38%|███▊      | 1447/3812 [35:41:22<58:17:46, 88.74s/it] 38%|███▊      | 1448/3812 [35:42:50<58:14:08, 88.68s/it] 38%|███▊      | 1449/3812 [35:44:19<58:12:13, 88.67s/it] 38%|███▊      | 1450/3812 [35:45:48<58:08:10, 88.61s/it]                                                         {'loss': 1.3206, 'learning_rate': 3.417552202198632e-05, 'epoch': 0.38}
 38%|███▊      | 1450/3812 [35:45:48<58:08:10, 88.61s/it] 38%|███▊      | 1451/3812 [35:47:16<58:08:12, 88.65s/it] 38%|███▊      | 1452/3812 [35:48:45<58:04:56, 88.60s/it] 38%|███▊      | 1453/3812 [35:50:14<58:05:14, 88.65s/it] 38%|███▊      | 1454/3812 [35:51:42<58:02:58, 88.63s/it] 38%|███▊      | 1455/3812 [35:53:11<58:03:11, 88.67s/it] 38%|███▊      | 1456/3812 [35:54:39<57:59:17, 88.61s/it] 38%|███▊      | 1457/3812 [35:56:08<58:01:05, 88.69s/it] 38%|███▊      | 1458/3812 [35:57:37<57:58:02, 88.65s/it] 38%|███▊      | 1459/3812 [35:59:06<57:57:40, 88.68s/it] 38%|███▊      | 1460/3812 [36:00:34<57:59:09, 88.75s/it]                                                         {'loss': 1.3219, 'learning_rate': 3.398355799168245e-05, 'epoch': 0.38}
 38%|███▊      | 1460/3812 [36:00:34<57:59:09, 88.75s/it] 38%|███▊      | 1461/3812 [36:02:04<58:03:14, 88.90s/it] 38%|███▊      | 1462/3812 [36:03:32<57:57:46, 88.79s/it] 38%|███▊      | 1463/3812 [36:05:01<57:56:16, 88.79s/it] 38%|███▊      | 1464/3812 [36:06:30<57:52:19, 88.73s/it] 38%|███▊      | 1465/3812 [36:07:58<57:51:15, 88.74s/it] 38%|███▊      | 1466/3812 [36:09:27<57:47:35, 88.69s/it] 38%|███▊      | 1467/3812 [36:10:56<57:46:43, 88.70s/it] 39%|███▊      | 1468/3812 [36:12:24<57:45:32, 88.71s/it] 39%|███▊      | 1469/3812 [36:13:53<57:41:55, 88.65s/it] 39%|███▊      | 1470/3812 [36:15:21<57:38:55, 88.61s/it]                                                         {'loss': 1.3166, 'learning_rate': 3.379098380679372e-05, 'epoch': 0.39}
 39%|███▊      | 1470/3812 [36:15:21<57:38:55, 88.61s/it] 39%|███▊      | 1471/3812 [36:16:50<57:40:16, 88.69s/it] 39%|███▊      | 1472/3812 [36:18:19<57:39:14, 88.70s/it] 39%|███▊      | 1473/3812 [36:19:48<57:40:08, 88.76s/it] 39%|███▊      | 1474/3812 [36:21:16<57:35:55, 88.69s/it] 39%|███▊      | 1475/3812 [36:22:45<57:35:02, 88.70s/it] 39%|███▊      | 1476/3812 [36:24:14<57:29:01, 88.59s/it] 39%|███▊      | 1477/3812 [36:25:42<57:28:35, 88.61s/it] 39%|███▉      | 1478/3812 [36:27:11<57:26:48, 88.61s/it] 39%|███▉      | 1479/3812 [36:28:40<57:28:41, 88.69s/it] 39%|███▉      | 1480/3812 [36:30:08<57:27:54, 88.71s/it]                                                         {'loss': 1.3142, 'learning_rate': 3.3597812546772824e-05, 'epoch': 0.39}
 39%|███▉      | 1480/3812 [36:30:08<57:27:54, 88.71s/it] 39%|███▉      | 1481/3812 [36:31:37<57:29:47, 88.80s/it] 39%|███▉      | 1482/3812 [36:33:06<57:22:52, 88.66s/it] 39%|███▉      | 1483/3812 [36:34:35<57:22:48, 88.69s/it] 39%|███▉      | 1484/3812 [36:36:03<57:22:36, 88.73s/it] 39%|███▉      | 1485/3812 [36:37:32<57:24:40, 88.82s/it] 39%|███▉      | 1486/3812 [36:39:01<57:25:49, 88.89s/it] 39%|███▉      | 1487/3812 [36:40:30<57:25:31, 88.92s/it] 39%|███▉      | 1488/3812 [36:41:59<57:21:19, 88.85s/it] 39%|███▉      | 1489/3812 [36:43:28<57:19:24, 88.84s/it] 39%|███▉      | 1490/3812 [36:44:57<57:16:37, 88.80s/it]                                                         {'loss': 1.3211, 'learning_rate': 3.340405733162524e-05, 'epoch': 0.39}
 39%|███▉      | 1490/3812 [36:44:57<57:16:37, 88.80s/it] 39%|███▉      | 1491/3812 [36:46:26<57:19:31, 88.92s/it] 39%|███▉      | 1492/3812 [36:47:54<57:13:22, 88.79s/it] 39%|███▉      | 1493/3812 [36:49:23<57:13:16, 88.83s/it] 39%|███▉      | 1494/3812 [36:50:52<57:08:36, 88.75s/it] 39%|███▉      | 1495/3812 [36:52:21<57:07:38, 88.76s/it] 39%|███▉      | 1496/3812 [36:53:49<57:04:12, 88.71s/it] 39%|███▉      | 1497/3812 [36:55:18<57:06:42, 88.81s/it] 39%|███▉      | 1498/3812 [36:56:47<57:00:54, 88.70s/it] 39%|███▉      | 1499/3812 [36:58:15<56:59:20, 88.70s/it] 39%|███▉      | 1500/3812 [36:59:44<56:56:26, 88.66s/it]                                                         {'loss': 1.3104, 'learning_rate': 3.320973132101809e-05, 'epoch': 0.39}
 39%|███▉      | 1500/3812 [36:59:44<56:56:26, 88.66s/it][INFO|trainer.py:2936] 2024-02-10 12:12:03,197 >> Saving model checkpoint to /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1500
[INFO|configuration_utils.py:473] 2024-02-10 12:12:03,199 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1500/config.json
[INFO|configuration_utils.py:594] 2024-02-10 12:12:03,199 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1500/generation_config.json
[INFO|modeling_utils.py:2501] 2024-02-10 12:12:20,859 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2433] 2024-02-10 12:12:20,859 >> tokenizer config file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-10 12:12:20,859 >> Special tokens file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-1500/special_tokens_map.json
 39%|███▉      | 1501/3812 [37:01:40<62:08:19, 96.80s/it] 39%|███▉      | 1502/3812 [37:03:09<60:34:45, 94.41s/it] 39%|███▉      | 1503/3812 [37:04:38<59:30:49, 92.79s/it] 39%|███▉      | 1504/3812 [37:06:06<58:43:44, 91.60s/it] 39%|███▉      | 1505/3812 [37:07:35<58:10:14, 90.77s/it] 40%|███▉      | 1506/3812 [37:09:04<57:42:11, 90.08s/it] 40%|███▉      | 1507/3812 [37:10:32<57:24:44, 89.67s/it] 40%|███▉      | 1508/3812 [37:12:01<57:08:56, 89.30s/it] 40%|███▉      | 1509/3812 [37:13:30<57:01:28, 89.14s/it] 40%|███▉      | 1510/3812 [37:14:58<56:54:16, 88.99s/it]                                                         {'loss': 1.3162, 'learning_rate': 3.3014847713386384e-05, 'epoch': 0.4}
 40%|███▉      | 1510/3812 [37:14:58<56:54:16, 88.99s/it] 40%|███▉      | 1511/3812 [37:16:27<56:52:03, 88.97s/it] 40%|███▉      | 1512/3812 [37:17:56<56:45:27, 88.84s/it] 40%|███▉      | 1513/3812 [37:19:24<56:41:06, 88.76s/it] 40%|███▉      | 1514/3812 [37:20:53<56:37:34, 88.71s/it] 40%|███▉      | 1515/3812 [37:22:21<56:35:06, 88.68s/it] 40%|███▉      | 1516/3812 [37:23:50<56:32:51, 88.66s/it] 40%|███▉      | 1517/3812 [37:25:19<56:32:25, 88.69s/it] 40%|███▉      | 1518/3812 [37:26:47<56:28:12, 88.62s/it] 40%|███▉      | 1519/3812 [37:28:16<56:28:08, 88.66s/it] 40%|███▉      | 1520/3812 [37:29:45<56:25:09, 88.62s/it]                                                         {'loss': 1.3185, 'learning_rate': 3.281941974503659e-05, 'epoch': 0.4}
 40%|███▉      | 1520/3812 [37:29:45<56:25:09, 88.62s/it] 40%|███▉      | 1521/3812 [37:31:13<56:23:56, 88.62s/it] 40%|███▉      | 1522/3812 [37:32:42<56:22:42, 88.63s/it] 40%|███▉      | 1523/3812 [37:34:11<56:23:28, 88.69s/it] 40%|███▉      | 1524/3812 [37:35:39<56:20:26, 88.65s/it] 40%|████      | 1525/3812 [37:37:08<56:19:37, 88.67s/it] 40%|████      | 1526/3812 [37:38:36<56:16:51, 88.63s/it] 40%|████      | 1527/3812 [37:40:06<56:19:50, 88.75s/it] 40%|████      | 1528/3812 [37:41:34<56:18:15, 88.75s/it] 40%|████      | 1529/3812 [37:43:03<56:20:26, 88.84s/it] 40%|████      | 1530/3812 [37:44:32<56:14:07, 88.71s/it]                                                         {'loss': 1.3188, 'learning_rate': 3.2623460689247644e-05, 'epoch': 0.4}
 40%|████      | 1530/3812 [37:44:32<56:14:07, 88.71s/it] 40%|████      | 1531/3812 [37:46:00<56:12:02, 88.70s/it] 40%|████      | 1532/3812 [37:47:29<56:08:46, 88.65s/it] 40%|████      | 1533/3812 [37:48:58<56:09:35, 88.71s/it] 40%|████      | 1534/3812 [37:50:26<56:06:53, 88.68s/it] 40%|████      | 1535/3812 [37:51:55<56:06:12, 88.70s/it] 40%|████      | 1536/3812 [37:53:23<55:59:24, 88.56s/it] 40%|████      | 1537/3812 [37:54:52<55:59:34, 88.60s/it] 40%|████      | 1538/3812 [37:56:21<55:56:40, 88.57s/it] 40%|████      | 1539/3812 [37:57:49<55:57:21, 88.62s/it] 40%|████      | 1540/3812 [37:59:18<55:58:33, 88.69s/it]                                                         {'loss': 1.3053, 'learning_rate': 3.242698385536939e-05, 'epoch': 0.4}
 40%|████      | 1540/3812 [37:59:18<55:58:33, 88.69s/it] 40%|████      | 1541/3812 [38:00:47<56:01:06, 88.80s/it] 40%|████      | 1542/3812 [38:02:16<55:56:59, 88.73s/it] 40%|████      | 1543/3812 [38:03:45<55:59:13, 88.83s/it] 41%|████      | 1544/3812 [38:05:13<55:54:47, 88.75s/it] 41%|████      | 1545/3812 [38:06:42<55:53:39, 88.76s/it] 41%|████      | 1546/3812 [38:08:11<55:51:20, 88.74s/it] 41%|████      | 1547/3812 [38:09:40<55:51:16, 88.78s/it] 41%|████      | 1548/3812 [38:11:08<55:46:38, 88.69s/it] 41%|████      | 1549/3812 [38:12:37<55:44:05, 88.66s/it] 41%|████      | 1550/3812 [38:14:06<55:43:25, 88.69s/it]                                                         {'loss': 1.3081, 'learning_rate': 3.223000258791868e-05, 'epoch': 0.41}
 41%|████      | 1550/3812 [38:14:06<55:43:25, 88.69s/it] 41%|████      | 1551/3812 [38:15:34<55:41:41, 88.68s/it] 41%|████      | 1552/3812 [38:17:03<55:42:41, 88.74s/it] 41%|████      | 1553/3812 [38:18:32<55:43:12, 88.80s/it] 41%|████      | 1554/3812 [38:20:00<55:37:13, 88.68s/it] 41%|████      | 1555/3812 [38:21:29<55:37:29, 88.72s/it] 41%|████      | 1556/3812 [38:22:58<55:39:05, 88.81s/it] 41%|████      | 1557/3812 [38:24:27<55:37:53, 88.81s/it] 41%|████      | 1558/3812 [38:25:56<55:35:49, 88.80s/it] 41%|████      | 1559/3812 [38:27:25<55:35:07, 88.82s/it] 41%|████      | 1560/3812 [38:28:53<55:32:09, 88.78s/it]                                                         {'loss': 1.3245, 'learning_rate': 3.203253026567302e-05, 'epoch': 0.41}
 41%|████      | 1560/3812 [38:28:53<55:32:09, 88.78s/it] 41%|████      | 1561/3812 [38:30:22<55:28:58, 88.73s/it] 41%|████      | 1562/3812 [38:31:51<55:27:05, 88.72s/it] 41%|████      | 1563/3812 [38:33:20<55:26:33, 88.75s/it] 41%|████      | 1564/3812 [38:34:48<55:24:01, 88.72s/it] 41%|████      | 1565/3812 [38:36:17<55:25:16, 88.79s/it] 41%|████      | 1566/3812 [38:37:46<55:20:46, 88.71s/it] 41%|████      | 1567/3812 [38:39:15<55:21:54, 88.78s/it] 41%|████      | 1568/3812 [38:40:43<55:19:01, 88.74s/it] 41%|████      | 1569/3812 [38:42:12<55:16:09, 88.71s/it] 41%|████      | 1570/3812 [38:43:41<55:15:19, 88.72s/it]                                                         {'loss': 1.3101, 'learning_rate': 3.183458030076185e-05, 'epoch': 0.41}
 41%|████      | 1570/3812 [38:43:41<55:15:19, 88.72s/it] 41%|████      | 1571/3812 [38:45:10<55:17:20, 88.82s/it] 41%|████      | 1572/3812 [38:46:38<55:12:04, 88.72s/it] 41%|████▏     | 1573/3812 [38:48:07<55:14:13, 88.81s/it] 41%|████▏     | 1574/3812 [38:49:36<55:12:04, 88.80s/it] 41%|████▏     | 1575/3812 [38:51:05<55:13:39, 88.88s/it] 41%|████▏     | 1576/3812 [38:52:34<55:10:44, 88.84s/it] 41%|████▏     | 1577/3812 [38:54:03<55:09:00, 88.83s/it] 41%|████▏     | 1578/3812 [38:55:32<55:07:54, 88.84s/it] 41%|████▏     | 1579/3812 [38:57:00<55:06:30, 88.84s/it] 41%|████▏     | 1580/3812 [38:58:29<55:02:21, 88.77s/it]                                                         {'loss': 1.3111, 'learning_rate': 3.163616613775568e-05, 'epoch': 0.41}
 41%|████▏     | 1580/3812 [38:58:29<55:02:21, 88.77s/it] 41%|████▏     | 1581/3812 [38:59:58<55:01:27, 88.79s/it] 42%|████▏     | 1582/3812 [39:01:27<55:00:16, 88.80s/it] 42%|████▏     | 1583/3812 [39:02:55<54:59:48, 88.82s/it] 42%|████▏     | 1584/3812 [39:04:24<54:55:24, 88.75s/it] 42%|████▏     | 1585/3812 [39:05:53<54:54:52, 88.77s/it] 42%|████▏     | 1586/3812 [39:07:21<54:50:56, 88.70s/it] 42%|████▏     | 1587/3812 [39:08:50<54:51:13, 88.75s/it] 42%|████▏     | 1588/3812 [39:10:19<54:49:13, 88.74s/it] 42%|████▏     | 1589/3812 [39:11:48<54:51:47, 88.85s/it] 42%|████▏     | 1590/3812 [39:13:17<54:46:23, 88.74s/it]                                                         {'loss': 1.312, 'learning_rate': 3.143730125275288e-05, 'epoch': 0.42}
 42%|████▏     | 1590/3812 [39:13:17<54:46:23, 88.74s/it] 42%|████▏     | 1591/3812 [39:14:45<54:45:30, 88.76s/it] 42%|████▏     | 1592/3812 [39:16:14<54:45:14, 88.79s/it] 42%|████▏     | 1593/3812 [39:17:43<54:43:56, 88.80s/it] 42%|████▏     | 1594/3812 [39:19:12<54:40:24, 88.74s/it] 42%|████▏     | 1595/3812 [39:20:40<54:39:46, 88.76s/it] 42%|████▏     | 1596/3812 [39:22:09<54:35:32, 88.69s/it] 42%|████▏     | 1597/3812 [39:23:38<54:36:15, 88.75s/it] 42%|████▏     | 1598/3812 [39:25:06<54:32:39, 88.69s/it] 42%|████▏     | 1599/3812 [39:26:35<54:33:54, 88.76s/it] 42%|████▏     | 1600/3812 [39:28:04<54:30:46, 88.72s/it]                                                         {'loss': 1.3107, 'learning_rate': 3.123799915246442e-05, 'epoch': 0.42}
 42%|████▏     | 1600/3812 [39:28:04<54:30:46, 88.72s/it] 42%|████▏     | 1601/3812 [39:29:33<54:29:08, 88.71s/it] 42%|████▏     | 1602/3812 [39:31:01<54:25:46, 88.66s/it] 42%|████▏     | 1603/3812 [39:32:30<54:30:15, 88.83s/it] 42%|████▏     | 1604/3812 [39:33:59<54:23:47, 88.69s/it] 42%|████▏     | 1605/3812 [39:35:28<54:25:01, 88.76s/it] 42%|████▏     | 1606/3812 [39:36:56<54:22:30, 88.74s/it] 42%|████▏     | 1607/3812 [39:38:25<54:21:47, 88.76s/it] 42%|████▏     | 1608/3812 [39:39:54<54:16:27, 88.65s/it] 42%|████▏     | 1609/3812 [39:41:23<54:17:17, 88.71s/it] 42%|████▏     | 1610/3812 [39:42:51<54:12:18, 88.62s/it]                                                         {'loss': 1.3173, 'learning_rate': 3.103827337329653e-05, 'epoch': 0.42}
 42%|████▏     | 1610/3812 [39:42:51<54:12:18, 88.62s/it] 42%|████▏     | 1611/3812 [39:44:20<54:12:52, 88.67s/it] 42%|████▏     | 1612/3812 [39:45:48<54:11:05, 88.67s/it] 42%|████▏     | 1613/3812 [39:47:17<54:12:45, 88.75s/it] 42%|████▏     | 1614/3812 [39:48:46<54:09:18, 88.70s/it] 42%|████▏     | 1615/3812 [39:50:15<54:10:55, 88.78s/it] 42%|████▏     | 1616/3812 [39:51:43<54:07:16, 88.72s/it] 42%|████▏     | 1617/3812 [39:53:12<54:05:42, 88.72s/it] 42%|████▏     | 1618/3812 [39:54:41<54:06:07, 88.77s/it] 42%|████▏     | 1619/3812 [39:56:10<54:07:51, 88.86s/it] 42%|████▏     | 1620/3812 [39:57:39<54:01:26, 88.73s/it]                                                         {'loss': 1.3098, 'learning_rate': 3.083813748043128e-05, 'epoch': 0.42}
 42%|████▏     | 1620/3812 [39:57:39<54:01:26, 88.73s/it] 43%|████▎     | 1621/3812 [39:59:07<54:00:38, 88.74s/it] 43%|████▎     | 1622/3812 [40:00:36<53:54:12, 88.61s/it] 43%|████▎     | 1623/3812 [40:02:04<53:53:20, 88.63s/it] 43%|████▎     | 1624/3812 [40:03:33<53:51:43, 88.62s/it] 43%|████▎     | 1625/3812 [40:05:02<53:53:00, 88.70s/it] 43%|████▎     | 1626/3812 [40:06:30<53:49:24, 88.64s/it] 43%|████▎     | 1627/3812 [40:07:59<53:49:00, 88.67s/it] 43%|████▎     | 1628/3812 [40:09:27<53:45:19, 88.61s/it] 43%|████▎     | 1629/3812 [40:10:56<53:47:02, 88.70s/it] 43%|████▎     | 1630/3812 [40:12:25<53:45:03, 88.68s/it]                                                         {'loss': 1.3022, 'learning_rate': 3.063760506690528e-05, 'epoch': 0.43}
 43%|████▎     | 1630/3812 [40:12:25<53:45:03, 88.68s/it] 43%|████▎     | 1631/3812 [40:13:54<53:46:51, 88.77s/it] 43%|████▎     | 1632/3812 [40:15:23<53:43:32, 88.72s/it] 43%|████▎     | 1633/3812 [40:16:51<53:41:57, 88.72s/it] 43%|████▎     | 1634/3812 [40:18:20<53:36:29, 88.61s/it] 43%|████▎     | 1635/3812 [40:19:48<53:37:20, 88.67s/it] 43%|████▎     | 1636/3812 [40:21:17<53:34:52, 88.65s/it] 43%|████▎     | 1637/3812 [40:22:46<53:34:37, 88.68s/it] 43%|████▎     | 1638/3812 [40:24:14<53:27:37, 88.53s/it] 43%|████▎     | 1639/3812 [40:25:43<53:27:11, 88.56s/it] 43%|████▎     | 1640/3812 [40:27:11<53:22:38, 88.47s/it]                                                         {'loss': 1.3075, 'learning_rate': 3.0436689752686443e-05, 'epoch': 0.43}
 43%|████▎     | 1640/3812 [40:27:11<53:22:38, 88.47s/it] 43%|████▎     | 1641/3812 [40:28:40<53:23:12, 88.53s/it] 43%|████▎     | 1642/3812 [40:30:08<53:23:12, 88.57s/it] 43%|████▎     | 1643/3812 [40:31:37<53:25:37, 88.68s/it] 43%|████▎     | 1644/3812 [40:33:06<53:21:03, 88.59s/it] 43%|████▎     | 1645/3812 [40:34:35<53:24:06, 88.72s/it] 43%|████▎     | 1646/3812 [40:36:03<53:20:52, 88.67s/it] 43%|████▎     | 1647/3812 [40:37:32<53:21:24, 88.72s/it] 43%|████▎     | 1648/3812 [40:39:01<53:21:01, 88.75s/it] 43%|████▎     | 1649/3812 [40:40:30<53:24:46, 88.90s/it] 43%|████▎     | 1650/3812 [40:41:58<53:17:56, 88.75s/it]                                                         {'loss': 1.3007, 'learning_rate': 3.0235405183748906e-05, 'epoch': 0.43}
 43%|████▎     | 1650/3812 [40:41:58<53:17:56, 88.75s/it] 43%|████▎     | 1651/3812 [40:43:27<53:18:30, 88.81s/it] 43%|████▎     | 1652/3812 [40:44:56<53:12:38, 88.68s/it] 43%|████▎     | 1653/3812 [40:46:25<53:13:08, 88.74s/it] 43%|████▎     | 1654/3812 [40:47:53<53:10:24, 88.70s/it] 43%|████▎     | 1655/3812 [40:49:22<53:09:27, 88.72s/it] 43%|████▎     | 1656/3812 [40:50:50<53:05:20, 88.65s/it] 43%|████▎     | 1657/3812 [40:52:19<53:04:35, 88.67s/it] 43%|████▎     | 1658/3812 [40:53:48<53:00:40, 88.60s/it] 44%|████▎     | 1659/3812 [40:55:17<53:02:24, 88.69s/it] 44%|████▎     | 1660/3812 [40:56:45<53:02:01, 88.72s/it]                                                         {'loss': 1.3092, 'learning_rate': 3.003376503114625e-05, 'epoch': 0.44}
 44%|████▎     | 1660/3812 [40:56:45<53:02:01, 88.72s/it] 44%|████▎     | 1661/3812 [40:58:14<53:02:52, 88.78s/it] 44%|████▎     | 1662/3812 [40:59:43<53:00:23, 88.76s/it] 44%|████▎     | 1663/3812 [41:01:12<53:03:34, 88.89s/it] 44%|████▎     | 1664/3812 [41:02:41<52:58:46, 88.79s/it] 44%|████▎     | 1665/3812 [41:04:09<52:56:05, 88.76s/it] 44%|████▎     | 1666/3812 [41:05:38<52:51:43, 88.68s/it] 44%|████▎     | 1667/3812 [41:07:06<52:48:45, 88.64s/it] 44%|████▍     | 1668/3812 [41:08:35<52:44:44, 88.57s/it] 44%|████▍     | 1669/3812 [41:10:03<52:42:55, 88.56s/it] 44%|████▍     | 1670/3812 [41:11:32<52:41:24, 88.56s/it]                                                         {'loss': 1.3092, 'learning_rate': 2.983178299008295e-05, 'epoch': 0.44}
 44%|████▍     | 1670/3812 [41:11:32<52:41:24, 88.56s/it] 44%|████▍     | 1671/3812 [41:13:01<52:41:46, 88.61s/it] 44%|████▍     | 1672/3812 [41:14:29<52:40:21, 88.61s/it] 44%|████▍     | 1673/3812 [41:15:58<52:41:03, 88.67s/it] 44%|████▍     | 1674/3812 [41:17:26<52:36:07, 88.57s/it] 44%|████▍     | 1675/3812 [41:18:55<52:36:31, 88.62s/it] 44%|████▍     | 1676/3812 [41:20:24<52:34:40, 88.61s/it] 44%|████▍     | 1677/3812 [41:21:53<52:36:07, 88.70s/it] 44%|████▍     | 1678/3812 [41:23:21<52:36:31, 88.75s/it] 44%|████▍     | 1679/3812 [41:24:51<52:38:46, 88.85s/it] 44%|████▍     | 1680/3812 [41:26:19<52:35:52, 88.81s/it]                                                         {'loss': 1.3048, 'learning_rate': 2.962947277898422e-05, 'epoch': 0.44}
 44%|████▍     | 1680/3812 [41:26:19<52:35:52, 88.81s/it] 44%|████▍     | 1681/3812 [41:27:48<52:35:32, 88.85s/it] 44%|████▍     | 1682/3812 [41:29:17<52:33:41, 88.84s/it] 44%|████▍     | 1683/3812 [41:30:46<52:30:58, 88.80s/it] 44%|████▍     | 1684/3812 [41:32:15<52:29:26, 88.80s/it] 44%|████▍     | 1685/3812 [41:33:43<52:27:44, 88.79s/it] 44%|████▍     | 1686/3812 [41:35:12<52:22:43, 88.69s/it] 44%|████▍     | 1687/3812 [41:36:40<52:18:10, 88.61s/it] 44%|████▍     | 1688/3812 [41:38:09<52:14:50, 88.55s/it] 44%|████▍     | 1689/3812 [41:39:37<52:13:58, 88.57s/it] 44%|████▍     | 1690/3812 [41:41:06<52:13:34, 88.60s/it]                                                         {'loss': 1.3069, 'learning_rate': 2.9426848138564245e-05, 'epoch': 0.44}
 44%|████▍     | 1690/3812 [41:41:06<52:13:34, 88.60s/it] 44%|████▍     | 1691/3812 [41:42:35<52:15:35, 88.70s/it] 44%|████▍     | 1692/3812 [41:44:03<52:12:15, 88.65s/it] 44%|████▍     | 1693/3812 [41:45:32<52:14:50, 88.76s/it] 44%|████▍     | 1694/3812 [41:47:01<52:11:19, 88.71s/it] 44%|████▍     | 1695/3812 [41:48:30<52:11:56, 88.77s/it] 44%|████▍     | 1696/3812 [41:49:58<52:08:22, 88.71s/it] 45%|████▍     | 1697/3812 [41:51:27<52:08:32, 88.75s/it] 45%|████▍     | 1698/3812 [41:52:56<52:03:16, 88.65s/it] 45%|████▍     | 1699/3812 [41:54:24<52:01:56, 88.65s/it] 45%|████▍     | 1700/3812 [41:55:53<52:00:51, 88.66s/it]                                                         {'loss': 1.302, 'learning_rate': 2.9223922830892975e-05, 'epoch': 0.45}
 45%|████▍     | 1700/3812 [41:55:53<52:00:51, 88.66s/it] 45%|████▍     | 1701/3812 [41:57:22<51:59:50, 88.67s/it] 45%|████▍     | 1702/3812 [41:58:50<51:58:08, 88.67s/it] 45%|████▍     | 1703/3812 [42:00:19<51:56:15, 88.66s/it] 45%|████▍     | 1704/3812 [42:01:48<51:52:55, 88.60s/it] 45%|████▍     | 1705/3812 [42:03:16<51:52:18, 88.63s/it] 45%|████▍     | 1706/3812 [42:04:45<51:50:06, 88.61s/it] 45%|████▍     | 1707/3812 [42:06:14<51:50:35, 88.66s/it] 45%|████▍     | 1708/3812 [42:07:42<51:51:19, 88.73s/it] 45%|████▍     | 1709/3812 [42:09:11<51:50:47, 88.75s/it] 45%|████▍     | 1710/3812 [42:10:40<51:48:08, 88.72s/it]                                                         {'loss': 1.3049, 'learning_rate': 2.9020710638461373e-05, 'epoch': 0.45}
 45%|████▍     | 1710/3812 [42:10:40<51:48:08, 88.72s/it] 45%|████▍     | 1711/3812 [42:12:09<51:47:04, 88.73s/it] 45%|████▍     | 1712/3812 [42:13:37<51:42:23, 88.64s/it] 45%|████▍     | 1713/3812 [42:15:06<51:41:52, 88.67s/it] 45%|████▍     | 1714/3812 [42:16:34<51:40:29, 88.67s/it] 45%|████▍     | 1715/3812 [42:18:04<51:43:09, 88.79s/it] 45%|████▌     | 1716/3812 [42:19:32<51:40:50, 88.76s/it] 45%|████▌     | 1717/3812 [42:21:01<51:41:16, 88.82s/it] 45%|████▌     | 1718/3812 [42:22:30<51:39:51, 88.82s/it] 45%|████▌     | 1719/3812 [42:23:59<51:41:08, 88.90s/it] 45%|████▌     | 1720/3812 [42:25:28<51:39:08, 88.89s/it]                                                         {'loss': 1.2976, 'learning_rate': 2.8817225363245344e-05, 'epoch': 0.45}
 45%|████▌     | 1720/3812 [42:25:28<51:39:08, 88.89s/it] 45%|████▌     | 1721/3812 [42:26:57<51:37:04, 88.87s/it] 45%|████▌     | 1722/3812 [42:28:26<51:34:22, 88.83s/it] 45%|████▌     | 1723/3812 [42:29:55<51:35:02, 88.90s/it] 45%|████▌     | 1724/3812 [42:31:23<51:30:41, 88.81s/it] 45%|████▌     | 1725/3812 [42:32:52<51:28:54, 88.80s/it] 45%|████▌     | 1726/3812 [42:34:21<51:25:48, 88.76s/it] 45%|████▌     | 1727/3812 [42:35:50<51:25:50, 88.80s/it] 45%|████▌     | 1728/3812 [42:37:18<51:20:28, 88.69s/it] 45%|████▌     | 1729/3812 [42:38:47<51:18:17, 88.67s/it] 45%|████▌     | 1730/3812 [42:40:15<51:15:32, 88.63s/it]                                                         {'loss': 1.3108, 'learning_rate': 2.8613480825768292e-05, 'epoch': 0.45}
 45%|████▌     | 1730/3812 [42:40:15<51:15:32, 88.63s/it] 45%|████▌     | 1731/3812 [42:41:44<51:13:35, 88.62s/it] 45%|████▌     | 1732/3812 [42:43:12<51:12:58, 88.64s/it] 45%|████▌     | 1733/3812 [42:44:41<51:14:51, 88.74s/it] 45%|████▌     | 1734/3812 [42:46:10<51:16:10, 88.82s/it] 46%|████▌     | 1735/3812 [42:47:39<51:16:58, 88.89s/it] 46%|████▌     | 1736/3812 [42:49:08<51:15:33, 88.89s/it] 46%|████▌     | 1737/3812 [42:50:37<51:15:46, 88.94s/it] 46%|████▌     | 1738/3812 [42:52:06<51:13:41, 88.92s/it] 46%|████▌     | 1739/3812 [42:53:35<51:11:39, 88.90s/it] 46%|████▌     | 1740/3812 [42:55:04<51:07:32, 88.83s/it]                                                         {'loss': 1.3017, 'learning_rate': 2.8409490864162504e-05, 'epoch': 0.46}
 46%|████▌     | 1740/3812 [42:55:04<51:07:32, 88.83s/it] 46%|████▌     | 1741/3812 [42:56:33<51:05:32, 88.81s/it] 46%|████▌     | 1742/3812 [42:58:01<51:00:44, 88.72s/it] 46%|████▌     | 1743/3812 [42:59:30<50:57:51, 88.68s/it] 46%|████▌     | 1744/3812 [43:00:58<50:55:18, 88.65s/it] 46%|████▌     | 1745/3812 [43:02:27<50:56:06, 88.71s/it] 46%|████▌     | 1746/3812 [43:03:56<50:56:44, 88.77s/it] 46%|████▌     | 1747/3812 [43:05:25<50:55:56, 88.79s/it] 46%|████▌     | 1748/3812 [43:06:53<50:52:32, 88.74s/it] 46%|████▌     | 1749/3812 [43:08:22<50:52:37, 88.78s/it] 46%|████▌     | 1750/3812 [43:09:51<50:50:05, 88.75s/it]                                                         {'loss': 1.3111, 'learning_rate': 2.8205269333229197e-05, 'epoch': 0.46}
 46%|████▌     | 1750/3812 [43:09:51<50:50:05, 88.75s/it] 46%|████▌     | 1751/3812 [43:11:20<50:52:47, 88.87s/it] 46%|████▌     | 1752/3812 [43:12:49<50:50:51, 88.86s/it] 46%|████▌     | 1753/3812 [43:14:18<50:49:23, 88.86s/it] 46%|████▌     | 1754/3812 [43:15:46<50:44:53, 88.77s/it] 46%|████▌     | 1755/3812 [43:17:15<50:42:16, 88.74s/it] 46%|████▌     | 1756/3812 [43:18:44<50:37:51, 88.65s/it] 46%|████▌     | 1757/3812 [43:20:12<50:36:48, 88.67s/it] 46%|████▌     | 1758/3812 [43:21:41<50:33:58, 88.63s/it] 46%|████▌     | 1759/3812 [43:23:09<50:32:32, 88.63s/it] 46%|████▌     | 1760/3812 [43:24:38<50:30:32, 88.61s/it]                                                         {'loss': 1.298, 'learning_rate': 2.8000830103497567e-05, 'epoch': 0.46}
 46%|████▌     | 1760/3812 [43:24:38<50:30:32, 88.61s/it] 46%|████▌     | 1761/3812 [43:26:07<50:29:43, 88.63s/it] 46%|████▌     | 1762/3812 [43:27:35<50:29:55, 88.68s/it] 46%|████▌     | 1763/3812 [43:29:04<50:31:18, 88.76s/it] 46%|████▋     | 1764/3812 [43:30:33<50:29:51, 88.77s/it] 46%|████▋     | 1765/3812 [43:32:02<50:29:16, 88.79s/it] 46%|████▋     | 1766/3812 [43:33:31<50:25:22, 88.72s/it] 46%|████▋     | 1767/3812 [43:34:59<50:22:59, 88.69s/it] 46%|████▋     | 1768/3812 [43:36:28<50:22:59, 88.74s/it] 46%|████▋     | 1769/3812 [43:37:57<50:24:01, 88.81s/it] 46%|████▋     | 1770/3812 [43:39:26<50:19:35, 88.72s/it]                                                         {'loss': 1.2982, 'learning_rate': 2.779618706028272e-05, 'epoch': 0.46}
 46%|████▋     | 1770/3812 [43:39:26<50:19:35, 88.72s/it] 46%|████▋     | 1771/3812 [43:40:54<50:18:57, 88.75s/it] 46%|████▋     | 1772/3812 [43:42:23<50:14:49, 88.67s/it] 47%|████▋     | 1773/3812 [43:43:52<50:13:35, 88.68s/it] 47%|████▋     | 1774/3812 [43:45:20<50:12:03, 88.68s/it] 47%|████▋     | 1775/3812 [43:46:49<50:13:59, 88.78s/it] 47%|████▋     | 1776/3812 [43:48:18<50:12:40, 88.78s/it] 47%|████▋     | 1777/3812 [43:49:47<50:11:57, 88.80s/it] 47%|████▋     | 1778/3812 [43:51:15<50:08:08, 88.74s/it] 47%|████▋     | 1779/3812 [43:52:44<50:08:57, 88.80s/it] 47%|████▋     | 1780/3812 [43:54:13<50:07:02, 88.79s/it]                                                         {'loss': 1.2884, 'learning_rate': 2.759135410274255e-05, 'epoch': 0.47}
 47%|████▋     | 1780/3812 [43:54:13<50:07:02, 88.79s/it] 47%|████▋     | 1781/3812 [43:55:42<50:10:46, 88.94s/it] 47%|████▋     | 1782/3812 [43:57:11<50:06:37, 88.87s/it] 47%|████▋     | 1783/3812 [43:58:40<50:04:46, 88.86s/it] 47%|████▋     | 1784/3812 [44:00:09<50:00:07, 88.76s/it] 47%|████▋     | 1785/3812 [44:01:37<49:57:49, 88.74s/it] 47%|████▋     | 1786/3812 [44:03:06<49:55:12, 88.70s/it] 47%|████▋     | 1787/3812 [44:04:35<49:55:03, 88.74s/it] 47%|████▋     | 1788/3812 [44:06:04<49:55:54, 88.81s/it] 47%|████▋     | 1789/3812 [44:07:32<49:53:54, 88.80s/it] 47%|████▋     | 1790/3812 [44:09:01<49:51:35, 88.77s/it]                                                         {'loss': 1.2905, 'learning_rate': 2.7386345142933763e-05, 'epoch': 0.47}
 47%|████▋     | 1790/3812 [44:09:01<49:51:35, 88.77s/it] 47%|████▋     | 1791/3812 [44:10:30<49:50:12, 88.77s/it] 47%|████▋     | 1792/3812 [44:11:59<49:47:13, 88.73s/it] 47%|████▋     | 1793/3812 [44:13:27<49:46:54, 88.76s/it] 47%|████▋     | 1794/3812 [44:14:56<49:43:04, 88.69s/it] 47%|████▋     | 1795/3812 [44:16:25<49:41:33, 88.69s/it] 47%|████▋     | 1796/3812 [44:17:53<49:38:57, 88.66s/it] 47%|████▋     | 1797/3812 [44:19:22<49:38:34, 88.69s/it] 47%|████▋     | 1798/3812 [44:20:51<49:37:11, 88.69s/it] 47%|████▋     | 1799/3812 [44:22:20<49:41:47, 88.88s/it] 47%|████▋     | 1800/3812 [44:23:49<49:38:38, 88.83s/it]                                                         {'loss': 1.2938, 'learning_rate': 2.7181174104866957e-05, 'epoch': 0.47}
 47%|████▋     | 1800/3812 [44:23:49<49:38:38, 88.83s/it] 47%|████▋     | 1801/3812 [44:25:18<49:37:45, 88.84s/it] 47%|████▋     | 1802/3812 [44:26:46<49:34:22, 88.79s/it] 47%|████▋     | 1803/3812 [44:28:15<49:32:08, 88.76s/it] 47%|████▋     | 1804/3812 [44:29:44<49:29:16, 88.72s/it] 47%|████▋     | 1805/3812 [44:31:12<49:28:23, 88.74s/it] 47%|████▋     | 1806/3812 [44:32:41<49:23:54, 88.65s/it] 47%|████▋     | 1807/3812 [44:34:09<49:21:46, 88.63s/it] 47%|████▋     | 1808/3812 [44:35:38<49:17:57, 88.56s/it] 47%|████▋     | 1809/3812 [44:37:06<49:17:33, 88.59s/it] 47%|████▋     | 1810/3812 [44:38:35<49:18:41, 88.67s/it]                                                         {'loss': 1.2933, 'learning_rate': 2.6975854923560956e-05, 'epoch': 0.47}
 47%|████▋     | 1810/3812 [44:38:35<49:18:41, 88.67s/it] 48%|████▊     | 1811/3812 [44:40:04<49:20:45, 88.78s/it] 48%|████▊     | 1812/3812 [44:41:33<49:17:02, 88.71s/it] 48%|████▊     | 1813/3812 [44:43:02<49:15:45, 88.72s/it] 48%|████▊     | 1814/3812 [44:44:30<49:12:32, 88.66s/it] 48%|████▊     | 1815/3812 [44:45:59<49:11:26, 88.68s/it] 48%|████▊     | 1816/3812 [44:47:27<49:08:35, 88.64s/it] 48%|████▊     | 1817/3812 [44:48:56<49:08:32, 88.68s/it] 48%|████▊     | 1818/3812 [44:50:25<49:04:23, 88.60s/it] 48%|████▊     | 1819/3812 [44:51:53<49:03:42, 88.62s/it] 48%|████▊     | 1820/3812 [44:53:22<49:01:03, 88.59s/it]                                                         {'loss': 1.294, 'learning_rate': 2.6770401544096303e-05, 'epoch': 0.48}
 48%|████▊     | 1820/3812 [44:53:22<49:01:03, 88.59s/it] 48%|████▊     | 1821/3812 [44:54:50<48:59:01, 88.57s/it] 48%|████▊     | 1822/3812 [44:56:19<48:57:40, 88.57s/it] 48%|████▊     | 1823/3812 [44:57:48<48:57:37, 88.62s/it] 48%|████▊     | 1824/3812 [44:59:16<48:55:32, 88.60s/it] 48%|████▊     | 1825/3812 [45:00:45<48:54:40, 88.62s/it] 48%|████▊     | 1826/3812 [45:02:13<48:52:09, 88.58s/it] 48%|████▊     | 1827/3812 [45:03:42<48:51:24, 88.61s/it] 48%|████▊     | 1828/3812 [45:05:11<48:49:56, 88.61s/it] 48%|████▊     | 1829/3812 [45:06:40<48:53:32, 88.76s/it] 48%|████▊     | 1830/3812 [45:08:09<48:52:51, 88.78s/it]                                                         {'loss': 1.2974, 'learning_rate': 2.656482792066815e-05, 'epoch': 0.48}
 48%|████▊     | 1830/3812 [45:08:09<48:52:51, 88.78s/it] 48%|████▊     | 1831/3812 [45:09:37<48:52:57, 88.83s/it] 48%|████▊     | 1832/3812 [45:11:06<48:46:39, 88.69s/it] 48%|████▊     | 1833/3812 [45:12:34<48:45:09, 88.69s/it] 48%|████▊     | 1834/3812 [45:14:03<48:44:00, 88.70s/it] 48%|████▊     | 1835/3812 [45:15:32<48:44:18, 88.75s/it] 48%|████▊     | 1836/3812 [45:17:00<48:39:24, 88.65s/it] 48%|████▊     | 1837/3812 [45:18:29<48:38:55, 88.68s/it] 48%|████▊     | 1838/3812 [45:19:58<48:38:45, 88.72s/it] 48%|████▊     | 1839/3812 [45:21:27<48:41:52, 88.86s/it] 48%|████▊     | 1840/3812 [45:22:56<48:37:50, 88.78s/it]                                                         {'loss': 1.3, 'learning_rate': 2.6359148015638523e-05, 'epoch': 0.48}
 48%|████▊     | 1840/3812 [45:22:56<48:37:50, 88.78s/it] 48%|████▊     | 1841/3812 [45:24:25<48:37:56, 88.83s/it] 48%|████▊     | 1842/3812 [45:25:53<48:35:21, 88.79s/it] 48%|████▊     | 1843/3812 [45:27:22<48:34:04, 88.80s/it] 48%|████▊     | 1844/3812 [45:28:51<48:31:06, 88.75s/it] 48%|████▊     | 1845/3812 [45:30:20<48:30:03, 88.77s/it] 48%|████▊     | 1846/3812 [45:31:48<48:26:49, 88.71s/it] 48%|████▊     | 1847/3812 [45:33:17<48:26:29, 88.75s/it] 48%|████▊     | 1848/3812 [45:34:46<48:21:52, 88.65s/it] 49%|████▊     | 1849/3812 [45:36:14<48:21:11, 88.68s/it] 49%|████▊     | 1850/3812 [45:37:43<48:17:11, 88.60s/it]                                                         {'loss': 1.2936, 'learning_rate': 2.615337579858798e-05, 'epoch': 0.49}
 49%|████▊     | 1850/3812 [45:37:43<48:17:11, 88.60s/it] 49%|████▊     | 1851/3812 [45:39:11<48:14:27, 88.56s/it] 49%|████▊     | 1852/3812 [45:40:40<48:16:42, 88.67s/it] 49%|████▊     | 1853/3812 [45:42:09<48:18:07, 88.76s/it] 49%|████▊     | 1854/3812 [45:43:38<48:16:58, 88.77s/it] 49%|████▊     | 1855/3812 [45:45:07<48:15:26, 88.77s/it] 49%|████▊     | 1856/3812 [45:46:35<48:11:09, 88.69s/it] 49%|████▊     | 1857/3812 [45:48:04<48:11:11, 88.73s/it] 49%|████▊     | 1858/3812 [45:49:33<48:09:48, 88.74s/it] 49%|████▉     | 1859/3812 [45:51:02<48:10:05, 88.79s/it] 49%|████▉     | 1860/3812 [45:52:30<48:06:59, 88.74s/it]                                                         {'loss': 1.3072, 'learning_rate': 2.594752524536682e-05, 'epoch': 0.49}
 49%|████▉     | 1860/3812 [45:52:30<48:06:59, 88.74s/it] 49%|████▉     | 1861/3812 [45:53:59<48:06:10, 88.76s/it] 49%|████▉     | 1862/3812 [45:55:28<48:03:46, 88.73s/it] 49%|████▉     | 1863/3812 [45:56:57<48:03:12, 88.76s/it] 49%|████▉     | 1864/3812 [45:58:25<48:00:39, 88.73s/it] 49%|████▉     | 1865/3812 [45:59:54<48:01:04, 88.78s/it] 49%|████▉     | 1866/3812 [46:01:23<47:56:49, 88.70s/it] 49%|████▉     | 1867/3812 [46:02:52<47:57:21, 88.76s/it] 49%|████▉     | 1868/3812 [46:04:20<47:52:41, 88.66s/it] 49%|████▉     | 1869/3812 [46:05:49<47:52:51, 88.71s/it] 49%|████▉     | 1870/3812 [46:07:18<47:54:15, 88.80s/it]                                                         {'loss': 1.2989, 'learning_rate': 2.574161033714586e-05, 'epoch': 0.49}
 49%|████▉     | 1870/3812 [46:07:18<47:54:15, 88.80s/it] 49%|████▉     | 1871/3812 [46:08:47<47:55:03, 88.87s/it] 49%|████▉     | 1872/3812 [46:10:15<47:49:44, 88.75s/it] 49%|████▉     | 1873/3812 [46:11:44<47:46:32, 88.70s/it] 49%|████▉     | 1874/3812 [46:13:12<47:42:39, 88.63s/it] 49%|████▉     | 1875/3812 [46:14:41<47:42:26, 88.67s/it] 49%|████▉     | 1876/3812 [46:16:10<47:40:28, 88.65s/it] 49%|████▉     | 1877/3812 [46:17:39<47:41:52, 88.74s/it] 49%|████▉     | 1878/3812 [46:19:07<47:37:55, 88.66s/it] 49%|████▉     | 1879/3812 [46:20:36<47:37:24, 88.69s/it] 49%|████▉     | 1880/3812 [46:22:04<47:33:33, 88.62s/it]                                                         {'loss': 1.2982, 'learning_rate': 2.553564505946685e-05, 'epoch': 0.49}
 49%|████▉     | 1880/3812 [46:22:04<47:33:33, 88.62s/it] 49%|████▉     | 1881/3812 [46:23:33<47:31:59, 88.62s/it] 49%|████▉     | 1882/3812 [46:25:02<47:31:46, 88.66s/it] 49%|████▉     | 1883/3812 [46:26:31<47:35:58, 88.83s/it] 49%|████▉     | 1884/3812 [46:28:00<47:33:20, 88.80s/it] 49%|████▉     | 1885/3812 [46:29:29<47:34:15, 88.87s/it] 49%|████▉     | 1886/3812 [46:30:57<47:29:50, 88.78s/it] 50%|████▉     | 1887/3812 [46:32:26<47:27:17, 88.75s/it] 50%|████▉     | 1888/3812 [46:33:55<47:23:45, 88.68s/it] 50%|████▉     | 1889/3812 [46:35:24<47:25:54, 88.80s/it] 50%|████▉     | 1890/3812 [46:36:52<47:24:48, 88.81s/it]                                                         {'loss': 1.2965, 'learning_rate': 2.5329643401292592e-05, 'epoch': 0.5}
 50%|████▉     | 1890/3812 [46:36:52<47:24:48, 88.81s/it] 50%|████▉     | 1891/3812 [46:38:21<47:24:58, 88.86s/it] 50%|████▉     | 1892/3812 [46:39:50<47:21:16, 88.79s/it] 50%|████▉     | 1893/3812 [46:41:19<47:20:07, 88.80s/it] 50%|████▉     | 1894/3812 [46:42:48<47:17:37, 88.77s/it] 50%|████▉     | 1895/3812 [46:44:17<47:18:17, 88.84s/it] 50%|████▉     | 1896/3812 [46:45:45<47:13:05, 88.72s/it] 50%|████▉     | 1897/3812 [46:47:14<47:12:21, 88.74s/it] 50%|████▉     | 1898/3812 [46:48:42<47:07:07, 88.62s/it] 50%|████▉     | 1899/3812 [46:50:11<47:06:35, 88.65s/it] 50%|████▉     | 1900/3812 [46:51:40<47:06:13, 88.69s/it]                                                         {'loss': 1.2926, 'learning_rate': 2.5123619354056794e-05, 'epoch': 0.5}
 50%|████▉     | 1900/3812 [46:51:40<47:06:13, 88.69s/it] 50%|████▉     | 1901/3812 [46:53:09<47:07:34, 88.78s/it] 50%|████▉     | 1902/3812 [46:54:37<47:05:00, 88.74s/it] 50%|████▉     | 1903/3812 [46:56:06<47:04:19, 88.77s/it] 50%|████▉     | 1904/3812 [46:57:35<47:01:21, 88.72s/it] 50%|████▉     | 1905/3812 [46:59:04<47:00:51, 88.75s/it] 50%|█████     | 1906/3812 [47:00:32<46:58:42, 88.73s/it] 50%|█████     | 1907/3812 [47:02:01<47:01:08, 88.85s/it] 50%|█████     | 1908/3812 [47:03:30<46:58:49, 88.83s/it] 50%|█████     | 1909/3812 [47:04:59<46:57:01, 88.82s/it] 50%|█████     | 1910/3812 [47:06:27<46:50:39, 88.66s/it]                                                         {'loss': 1.2902, 'learning_rate': 2.4917586910713825e-05, 'epoch': 0.5}
 50%|█████     | 1910/3812 [47:06:27<46:50:39, 88.66s/it] 50%|█████     | 1911/3812 [47:07:56<46:47:58, 88.63s/it] 50%|█████     | 1912/3812 [47:09:24<46:46:04, 88.61s/it] 50%|█████     | 1913/3812 [47:10:53<46:47:04, 88.69s/it] 50%|█████     | 1914/3812 [47:12:22<46:43:55, 88.64s/it] 50%|█████     | 1915/3812 [47:13:50<46:42:38, 88.64s/it] 50%|█████     | 1916/3812 [47:15:19<46:39:53, 88.60s/it] 50%|█████     | 1917/3812 [47:16:48<46:39:54, 88.65s/it] 50%|█████     | 1918/3812 [47:18:16<46:38:51, 88.67s/it] 50%|█████     | 1919/3812 [47:19:45<46:39:59, 88.75s/it] 50%|█████     | 1920/3812 [47:21:14<46:37:29, 88.72s/it]                                                         {'loss': 1.2964, 'learning_rate': 2.471156006478832e-05, 'epoch': 0.5}
 50%|█████     | 1920/3812 [47:21:14<46:37:29, 88.72s/it] 50%|█████     | 1921/3812 [47:22:43<46:38:15, 88.79s/it] 50%|█████     | 1922/3812 [47:24:11<46:34:11, 88.70s/it] 50%|█████     | 1923/3812 [47:25:40<46:33:29, 88.73s/it] 50%|█████     | 1924/3812 [47:27:09<46:31:40, 88.72s/it] 50%|█████     | 1925/3812 [47:28:38<46:32:04, 88.78s/it] 51%|█████     | 1926/3812 [47:30:06<46:28:46, 88.72s/it] 51%|█████     | 1927/3812 [47:31:35<46:28:30, 88.76s/it] 51%|█████     | 1928/3812 [47:33:04<46:25:19, 88.70s/it] 51%|█████     | 1929/3812 [47:34:33<46:26:27, 88.79s/it] 51%|█████     | 1930/3812 [47:36:01<46:23:02, 88.73s/it]                                                         {'loss': 1.2949, 'learning_rate': 2.45055528094247e-05, 'epoch': 0.51}
 51%|█████     | 1930/3812 [47:36:01<46:23:02, 88.73s/it] 51%|█████     | 1931/3812 [47:37:30<46:24:33, 88.82s/it] 51%|█████     | 1932/3812 [47:38:59<46:20:51, 88.75s/it] 51%|█████     | 1933/3812 [47:40:28<46:19:53, 88.77s/it] 51%|█████     | 1934/3812 [47:41:56<46:15:47, 88.68s/it] 51%|█████     | 1935/3812 [47:43:25<46:14:46, 88.70s/it] 51%|█████     | 1936/3812 [47:44:54<46:13:00, 88.69s/it] 51%|█████     | 1937/3812 [47:46:23<46:12:45, 88.73s/it] 51%|█████     | 1938/3812 [47:47:51<46:10:22, 88.70s/it] 51%|█████     | 1939/3812 [47:49:20<46:09:32, 88.72s/it] 51%|█████     | 1940/3812 [47:50:48<46:06:12, 88.66s/it]                                                         {'loss': 1.2948, 'learning_rate': 2.429957913643686e-05, 'epoch': 0.51}
 51%|█████     | 1940/3812 [47:50:48<46:06:12, 88.66s/it] 51%|█████     | 1941/3812 [47:52:17<46:08:04, 88.77s/it] 51%|█████     | 1942/3812 [47:53:46<46:05:26, 88.73s/it] 51%|█████     | 1943/3812 [47:55:15<46:07:18, 88.84s/it] 51%|█████     | 1944/3812 [47:56:44<46:03:31, 88.76s/it] 51%|█████     | 1945/3812 [47:58:13<46:03:29, 88.81s/it] 51%|█████     | 1946/3812 [47:59:41<45:59:19, 88.72s/it] 51%|█████     | 1947/3812 [48:01:10<46:00:03, 88.80s/it] 51%|█████     | 1948/3812 [48:02:39<46:01:20, 88.88s/it] 51%|█████     | 1949/3812 [48:04:08<46:01:19, 88.93s/it] 51%|█████     | 1950/3812 [48:05:37<45:57:04, 88.84s/it]                                                         {'loss': 1.2939, 'learning_rate': 2.409365303535781e-05, 'epoch': 0.51}
 51%|█████     | 1950/3812 [48:05:37<45:57:04, 88.84s/it] 51%|█████     | 1951/3812 [48:07:06<45:57:03, 88.89s/it] 51%|█████     | 1952/3812 [48:08:35<45:55:21, 88.88s/it] 51%|█████     | 1953/3812 [48:10:04<45:52:25, 88.84s/it] 51%|█████▏    | 1954/3812 [48:11:32<45:50:27, 88.82s/it] 51%|█████▏    | 1955/3812 [48:13:01<45:49:33, 88.84s/it] 51%|█████▏    | 1956/3812 [48:14:30<45:45:16, 88.75s/it] 51%|█████▏    | 1957/3812 [48:15:58<45:42:55, 88.72s/it] 51%|█████▏    | 1958/3812 [48:17:27<45:40:58, 88.70s/it] 51%|█████▏    | 1959/3812 [48:18:56<45:39:53, 88.72s/it] 51%|█████▏    | 1960/3812 [48:20:24<45:37:45, 88.70s/it]                                                         {'loss': 1.2933, 'learning_rate': 2.388778849248947e-05, 'epoch': 0.51}
 51%|█████▏    | 1960/3812 [48:20:24<45:37:45, 88.70s/it] 51%|█████▏    | 1961/3812 [48:21:53<45:37:12, 88.73s/it] 51%|█████▏    | 1962/3812 [48:23:22<45:35:46, 88.73s/it] 51%|█████▏    | 1963/3812 [48:24:51<45:37:16, 88.82s/it] 52%|█████▏    | 1964/3812 [48:26:20<45:32:41, 88.72s/it] 52%|█████▏    | 1965/3812 [48:27:48<45:30:49, 88.71s/it] 52%|█████▏    | 1966/3812 [48:29:17<45:29:02, 88.70s/it] 52%|█████▏    | 1967/3812 [48:30:46<45:28:30, 88.73s/it] 52%|█████▏    | 1968/3812 [48:32:14<45:26:17, 88.71s/it] 52%|█████▏    | 1969/3812 [48:33:43<45:27:16, 88.79s/it] 52%|█████▏    | 1970/3812 [48:35:12<45:25:17, 88.77s/it]                                                         {'loss': 1.2928, 'learning_rate': 2.368199948995285e-05, 'epoch': 0.52}
 52%|█████▏    | 1970/3812 [48:35:12<45:25:17, 88.77s/it] 52%|█████▏    | 1971/3812 [48:36:41<45:26:28, 88.86s/it] 52%|█████▏    | 1972/3812 [48:38:10<45:22:01, 88.76s/it] 52%|█████▏    | 1973/3812 [48:39:39<45:22:57, 88.84s/it] 52%|█████▏    | 1974/3812 [48:41:07<45:19:31, 88.78s/it] 52%|█████▏    | 1975/3812 [48:42:36<45:17:19, 88.75s/it] 52%|█████▏    | 1976/3812 [48:44:05<45:13:23, 88.67s/it] 52%|█████▏    | 1977/3812 [48:45:33<45:11:01, 88.64s/it] 52%|█████▏    | 1978/3812 [48:47:02<45:08:10, 88.60s/it] 52%|█████▏    | 1979/3812 [48:48:30<45:07:55, 88.64s/it] 52%|█████▏    | 1980/3812 [48:49:59<45:04:03, 88.56s/it]                                                         {'loss': 1.2887, 'learning_rate': 2.3476300004738303e-05, 'epoch': 0.52}
 52%|█████▏    | 1980/3812 [48:49:59<45:04:03, 88.56s/it] 52%|█████▏    | 1981/3812 [48:51:27<45:03:23, 88.59s/it] 52%|█████▏    | 1982/3812 [48:52:56<45:02:25, 88.60s/it] 52%|█████▏    | 1983/3812 [48:54:25<45:03:06, 88.67s/it] 52%|█████▏    | 1984/3812 [48:55:54<45:01:54, 88.68s/it] 52%|█████▏    | 1985/3812 [48:57:23<45:03:41, 88.79s/it] 52%|█████▏    | 1986/3812 [48:58:51<44:59:58, 88.72s/it] 52%|█████▏    | 1987/3812 [49:00:20<45:00:43, 88.79s/it] 52%|█████▏    | 1988/3812 [49:01:48<44:55:44, 88.68s/it] 52%|█████▏    | 1989/3812 [49:03:18<44:57:24, 88.78s/it] 52%|█████▏    | 1990/3812 [49:04:46<44:54:16, 88.72s/it]                                                         {'loss': 1.2988, 'learning_rate': 2.327070400775624e-05, 'epoch': 0.52}
 52%|█████▏    | 1990/3812 [49:04:46<44:54:16, 88.72s/it] 52%|█████▏    | 1991/3812 [49:06:15<44:57:30, 88.88s/it] 52%|█████▏    | 1992/3812 [49:07:44<44:51:09, 88.72s/it] 52%|█████▏    | 1993/3812 [49:09:13<44:51:15, 88.77s/it] 52%|█████▏    | 1994/3812 [49:10:41<44:46:13, 88.65s/it] 52%|█████▏    | 1995/3812 [49:12:10<44:47:35, 88.75s/it] 52%|█████▏    | 1996/3812 [49:13:39<44:45:04, 88.71s/it] 52%|█████▏    | 1997/3812 [49:15:07<44:45:15, 88.77s/it] 52%|█████▏    | 1998/3812 [49:16:36<44:41:42, 88.70s/it] 52%|█████▏    | 1999/3812 [49:18:05<44:42:17, 88.77s/it] 52%|█████▏    | 2000/3812 [49:19:33<44:38:00, 88.68s/it]                                                         {'loss': 1.2864, 'learning_rate': 2.3065225462888285e-05, 'epoch': 0.52}
 52%|█████▏    | 2000/3812 [49:19:33<44:38:00, 88.68s/it][INFO|trainer.py:2936] 2024-02-11 00:31:52,771 >> Saving model checkpoint to /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2000
[INFO|configuration_utils.py:473] 2024-02-11 00:31:52,772 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2000/config.json
[INFO|configuration_utils.py:594] 2024-02-11 00:31:52,772 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2000/generation_config.json
[INFO|modeling_utils.py:2501] 2024-02-11 00:32:09,820 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2000/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2433] 2024-02-11 00:32:09,821 >> tokenizer config file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-11 00:32:09,821 >> Special tokens file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2000/special_tokens_map.json
 52%|█████▏    | 2001/3812 [49:21:28<48:35:18, 96.59s/it] 53%|█████▎    | 2002/3812 [49:22:57<47:19:16, 94.12s/it] 53%|█████▎    | 2003/3812 [49:24:26<46:34:26, 92.68s/it] 53%|█████▎    | 2004/3812 [49:25:55<45:56:07, 91.46s/it] 53%|█████▎    | 2005/3812 [49:27:24<45:33:54, 90.78s/it] 53%|█████▎    | 2006/3812 [49:28:52<45:11:09, 90.07s/it] 53%|█████▎    | 2007/3812 [49:30:21<45:00:30, 89.77s/it] 53%|█████▎    | 2008/3812 [49:31:50<44:46:05, 89.34s/it] 53%|█████▎    | 2009/3812 [49:33:19<44:41:45, 89.24s/it] 53%|█████▎    | 2010/3812 [49:34:48<44:35:54, 89.10s/it]                                                         {'loss': 1.2984, 'learning_rate': 2.2859878326038798e-05, 'epoch': 0.53}
 53%|█████▎    | 2010/3812 [49:34:48<44:35:54, 89.10s/it] 53%|█████▎    | 2011/3812 [49:36:17<44:33:58, 89.08s/it] 53%|█████▎    | 2012/3812 [49:37:45<44:26:22, 88.88s/it] 53%|█████▎    | 2013/3812 [49:39:14<44:25:58, 88.92s/it] 53%|█████▎    | 2014/3812 [49:40:42<44:19:56, 88.76s/it] 53%|█████▎    | 2015/3812 [49:42:11<44:19:13, 88.79s/it] 53%|█████▎    | 2016/3812 [49:43:40<44:17:32, 88.78s/it] 53%|█████▎    | 2017/3812 [49:45:09<44:17:55, 88.84s/it] 53%|█████▎    | 2018/3812 [49:46:37<44:12:44, 88.72s/it] 53%|█████▎    | 2019/3812 [49:48:06<44:12:40, 88.77s/it] 53%|█████▎    | 2020/3812 [49:49:35<44:10:48, 88.75s/it]                                                         {'loss': 1.2845, 'learning_rate': 2.2654676544187043e-05, 'epoch': 0.53}
 53%|█████▎    | 2020/3812 [49:49:35<44:10:48, 88.75s/it] 53%|█████▎    | 2021/3812 [49:51:04<44:12:44, 88.87s/it] 53%|█████▎    | 2022/3812 [49:52:33<44:11:58, 88.89s/it] 53%|█████▎    | 2023/3812 [49:54:02<44:10:52, 88.91s/it] 53%|█████▎    | 2024/3812 [49:55:31<44:05:46, 88.78s/it] 53%|█████▎    | 2025/3812 [49:56:59<44:04:54, 88.80s/it] 53%|█████▎    | 2026/3812 [49:58:28<43:58:54, 88.65s/it] 53%|█████▎    | 2027/3812 [49:59:57<43:59:46, 88.73s/it] 53%|█████▎    | 2028/3812 [50:01:25<43:59:03, 88.76s/it] 53%|█████▎    | 2029/3812 [50:02:54<43:59:02, 88.81s/it] 53%|█████▎    | 2030/3812 [50:04:23<43:54:39, 88.71s/it]                                                         {'loss': 1.2784, 'learning_rate': 2.2449634054439918e-05, 'epoch': 0.53}
 53%|█████▎    | 2030/3812 [50:04:23<43:54:39, 88.71s/it] 53%|█████▎    | 2031/3812 [50:05:52<43:56:18, 88.81s/it] 53%|█████▎    | 2032/3812 [50:07:20<43:51:32, 88.70s/it] 53%|█████▎    | 2033/3812 [50:08:49<43:50:43, 88.73s/it] 53%|█████▎    | 2034/3812 [50:10:18<43:49:13, 88.73s/it] 53%|█████▎    | 2035/3812 [50:11:47<43:50:31, 88.82s/it] 53%|█████▎    | 2036/3812 [50:13:15<43:46:24, 88.73s/it] 53%|█████▎    | 2037/3812 [50:14:44<43:47:45, 88.83s/it] 53%|█████▎    | 2038/3812 [50:16:13<43:43:59, 88.75s/it] 53%|█████▎    | 2039/3812 [50:17:42<43:44:05, 88.80s/it] 54%|█████▎    | 2040/3812 [50:19:11<43:41:18, 88.76s/it]                                                         {'loss': 1.2916, 'learning_rate': 2.2244764783085357e-05, 'epoch': 0.54}
 54%|█████▎    | 2040/3812 [50:19:11<43:41:18, 88.76s/it] 54%|█████▎    | 2041/3812 [50:20:40<43:41:28, 88.81s/it] 54%|█████▎    | 2042/3812 [50:22:08<43:38:11, 88.75s/it] 54%|█████▎    | 2043/3812 [50:23:37<43:39:28, 88.85s/it] 54%|█████▎    | 2044/3812 [50:25:06<43:34:52, 88.74s/it] 54%|█████▎    | 2045/3812 [50:26:35<43:35:23, 88.81s/it] 54%|█████▎    | 2046/3812 [50:28:03<43:33:10, 88.78s/it] 54%|█████▎    | 2047/3812 [50:29:33<43:35:37, 88.92s/it] 54%|█████▎    | 2048/3812 [50:31:01<43:29:32, 88.76s/it] 54%|█████▍    | 2049/3812 [50:32:30<43:30:01, 88.83s/it] 54%|█████▍    | 2050/3812 [50:33:59<43:26:23, 88.75s/it]                                                         {'loss': 1.2873, 'learning_rate': 2.2040082644646446e-05, 'epoch': 0.54}
 54%|█████▍    | 2050/3812 [50:33:59<43:26:23, 88.75s/it] 54%|█████▍    | 2051/3812 [50:35:27<43:25:03, 88.76s/it] 54%|█████▍    | 2052/3812 [50:36:56<43:24:19, 88.78s/it] 54%|█████▍    | 2053/3812 [50:38:25<43:23:45, 88.81s/it] 54%|█████▍    | 2054/3812 [50:39:54<43:19:38, 88.72s/it] 54%|█████▍    | 2055/3812 [50:41:23<43:23:50, 88.92s/it] 54%|█████▍    | 2056/3812 [50:42:51<43:18:50, 88.80s/it] 54%|█████▍    | 2057/3812 [50:44:21<43:21:37, 88.94s/it] 54%|█████▍    | 2058/3812 [50:45:50<43:18:55, 88.90s/it] 54%|█████▍    | 2059/3812 [50:47:19<43:19:23, 88.97s/it] 54%|█████▍    | 2060/3812 [50:48:47<43:15:02, 88.87s/it]                                                         {'loss': 1.289, 'learning_rate': 2.1835601540936413e-05, 'epoch': 0.54}
 54%|█████▍    | 2060/3812 [50:48:47<43:15:02, 88.87s/it] 54%|█████▍    | 2061/3812 [50:50:17<43:17:31, 89.01s/it] 54%|█████▍    | 2062/3812 [50:51:45<43:10:03, 88.80s/it] 54%|█████▍    | 2063/3812 [50:53:14<43:10:46, 88.88s/it] 54%|█████▍    | 2064/3812 [50:54:43<43:08:45, 88.86s/it] 54%|█████▍    | 2065/3812 [50:56:12<43:07:31, 88.87s/it] 54%|█████▍    | 2066/3812 [50:57:40<43:02:15, 88.74s/it] 54%|█████▍    | 2067/3812 [50:59:09<43:01:38, 88.77s/it] 54%|█████▍    | 2068/3812 [51:00:38<42:57:54, 88.69s/it] 54%|█████▍    | 2069/3812 [51:02:06<42:56:22, 88.69s/it] 54%|█████▍    | 2070/3812 [51:03:35<42:55:54, 88.72s/it]                                                         {'loss': 1.2796, 'learning_rate': 2.1631335360114387e-05, 'epoch': 0.54}
 54%|█████▍    | 2070/3812 [51:03:35<42:55:54, 88.72s/it] 54%|█████▍    | 2071/3812 [51:05:04<42:55:51, 88.77s/it] 54%|█████▍    | 2072/3812 [51:06:32<42:50:00, 88.62s/it] 54%|█████▍    | 2073/3812 [51:08:01<42:51:34, 88.73s/it] 54%|█████▍    | 2074/3812 [51:09:30<42:49:04, 88.69s/it] 54%|█████▍    | 2075/3812 [51:10:59<42:49:41, 88.76s/it] 54%|█████▍    | 2076/3812 [51:12:28<42:49:07, 88.79s/it] 54%|█████▍    | 2077/3812 [51:13:57<42:49:31, 88.86s/it] 55%|█████▍    | 2078/3812 [51:15:25<42:43:59, 88.72s/it] 55%|█████▍    | 2079/3812 [51:16:54<42:43:26, 88.75s/it] 55%|█████▍    | 2080/3812 [51:18:22<42:40:42, 88.71s/it]                                                         {'loss': 1.2892, 'learning_rate': 2.1427297975742137e-05, 'epoch': 0.55}
 55%|█████▍    | 2080/3812 [51:18:22<42:40:42, 88.71s/it] 55%|█████▍    | 2081/3812 [51:19:51<42:40:04, 88.74s/it] 55%|█████▍    | 2082/3812 [51:21:20<42:40:24, 88.80s/it] 55%|█████▍    | 2083/3812 [51:22:49<42:41:18, 88.88s/it] 55%|█████▍    | 2084/3812 [51:24:18<42:36:43, 88.78s/it] 55%|█████▍    | 2085/3812 [51:25:47<42:38:20, 88.88s/it] 55%|█████▍    | 2086/3812 [51:27:15<42:33:36, 88.77s/it] 55%|█████▍    | 2087/3812 [51:28:44<42:34:03, 88.84s/it] 55%|█████▍    | 2088/3812 [51:30:13<42:30:39, 88.77s/it] 55%|█████▍    | 2089/3812 [51:31:42<42:31:11, 88.84s/it] 55%|█████▍    | 2090/3812 [51:33:10<42:25:41, 88.70s/it]                                                         {'loss': 1.2801, 'learning_rate': 2.122350324584182e-05, 'epoch': 0.55}
 55%|█████▍    | 2090/3812 [51:33:10<42:25:41, 88.70s/it] 55%|█████▍    | 2091/3812 [51:34:39<42:27:05, 88.80s/it] 55%|█████▍    | 2092/3812 [51:36:08<42:21:55, 88.67s/it] 55%|█████▍    | 2093/3812 [51:37:37<42:21:24, 88.71s/it] 55%|█████▍    | 2094/3812 [51:39:05<42:17:56, 88.64s/it] 55%|█████▍    | 2095/3812 [51:40:34<42:19:34, 88.74s/it] 55%|█████▍    | 2096/3812 [51:42:02<42:15:09, 88.64s/it] 55%|█████▌    | 2097/3812 [51:43:31<42:15:07, 88.69s/it] 55%|█████▌    | 2098/3812 [51:45:00<42:11:41, 88.62s/it] 55%|█████▌    | 2099/3812 [51:46:28<42:09:46, 88.61s/it] 55%|█████▌    | 2100/3812 [51:47:57<42:10:46, 88.70s/it]                                                         {'loss': 1.2827, 'learning_rate': 2.1019965011954697e-05, 'epoch': 0.55}
 55%|█████▌    | 2100/3812 [51:47:57<42:10:46, 88.70s/it] 55%|█████▌    | 2101/3812 [51:49:26<42:11:09, 88.76s/it] 55%|█████▌    | 2102/3812 [51:50:55<42:07:02, 88.67s/it] 55%|█████▌    | 2103/3812 [51:52:23<42:06:51, 88.71s/it] 55%|█████▌    | 2104/3812 [51:53:52<42:03:41, 88.65s/it] 55%|█████▌    | 2105/3812 [51:55:21<42:07:03, 88.82s/it] 55%|█████▌    | 2106/3812 [51:56:50<42:03:36, 88.76s/it] 55%|█████▌    | 2107/3812 [51:58:19<42:05:44, 88.88s/it] 55%|█████▌    | 2108/3812 [51:59:48<42:03:11, 88.84s/it] 55%|█████▌    | 2109/3812 [52:01:17<42:02:42, 88.88s/it] 55%|█████▌    | 2110/3812 [52:02:45<41:58:17, 88.78s/it]                                                         {'loss': 1.2836, 'learning_rate': 2.0816697098201105e-05, 'epoch': 0.55}
 55%|█████▌    | 2110/3812 [52:02:45<41:58:17, 88.78s/it] 55%|█████▌    | 2111/3812 [52:04:14<41:56:11, 88.75s/it] 55%|█████▌    | 2112/3812 [52:05:43<41:56:57, 88.83s/it] 55%|█████▌    | 2113/3812 [52:07:12<41:56:36, 88.87s/it] 55%|█████▌    | 2114/3812 [52:08:40<41:53:08, 88.80s/it] 55%|█████▌    | 2115/3812 [52:10:09<41:51:28, 88.80s/it] 56%|█████▌    | 2116/3812 [52:11:38<41:46:13, 88.66s/it] 56%|█████▌    | 2117/3812 [52:13:06<41:45:49, 88.70s/it] 56%|█████▌    | 2118/3812 [52:14:35<41:42:33, 88.64s/it] 56%|█████▌    | 2119/3812 [52:16:04<41:43:01, 88.71s/it] 56%|█████▌    | 2120/3812 [52:17:32<41:38:03, 88.58s/it]                                                         {'loss': 1.2777, 'learning_rate': 2.0613713310341492e-05, 'epoch': 0.56}
 56%|█████▌    | 2120/3812 [52:17:32<41:38:03, 88.58s/it] 56%|█████▌    | 2121/3812 [52:19:01<41:38:23, 88.65s/it] 56%|█████▌    | 2122/3812 [52:20:29<41:34:25, 88.56s/it] 56%|█████▌    | 2123/3812 [52:21:58<41:34:19, 88.61s/it] 56%|█████▌    | 2124/3812 [52:23:27<41:33:14, 88.62s/it] 56%|█████▌    | 2125/3812 [52:24:55<41:34:15, 88.71s/it] 56%|█████▌    | 2126/3812 [52:26:24<41:31:21, 88.66s/it] 56%|█████▌    | 2127/3812 [52:27:53<41:30:55, 88.70s/it] 56%|█████▌    | 2128/3812 [52:29:21<41:28:04, 88.65s/it] 56%|█████▌    | 2129/3812 [52:30:50<41:27:24, 88.68s/it] 56%|█████▌    | 2130/3812 [52:32:19<41:25:43, 88.67s/it]                                                         {'loss': 1.2844, 'learning_rate': 2.041102743483872e-05, 'epoch': 0.56}
 56%|█████▌    | 2130/3812 [52:32:19<41:25:43, 88.67s/it] 56%|█████▌    | 2131/3812 [52:33:48<41:25:53, 88.73s/it] 56%|█████▌    | 2132/3812 [52:35:16<41:21:12, 88.61s/it] 56%|█████▌    | 2133/3812 [52:36:45<41:22:10, 88.70s/it] 56%|█████▌    | 2134/3812 [52:38:13<41:17:59, 88.61s/it] 56%|█████▌    | 2135/3812 [52:39:42<41:19:58, 88.73s/it] 56%|█████▌    | 2136/3812 [52:41:11<41:17:15, 88.68s/it] 56%|█████▌    | 2137/3812 [52:42:40<41:19:22, 88.81s/it] 56%|█████▌    | 2138/3812 [52:44:08<41:14:50, 88.70s/it] 56%|█████▌    | 2139/3812 [52:45:37<41:14:36, 88.75s/it] 56%|█████▌    | 2140/3812 [52:47:06<41:10:58, 88.67s/it]                                                         {'loss': 1.2793, 'learning_rate': 2.0208653237921766e-05, 'epoch': 0.56}
 56%|█████▌    | 2140/3812 [52:47:06<41:10:58, 88.67s/it] 56%|█████▌    | 2141/3812 [52:48:35<41:12:08, 88.77s/it] 56%|█████▌    | 2142/3812 [52:50:03<41:09:34, 88.73s/it] 56%|█████▌    | 2143/3812 [52:51:33<41:11:29, 88.85s/it] 56%|█████▌    | 2144/3812 [52:53:01<41:06:52, 88.74s/it] 56%|█████▋    | 2145/3812 [52:54:30<41:06:21, 88.77s/it] 56%|█████▋    | 2146/3812 [52:55:59<41:04:08, 88.74s/it] 56%|█████▋    | 2147/3812 [52:57:27<41:04:07, 88.80s/it] 56%|█████▋    | 2148/3812 [52:58:56<41:01:13, 88.75s/it] 56%|█████▋    | 2149/3812 [53:00:25<41:01:56, 88.83s/it] 56%|█████▋    | 2150/3812 [53:01:53<40:56:19, 88.68s/it]                                                         {'loss': 1.2822, 'learning_rate': 2.0006604464650694e-05, 'epoch': 0.56}
 56%|█████▋    | 2150/3812 [53:01:53<40:56:19, 88.68s/it] 56%|█████▋    | 2151/3812 [53:03:22<40:54:47, 88.67s/it] 56%|█████▋    | 2152/3812 [53:04:51<40:51:18, 88.60s/it] 56%|█████▋    | 2153/3812 [53:06:19<40:49:38, 88.59s/it] 57%|█████▋    | 2154/3812 [53:07:48<40:49:17, 88.64s/it] 57%|█████▋    | 2155/3812 [53:09:17<40:50:58, 88.75s/it] 57%|█████▋    | 2156/3812 [53:10:45<40:47:33, 88.68s/it] 57%|█████▋    | 2157/3812 [53:12:14<40:47:19, 88.72s/it] 57%|█████▋    | 2158/3812 [53:13:43<40:44:56, 88.69s/it] 57%|█████▋    | 2159/3812 [53:15:12<40:44:19, 88.72s/it] 57%|█████▋    | 2160/3812 [53:16:40<40:42:32, 88.71s/it]                                                         {'loss': 1.2808, 'learning_rate': 1.980489483798308e-05, 'epoch': 0.57}
 57%|█████▋    | 2160/3812 [53:16:40<40:42:32, 88.71s/it] 57%|█████▋    | 2161/3812 [53:18:10<40:45:21, 88.87s/it] 57%|█████▋    | 2162/3812 [53:19:38<40:41:30, 88.78s/it] 57%|█████▋    | 2163/3812 [53:21:07<40:38:52, 88.74s/it] 57%|█████▋    | 2164/3812 [53:22:35<40:34:25, 88.63s/it] 57%|█████▋    | 2165/3812 [53:24:04<40:33:30, 88.65s/it] 57%|█████▋    | 2166/3812 [53:25:32<40:31:03, 88.62s/it] 57%|█████▋    | 2167/3812 [53:27:01<40:32:23, 88.72s/it] 57%|█████▋    | 2168/3812 [53:28:30<40:29:02, 88.65s/it] 57%|█████▋    | 2169/3812 [53:29:59<40:29:12, 88.71s/it] 57%|█████▋    | 2170/3812 [53:31:27<40:27:00, 88.68s/it]                                                         {'loss': 1.2802, 'learning_rate': 1.960353805784203e-05, 'epoch': 0.57}
 57%|█████▋    | 2170/3812 [53:31:27<40:27:00, 88.68s/it] 57%|█████▋    | 2171/3812 [53:32:56<40:24:27, 88.65s/it] 57%|█████▋    | 2172/3812 [53:34:25<40:23:44, 88.67s/it] 57%|█████▋    | 2173/3812 [53:35:54<40:26:49, 88.84s/it] 57%|█████▋    | 2174/3812 [53:37:22<40:21:37, 88.70s/it] 57%|█████▋    | 2175/3812 [53:38:51<40:21:34, 88.76s/it] 57%|█████▋    | 2176/3812 [53:40:20<40:19:46, 88.75s/it] 57%|█████▋    | 2177/3812 [53:41:49<40:19:07, 88.78s/it] 57%|█████▋    | 2178/3812 [53:43:17<40:17:22, 88.77s/it] 57%|█████▋    | 2179/3812 [53:44:46<40:17:38, 88.83s/it] 57%|█████▋    | 2180/3812 [53:46:15<40:14:21, 88.76s/it]                                                         {'loss': 1.2765, 'learning_rate': 1.9402547800185613e-05, 'epoch': 0.57}
 57%|█████▋    | 2180/3812 [53:46:15<40:14:21, 88.76s/it] 57%|█████▋    | 2181/3812 [53:47:44<40:15:04, 88.84s/it] 57%|█████▋    | 2182/3812 [53:49:12<40:10:17, 88.72s/it] 57%|█████▋    | 2183/3812 [53:50:41<40:09:06, 88.73s/it] 57%|█████▋    | 2184/3812 [53:52:10<40:07:01, 88.71s/it] 57%|█████▋    | 2185/3812 [53:53:39<40:05:42, 88.72s/it] 57%|█████▋    | 2186/3812 [53:55:07<40:03:06, 88.68s/it] 57%|█████▋    | 2187/3812 [53:56:36<40:02:50, 88.72s/it] 57%|█████▋    | 2188/3812 [53:58:04<39:59:40, 88.66s/it] 57%|█████▋    | 2189/3812 [53:59:33<39:59:42, 88.71s/it] 57%|█████▋    | 2190/3812 [54:01:02<39:57:35, 88.69s/it]                                                         {'loss': 1.2822, 'learning_rate': 1.9201937716078077e-05, 'epoch': 0.57}
 57%|█████▋    | 2190/3812 [54:01:02<39:57:35, 88.69s/it] 57%|█████▋    | 2191/3812 [54:02:31<39:58:27, 88.78s/it] 58%|█████▊    | 2192/3812 [54:03:59<39:53:58, 88.67s/it] 58%|█████▊    | 2193/3812 [54:05:28<39:52:47, 88.68s/it] 58%|█████▊    | 2194/3812 [54:06:56<39:49:22, 88.60s/it] 58%|█████▊    | 2195/3812 [54:08:25<39:48:09, 88.61s/it] 58%|█████▊    | 2196/3812 [54:09:54<39:47:07, 88.63s/it] 58%|█████▊    | 2197/3812 [54:11:23<39:47:47, 88.71s/it] 58%|█████▊    | 2198/3812 [54:12:51<39:45:23, 88.68s/it] 58%|█████▊    | 2199/3812 [54:14:20<39:44:53, 88.71s/it] 58%|█████▊    | 2200/3812 [54:15:49<39:41:16, 88.63s/it]                                                         {'loss': 1.2718, 'learning_rate': 1.900172143076265e-05, 'epoch': 0.58}
 58%|█████▊    | 2200/3812 [54:15:49<39:41:16, 88.63s/it] 58%|█████▊    | 2201/3812 [54:17:17<39:39:39, 88.63s/it] 58%|█████▊    | 2202/3812 [54:18:46<39:39:32, 88.68s/it] 58%|█████▊    | 2203/3812 [54:20:15<39:40:12, 88.76s/it] 58%|█████▊    | 2204/3812 [54:21:43<39:36:04, 88.66s/it] 58%|█████▊    | 2205/3812 [54:23:12<39:34:29, 88.66s/it] 58%|█████▊    | 2206/3812 [54:24:40<39:30:12, 88.55s/it] 58%|█████▊    | 2207/3812 [54:26:09<39:29:20, 88.57s/it] 58%|█████▊    | 2208/3812 [54:27:37<39:27:41, 88.57s/it] 58%|█████▊    | 2209/3812 [54:29:06<39:27:58, 88.63s/it] 58%|█████▊    | 2210/3812 [54:30:35<39:25:01, 88.58s/it]                                                         {'loss': 1.2694, 'learning_rate': 1.8801912542736093e-05, 'epoch': 0.58}
 58%|█████▊    | 2210/3812 [54:30:35<39:25:01, 88.58s/it] 58%|█████▊    | 2211/3812 [54:32:03<39:23:28, 88.58s/it] 58%|█████▊    | 2212/3812 [54:33:32<39:21:32, 88.56s/it] 58%|█████▊    | 2213/3812 [54:35:00<39:21:05, 88.60s/it] 58%|█████▊    | 2214/3812 [54:36:29<39:20:55, 88.65s/it] 58%|█████▊    | 2215/3812 [54:37:58<39:21:03, 88.71s/it] 58%|█████▊    | 2216/3812 [54:39:27<39:19:30, 88.70s/it] 58%|█████▊    | 2217/3812 [54:40:55<39:18:00, 88.70s/it] 58%|█████▊    | 2218/3812 [54:42:24<39:16:39, 88.71s/it] 58%|█████▊    | 2219/3812 [54:43:53<39:14:54, 88.70s/it] 58%|█████▊    | 2220/3812 [54:45:21<39:12:52, 88.68s/it]                                                         {'loss': 1.2833, 'learning_rate': 1.860252462282517e-05, 'epoch': 0.58}
 58%|█████▊    | 2220/3812 [54:45:21<39:12:52, 88.68s/it] 58%|█████▊    | 2221/3812 [54:46:50<39:14:03, 88.78s/it] 58%|█████▊    | 2222/3812 [54:48:19<39:11:27, 88.73s/it] 58%|█████▊    | 2223/3812 [54:49:48<39:13:38, 88.87s/it] 58%|█████▊    | 2224/3812 [54:51:17<39:08:47, 88.75s/it] 58%|█████▊    | 2225/3812 [54:52:46<39:10:05, 88.85s/it] 58%|█████▊    | 2226/3812 [54:54:14<39:06:28, 88.77s/it] 58%|█████▊    | 2227/3812 [54:55:43<39:05:38, 88.79s/it] 58%|█████▊    | 2228/3812 [54:57:12<39:02:39, 88.74s/it] 58%|█████▊    | 2229/3812 [54:58:41<39:01:36, 88.75s/it] 58%|█████▊    | 2230/3812 [55:00:09<38:57:13, 88.64s/it]                                                         {'loss': 1.2776, 'learning_rate': 1.8403571213264893e-05, 'epoch': 0.58}
 58%|█████▊    | 2230/3812 [55:00:09<38:57:13, 88.64s/it] 59%|█████▊    | 2231/3812 [55:01:38<38:58:23, 88.74s/it] 59%|█████▊    | 2232/3812 [55:03:07<38:56:12, 88.72s/it] 59%|█████▊    | 2233/3812 [55:04:36<38:55:31, 88.75s/it] 59%|█████▊    | 2234/3812 [55:06:04<38:52:19, 88.68s/it] 59%|█████▊    | 2235/3812 [55:07:33<38:52:24, 88.74s/it] 59%|█████▊    | 2236/3812 [55:09:02<38:49:41, 88.69s/it] 59%|█████▊    | 2237/3812 [55:10:30<38:49:47, 88.75s/it] 59%|█████▊    | 2238/3812 [55:11:59<38:47:22, 88.72s/it] 59%|█████▊    | 2239/3812 [55:13:28<38:48:49, 88.83s/it] 59%|█████▉    | 2240/3812 [55:14:57<38:46:12, 88.79s/it]                                                         {'loss': 1.2813, 'learning_rate': 1.8205065826778723e-05, 'epoch': 0.59}
 59%|█████▉    | 2240/3812 [55:14:57<38:46:12, 88.79s/it] 59%|█████▉    | 2241/3812 [55:16:26<38:45:25, 88.81s/it] 59%|█████▉    | 2242/3812 [55:17:54<38:41:48, 88.73s/it] 59%|█████▉    | 2243/3812 [55:19:23<38:39:59, 88.72s/it] 59%|█████▉    | 2244/3812 [55:20:52<38:37:43, 88.69s/it] 59%|█████▉    | 2245/3812 [55:22:20<38:38:16, 88.77s/it] 59%|█████▉    | 2246/3812 [55:23:49<38:35:09, 88.70s/it] 59%|█████▉    | 2247/3812 [55:25:18<38:33:43, 88.71s/it] 59%|█████▉    | 2248/3812 [55:26:47<38:32:35, 88.72s/it] 59%|█████▉    | 2249/3812 [55:28:15<38:32:01, 88.75s/it] 59%|█████▉    | 2250/3812 [55:29:44<38:30:39, 88.76s/it]                                                         {'loss': 1.2822, 'learning_rate': 1.800702194566086e-05, 'epoch': 0.59}
 59%|█████▉    | 2250/3812 [55:29:44<38:30:39, 88.76s/it] 59%|█████▉    | 2251/3812 [55:31:13<38:30:54, 88.82s/it] 59%|█████▉    | 2252/3812 [55:32:41<38:25:58, 88.69s/it] 59%|█████▉    | 2253/3812 [55:34:10<38:25:35, 88.73s/it] 59%|█████▉    | 2254/3812 [55:35:39<38:20:47, 88.61s/it] 59%|█████▉    | 2255/3812 [55:37:07<38:20:47, 88.66s/it] 59%|█████▉    | 2256/3812 [55:38:36<38:18:43, 88.64s/it] 59%|█████▉    | 2257/3812 [55:40:05<38:21:02, 88.79s/it] 59%|█████▉    | 2258/3812 [55:41:33<38:15:58, 88.65s/it] 59%|█████▉    | 2259/3812 [55:43:02<38:16:11, 88.71s/it] 59%|█████▉    | 2260/3812 [55:44:31<38:12:21, 88.62s/it]                                                         {'loss': 1.2706, 'learning_rate': 1.78094530208605e-05, 'epoch': 0.59}
 59%|█████▉    | 2260/3812 [55:44:31<38:12:21, 88.62s/it] 59%|█████▉    | 2261/3812 [55:46:00<38:12:19, 88.68s/it] 59%|█████▉    | 2262/3812 [55:47:28<38:09:53, 88.64s/it] 59%|█████▉    | 2263/3812 [55:48:57<38:10:37, 88.73s/it] 59%|█████▉    | 2264/3812 [55:50:26<38:07:20, 88.66s/it] 59%|█████▉    | 2265/3812 [55:51:54<38:07:01, 88.70s/it] 59%|█████▉    | 2266/3812 [55:53:23<38:04:16, 88.65s/it] 59%|█████▉    | 2267/3812 [55:54:52<38:05:26, 88.76s/it] 59%|█████▉    | 2268/3812 [55:56:21<38:03:17, 88.73s/it] 60%|█████▉    | 2269/3812 [55:57:49<38:02:23, 88.75s/it] 60%|█████▉    | 2270/3812 [55:59:18<37:59:14, 88.69s/it]                                                         {'loss': 1.2805, 'learning_rate': 1.7612372471068233e-05, 'epoch': 0.6}
 60%|█████▉    | 2270/3812 [55:59:18<37:59:14, 88.69s/it] 60%|█████▉    | 2271/3812 [56:00:47<37:59:41, 88.76s/it] 60%|█████▉    | 2272/3812 [56:02:15<37:57:22, 88.73s/it] 60%|█████▉    | 2273/3812 [56:03:44<37:56:59, 88.77s/it] 60%|█████▉    | 2274/3812 [56:05:13<37:54:26, 88.73s/it] 60%|█████▉    | 2275/3812 [56:06:42<37:54:04, 88.77s/it] 60%|█████▉    | 2276/3812 [56:08:10<37:51:40, 88.74s/it] 60%|█████▉    | 2277/3812 [56:09:40<37:54:04, 88.89s/it] 60%|█████▉    | 2278/3812 [56:11:08<37:51:15, 88.84s/it] 60%|█████▉    | 2279/3812 [56:12:37<37:50:46, 88.88s/it] 60%|█████▉    | 2280/3812 [56:14:06<37:47:48, 88.82s/it]                                                         {'loss': 1.2737, 'learning_rate': 1.7415793681804742e-05, 'epoch': 0.6}
 60%|█████▉    | 2280/3812 [56:14:06<37:47:48, 88.82s/it] 60%|█████▉    | 2281/3812 [56:15:35<37:47:37, 88.87s/it] 60%|█████▉    | 2282/3812 [56:17:04<37:45:53, 88.86s/it] 60%|█████▉    | 2283/3812 [56:18:33<37:43:36, 88.83s/it] 60%|█████▉    | 2284/3812 [56:20:01<37:38:23, 88.68s/it] 60%|█████▉    | 2285/3812 [56:21:30<37:37:00, 88.68s/it] 60%|█████▉    | 2286/3812 [56:22:58<37:35:08, 88.67s/it] 60%|█████▉    | 2287/3812 [56:24:27<37:34:47, 88.71s/it] 60%|██████    | 2288/3812 [56:25:56<37:32:55, 88.70s/it] 60%|██████    | 2289/3812 [56:27:25<37:32:10, 88.73s/it] 60%|██████    | 2290/3812 [56:28:53<37:30:55, 88.74s/it]                                                         {'loss': 1.2726, 'learning_rate': 1.7219730004511558e-05, 'epoch': 0.6}
 60%|██████    | 2290/3812 [56:28:53<37:30:55, 88.74s/it] 60%|██████    | 2291/3812 [56:30:22<37:30:36, 88.78s/it] 60%|██████    | 2292/3812 [56:31:51<37:28:09, 88.74s/it] 60%|██████    | 2293/3812 [56:33:20<37:29:36, 88.86s/it] 60%|██████    | 2294/3812 [56:34:48<37:25:00, 88.74s/it] 60%|██████    | 2295/3812 [56:36:17<37:24:19, 88.77s/it] 60%|██████    | 2296/3812 [56:37:46<37:20:01, 88.66s/it] 60%|██████    | 2297/3812 [56:39:14<37:17:52, 88.63s/it] 60%|██████    | 2298/3812 [56:40:43<37:17:31, 88.67s/it] 60%|██████    | 2299/3812 [56:42:12<37:16:56, 88.71s/it] 60%|██████    | 2300/3812 [56:43:40<37:13:01, 88.61s/it]                                                         {'loss': 1.278, 'learning_rate': 1.7024194755644356e-05, 'epoch': 0.6}
 60%|██████    | 2300/3812 [56:43:40<37:13:01, 88.61s/it] 60%|██████    | 2301/3812 [56:45:09<37:12:09, 88.64s/it] 60%|██████    | 2302/3812 [56:46:37<37:08:58, 88.57s/it] 60%|██████    | 2303/3812 [56:48:06<37:08:09, 88.59s/it] 60%|██████    | 2304/3812 [56:49:35<37:06:18, 88.58s/it] 60%|██████    | 2305/3812 [56:51:03<37:06:38, 88.65s/it] 60%|██████    | 2306/3812 [56:52:32<37:04:39, 88.63s/it] 61%|██████    | 2307/3812 [56:54:01<37:04:41, 88.69s/it] 61%|██████    | 2308/3812 [56:55:29<37:01:22, 88.62s/it] 61%|██████    | 2309/3812 [56:56:58<37:01:12, 88.67s/it] 61%|██████    | 2310/3812 [56:58:27<37:00:09, 88.69s/it]                                                         {'loss': 1.2721, 'learning_rate': 1.6829201215768447e-05, 'epoch': 0.61}
 61%|██████    | 2310/3812 [56:58:27<37:00:09, 88.69s/it] 61%|██████    | 2311/3812 [56:59:56<36:59:37, 88.73s/it] 61%|██████    | 2312/3812 [57:01:24<36:55:28, 88.62s/it] 61%|██████    | 2313/3812 [57:02:53<36:54:34, 88.64s/it] 61%|██████    | 2314/3812 [57:04:21<36:52:23, 88.61s/it] 61%|██████    | 2315/3812 [57:05:50<36:52:10, 88.66s/it] 61%|██████    | 2316/3812 [57:07:19<36:52:33, 88.74s/it] 61%|██████    | 2317/3812 [57:08:48<36:51:54, 88.77s/it] 61%|██████    | 2318/3812 [57:10:16<36:47:30, 88.65s/it] 61%|██████    | 2319/3812 [57:11:45<36:45:51, 88.65s/it] 61%|██████    | 2320/3812 [57:13:13<36:42:51, 88.59s/it]                                                         {'loss': 1.2713, 'learning_rate': 1.663476262865677e-05, 'epoch': 0.61}
 61%|██████    | 2320/3812 [57:13:13<36:42:51, 88.59s/it] 61%|██████    | 2321/3812 [57:14:42<36:40:45, 88.56s/it] 61%|██████    | 2322/3812 [57:16:10<36:39:42, 88.58s/it] 61%|██████    | 2323/3812 [57:17:39<36:39:52, 88.64s/it] 61%|██████    | 2324/3812 [57:19:08<36:38:25, 88.65s/it] 61%|██████    | 2325/3812 [57:20:37<36:38:32, 88.71s/it] 61%|██████    | 2326/3812 [57:22:05<36:35:10, 88.63s/it] 61%|██████    | 2327/3812 [57:23:34<36:33:43, 88.64s/it] 61%|██████    | 2328/3812 [57:25:02<36:31:06, 88.59s/it] 61%|██████    | 2329/3812 [57:26:31<36:30:57, 88.64s/it] 61%|██████    | 2330/3812 [57:27:59<36:28:14, 88.59s/it]                                                         {'loss': 1.2829, 'learning_rate': 1.6440892200390437e-05, 'epoch': 0.61}
 61%|██████    | 2330/3812 [57:27:59<36:28:14, 88.59s/it] 61%|██████    | 2331/3812 [57:29:28<36:28:41, 88.67s/it] 61%|██████    | 2332/3812 [57:30:57<36:27:37, 88.69s/it] 61%|██████    | 2333/3812 [57:32:26<36:26:40, 88.71s/it] 61%|██████    | 2334/3812 [57:33:55<36:26:28, 88.76s/it] 61%|██████▏   | 2335/3812 [57:35:24<36:26:33, 88.82s/it] 61%|██████▏   | 2336/3812 [57:36:52<36:22:10, 88.71s/it] 61%|██████▏   | 2337/3812 [57:38:21<36:22:06, 88.76s/it] 61%|██████▏   | 2338/3812 [57:39:49<36:18:45, 88.69s/it] 61%|██████▏   | 2339/3812 [57:41:19<36:20:32, 88.82s/it] 61%|██████▏   | 2340/3812 [57:42:47<36:19:22, 88.83s/it]                                                         {'loss': 1.2755, 'learning_rate': 1.6247603098461744e-05, 'epoch': 0.61}
 61%|██████▏   | 2340/3812 [57:42:47<36:19:22, 88.83s/it] 61%|██████▏   | 2341/3812 [57:44:16<36:19:28, 88.90s/it] 61%|██████▏   | 2342/3812 [57:45:45<36:14:27, 88.75s/it] 61%|██████▏   | 2343/3812 [57:47:14<36:13:45, 88.79s/it] 61%|██████▏   | 2344/3812 [57:48:42<36:10:59, 88.73s/it] 62%|██████▏   | 2345/3812 [57:50:11<36:10:48, 88.79s/it] 62%|██████▏   | 2346/3812 [57:51:40<36:08:46, 88.76s/it] 62%|██████▏   | 2347/3812 [57:53:09<36:08:53, 88.83s/it] 62%|██████▏   | 2348/3812 [57:54:37<36:05:08, 88.74s/it] 62%|██████▏   | 2349/3812 [57:56:07<36:06:41, 88.86s/it] 62%|██████▏   | 2350/3812 [57:57:35<36:01:53, 88.72s/it]                                                         {'loss': 1.2833, 'learning_rate': 1.6054908450879862e-05, 'epoch': 0.62}
 62%|██████▏   | 2350/3812 [57:57:35<36:01:53, 88.72s/it] 62%|██████▏   | 2351/3812 [57:59:04<36:01:08, 88.75s/it] 62%|██████▏   | 2352/3812 [58:00:32<35:58:33, 88.71s/it] 62%|██████▏   | 2353/3812 [58:02:01<35:58:25, 88.76s/it] 62%|██████▏   | 2354/3812 [58:03:30<35:54:58, 88.68s/it] 62%|██████▏   | 2355/3812 [58:04:59<35:54:21, 88.72s/it] 62%|██████▏   | 2356/3812 [58:06:27<35:49:37, 88.58s/it] 62%|██████▏   | 2357/3812 [58:07:56<35:48:45, 88.61s/it] 62%|██████▏   | 2358/3812 [58:09:24<35:48:14, 88.65s/it] 62%|██████▏   | 2359/3812 [58:10:53<35:48:11, 88.71s/it] 62%|██████▏   | 2360/3812 [58:12:22<35:46:43, 88.71s/it]                                                         {'loss': 1.2688, 'learning_rate': 1.5862821345279212e-05, 'epoch': 0.62}
 62%|██████▏   | 2360/3812 [58:12:22<35:46:43, 88.71s/it] 62%|██████▏   | 2361/3812 [58:13:51<35:48:18, 88.83s/it] 62%|██████▏   | 2362/3812 [58:15:19<35:43:26, 88.69s/it] 62%|██████▏   | 2363/3812 [58:16:48<35:43:58, 88.78s/it] 62%|██████▏   | 2364/3812 [58:18:17<35:41:17, 88.73s/it] 62%|██████▏   | 2365/3812 [58:19:46<35:42:12, 88.83s/it] 62%|██████▏   | 2366/3812 [58:21:14<35:38:09, 88.72s/it] 62%|██████▏   | 2367/3812 [58:22:44<35:40:38, 88.88s/it] 62%|██████▏   | 2368/3812 [58:24:12<35:35:18, 88.72s/it] 62%|██████▏   | 2369/3812 [58:25:41<35:35:00, 88.77s/it] 62%|██████▏   | 2370/3812 [58:27:10<35:32:15, 88.72s/it]                                                         {'loss': 1.2731, 'learning_rate': 1.5671354828030537e-05, 'epoch': 0.62}
 62%|██████▏   | 2370/3812 [58:27:10<35:32:15, 88.72s/it] 62%|██████▏   | 2371/3812 [58:28:39<35:33:00, 88.81s/it] 62%|██████▏   | 2372/3812 [58:30:07<35:30:10, 88.76s/it] 62%|██████▏   | 2373/3812 [58:31:36<35:30:05, 88.82s/it] 62%|██████▏   | 2374/3812 [58:33:05<35:25:53, 88.70s/it] 62%|██████▏   | 2375/3812 [58:34:33<35:25:24, 88.74s/it] 62%|██████▏   | 2376/3812 [58:36:02<35:23:47, 88.74s/it] 62%|██████▏   | 2377/3812 [58:37:31<35:23:23, 88.78s/it] 62%|██████▏   | 2378/3812 [58:39:00<35:19:58, 88.70s/it] 62%|██████▏   | 2379/3812 [58:40:28<35:19:40, 88.75s/it] 62%|██████▏   | 2380/3812 [58:41:57<35:16:28, 88.68s/it]                                                         {'loss': 1.2784, 'learning_rate': 1.5480521903354813e-05, 'epoch': 0.62}
 62%|██████▏   | 2380/3812 [58:41:57<35:16:28, 88.68s/it] 62%|██████▏   | 2381/3812 [58:43:26<35:16:12, 88.73s/it] 62%|██████▏   | 2382/3812 [58:44:55<35:15:36, 88.77s/it] 63%|██████▎   | 2383/3812 [58:46:24<35:16:47, 88.88s/it] 63%|██████▎   | 2384/3812 [58:47:52<35:12:38, 88.77s/it] 63%|██████▎   | 2385/3812 [58:49:21<35:12:37, 88.83s/it] 63%|██████▎   | 2386/3812 [58:50:50<35:09:05, 88.74s/it] 63%|██████▎   | 2387/3812 [58:52:19<35:08:32, 88.78s/it] 63%|██████▎   | 2388/3812 [58:53:48<35:09:28, 88.88s/it] 63%|██████▎   | 2389/3812 [58:55:17<35:07:59, 88.88s/it] 63%|██████▎   | 2390/3812 [58:56:45<35:02:52, 88.73s/it]                                                         {'loss': 1.2651, 'learning_rate': 1.5290335532440036e-05, 'epoch': 0.63}
 63%|██████▎   | 2390/3812 [58:56:45<35:02:52, 88.73s/it] 63%|██████▎   | 2391/3812 [58:58:14<35:01:12, 88.72s/it] 63%|██████▎   | 2392/3812 [58:59:42<34:57:10, 88.61s/it] 63%|██████▎   | 2393/3812 [59:01:11<34:58:33, 88.73s/it] 63%|██████▎   | 2394/3812 [59:02:40<34:56:39, 88.72s/it] 63%|██████▎   | 2395/3812 [59:04:09<34:57:39, 88.82s/it] 63%|██████▎   | 2396/3812 [59:05:37<34:52:57, 88.68s/it] 63%|██████▎   | 2397/3812 [59:07:06<34:51:54, 88.70s/it] 63%|██████▎   | 2398/3812 [59:08:35<34:49:22, 88.66s/it] 63%|██████▎   | 2399/3812 [59:10:03<34:48:53, 88.70s/it] 63%|██████▎   | 2400/3812 [59:11:32<34:46:12, 88.65s/it]                                                         {'loss': 1.2794, 'learning_rate': 1.5100808632560878e-05, 'epoch': 0.63}
 63%|██████▎   | 2400/3812 [59:11:32<34:46:12, 88.65s/it] 63%|██████▎   | 2401/3812 [59:13:01<34:46:20, 88.72s/it] 63%|██████▎   | 2402/3812 [59:14:29<34:43:31, 88.66s/it] 63%|██████▎   | 2403/3812 [59:15:58<34:42:07, 88.66s/it] 63%|██████▎   | 2404/3812 [59:17:26<34:37:58, 88.55s/it] 63%|██████▎   | 2405/3812 [59:18:55<34:38:12, 88.62s/it] 63%|██████▎   | 2406/3812 [59:20:24<34:37:58, 88.68s/it] 63%|██████▎   | 2407/3812 [59:21:53<34:39:45, 88.82s/it] 63%|██████▎   | 2408/3812 [59:23:21<34:35:40, 88.70s/it] 63%|██████▎   | 2409/3812 [59:24:50<34:36:19, 88.80s/it] 63%|██████▎   | 2410/3812 [59:26:19<34:31:27, 88.65s/it]                                                         {'loss': 1.2728, 'learning_rate': 1.4911954076201396e-05, 'epoch': 0.63}
 63%|██████▎   | 2410/3812 [59:26:19<34:31:27, 88.65s/it] 63%|██████▎   | 2411/3812 [59:27:48<34:32:41, 88.77s/it] 63%|██████▎   | 2412/3812 [59:29:16<34:29:35, 88.70s/it] 63%|██████▎   | 2413/3812 [59:30:46<34:32:21, 88.88s/it] 63%|██████▎   | 2414/3812 [59:32:14<34:27:40, 88.74s/it] 63%|██████▎   | 2415/3812 [59:33:43<34:28:14, 88.83s/it] 63%|██████▎   | 2416/3812 [59:35:12<34:24:10, 88.72s/it] 63%|██████▎   | 2417/3812 [59:36:40<34:24:04, 88.78s/it] 63%|██████▎   | 2418/3812 [59:38:09<34:21:34, 88.73s/it] 63%|██████▎   | 2419/3812 [59:39:38<34:20:54, 88.77s/it] 63%|██████▎   | 2420/3812 [59:41:07<34:18:30, 88.73s/it]                                                         {'loss': 1.2739, 'learning_rate': 1.4723784690180708e-05, 'epoch': 0.63}
 63%|██████▎   | 2420/3812 [59:41:07<34:18:30, 88.73s/it] 64%|██████▎   | 2421/3812 [59:42:36<34:20:22, 88.87s/it] 64%|██████▎   | 2422/3812 [59:44:04<34:17:40, 88.82s/it] 64%|██████▎   | 2423/3812 [59:45:34<34:19:12, 88.95s/it] 64%|██████▎   | 2424/3812 [59:47:02<34:14:15, 88.80s/it] 64%|██████▎   | 2425/3812 [59:48:32<34:16:34, 88.97s/it] 64%|██████▎   | 2426/3812 [59:50:00<34:11:16, 88.80s/it] 64%|██████▎   | 2427/3812 [59:51:29<34:12:20, 88.91s/it] 64%|██████▎   | 2428/3812 [59:52:58<34:09:20, 88.84s/it] 64%|██████▎   | 2429/3812 [59:54:27<34:08:59, 88.89s/it] 64%|██████▎   | 2430/3812 [59:55:56<34:06:49, 88.86s/it]                                                         {'loss': 1.2696, 'learning_rate': 1.4536313254781824e-05, 'epoch': 0.64}
 64%|██████▎   | 2430/3812 [59:55:56<34:06:49, 88.86s/it] 64%|██████▍   | 2431/3812 [59:57:25<34:05:54, 88.89s/it] 64%|██████▍   | 2432/3812 [59:58:53<34:03:03, 88.83s/it] 64%|██████▍   | 2433/3812 [60:00:22<34:02:08, 88.85s/it] 64%|██████▍   | 2434/3812 [60:01:51<33:59:08, 88.79s/it] 64%|██████▍   | 2435/3812 [60:03:20<33:57:51, 88.80s/it] 64%|██████▍   | 2436/3812 [60:04:48<33:54:11, 88.70s/it] 64%|██████▍   | 2437/3812 [60:06:17<33:54:36, 88.78s/it] 64%|██████▍   | 2438/3812 [60:07:45<33:50:34, 88.67s/it] 64%|██████▍   | 2439/3812 [60:09:14<33:51:24, 88.77s/it] 64%|██████▍   | 2440/3812 [60:10:43<33:48:44, 88.72s/it]                                                         {'loss': 1.2778, 'learning_rate': 1.4349552502883628e-05, 'epoch': 0.64}
 64%|██████▍   | 2440/3812 [60:10:43<33:48:44, 88.72s/it] 64%|██████▍   | 2441/3812 [60:12:12<33:49:03, 88.80s/it] 64%|██████▍   | 2442/3812 [60:13:41<33:47:27, 88.79s/it] 64%|██████▍   | 2443/3812 [60:15:10<33:49:06, 88.93s/it] 64%|██████▍   | 2444/3812 [60:16:39<33:45:08, 88.82s/it] 64%|██████▍   | 2445/3812 [60:18:08<33:44:18, 88.85s/it] 64%|██████▍   | 2446/3812 [60:19:36<33:40:50, 88.76s/it] 64%|██████▍   | 2447/3812 [60:21:05<33:40:10, 88.80s/it] 64%|██████▍   | 2448/3812 [60:22:33<33:36:19, 88.69s/it] 64%|██████▍   | 2449/3812 [60:24:02<33:35:41, 88.73s/it] 64%|██████▍   | 2450/3812 [60:25:31<33:33:19, 88.69s/it]                                                         {'loss': 1.2791, 'learning_rate': 1.4163515119096077e-05, 'epoch': 0.64}
 64%|██████▍   | 2450/3812 [60:25:31<33:33:19, 88.69s/it] 64%|██████▍   | 2451/3812 [60:27:00<33:31:39, 88.68s/it] 64%|██████▍   | 2452/3812 [60:28:28<33:31:05, 88.72s/it] 64%|██████▍   | 2453/3812 [60:29:57<33:32:01, 88.83s/it] 64%|██████▍   | 2454/3812 [60:31:26<33:29:44, 88.80s/it] 64%|██████▍   | 2455/3812 [60:32:55<33:29:44, 88.86s/it] 64%|██████▍   | 2456/3812 [60:34:23<33:24:16, 88.68s/it] 64%|██████▍   | 2457/3812 [60:35:53<33:26:09, 88.83s/it] 64%|██████▍   | 2458/3812 [60:37:21<33:22:51, 88.75s/it] 65%|██████▍   | 2459/3812 [60:38:50<33:20:48, 88.73s/it] 65%|██████▍   | 2460/3812 [60:40:19<33:18:59, 88.71s/it]                                                         {'loss': 1.2604, 'learning_rate': 1.3978213738898632e-05, 'epoch': 0.65}
 65%|██████▍   | 2460/3812 [60:40:19<33:18:59, 88.71s/it] 65%|██████▍   | 2461/3812 [60:41:47<33:19:01, 88.78s/it] 65%|██████▍   | 2462/3812 [60:43:16<33:15:37, 88.69s/it] 65%|██████▍   | 2463/3812 [60:44:45<33:15:45, 88.77s/it] 65%|██████▍   | 2464/3812 [60:46:14<33:14:02, 88.76s/it] 65%|██████▍   | 2465/3812 [60:47:43<33:13:23, 88.79s/it] 65%|██████▍   | 2466/3812 [60:49:11<33:11:47, 88.79s/it] 65%|██████▍   | 2467/3812 [60:50:40<33:11:16, 88.83s/it] 65%|██████▍   | 2468/3812 [60:52:09<33:07:38, 88.73s/it] 65%|██████▍   | 2469/3812 [60:53:38<33:08:00, 88.82s/it] 65%|██████▍   | 2470/3812 [60:55:06<33:04:19, 88.72s/it]                                                         {'loss': 1.2679, 'learning_rate': 1.3793660947782133e-05, 'epoch': 0.65}
 65%|██████▍   | 2470/3812 [60:55:06<33:04:19, 88.72s/it] 65%|██████▍   | 2471/3812 [60:56:35<33:03:06, 88.73s/it] 65%|██████▍   | 2472/3812 [60:58:04<33:01:43, 88.73s/it] 65%|██████▍   | 2473/3812 [60:59:33<33:02:49, 88.85s/it] 65%|██████▍   | 2474/3812 [61:01:01<32:59:01, 88.75s/it] 65%|██████▍   | 2475/3812 [61:02:30<32:58:20, 88.78s/it] 65%|██████▍   | 2476/3812 [61:03:59<32:54:23, 88.67s/it] 65%|██████▍   | 2477/3812 [61:05:27<32:53:37, 88.70s/it] 65%|██████▌   | 2478/3812 [61:06:56<32:51:19, 88.67s/it] 65%|██████▌   | 2479/3812 [61:08:25<32:51:23, 88.73s/it] 65%|██████▌   | 2480/3812 [61:09:53<32:47:23, 88.62s/it]                                                         {'loss': 1.2744, 'learning_rate': 1.3609869280393967e-05, 'epoch': 0.65}
 65%|██████▌   | 2480/3812 [61:09:53<32:47:23, 88.62s/it] 65%|██████▌   | 2481/3812 [61:11:22<32:47:45, 88.70s/it] 65%|██████▌   | 2482/3812 [61:12:51<32:44:28, 88.62s/it] 65%|██████▌   | 2483/3812 [61:14:19<32:43:21, 88.64s/it] 65%|██████▌   | 2484/3812 [61:15:48<32:41:34, 88.63s/it] 65%|██████▌   | 2485/3812 [61:17:17<32:44:19, 88.82s/it] 65%|██████▌   | 2486/3812 [61:18:46<32:41:38, 88.76s/it] 65%|██████▌   | 2487/3812 [61:20:15<32:43:01, 88.89s/it] 65%|██████▌   | 2488/3812 [61:21:43<32:38:36, 88.76s/it] 65%|██████▌   | 2489/3812 [61:23:12<32:39:00, 88.84s/it] 65%|██████▌   | 2490/3812 [61:24:41<32:36:21, 88.79s/it]                                                         {'loss': 1.2649, 'learning_rate': 1.3426851219686709e-05, 'epoch': 0.65}
 65%|██████▌   | 2490/3812 [61:24:41<32:36:21, 88.79s/it] 65%|██████▌   | 2491/3812 [61:26:10<32:35:23, 88.81s/it] 65%|██████▌   | 2492/3812 [61:27:39<32:32:45, 88.76s/it] 65%|██████▌   | 2493/3812 [61:29:08<32:33:11, 88.85s/it] 65%|██████▌   | 2494/3812 [61:30:36<32:28:54, 88.72s/it] 65%|██████▌   | 2495/3812 [61:32:05<32:28:29, 88.77s/it] 65%|██████▌   | 2496/3812 [61:33:34<32:25:33, 88.70s/it] 66%|██████▌   | 2497/3812 [61:35:03<32:27:31, 88.86s/it] 66%|██████▌   | 2498/3812 [61:36:31<32:23:15, 88.73s/it] 66%|██████▌   | 2499/3812 [61:38:00<32:24:54, 88.88s/it] 66%|██████▌   | 2500/3812 [61:39:29<32:22:00, 88.81s/it]                                                         {'loss': 1.2696, 'learning_rate': 1.3244619196070352e-05, 'epoch': 0.66}
 66%|██████▌   | 2500/3812 [61:39:29<32:22:00, 88.81s/it][INFO|trainer.py:2936] 2024-02-11 12:51:48,817 >> Saving model checkpoint to /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2500
[INFO|configuration_utils.py:473] 2024-02-11 12:51:48,819 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2500/config.json
[INFO|configuration_utils.py:594] 2024-02-11 12:51:48,819 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2500/generation_config.json
[INFO|modeling_utils.py:2501] 2024-02-11 12:52:06,994 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2433] 2024-02-11 12:52:06,995 >> tokenizer config file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-11 12:52:06,995 >> Special tokens file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-2500/special_tokens_map.json
 66%|██████▌   | 2501/3812 [61:41:26<35:26:21, 97.32s/it] 66%|██████▌   | 2502/3812 [61:42:55<34:26:43, 94.66s/it] 66%|██████▌   | 2503/3812 [61:44:24<33:48:35, 92.98s/it] 66%|██████▌   | 2504/3812 [61:45:52<33:18:24, 91.67s/it] 66%|██████▌   | 2505/3812 [61:47:21<32:59:06, 90.85s/it] 66%|██████▌   | 2506/3812 [61:48:50<32:41:56, 90.13s/it] 66%|██████▌   | 2507/3812 [61:50:19<32:31:40, 89.73s/it] 66%|██████▌   | 2508/3812 [61:51:47<32:21:09, 89.32s/it] 66%|██████▌   | 2509/3812 [61:53:16<32:16:19, 89.16s/it] 66%|██████▌   | 2510/3812 [61:54:44<32:10:33, 88.97s/it]                                                         {'loss': 1.2731, 'learning_rate': 1.3063185586567966e-05, 'epoch': 0.66}
 66%|██████▌   | 2510/3812 [61:54:44<32:10:33, 88.97s/it] 66%|██████▌   | 2511/3812 [61:56:13<32:08:08, 88.92s/it] 66%|██████▌   | 2512/3812 [61:57:41<32:03:07, 88.76s/it] 66%|██████▌   | 2513/3812 [61:59:10<32:00:44, 88.72s/it] 66%|██████▌   | 2514/3812 [62:00:38<31:57:02, 88.62s/it] 66%|██████▌   | 2515/3812 [62:02:07<31:56:37, 88.66s/it] 66%|██████▌   | 2516/3812 [62:03:36<31:54:17, 88.62s/it] 66%|██████▌   | 2517/3812 [62:05:05<31:54:32, 88.70s/it] 66%|██████▌   | 2518/3812 [62:06:33<31:51:35, 88.64s/it] 66%|██████▌   | 2519/3812 [62:08:02<31:51:45, 88.71s/it] 66%|██████▌   | 2520/3812 [62:09:31<31:51:01, 88.75s/it]                                                         {'loss': 1.27, 'learning_rate': 1.2882562713975144e-05, 'epoch': 0.66}
 66%|██████▌   | 2520/3812 [62:09:31<31:51:01, 88.75s/it] 66%|██████▌   | 2521/3812 [62:11:00<31:51:18, 88.83s/it] 66%|██████▌   | 2522/3812 [62:12:29<31:50:06, 88.84s/it] 66%|██████▌   | 2523/3812 [62:13:58<31:48:21, 88.83s/it] 66%|██████▌   | 2524/3812 [62:15:26<31:43:46, 88.68s/it] 66%|██████▌   | 2525/3812 [62:16:55<31:42:22, 88.69s/it] 66%|██████▋   | 2526/3812 [62:18:23<31:38:24, 88.57s/it] 66%|██████▋   | 2527/3812 [62:19:52<31:38:59, 88.67s/it] 66%|██████▋   | 2528/3812 [62:21:20<31:36:50, 88.64s/it] 66%|██████▋   | 2529/3812 [62:22:49<31:36:16, 88.68s/it] 66%|██████▋   | 2530/3812 [62:24:18<31:33:38, 88.63s/it]                                                         {'loss': 1.2766, 'learning_rate': 1.2702762846022997e-05, 'epoch': 0.66}
 66%|██████▋   | 2530/3812 [62:24:18<31:33:38, 88.63s/it] 66%|██████▋   | 2531/3812 [62:25:46<31:33:57, 88.71s/it] 66%|██████▋   | 2532/3812 [62:27:15<31:31:16, 88.65s/it] 66%|██████▋   | 2533/3812 [62:28:44<31:31:09, 88.72s/it] 66%|██████▋   | 2534/3812 [62:30:13<31:30:00, 88.73s/it] 67%|██████▋   | 2535/3812 [62:31:42<31:31:00, 88.85s/it] 67%|██████▋   | 2536/3812 [62:33:10<31:27:31, 88.76s/it] 67%|██████▋   | 2537/3812 [62:34:39<31:27:36, 88.83s/it] 67%|██████▋   | 2538/3812 [62:36:08<31:26:02, 88.82s/it] 67%|██████▋   | 2539/3812 [62:37:37<31:25:16, 88.86s/it] 67%|██████▋   | 2540/3812 [62:39:06<31:23:02, 88.82s/it]                                                         {'loss': 1.2712, 'learning_rate': 1.252379819454494e-05, 'epoch': 0.67}
 67%|██████▋   | 2540/3812 [62:39:06<31:23:02, 88.82s/it] 67%|██████▋   | 2541/3812 [62:40:35<31:22:27, 88.87s/it] 67%|██████▋   | 2542/3812 [62:42:03<31:18:21, 88.74s/it] 67%|██████▋   | 2543/3812 [62:43:32<31:18:22, 88.81s/it] 67%|██████▋   | 2544/3812 [62:45:01<31:16:14, 88.78s/it] 67%|██████▋   | 2545/3812 [62:46:30<31:14:47, 88.78s/it] 67%|██████▋   | 2546/3812 [62:47:58<31:11:26, 88.69s/it] 67%|██████▋   | 2547/3812 [62:49:27<31:11:49, 88.78s/it] 67%|██████▋   | 2548/3812 [62:50:55<31:07:20, 88.64s/it] 67%|██████▋   | 2549/3812 [62:52:24<31:07:59, 88.74s/it] 67%|██████▋   | 2550/3812 [62:53:53<31:05:57, 88.71s/it]                                                         {'loss': 1.2666, 'learning_rate': 1.2345680914647323e-05, 'epoch': 0.67}
 67%|██████▋   | 2550/3812 [62:53:53<31:05:57, 88.71s/it] 67%|██████▋   | 2551/3812 [62:55:22<31:06:13, 88.80s/it] 67%|██████▋   | 2552/3812 [62:56:51<31:04:07, 88.77s/it] 67%|██████▋   | 2553/3812 [62:58:20<31:06:35, 88.96s/it] 67%|██████▋   | 2554/3812 [62:59:49<31:02:19, 88.82s/it] 67%|██████▋   | 2555/3812 [63:01:18<31:02:57, 88.92s/it] 67%|██████▋   | 2556/3812 [63:02:46<30:58:56, 88.80s/it] 67%|██████▋   | 2557/3812 [63:04:15<30:58:27, 88.85s/it] 67%|██████▋   | 2558/3812 [63:05:44<30:57:14, 88.86s/it] 67%|██████▋   | 2559/3812 [63:07:13<30:57:10, 88.93s/it] 67%|██████▋   | 2560/3812 [63:08:42<30:52:19, 88.77s/it]                                                         {'loss': 1.2591, 'learning_rate': 1.2168423103883797e-05, 'epoch': 0.67}
 67%|██████▋   | 2560/3812 [63:08:42<30:52:19, 88.77s/it] 67%|██████▋   | 2561/3812 [63:10:11<30:52:55, 88.87s/it] 67%|██████▋   | 2562/3812 [63:11:39<30:48:49, 88.74s/it] 67%|██████▋   | 2563/3812 [63:13:08<30:49:04, 88.83s/it] 67%|██████▋   | 2564/3812 [63:14:37<30:48:39, 88.88s/it] 67%|██████▋   | 2565/3812 [63:16:07<30:49:44, 89.00s/it] 67%|██████▋   | 2566/3812 [63:17:35<30:45:07, 88.85s/it] 67%|██████▋   | 2567/3812 [63:19:04<30:44:56, 88.91s/it] 67%|██████▋   | 2568/3812 [63:20:33<30:40:21, 88.76s/it] 67%|██████▋   | 2569/3812 [63:22:02<30:40:29, 88.84s/it] 67%|██████▋   | 2570/3812 [63:23:30<30:37:37, 88.77s/it]                                                         {'loss': 1.2602, 'learning_rate': 1.199203680143374e-05, 'epoch': 0.67}
 67%|██████▋   | 2570/3812 [63:23:30<30:37:37, 88.77s/it] 67%|██████▋   | 2571/3812 [63:24:59<30:38:17, 88.88s/it] 67%|██████▋   | 2572/3812 [63:26:28<30:33:20, 88.71s/it] 67%|██████▋   | 2573/3812 [63:27:57<30:33:06, 88.77s/it] 68%|██████▊   | 2574/3812 [63:29:25<30:29:00, 88.64s/it] 68%|██████▊   | 2575/3812 [63:30:54<30:28:37, 88.70s/it] 68%|██████▊   | 2576/3812 [63:32:23<30:28:05, 88.74s/it] 68%|██████▊   | 2577/3812 [63:33:52<30:28:16, 88.82s/it] 68%|██████▊   | 2578/3812 [63:35:20<30:24:57, 88.73s/it] 68%|██████▊   | 2579/3812 [63:36:49<30:25:11, 88.82s/it] 68%|██████▊   | 2580/3812 [63:38:18<30:21:49, 88.73s/it]                                                         {'loss': 1.268, 'learning_rate': 1.181653398728449e-05, 'epoch': 0.68}
 68%|██████▊   | 2580/3812 [63:38:18<30:21:49, 88.73s/it] 68%|██████▊   | 2581/3812 [63:39:47<30:22:30, 88.83s/it] 68%|██████▊   | 2582/3812 [63:41:15<30:20:33, 88.81s/it] 68%|██████▊   | 2583/3812 [63:42:45<30:21:22, 88.92s/it] 68%|██████▊   | 2584/3812 [63:44:13<30:16:45, 88.77s/it] 68%|██████▊   | 2585/3812 [63:45:42<30:15:51, 88.79s/it] 68%|██████▊   | 2586/3812 [63:47:10<30:12:55, 88.72s/it] 68%|██████▊   | 2587/3812 [63:48:39<30:11:46, 88.74s/it] 68%|██████▊   | 2588/3812 [63:50:08<30:09:12, 88.69s/it] 68%|██████▊   | 2589/3812 [63:51:37<30:09:29, 88.77s/it] 68%|██████▊   | 2590/3812 [63:53:05<30:05:57, 88.67s/it]                                                         {'loss': 1.2675, 'learning_rate': 1.1641926581417739e-05, 'epoch': 0.68}
 68%|██████▊   | 2590/3812 [63:53:05<30:05:57, 88.67s/it] 68%|██████▊   | 2591/3812 [63:54:34<30:06:03, 88.75s/it] 68%|██████▊   | 2592/3812 [63:56:03<30:03:26, 88.69s/it] 68%|██████▊   | 2593/3812 [63:57:32<30:03:29, 88.77s/it] 68%|██████▊   | 2594/3812 [63:59:00<30:02:08, 88.78s/it] 68%|██████▊   | 2595/3812 [64:00:29<30:02:23, 88.86s/it] 68%|██████▊   | 2596/3812 [64:01:58<30:00:24, 88.84s/it] 68%|██████▊   | 2597/3812 [64:03:27<30:00:40, 88.92s/it] 68%|██████▊   | 2598/3812 [64:04:56<29:55:44, 88.75s/it] 68%|██████▊   | 2599/3812 [64:06:25<29:55:00, 88.79s/it] 68%|██████▊   | 2600/3812 [64:07:53<29:52:23, 88.73s/it]                                                         {'loss': 1.2727, 'learning_rate': 1.1468226442999913e-05, 'epoch': 0.68}
 68%|██████▊   | 2600/3812 [64:07:53<29:52:23, 88.73s/it] 68%|██████▊   | 2601/3812 [64:09:22<29:51:55, 88.78s/it] 68%|██████▊   | 2602/3812 [64:10:51<29:48:34, 88.69s/it] 68%|██████▊   | 2603/3812 [64:12:19<29:48:25, 88.76s/it] 68%|██████▊   | 2604/3812 [64:13:48<29:44:55, 88.66s/it] 68%|██████▊   | 2605/3812 [64:15:17<29:44:08, 88.69s/it] 68%|██████▊   | 2606/3812 [64:16:46<29:43:34, 88.73s/it] 68%|██████▊   | 2607/3812 [64:18:14<29:42:57, 88.78s/it] 68%|██████▊   | 2608/3812 [64:19:43<29:38:53, 88.65s/it] 68%|██████▊   | 2609/3812 [64:21:12<29:38:19, 88.69s/it] 68%|██████▊   | 2610/3812 [64:22:40<29:35:42, 88.64s/it]                                                         {'loss': 1.2659, 'learning_rate': 1.1295445369576682e-05, 'epoch': 0.68}
 68%|██████▊   | 2610/3812 [64:22:40<29:35:42, 88.64s/it] 68%|██████▊   | 2611/3812 [64:24:09<29:36:07, 88.73s/it] 69%|██████▊   | 2612/3812 [64:25:38<29:33:54, 88.70s/it] 69%|██████▊   | 2613/3812 [64:27:07<29:34:00, 88.77s/it] 69%|██████▊   | 2614/3812 [64:28:35<29:30:01, 88.65s/it] 69%|██████▊   | 2615/3812 [64:30:04<29:30:55, 88.77s/it] 69%|██████▊   | 2616/3812 [64:31:32<29:26:20, 88.61s/it] 69%|██████▊   | 2617/3812 [64:33:01<29:26:13, 88.68s/it] 69%|██████▊   | 2618/3812 [64:34:30<29:24:17, 88.66s/it] 69%|██████▊   | 2619/3812 [64:35:59<29:24:12, 88.73s/it] 69%|██████▊   | 2620/3812 [64:37:27<29:21:27, 88.66s/it]                                                         {'loss': 1.2684, 'learning_rate': 1.1123595096271746e-05, 'epoch': 0.69}
 69%|██████▊   | 2620/3812 [64:37:27<29:21:27, 88.66s/it] 69%|██████▉   | 2621/3812 [64:38:56<29:20:25, 88.69s/it] 69%|██████▉   | 2622/3812 [64:40:24<29:17:04, 88.59s/it] 69%|██████▉   | 2623/3812 [64:41:53<29:15:40, 88.60s/it] 69%|██████▉   | 2624/3812 [64:43:21<29:14:42, 88.62s/it] 69%|██████▉   | 2625/3812 [64:44:51<29:15:38, 88.74s/it] 69%|██████▉   | 2626/3812 [64:46:19<29:12:55, 88.68s/it] 69%|██████▉   | 2627/3812 [64:47:48<29:12:50, 88.75s/it] 69%|██████▉   | 2628/3812 [64:49:17<29:10:53, 88.73s/it] 69%|██████▉   | 2629/3812 [64:50:46<29:10:42, 88.79s/it] 69%|██████▉   | 2630/3812 [64:52:15<29:10:29, 88.86s/it]                                                         {'loss': 1.2647, 'learning_rate': 1.0952687294989722e-05, 'epoch': 0.69}
 69%|██████▉   | 2630/3812 [64:52:15<29:10:29, 88.86s/it] 69%|██████▉   | 2631/3812 [64:53:44<29:10:35, 88.94s/it] 69%|██████▉   | 2632/3812 [64:55:12<29:05:54, 88.78s/it] 69%|██████▉   | 2633/3812 [64:56:41<29:06:06, 88.86s/it] 69%|██████▉   | 2634/3812 [64:58:10<29:01:41, 88.71s/it] 69%|██████▉   | 2635/3812 [64:59:40<29:11:36, 89.29s/it] 69%|██████▉   | 2636/3812 [65:01:09<29:07:53, 89.18s/it] 69%|██████▉   | 2637/3812 [65:02:38<29:06:48, 89.20s/it] 69%|██████▉   | 2638/3812 [65:04:07<29:00:44, 88.96s/it] 69%|██████▉   | 2639/3812 [65:05:36<28:59:49, 88.99s/it] 69%|██████▉   | 2640/3812 [65:07:04<28:54:41, 88.81s/it]                                                         {'loss': 1.2676, 'learning_rate': 1.0782733573623471e-05, 'epoch': 0.69}
 69%|██████▉   | 2640/3812 [65:07:04<28:54:41, 88.81s/it] 69%|██████▉   | 2641/3812 [65:08:33<28:52:53, 88.79s/it] 69%|██████▉   | 2642/3812 [65:10:02<28:52:20, 88.84s/it] 69%|██████▉   | 2643/3812 [65:11:31<28:51:44, 88.88s/it] 69%|██████▉   | 2644/3812 [65:12:59<28:48:14, 88.78s/it] 69%|██████▉   | 2645/3812 [65:14:29<28:48:48, 88.88s/it] 69%|██████▉   | 2646/3812 [65:15:57<28:46:06, 88.82s/it] 69%|██████▉   | 2647/3812 [65:17:26<28:45:29, 88.87s/it] 69%|██████▉   | 2648/3812 [65:18:55<28:42:15, 88.78s/it] 69%|██████▉   | 2649/3812 [65:20:24<28:41:27, 88.81s/it] 70%|██████▉   | 2650/3812 [65:21:52<28:37:38, 88.69s/it]                                                         {'loss': 1.2705, 'learning_rate': 1.0613745475265638e-05, 'epoch': 0.7}
 70%|██████▉   | 2650/3812 [65:21:52<28:37:38, 88.69s/it] 70%|██████▉   | 2651/3812 [65:23:21<28:36:51, 88.73s/it] 70%|██████▉   | 2652/3812 [65:24:49<28:33:31, 88.63s/it] 70%|██████▉   | 2653/3812 [65:26:18<28:33:21, 88.70s/it] 70%|██████▉   | 2654/3812 [65:27:47<28:32:25, 88.73s/it] 70%|██████▉   | 2655/3812 [65:29:16<28:32:27, 88.81s/it] 70%|██████▉   | 2656/3812 [65:30:44<28:29:17, 88.72s/it] 70%|██████▉   | 2657/3812 [65:32:14<28:29:55, 88.83s/it] 70%|██████▉   | 2658/3812 [65:33:42<28:25:57, 88.70s/it] 70%|██████▉   | 2659/3812 [65:35:11<28:26:24, 88.80s/it] 70%|██████▉   | 2660/3812 [65:36:40<28:26:21, 88.87s/it]                                                         {'loss': 1.2761, 'learning_rate': 1.0445734477424707e-05, 'epoch': 0.7}
 70%|██████▉   | 2660/3812 [65:36:40<28:26:21, 88.87s/it] 70%|██████▉   | 2661/3812 [65:38:09<28:26:01, 88.93s/it] 70%|██████▉   | 2662/3812 [65:39:38<28:22:41, 88.84s/it] 70%|██████▉   | 2663/3812 [65:41:07<28:21:33, 88.85s/it] 70%|██████▉   | 2664/3812 [65:42:35<28:17:15, 88.71s/it] 70%|██████▉   | 2665/3812 [65:44:04<28:17:22, 88.79s/it] 70%|██████▉   | 2666/3812 [65:45:33<28:15:32, 88.77s/it] 70%|██████▉   | 2667/3812 [65:47:02<28:14:40, 88.80s/it] 70%|██████▉   | 2668/3812 [65:48:30<28:11:36, 88.72s/it] 70%|███████   | 2669/3812 [65:49:59<28:11:14, 88.78s/it] 70%|███████   | 2670/3812 [65:51:27<28:07:24, 88.65s/it]                                                         {'loss': 1.2664, 'learning_rate': 1.0278711991245438e-05, 'epoch': 0.7}
 70%|███████   | 2670/3812 [65:51:27<28:07:24, 88.65s/it] 70%|███████   | 2671/3812 [65:52:56<28:07:48, 88.75s/it] 70%|███████   | 2672/3812 [65:54:25<28:05:08, 88.69s/it] 70%|███████   | 2673/3812 [65:55:54<28:05:10, 88.77s/it] 70%|███████   | 2674/3812 [65:57:22<28:01:10, 88.64s/it] 70%|███████   | 2675/3812 [65:58:51<28:00:56, 88.70s/it] 70%|███████   | 2676/3812 [66:00:19<27:58:18, 88.64s/it] 70%|███████   | 2677/3812 [66:01:48<27:57:40, 88.69s/it] 70%|███████   | 2678/3812 [66:03:17<27:56:56, 88.73s/it] 70%|███████   | 2679/3812 [66:04:46<27:57:02, 88.81s/it] 70%|███████   | 2680/3812 [66:06:14<27:52:39, 88.66s/it]                                                         {'loss': 1.2647, 'learning_rate': 1.0112689360733813e-05, 'epoch': 0.7}
 70%|███████   | 2680/3812 [66:06:14<27:52:39, 88.66s/it] 70%|███████   | 2681/3812 [66:07:43<27:52:00, 88.70s/it] 70%|███████   | 2682/3812 [66:09:12<27:48:17, 88.58s/it] 70%|███████   | 2683/3812 [66:10:40<27:48:05, 88.65s/it] 70%|███████   | 2684/3812 [66:12:09<27:46:47, 88.66s/it] 70%|███████   | 2685/3812 [66:13:38<27:47:08, 88.76s/it] 70%|███████   | 2686/3812 [66:15:07<27:44:33, 88.70s/it] 70%|███████   | 2687/3812 [66:16:36<27:44:59, 88.80s/it] 71%|███████   | 2688/3812 [66:18:04<27:40:56, 88.66s/it] 71%|███████   | 2689/3812 [66:19:33<27:41:34, 88.77s/it] 71%|███████   | 2690/3812 [66:21:02<27:40:07, 88.78s/it]                                                         {'loss': 1.2612, 'learning_rate': 9.9476778619866e-06, 'epoch': 0.71}
 71%|███████   | 2690/3812 [66:21:02<27:40:07, 88.78s/it] 71%|███████   | 2691/3812 [66:22:31<27:39:36, 88.83s/it] 71%|███████   | 2692/3812 [66:23:59<27:35:40, 88.70s/it] 71%|███████   | 2693/3812 [66:25:28<27:35:23, 88.76s/it] 71%|███████   | 2694/3812 [66:26:56<27:32:10, 88.67s/it] 71%|███████   | 2695/3812 [66:28:25<27:32:17, 88.75s/it] 71%|███████   | 2696/3812 [66:29:54<27:29:46, 88.70s/it] 71%|███████   | 2697/3812 [66:31:23<27:29:09, 88.74s/it] 71%|███████   | 2698/3812 [66:32:51<27:25:30, 88.63s/it] 71%|███████   | 2699/3812 [66:34:20<27:25:50, 88.72s/it] 71%|███████   | 2700/3812 [66:35:48<27:22:13, 88.61s/it]                                                         {'loss': 1.2578, 'learning_rate': 9.783688702425483e-06, 'epoch': 0.71}
 71%|███████   | 2700/3812 [66:35:48<27:22:13, 88.61s/it] 71%|███████   | 2701/3812 [66:37:17<27:22:05, 88.68s/it] 71%|███████   | 2702/3812 [66:38:46<27:19:13, 88.61s/it] 71%|███████   | 2703/3812 [66:40:15<27:19:53, 88.72s/it] 71%|███████   | 2704/3812 [66:41:43<27:17:07, 88.65s/it] 71%|███████   | 2705/3812 [66:43:12<27:16:19, 88.69s/it] 71%|███████   | 2706/3812 [66:44:41<27:15:27, 88.72s/it] 71%|███████   | 2707/3812 [66:46:10<27:15:28, 88.80s/it] 71%|███████   | 2708/3812 [66:47:38<27:12:47, 88.74s/it] 71%|███████   | 2709/3812 [66:49:07<27:12:16, 88.79s/it] 71%|███████   | 2710/3812 [66:50:36<27:09:13, 88.71s/it]                                                         {'loss': 1.2666, 'learning_rate': 9.620733020035822e-06, 'epoch': 0.71}
 71%|███████   | 2710/3812 [66:50:36<27:09:13, 88.71s/it] 71%|███████   | 2711/3812 [66:52:05<27:08:55, 88.77s/it] 71%|███████   | 2712/3812 [66:53:33<27:06:33, 88.72s/it] 71%|███████   | 2713/3812 [66:55:02<27:05:01, 88.72s/it] 71%|███████   | 2714/3812 [66:56:31<27:03:12, 88.70s/it] 71%|███████   | 2715/3812 [66:58:00<27:02:33, 88.75s/it] 71%|███████   | 2716/3812 [66:59:28<26:58:46, 88.62s/it] 71%|███████▏  | 2717/3812 [67:00:57<26:58:48, 88.70s/it] 71%|███████▏  | 2718/3812 [67:02:25<26:55:36, 88.61s/it] 71%|███████▏  | 2719/3812 [67:03:54<26:57:08, 88.77s/it] 71%|███████▏  | 2720/3812 [67:05:23<26:55:35, 88.77s/it]                                                         {'loss': 1.2677, 'learning_rate': 9.458821882610233e-06, 'epoch': 0.71}
 71%|███████▏  | 2720/3812 [67:05:23<26:55:35, 88.77s/it] 71%|███████▏  | 2721/3812 [67:06:52<26:55:30, 88.85s/it] 71%|███████▏  | 2722/3812 [67:08:21<26:52:45, 88.78s/it] 71%|███████▏  | 2723/3812 [67:09:50<26:52:51, 88.86s/it] 71%|███████▏  | 2724/3812 [67:11:18<26:49:54, 88.78s/it] 71%|███████▏  | 2725/3812 [67:12:47<26:49:48, 88.86s/it] 72%|███████▏  | 2726/3812 [67:14:16<26:47:07, 88.79s/it] 72%|███████▏  | 2727/3812 [67:15:45<26:46:13, 88.82s/it] 72%|███████▏  | 2728/3812 [67:17:13<26:41:51, 88.66s/it] 72%|███████▏  | 2729/3812 [67:18:42<26:41:08, 88.71s/it] 72%|███████▏  | 2730/3812 [67:20:11<26:38:49, 88.66s/it]                                                         {'loss': 1.2598, 'learning_rate': 9.297966286996853e-06, 'epoch': 0.72}
 72%|███████▏  | 2730/3812 [67:20:11<26:38:49, 88.66s/it] 72%|███████▏  | 2731/3812 [67:21:39<26:38:36, 88.73s/it] 72%|███████▏  | 2732/3812 [67:23:08<26:36:01, 88.67s/it] 72%|███████▏  | 2733/3812 [67:24:37<26:37:27, 88.83s/it] 72%|███████▏  | 2734/3812 [67:26:05<26:32:55, 88.66s/it] 72%|███████▏  | 2735/3812 [67:27:35<26:34:18, 88.82s/it] 72%|███████▏  | 2736/3812 [67:29:03<26:30:49, 88.71s/it] 72%|███████▏  | 2737/3812 [67:30:32<26:30:50, 88.79s/it] 72%|███████▏  | 2738/3812 [67:32:01<26:30:02, 88.83s/it] 72%|███████▏  | 2739/3812 [67:33:30<26:29:14, 88.87s/it] 72%|███████▏  | 2740/3812 [67:34:58<26:25:34, 88.75s/it]                                                         {'loss': 1.2532, 'learning_rate': 9.138177158352403e-06, 'epoch': 0.72}
 72%|███████▏  | 2740/3812 [67:34:58<26:25:34, 88.75s/it] 72%|███████▏  | 2741/3812 [67:36:27<26:24:31, 88.77s/it] 72%|███████▏  | 2742/3812 [67:37:56<26:20:49, 88.64s/it] 72%|███████▏  | 2743/3812 [67:39:24<26:20:06, 88.69s/it] 72%|███████▏  | 2744/3812 [67:40:53<26:17:26, 88.62s/it] 72%|███████▏  | 2745/3812 [67:42:22<26:17:11, 88.69s/it] 72%|███████▏  | 2746/3812 [67:43:50<26:13:33, 88.57s/it] 72%|███████▏  | 2747/3812 [67:45:19<26:14:32, 88.71s/it] 72%|███████▏  | 2748/3812 [67:46:48<26:11:51, 88.64s/it] 72%|███████▏  | 2749/3812 [67:48:16<26:11:54, 88.73s/it] 72%|███████▏  | 2750/3812 [67:49:45<26:10:11, 88.71s/it]                                                         {'loss': 1.2696, 'learning_rate': 8.979465349400237e-06, 'epoch': 0.72}
 72%|███████▏  | 2750/3812 [67:49:45<26:10:11, 88.71s/it] 72%|███████▏  | 2751/3812 [67:51:14<26:11:20, 88.86s/it] 72%|███████▏  | 2752/3812 [67:52:43<26:07:10, 88.71s/it] 72%|███████▏  | 2753/3812 [67:54:12<26:07:05, 88.79s/it] 72%|███████▏  | 2754/3812 [67:55:40<26:03:02, 88.64s/it] 72%|███████▏  | 2755/3812 [67:57:09<26:01:57, 88.66s/it] 72%|███████▏  | 2756/3812 [67:58:37<26:00:01, 88.64s/it] 72%|███████▏  | 2757/3812 [68:00:06<25:59:50, 88.71s/it] 72%|███████▏  | 2758/3812 [68:01:34<25:56:09, 88.59s/it] 72%|███████▏  | 2759/3812 [68:03:03<25:55:54, 88.66s/it] 72%|███████▏  | 2760/3812 [68:04:32<25:53:11, 88.59s/it]                                                         {'loss': 1.2526, 'learning_rate': 8.82184163969316e-06, 'epoch': 0.72}
 72%|███████▏  | 2760/3812 [68:04:32<25:53:11, 88.59s/it] 72%|███████▏  | 2761/3812 [68:06:01<25:53:07, 88.67s/it] 72%|███████▏  | 2762/3812 [68:07:29<25:51:47, 88.67s/it] 72%|███████▏  | 2763/3812 [68:08:58<25:52:34, 88.80s/it] 73%|███████▎  | 2764/3812 [68:10:27<25:49:27, 88.71s/it] 73%|███████▎  | 2765/3812 [68:11:56<25:49:45, 88.81s/it] 73%|███████▎  | 2766/3812 [68:13:24<25:46:43, 88.72s/it] 73%|███████▎  | 2767/3812 [68:14:53<25:45:50, 88.76s/it] 73%|███████▎  | 2768/3812 [68:16:22<25:43:54, 88.73s/it] 73%|███████▎  | 2769/3812 [68:17:51<25:43:39, 88.80s/it] 73%|███████▎  | 2770/3812 [68:19:19<25:39:44, 88.66s/it]                                                         {'loss': 1.2633, 'learning_rate': 8.665316734881354e-06, 'epoch': 0.73}
 73%|███████▎  | 2770/3812 [68:19:19<25:39:44, 88.66s/it] 73%|███████▎  | 2771/3812 [68:20:48<25:39:06, 88.71s/it] 73%|███████▎  | 2772/3812 [68:22:16<25:35:25, 88.58s/it] 73%|███████▎  | 2773/3812 [68:23:45<25:36:52, 88.75s/it] 73%|███████▎  | 2774/3812 [68:25:14<25:35:32, 88.76s/it] 73%|███████▎  | 2775/3812 [68:26:43<25:34:58, 88.81s/it] 73%|███████▎  | 2776/3812 [68:28:12<25:32:14, 88.74s/it] 73%|███████▎  | 2777/3812 [68:29:41<25:32:15, 88.83s/it] 73%|███████▎  | 2778/3812 [68:31:09<25:29:40, 88.76s/it] 73%|███████▎  | 2779/3812 [68:32:38<25:29:18, 88.83s/it] 73%|███████▎  | 2780/3812 [68:34:07<25:28:11, 88.85s/it]                                                         {'loss': 1.2583, 'learning_rate': 8.50990126598523e-06, 'epoch': 0.73}
 73%|███████▎  | 2780/3812 [68:34:07<25:28:11, 88.85s/it] 73%|███████▎  | 2781/3812 [68:35:36<25:28:20, 88.94s/it] 73%|███████▎  | 2782/3812 [68:37:05<25:23:53, 88.77s/it] 73%|███████▎  | 2783/3812 [68:38:34<25:23:09, 88.81s/it] 73%|███████▎  | 2784/3812 [68:40:02<25:20:38, 88.75s/it] 73%|███████▎  | 2785/3812 [68:41:31<25:19:29, 88.77s/it] 73%|███████▎  | 2786/3812 [68:43:00<25:16:07, 88.66s/it] 73%|███████▎  | 2787/3812 [68:44:29<25:16:23, 88.76s/it] 73%|███████▎  | 2788/3812 [68:45:57<25:13:21, 88.67s/it] 73%|███████▎  | 2789/3812 [68:47:26<25:12:09, 88.69s/it] 73%|███████▎  | 2790/3812 [68:48:54<25:09:24, 88.62s/it]                                                         {'loss': 1.2688, 'learning_rate': 8.35560578867336e-06, 'epoch': 0.73}
 73%|███████▎  | 2790/3812 [68:48:54<25:09:24, 88.62s/it] 73%|███████▎  | 2791/3812 [68:50:23<25:08:31, 88.65s/it] 73%|███████▎  | 2792/3812 [68:51:51<25:06:40, 88.63s/it] 73%|███████▎  | 2793/3812 [68:53:20<25:06:22, 88.70s/it] 73%|███████▎  | 2794/3812 [68:54:49<25:02:46, 88.57s/it] 73%|███████▎  | 2795/3812 [68:56:17<25:01:57, 88.61s/it] 73%|███████▎  | 2796/3812 [68:57:46<24:59:01, 88.53s/it] 73%|███████▎  | 2797/3812 [68:59:14<24:58:19, 88.57s/it] 73%|███████▎  | 2798/3812 [69:00:43<24:56:34, 88.56s/it] 73%|███████▎  | 2799/3812 [69:02:12<24:57:05, 88.67s/it] 73%|███████▎  | 2800/3812 [69:03:40<24:55:28, 88.66s/it]                                                         {'loss': 1.2616, 'learning_rate': 8.202440782545597e-06, 'epoch': 0.73}
 73%|███████▎  | 2800/3812 [69:03:40<24:55:28, 88.66s/it] 73%|███████▎  | 2801/3812 [69:05:09<24:55:30, 88.75s/it] 74%|███████▎  | 2802/3812 [69:06:38<24:52:45, 88.68s/it] 74%|███████▎  | 2803/3812 [69:08:07<24:52:11, 88.73s/it] 74%|███████▎  | 2804/3812 [69:09:35<24:49:26, 88.66s/it] 74%|███████▎  | 2805/3812 [69:11:04<24:49:21, 88.74s/it] 74%|███████▎  | 2806/3812 [69:12:33<24:46:40, 88.67s/it] 74%|███████▎  | 2807/3812 [69:14:01<24:45:48, 88.70s/it] 74%|███████▎  | 2808/3812 [69:15:31<24:46:24, 88.83s/it] 74%|███████▎  | 2809/3812 [69:17:00<24:45:55, 88.89s/it] 74%|███████▎  | 2810/3812 [69:18:28<24:43:03, 88.81s/it]                                                         {'loss': 1.2651, 'learning_rate': 8.050416650421274e-06, 'epoch': 0.74}
 74%|███████▎  | 2810/3812 [69:18:28<24:43:03, 88.81s/it] 74%|███████▎  | 2811/3812 [69:19:57<24:41:51, 88.82s/it] 74%|███████▍  | 2812/3812 [69:21:26<24:38:30, 88.71s/it] 74%|███████▍  | 2813/3812 [69:22:54<24:38:16, 88.79s/it] 74%|███████▍  | 2814/3812 [69:24:23<24:36:18, 88.76s/it] 74%|███████▍  | 2815/3812 [69:25:52<24:35:02, 88.77s/it] 74%|███████▍  | 2816/3812 [69:27:21<24:32:34, 88.71s/it] 74%|███████▍  | 2817/3812 [69:28:50<24:33:23, 88.85s/it] 74%|███████▍  | 2818/3812 [69:30:18<24:29:54, 88.73s/it] 74%|███████▍  | 2819/3812 [69:31:47<24:28:51, 88.75s/it] 74%|███████▍  | 2820/3812 [69:33:15<24:26:18, 88.69s/it]                                                         {'loss': 1.2541, 'learning_rate': 7.899543717632651e-06, 'epoch': 0.74}
 74%|███████▍  | 2820/3812 [69:33:15<24:26:18, 88.69s/it] 74%|███████▍  | 2821/3812 [69:34:44<24:25:27, 88.73s/it] 74%|███████▍  | 2822/3812 [69:36:13<24:23:34, 88.70s/it] 74%|███████▍  | 2823/3812 [69:37:42<24:22:59, 88.76s/it] 74%|███████▍  | 2824/3812 [69:39:10<24:20:17, 88.68s/it] 74%|███████▍  | 2825/3812 [69:40:39<24:19:33, 88.73s/it] 74%|███████▍  | 2826/3812 [69:42:08<24:17:51, 88.71s/it] 74%|███████▍  | 2827/3812 [69:43:37<24:16:28, 88.72s/it] 74%|███████▍  | 2828/3812 [69:45:05<24:14:30, 88.69s/it] 74%|███████▍  | 2829/3812 [69:46:34<24:13:46, 88.73s/it] 74%|███████▍  | 2830/3812 [69:48:02<24:10:45, 88.64s/it]                                                         {'loss': 1.2614, 'learning_rate': 7.749832231323653e-06, 'epoch': 0.74}
 74%|███████▍  | 2830/3812 [69:48:02<24:10:45, 88.64s/it] 74%|███████▍  | 2831/3812 [69:49:31<24:09:49, 88.67s/it] 74%|███████▍  | 2832/3812 [69:51:00<24:08:00, 88.65s/it] 74%|███████▍  | 2833/3812 [69:52:29<24:07:56, 88.74s/it] 74%|███████▍  | 2834/3812 [69:53:58<24:07:03, 88.78s/it] 74%|███████▍  | 2835/3812 [69:55:27<24:06:39, 88.84s/it] 74%|███████▍  | 2836/3812 [69:56:55<24:04:30, 88.80s/it] 74%|███████▍  | 2837/3812 [69:58:24<24:03:56, 88.86s/it] 74%|███████▍  | 2838/3812 [69:59:53<24:00:53, 88.76s/it] 74%|███████▍  | 2839/3812 [70:01:22<24:01:06, 88.87s/it] 75%|███████▍  | 2840/3812 [70:02:51<23:58:51, 88.82s/it]                                                         {'loss': 1.2614, 'learning_rate': 7.601292359753889e-06, 'epoch': 0.74}
 75%|███████▍  | 2840/3812 [70:02:51<23:58:51, 88.82s/it] 75%|███████▍  | 2841/3812 [70:04:20<23:58:01, 88.86s/it] 75%|███████▍  | 2842/3812 [70:05:48<23:55:20, 88.78s/it] 75%|███████▍  | 2843/3812 [70:07:17<23:53:55, 88.79s/it] 75%|███████▍  | 2844/3812 [70:08:46<23:51:38, 88.74s/it] 75%|███████▍  | 2845/3812 [70:10:14<23:50:28, 88.76s/it] 75%|███████▍  | 2846/3812 [70:11:43<23:49:22, 88.78s/it] 75%|███████▍  | 2847/3812 [70:13:13<23:50:25, 88.94s/it] 75%|███████▍  | 2848/3812 [70:14:41<23:46:46, 88.80s/it] 75%|███████▍  | 2849/3812 [70:16:10<23:46:14, 88.86s/it] 75%|███████▍  | 2850/3812 [70:17:39<23:42:49, 88.74s/it]                                                         {'loss': 1.2618, 'learning_rate': 7.453934191607998e-06, 'epoch': 0.75}
 75%|███████▍  | 2850/3812 [70:17:39<23:42:49, 88.74s/it] 75%|███████▍  | 2851/3812 [70:19:08<23:42:39, 88.82s/it] 75%|███████▍  | 2852/3812 [70:20:36<23:40:37, 88.79s/it] 75%|███████▍  | 2853/3812 [70:22:05<23:39:48, 88.83s/it] 75%|███████▍  | 2854/3812 [70:23:34<23:36:49, 88.74s/it] 75%|███████▍  | 2855/3812 [70:25:02<23:35:29, 88.75s/it] 75%|███████▍  | 2856/3812 [70:26:31<23:33:51, 88.74s/it] 75%|███████▍  | 2857/3812 [70:28:00<23:31:48, 88.70s/it] 75%|███████▍  | 2858/3812 [70:29:29<23:30:46, 88.73s/it] 75%|███████▌  | 2859/3812 [70:30:57<23:29:48, 88.76s/it] 75%|███████▌  | 2860/3812 [70:32:26<23:27:08, 88.69s/it]                                                         {'loss': 1.2497, 'learning_rate': 7.307767735310497e-06, 'epoch': 0.75}
 75%|███████▌  | 2860/3812 [70:32:26<23:27:08, 88.69s/it] 75%|███████▌  | 2861/3812 [70:33:55<23:25:53, 88.70s/it] 75%|███████▌  | 2862/3812 [70:35:24<23:24:55, 88.73s/it] 75%|███████▌  | 2863/3812 [70:36:52<23:23:08, 88.71s/it] 75%|███████▌  | 2864/3812 [70:38:21<23:21:23, 88.70s/it] 75%|███████▌  | 2865/3812 [70:39:50<23:20:27, 88.73s/it] 75%|███████▌  | 2866/3812 [70:41:18<23:17:07, 88.61s/it] 75%|███████▌  | 2867/3812 [70:42:47<23:15:56, 88.63s/it] 75%|███████▌  | 2868/3812 [70:44:15<23:14:05, 88.61s/it] 75%|███████▌  | 2869/3812 [70:45:44<23:13:29, 88.66s/it] 75%|███████▌  | 2870/3812 [70:47:13<23:12:01, 88.66s/it]                                                         {'loss': 1.2593, 'learning_rate': 7.162802918345954e-06, 'epoch': 0.75}
 75%|███████▌  | 2870/3812 [70:47:13<23:12:01, 88.66s/it] 75%|███████▌  | 2871/3812 [70:48:42<23:12:38, 88.80s/it] 75%|███████▌  | 2872/3812 [70:50:10<23:10:36, 88.76s/it] 75%|███████▌  | 2873/3812 [70:51:39<23:09:37, 88.79s/it] 75%|███████▌  | 2874/3812 [70:53:08<23:07:23, 88.75s/it] 75%|███████▌  | 2875/3812 [70:54:37<23:05:32, 88.72s/it] 75%|███████▌  | 2876/3812 [70:56:05<23:03:06, 88.66s/it] 75%|███████▌  | 2877/3812 [70:57:34<23:03:10, 88.76s/it] 75%|███████▌  | 2878/3812 [70:59:03<23:00:52, 88.71s/it] 76%|███████▌  | 2879/3812 [71:00:32<23:00:05, 88.75s/it] 76%|███████▌  | 2880/3812 [71:02:00<22:58:57, 88.77s/it]                                                         {'loss': 1.2626, 'learning_rate': 7.019049586584778e-06, 'epoch': 0.76}
 76%|███████▌  | 2880/3812 [71:02:00<22:58:57, 88.77s/it] 76%|███████▌  | 2881/3812 [71:03:29<22:57:45, 88.79s/it] 76%|███████▌  | 2882/3812 [71:04:58<22:55:23, 88.73s/it] 76%|███████▌  | 2883/3812 [71:06:27<22:54:12, 88.75s/it] 76%|███████▌  | 2884/3812 [71:07:55<22:51:21, 88.67s/it] 76%|███████▌  | 2885/3812 [71:09:24<22:50:17, 88.69s/it] 76%|███████▌  | 2886/3812 [71:10:52<22:47:10, 88.59s/it] 76%|███████▌  | 2887/3812 [71:12:21<22:46:29, 88.64s/it] 76%|███████▌  | 2888/3812 [71:13:50<22:46:48, 88.75s/it] 76%|███████▌  | 2889/3812 [71:15:19<22:46:35, 88.84s/it] 76%|███████▌  | 2890/3812 [71:16:47<22:43:25, 88.73s/it]                                                         {'loss': 1.2603, 'learning_rate': 6.876517503614469e-06, 'epoch': 0.76}
 76%|███████▌  | 2890/3812 [71:16:47<22:43:25, 88.73s/it] 76%|███████▌  | 2891/3812 [71:18:17<22:43:47, 88.85s/it] 76%|███████▌  | 2892/3812 [71:19:45<22:41:46, 88.81s/it] 76%|███████▌  | 2893/3812 [71:21:14<22:41:05, 88.86s/it] 76%|███████▌  | 2894/3812 [71:22:43<22:39:19, 88.85s/it] 76%|███████▌  | 2895/3812 [71:24:12<22:38:18, 88.87s/it] 76%|███████▌  | 2896/3812 [71:25:40<22:33:59, 88.69s/it] 76%|███████▌  | 2897/3812 [71:27:09<22:33:22, 88.75s/it] 76%|███████▌  | 2898/3812 [71:28:38<22:30:51, 88.68s/it] 76%|███████▌  | 2899/3812 [71:30:06<22:29:15, 88.67s/it] 76%|███████▌  | 2900/3812 [71:31:35<22:27:44, 88.67s/it]                                                         {'loss': 1.2622, 'learning_rate': 6.735216350076467e-06, 'epoch': 0.76}
 76%|███████▌  | 2900/3812 [71:31:35<22:27:44, 88.67s/it] 76%|███████▌  | 2901/3812 [71:33:04<22:27:12, 88.73s/it] 76%|███████▌  | 2902/3812 [71:34:32<22:25:03, 88.69s/it] 76%|███████▌  | 2903/3812 [71:36:01<22:24:19, 88.73s/it] 76%|███████▌  | 2904/3812 [71:37:30<22:22:34, 88.72s/it] 76%|███████▌  | 2905/3812 [71:38:59<22:22:36, 88.82s/it] 76%|███████▌  | 2906/3812 [71:40:28<22:21:50, 88.86s/it] 76%|███████▋  | 2907/3812 [71:41:57<22:21:17, 88.93s/it] 76%|███████▋  | 2908/3812 [71:43:26<22:17:47, 88.79s/it] 76%|███████▋  | 2909/3812 [71:44:54<22:16:12, 88.78s/it] 76%|███████▋  | 2910/3812 [71:46:23<22:14:04, 88.74s/it]                                                         {'loss': 1.2546, 'learning_rate': 6.5951557230087e-06, 'epoch': 0.76}
 76%|███████▋  | 2910/3812 [71:46:23<22:14:04, 88.74s/it] 76%|███████▋  | 2911/3812 [71:47:52<22:12:56, 88.76s/it] 76%|███████▋  | 2912/3812 [71:49:21<22:11:24, 88.76s/it] 76%|███████▋  | 2913/3812 [71:50:50<22:11:29, 88.87s/it] 76%|███████▋  | 2914/3812 [71:52:18<22:08:47, 88.78s/it] 76%|███████▋  | 2915/3812 [71:53:47<22:07:59, 88.83s/it] 76%|███████▋  | 2916/3812 [71:55:16<22:05:19, 88.75s/it] 77%|███████▋  | 2917/3812 [71:56:45<22:03:52, 88.75s/it] 77%|███████▋  | 2918/3812 [71:58:13<22:02:03, 88.73s/it] 77%|███████▋  | 2919/3812 [71:59:42<22:00:04, 88.69s/it] 77%|███████▋  | 2920/3812 [72:01:10<21:57:54, 88.65s/it]                                                         {'loss': 1.2554, 'learning_rate': 6.4563451351937286e-06, 'epoch': 0.77}
 77%|███████▋  | 2920/3812 [72:01:10<21:57:54, 88.65s/it] 77%|███████▋  | 2921/3812 [72:02:39<21:56:48, 88.67s/it] 77%|███████▋  | 2922/3812 [72:04:07<21:54:08, 88.59s/it] 77%|███████▋  | 2923/3812 [72:05:36<21:53:47, 88.67s/it] 77%|███████▋  | 2924/3812 [72:07:05<21:51:56, 88.65s/it] 77%|███████▋  | 2925/3812 [72:08:34<21:51:28, 88.71s/it] 77%|███████▋  | 2926/3812 [72:10:02<21:48:12, 88.59s/it] 77%|███████▋  | 2927/3812 [72:11:31<21:47:28, 88.64s/it] 77%|███████▋  | 2928/3812 [72:12:59<21:44:25, 88.54s/it] 77%|███████▋  | 2929/3812 [72:14:28<21:44:21, 88.63s/it] 77%|███████▋  | 2930/3812 [72:15:57<21:42:42, 88.62s/it]                                                         {'loss': 1.2637, 'learning_rate': 6.318794014512641e-06, 'epoch': 0.77}
 77%|███████▋  | 2930/3812 [72:15:57<21:42:42, 88.62s/it] 77%|███████▋  | 2931/3812 [72:17:25<21:42:02, 88.68s/it] 77%|███████▋  | 2932/3812 [72:18:54<21:39:40, 88.61s/it] 77%|███████▋  | 2933/3812 [72:20:23<21:38:59, 88.67s/it] 77%|███████▋  | 2934/3812 [72:21:51<21:35:54, 88.56s/it] 77%|███████▋  | 2935/3812 [72:23:20<21:34:51, 88.59s/it] 77%|███████▋  | 2936/3812 [72:24:48<21:33:40, 88.61s/it] 77%|███████▋  | 2937/3812 [72:26:17<21:34:03, 88.74s/it] 77%|███████▋  | 2938/3812 [72:27:46<21:30:49, 88.61s/it] 77%|███████▋  | 2939/3812 [72:29:15<21:30:40, 88.71s/it] 77%|███████▋  | 2940/3812 [72:30:43<21:28:23, 88.65s/it]                                                         {'loss': 1.2643, 'learning_rate': 6.182511703304747e-06, 'epoch': 0.77}
 77%|███████▋  | 2940/3812 [72:30:43<21:28:23, 88.65s/it] 77%|███████▋  | 2941/3812 [72:32:12<21:27:45, 88.71s/it] 77%|███████▋  | 2942/3812 [72:33:41<21:26:24, 88.72s/it] 77%|███████▋  | 2943/3812 [72:35:10<21:26:40, 88.84s/it] 77%|███████▋  | 2944/3812 [72:36:38<21:23:51, 88.75s/it] 77%|███████▋  | 2945/3812 [72:38:07<21:23:33, 88.83s/it] 77%|███████▋  | 2946/3812 [72:39:36<21:20:03, 88.69s/it] 77%|███████▋  | 2947/3812 [72:41:04<21:19:00, 88.72s/it] 77%|███████▋  | 2948/3812 [72:42:33<21:17:28, 88.71s/it] 77%|███████▋  | 2949/3812 [72:44:02<21:16:41, 88.76s/it] 77%|███████▋  | 2950/3812 [72:45:31<21:14:16, 88.70s/it]                                                         {'loss': 1.2586, 'learning_rate': 6.047507457733048e-06, 'epoch': 0.77}
 77%|███████▋  | 2950/3812 [72:45:31<21:14:16, 88.70s/it] 77%|███████▋  | 2951/3812 [72:47:00<21:14:20, 88.80s/it] 77%|███████▋  | 2952/3812 [72:48:28<21:11:59, 88.74s/it] 77%|███████▋  | 2953/3812 [72:49:57<21:11:27, 88.81s/it] 77%|███████▋  | 2954/3812 [72:51:26<21:09:33, 88.78s/it] 78%|███████▊  | 2955/3812 [72:52:55<21:08:25, 88.81s/it] 78%|███████▊  | 2956/3812 [72:54:23<21:05:36, 88.71s/it] 78%|███████▊  | 2957/3812 [72:55:52<21:05:33, 88.81s/it] 78%|███████▊  | 2958/3812 [72:57:21<21:03:47, 88.79s/it] 78%|███████▊  | 2959/3812 [72:58:50<21:03:38, 88.89s/it] 78%|███████▊  | 2960/3812 [73:00:19<21:02:00, 88.87s/it]                                                         {'loss': 1.2572, 'learning_rate': 5.9137904471555345e-06, 'epoch': 0.78}
 78%|███████▊  | 2960/3812 [73:00:19<21:02:00, 88.87s/it] 78%|███████▊  | 2961/3812 [73:01:48<21:01:17, 88.93s/it] 78%|███████▊  | 2962/3812 [73:03:17<20:58:23, 88.83s/it] 78%|███████▊  | 2963/3812 [73:04:46<20:57:15, 88.85s/it] 78%|███████▊  | 2964/3812 [73:06:14<20:54:37, 88.77s/it] 78%|███████▊  | 2965/3812 [73:07:43<20:53:23, 88.79s/it] 78%|███████▊  | 2966/3812 [73:09:12<20:51:39, 88.77s/it] 78%|███████▊  | 2967/3812 [73:10:41<20:50:47, 88.81s/it] 78%|███████▊  | 2968/3812 [73:12:09<20:48:16, 88.74s/it] 78%|███████▊  | 2969/3812 [73:13:38<20:46:56, 88.75s/it] 78%|███████▊  | 2970/3812 [73:15:06<20:42:44, 88.56s/it]                                                         {'loss': 1.2614, 'learning_rate': 5.7813697535024745e-06, 'epoch': 0.78}
 78%|███████▊  | 2970/3812 [73:15:06<20:42:44, 88.56s/it] 78%|███████▊  | 2971/3812 [73:16:35<20:43:19, 88.70s/it] 78%|███████▊  | 2972/3812 [73:18:04<20:41:40, 88.69s/it] 78%|███████▊  | 2973/3812 [73:19:33<20:41:37, 88.79s/it] 78%|███████▊  | 2974/3812 [73:21:01<20:38:40, 88.69s/it] 78%|███████▊  | 2975/3812 [73:22:30<20:38:32, 88.78s/it] 78%|███████▊  | 2976/3812 [73:23:59<20:36:08, 88.72s/it] 78%|███████▊  | 2977/3812 [73:25:28<20:35:39, 88.79s/it] 78%|███████▊  | 2978/3812 [73:26:57<20:35:06, 88.86s/it] 78%|███████▊  | 2979/3812 [73:28:26<20:34:33, 88.92s/it] 78%|███████▊  | 2980/3812 [73:29:54<20:31:29, 88.81s/it]                                                         {'loss': 1.2545, 'learning_rate': 5.650254370659508e-06, 'epoch': 0.78}
 78%|███████▊  | 2980/3812 [73:29:54<20:31:29, 88.81s/it] 78%|███████▊  | 2981/3812 [73:31:23<20:30:46, 88.86s/it] 78%|███████▊  | 2982/3812 [73:32:52<20:26:30, 88.66s/it] 78%|███████▊  | 2983/3812 [73:34:20<20:25:08, 88.67s/it] 78%|███████▊  | 2984/3812 [73:35:49<20:24:39, 88.74s/it] 78%|███████▊  | 2985/3812 [73:37:18<20:25:20, 88.90s/it] 78%|███████▊  | 2986/3812 [73:38:47<20:22:34, 88.81s/it] 78%|███████▊  | 2987/3812 [73:40:16<20:22:14, 88.89s/it] 78%|███████▊  | 2988/3812 [73:41:45<20:19:14, 88.78s/it] 78%|███████▊  | 2989/3812 [73:43:14<20:18:29, 88.83s/it] 78%|███████▊  | 2990/3812 [73:44:42<20:16:25, 88.79s/it]                                                         {'loss': 1.2522, 'learning_rate': 5.520453203856849e-06, 'epoch': 0.78}
 78%|███████▊  | 2990/3812 [73:44:42<20:16:25, 88.79s/it] 78%|███████▊  | 2991/3812 [73:46:11<20:15:28, 88.83s/it] 78%|███████▊  | 2992/3812 [73:47:40<20:13:53, 88.82s/it] 79%|███████▊  | 2993/3812 [73:49:09<20:12:59, 88.86s/it] 79%|███████▊  | 2994/3812 [73:50:38<20:10:13, 88.77s/it] 79%|███████▊  | 2995/3812 [73:52:06<20:08:23, 88.74s/it] 79%|███████▊  | 2996/3812 [73:53:35<20:05:47, 88.66s/it] 79%|███████▊  | 2997/3812 [73:55:04<20:04:57, 88.71s/it] 79%|███████▊  | 2998/3812 [73:56:32<20:02:20, 88.62s/it] 79%|███████▊  | 2999/3812 [73:58:01<20:02:09, 88.72s/it] 79%|███████▊  | 3000/3812 [73:59:29<20:00:00, 88.67s/it]                                                         {'loss': 1.2589, 'learning_rate': 5.391975069064426e-06, 'epoch': 0.79}
 79%|███████▊  | 3000/3812 [73:59:29<20:00:00, 88.67s/it][INFO|trainer.py:2936] 2024-02-12 01:11:49,552 >> Saving model checkpoint to /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3000
[INFO|configuration_utils.py:473] 2024-02-12 01:11:49,553 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3000/config.json
[INFO|configuration_utils.py:594] 2024-02-12 01:11:49,553 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3000/generation_config.json
[INFO|modeling_utils.py:2501] 2024-02-12 01:12:08,118 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3000/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2433] 2024-02-12 01:12:08,119 >> tokenizer config file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-12 01:12:08,119 >> Special tokens file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3000/special_tokens_map.json
[INFO|trainer.py:3028] 2024-02-12 01:12:08,922 >> Deleting older checkpoint [/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/checkpoint-1500] due to args.save_total_limit
 79%|███████▊  | 3001/3812 [74:01:27<21:56:57, 97.43s/it] 79%|███████▉  | 3002/3812 [74:02:56<21:18:38, 94.71s/it] 79%|███████▉  | 3003/3812 [74:04:24<20:52:58, 92.93s/it] 79%|███████▉  | 3004/3812 [74:05:53<20:33:42, 91.61s/it] 79%|███████▉  | 3005/3812 [74:07:22<20:21:29, 90.82s/it] 79%|███████▉  | 3006/3812 [74:08:50<20:10:26, 90.11s/it] 79%|███████▉  | 3007/3812 [74:10:19<20:03:40, 89.71s/it] 79%|███████▉  | 3008/3812 [74:11:48<19:57:32, 89.37s/it] 79%|███████▉  | 3009/3812 [74:13:16<19:52:56, 89.14s/it] 79%|███████▉  | 3010/3812 [74:14:45<19:49:08, 88.96s/it]                                                         {'loss': 1.2597, 'learning_rate': 5.264828692393092e-06, 'epoch': 0.79}
 79%|███████▉  | 3010/3812 [74:14:45<19:49:08, 88.96s/it] 79%|███████▉  | 3011/3812 [74:16:14<19:46:37, 88.89s/it] 79%|███████▉  | 3012/3812 [74:17:42<19:43:26, 88.76s/it] 79%|███████▉  | 3013/3812 [74:19:11<19:42:05, 88.77s/it] 79%|███████▉  | 3014/3812 [74:20:39<19:39:46, 88.70s/it] 79%|███████▉  | 3015/3812 [74:22:08<19:38:34, 88.73s/it] 79%|███████▉  | 3016/3812 [74:23:37<19:36:11, 88.66s/it] 79%|███████▉  | 3017/3812 [74:25:06<19:35:21, 88.71s/it] 79%|███████▉  | 3018/3812 [74:26:34<19:33:02, 88.64s/it] 79%|███████▉  | 3019/3812 [74:28:03<19:31:14, 88.62s/it] 79%|███████▉  | 3020/3812 [74:29:31<19:29:09, 88.57s/it]                                                         {'loss': 1.2575, 'learning_rate': 5.139022709501989e-06, 'epoch': 0.79}
 79%|███████▉  | 3020/3812 [74:29:31<19:29:09, 88.57s/it] 79%|███████▉  | 3021/3812 [74:31:00<19:29:11, 88.69s/it] 79%|███████▉  | 3022/3812 [74:32:29<19:27:59, 88.71s/it] 79%|███████▉  | 3023/3812 [74:33:58<19:28:13, 88.84s/it] 79%|███████▉  | 3024/3812 [74:35:27<19:26:16, 88.80s/it] 79%|███████▉  | 3025/3812 [74:36:56<19:25:08, 88.83s/it] 79%|███████▉  | 3026/3812 [74:38:24<19:22:30, 88.74s/it] 79%|███████▉  | 3027/3812 [74:39:53<19:20:56, 88.73s/it] 79%|███████▉  | 3028/3812 [74:41:21<19:18:37, 88.67s/it] 79%|███████▉  | 3029/3812 [74:42:50<19:17:55, 88.73s/it] 79%|███████▉  | 3030/3812 [74:44:19<19:15:40, 88.67s/it]                                                         {'loss': 1.2615, 'learning_rate': 5.0145656650120165e-06, 'epoch': 0.79}
 79%|███████▉  | 3030/3812 [74:44:19<19:15:40, 88.67s/it] 80%|███████▉  | 3031/3812 [74:45:47<19:14:29, 88.69s/it] 80%|███████▉  | 3032/3812 [74:47:16<19:12:15, 88.64s/it] 80%|███████▉  | 3033/3812 [74:48:45<19:11:41, 88.71s/it] 80%|███████▉  | 3034/3812 [74:50:13<19:09:48, 88.67s/it] 80%|███████▉  | 3035/3812 [74:51:42<19:09:42, 88.78s/it] 80%|███████▉  | 3036/3812 [74:53:11<19:07:36, 88.73s/it] 80%|███████▉  | 3037/3812 [74:54:40<19:06:44, 88.78s/it] 80%|███████▉  | 3038/3812 [74:56:08<19:03:41, 88.66s/it] 80%|███████▉  | 3039/3812 [74:57:37<19:02:26, 88.68s/it] 80%|███████▉  | 3040/3812 [74:59:06<19:00:36, 88.65s/it]                                                         {'loss': 1.2557, 'learning_rate': 4.891466011925455e-06, 'epoch': 0.8}
 80%|███████▉  | 3040/3812 [74:59:06<19:00:36, 88.65s/it] 80%|███████▉  | 3041/3812 [75:00:35<19:00:01, 88.72s/it] 80%|███████▉  | 3042/3812 [75:02:03<18:58:05, 88.68s/it] 80%|███████▉  | 3043/3812 [75:03:32<18:58:13, 88.81s/it] 80%|███████▉  | 3044/3812 [75:05:01<18:55:38, 88.72s/it] 80%|███████▉  | 3045/3812 [75:06:30<18:54:49, 88.77s/it] 80%|███████▉  | 3046/3812 [75:07:58<18:52:38, 88.72s/it] 80%|███████▉  | 3047/3812 [75:09:27<18:52:35, 88.83s/it] 80%|███████▉  | 3048/3812 [75:10:56<18:50:09, 88.76s/it] 80%|███████▉  | 3049/3812 [75:12:25<18:49:07, 88.79s/it] 80%|████████  | 3050/3812 [75:13:53<18:46:17, 88.68s/it]                                                         {'loss': 1.2486, 'learning_rate': 4.769732111051886e-06, 'epoch': 0.8}
 80%|████████  | 3050/3812 [75:13:53<18:46:17, 88.68s/it] 80%|████████  | 3051/3812 [75:15:22<18:45:16, 88.72s/it] 80%|████████  | 3052/3812 [75:16:51<18:44:12, 88.75s/it] 80%|████████  | 3053/3812 [75:18:20<18:43:42, 88.83s/it] 80%|████████  | 3054/3812 [75:19:49<18:42:02, 88.82s/it] 80%|████████  | 3055/3812 [75:21:18<18:40:49, 88.84s/it] 80%|████████  | 3056/3812 [75:22:46<18:38:20, 88.76s/it] 80%|████████  | 3057/3812 [75:24:15<18:36:29, 88.73s/it] 80%|████████  | 3058/3812 [75:25:44<18:35:18, 88.75s/it] 80%|████████  | 3059/3812 [75:27:12<18:34:21, 88.79s/it] 80%|████████  | 3060/3812 [75:28:41<18:32:20, 88.75s/it]                                                         {'loss': 1.2518, 'learning_rate': 4.649372230440333e-06, 'epoch': 0.8}
 80%|████████  | 3060/3812 [75:28:41<18:32:20, 88.75s/it] 80%|████████  | 3061/3812 [75:30:10<18:30:38, 88.73s/it] 80%|████████  | 3062/3812 [75:31:38<18:28:32, 88.68s/it] 80%|████████  | 3063/3812 [75:33:07<18:27:00, 88.68s/it] 80%|████████  | 3064/3812 [75:34:36<18:25:25, 88.67s/it] 80%|████████  | 3065/3812 [75:36:05<18:24:37, 88.73s/it] 80%|████████  | 3066/3812 [75:37:33<18:23:48, 88.78s/it] 80%|████████  | 3067/3812 [75:39:02<18:22:55, 88.83s/it] 80%|████████  | 3068/3812 [75:40:31<18:20:21, 88.74s/it] 81%|████████  | 3069/3812 [75:42:00<18:19:03, 88.75s/it] 81%|████████  | 3070/3812 [75:43:28<18:16:52, 88.70s/it]                                                         {'loss': 1.2528, 'learning_rate': 4.5303945448176565e-06, 'epoch': 0.81}
 81%|████████  | 3070/3812 [75:43:28<18:16:52, 88.70s/it] 81%|████████  | 3071/3812 [75:44:57<18:15:26, 88.70s/it] 81%|████████  | 3072/3812 [75:46:25<18:13:15, 88.64s/it] 81%|████████  | 3073/3812 [75:47:54<18:12:17, 88.68s/it] 81%|████████  | 3074/3812 [75:49:23<18:10:41, 88.67s/it] 81%|████████  | 3075/3812 [75:50:52<18:09:43, 88.72s/it] 81%|████████  | 3076/3812 [75:52:20<18:08:08, 88.71s/it] 81%|████████  | 3077/3812 [75:53:50<18:08:41, 88.87s/it] 81%|████████  | 3078/3812 [75:55:18<18:05:40, 88.75s/it] 81%|████████  | 3079/3812 [75:56:47<18:04:37, 88.78s/it] 81%|████████  | 3080/3812 [75:58:15<18:02:05, 88.70s/it]                                                         {'loss': 1.2526, 'learning_rate': 4.412807135033406e-06, 'epoch': 0.81}
 81%|████████  | 3080/3812 [75:58:15<18:02:05, 88.70s/it] 81%|████████  | 3081/3812 [75:59:44<18:01:07, 88.74s/it] 81%|████████  | 3082/3812 [76:01:13<17:59:10, 88.70s/it] 81%|████████  | 3083/3812 [76:02:42<17:59:19, 88.83s/it] 81%|████████  | 3084/3812 [76:04:11<17:56:38, 88.73s/it] 81%|████████  | 3085/3812 [76:05:39<17:55:43, 88.78s/it] 81%|████████  | 3086/3812 [76:07:08<17:52:49, 88.66s/it] 81%|████████  | 3087/3812 [76:08:36<17:50:42, 88.61s/it] 81%|████████  | 3088/3812 [76:10:05<17:48:37, 88.56s/it] 81%|████████  | 3089/3812 [76:11:34<17:48:45, 88.69s/it] 81%|████████  | 3090/3812 [76:13:02<17:46:32, 88.63s/it]                                                         {'loss': 1.2523, 'learning_rate': 4.296617987510898e-06, 'epoch': 0.81}
 81%|████████  | 3090/3812 [76:13:02<17:46:32, 88.63s/it] 81%|████████  | 3091/3812 [76:14:31<17:46:25, 88.75s/it] 81%|████████  | 3092/3812 [76:16:00<17:43:53, 88.66s/it] 81%|████████  | 3093/3812 [76:17:28<17:42:37, 88.67s/it] 81%|████████  | 3094/3812 [76:18:57<17:41:01, 88.66s/it] 81%|████████  | 3095/3812 [76:20:26<17:40:55, 88.78s/it] 81%|████████  | 3096/3812 [76:21:55<17:38:00, 88.66s/it] 81%|████████  | 3097/3812 [76:23:23<17:36:55, 88.69s/it] 81%|████████▏ | 3098/3812 [76:24:52<17:34:42, 88.63s/it] 81%|████████▏ | 3099/3812 [76:26:21<17:33:33, 88.66s/it] 81%|████████▏ | 3100/3812 [76:27:49<17:32:00, 88.65s/it]                                                         {'loss': 1.2558, 'learning_rate': 4.181834993704864e-06, 'epoch': 0.81}
 81%|████████▏ | 3100/3812 [76:27:49<17:32:00, 88.65s/it] 81%|████████▏ | 3101/3812 [76:29:18<17:31:35, 88.74s/it] 81%|████████▏ | 3102/3812 [76:30:47<17:29:47, 88.71s/it] 81%|████████▏ | 3103/3812 [76:32:16<17:30:23, 88.89s/it] 81%|████████▏ | 3104/3812 [76:33:45<17:27:24, 88.76s/it] 81%|████████▏ | 3105/3812 [76:35:13<17:26:40, 88.83s/it] 81%|████████▏ | 3106/3812 [76:36:42<17:24:14, 88.75s/it] 82%|████████▏ | 3107/3812 [76:38:11<17:23:10, 88.78s/it] 82%|████████▏ | 3108/3812 [76:39:39<17:20:24, 88.67s/it] 82%|████████▏ | 3109/3812 [76:41:08<17:20:19, 88.79s/it] 82%|████████▏ | 3110/3812 [76:42:37<17:18:10, 88.73s/it]                                                         {'loss': 1.254, 'learning_rate': 4.068465949565414e-06, 'epoch': 0.82}
 82%|████████▏ | 3110/3812 [76:42:37<17:18:10, 88.73s/it] 82%|████████▏ | 3111/3812 [76:44:06<17:16:56, 88.75s/it] 82%|████████▏ | 3112/3812 [76:45:35<17:15:23, 88.75s/it] 82%|████████▏ | 3113/3812 [76:47:03<17:13:45, 88.74s/it] 82%|████████▏ | 3114/3812 [76:48:32<17:11:32, 88.67s/it] 82%|████████▏ | 3115/3812 [76:50:00<17:10:09, 88.68s/it] 82%|████████▏ | 3116/3812 [76:51:29<17:07:44, 88.60s/it] 82%|████████▏ | 3117/3812 [76:52:58<17:07:02, 88.67s/it] 82%|████████▏ | 3118/3812 [76:54:26<17:05:16, 88.64s/it] 82%|████████▏ | 3119/3812 [76:55:55<17:04:54, 88.74s/it] 82%|████████▏ | 3120/3812 [76:57:24<17:03:11, 88.72s/it]                                                         {'loss': 1.2507, 'learning_rate': 3.9565185550085565e-06, 'epoch': 0.82}
 82%|████████▏ | 3120/3812 [76:57:24<17:03:11, 88.72s/it] 82%|████████▏ | 3121/3812 [76:58:53<17:01:46, 88.72s/it] 82%|████████▏ | 3122/3812 [77:00:21<17:00:05, 88.70s/it] 82%|████████▏ | 3123/3812 [77:01:50<16:59:42, 88.80s/it] 82%|████████▏ | 3124/3812 [77:03:19<16:57:48, 88.76s/it] 82%|████████▏ | 3125/3812 [77:04:48<16:57:00, 88.82s/it] 82%|████████▏ | 3126/3812 [77:06:17<16:54:47, 88.76s/it] 82%|████████▏ | 3127/3812 [77:07:45<16:53:15, 88.75s/it] 82%|████████▏ | 3128/3812 [77:09:14<16:50:52, 88.67s/it] 82%|████████▏ | 3129/3812 [77:10:43<16:49:53, 88.72s/it] 82%|████████▏ | 3130/3812 [77:12:11<16:48:28, 88.72s/it]                                                         {'loss': 1.2601, 'learning_rate': 3.846000413393247e-06, 'epoch': 0.82}
 82%|████████▏ | 3130/3812 [77:12:11<16:48:28, 88.72s/it] 82%|████████▏ | 3131/3812 [77:13:40<16:47:19, 88.75s/it] 82%|████████▏ | 3132/3812 [77:15:09<16:45:00, 88.68s/it] 82%|████████▏ | 3133/3812 [77:16:37<16:43:27, 88.67s/it] 82%|████████▏ | 3134/3812 [77:18:06<16:40:38, 88.55s/it] 82%|████████▏ | 3135/3812 [77:19:34<16:39:55, 88.62s/it] 82%|████████▏ | 3136/3812 [77:21:03<16:38:30, 88.62s/it] 82%|████████▏ | 3137/3812 [77:22:32<16:37:43, 88.69s/it] 82%|████████▏ | 3138/3812 [77:24:00<16:36:00, 88.67s/it] 82%|████████▏ | 3139/3812 [77:25:29<16:35:19, 88.74s/it] 82%|████████▏ | 3140/3812 [77:26:58<16:33:46, 88.73s/it]                                                         {'loss': 1.25, 'learning_rate': 3.7369190310049597e-06, 'epoch': 0.82}
 82%|████████▏ | 3140/3812 [77:26:58<16:33:46, 88.73s/it] 82%|████████▏ | 3141/3812 [77:28:27<16:32:40, 88.76s/it] 82%|████████▏ | 3142/3812 [77:29:56<16:31:02, 88.75s/it] 82%|████████▏ | 3143/3812 [77:31:25<16:30:21, 88.82s/it] 82%|████████▏ | 3144/3812 [77:32:53<16:27:32, 88.70s/it] 83%|████████▎ | 3145/3812 [77:34:22<16:26:15, 88.72s/it] 83%|████████▎ | 3146/3812 [77:35:50<16:23:10, 88.57s/it] 83%|████████▎ | 3147/3812 [77:37:19<16:22:39, 88.66s/it] 83%|████████▎ | 3148/3812 [77:38:48<16:21:55, 88.73s/it] 83%|████████▎ | 3149/3812 [77:40:17<16:21:33, 88.83s/it] 83%|████████▎ | 3150/3812 [77:41:46<16:19:54, 88.81s/it]                                                         {'loss': 1.2537, 'learning_rate': 3.6292818165458516e-06, 'epoch': 0.83}
 83%|████████▎ | 3150/3812 [77:41:46<16:19:54, 88.81s/it] 83%|████████▎ | 3151/3812 [77:43:15<16:19:18, 88.89s/it] 83%|████████▎ | 3152/3812 [77:44:43<16:17:06, 88.83s/it] 83%|████████▎ | 3153/3812 [77:46:12<16:16:17, 88.89s/it] 83%|████████▎ | 3154/3812 [77:47:41<16:14:56, 88.90s/it] 83%|████████▎ | 3155/3812 [77:49:10<16:13:54, 88.94s/it] 83%|████████▎ | 3156/3812 [77:50:39<16:11:55, 88.89s/it] 83%|████████▎ | 3157/3812 [77:52:08<16:10:55, 88.94s/it] 83%|████████▎ | 3158/3812 [77:53:37<16:08:11, 88.83s/it] 83%|████████▎ | 3159/3812 [77:55:06<16:07:35, 88.91s/it] 83%|████████▎ | 3160/3812 [77:56:35<16:06:05, 88.90s/it]                                                         {'loss': 1.2494, 'learning_rate': 3.5230960806316104e-06, 'epoch': 0.83}
 83%|████████▎ | 3160/3812 [77:56:35<16:06:05, 88.90s/it] 83%|████████▎ | 3161/3812 [77:58:04<16:05:13, 88.96s/it] 83%|████████▎ | 3162/3812 [77:59:32<16:02:11, 88.82s/it] 83%|████████▎ | 3163/3812 [78:01:01<16:01:50, 88.92s/it] 83%|████████▎ | 3164/3812 [78:02:30<15:58:13, 88.72s/it] 83%|████████▎ | 3165/3812 [78:03:59<15:57:17, 88.77s/it] 83%|████████▎ | 3166/3812 [78:05:27<15:55:09, 88.71s/it] 83%|████████▎ | 3167/3812 [78:06:56<15:54:55, 88.83s/it] 83%|████████▎ | 3168/3812 [78:08:25<15:51:41, 88.67s/it] 83%|████████▎ | 3169/3812 [78:09:54<15:51:38, 88.80s/it] 83%|████████▎ | 3170/3812 [78:11:22<15:48:56, 88.69s/it]                                                         {'loss': 1.2412, 'learning_rate': 3.4183690352948964e-06, 'epoch': 0.83}
 83%|████████▎ | 3170/3812 [78:11:22<15:48:56, 88.69s/it] 83%|████████▎ | 3171/3812 [78:12:51<15:48:04, 88.74s/it] 83%|████████▎ | 3172/3812 [78:14:20<15:45:53, 88.68s/it] 83%|████████▎ | 3173/3812 [78:15:48<15:44:42, 88.71s/it] 83%|████████▎ | 3174/3812 [78:17:17<15:42:29, 88.64s/it] 83%|████████▎ | 3175/3812 [78:18:45<15:41:13, 88.66s/it] 83%|████████▎ | 3176/3812 [78:20:14<15:38:53, 88.57s/it] 83%|████████▎ | 3177/3812 [78:21:43<15:38:24, 88.67s/it] 83%|████████▎ | 3178/3812 [78:23:11<15:36:57, 88.67s/it] 83%|████████▎ | 3179/3812 [78:24:40<15:36:10, 88.74s/it] 83%|████████▎ | 3180/3812 [78:26:09<15:34:08, 88.69s/it]                                                         {'loss': 1.2574, 'learning_rate': 3.3151077934955003e-06, 'epoch': 0.83}
 83%|████████▎ | 3180/3812 [78:26:09<15:34:08, 88.69s/it] 83%|████████▎ | 3181/3812 [78:27:38<15:33:16, 88.74s/it] 83%|████████▎ | 3182/3812 [78:29:06<15:31:24, 88.71s/it] 83%|████████▎ | 3183/3812 [78:30:35<15:29:56, 88.71s/it] 84%|████████▎ | 3184/3812 [78:32:04<15:28:45, 88.73s/it] 84%|████████▎ | 3185/3812 [78:33:33<15:27:30, 88.76s/it] 84%|████████▎ | 3186/3812 [78:35:01<15:25:55, 88.75s/it] 84%|████████▎ | 3187/3812 [78:36:30<15:25:22, 88.84s/it] 84%|████████▎ | 3188/3812 [78:37:59<15:23:22, 88.79s/it] 84%|████████▎ | 3189/3812 [78:39:28<15:23:06, 88.90s/it] 84%|████████▎ | 3190/3812 [78:40:57<15:21:56, 88.93s/it]                                                         {'loss': 1.2486, 'learning_rate': 3.2133193686372713e-06, 'epoch': 0.84}
 84%|████████▎ | 3190/3812 [78:40:57<15:21:56, 88.93s/it] 84%|████████▎ | 3191/3812 [78:42:26<15:20:53, 88.98s/it] 84%|████████▎ | 3192/3812 [78:43:55<15:17:44, 88.81s/it] 84%|████████▍ | 3193/3812 [78:45:24<15:16:23, 88.83s/it] 84%|████████▍ | 3194/3812 [78:46:52<15:14:14, 88.76s/it] 84%|████████▍ | 3195/3812 [78:48:21<15:12:31, 88.74s/it] 84%|████████▍ | 3196/3812 [78:49:50<15:11:10, 88.75s/it] 84%|████████▍ | 3197/3812 [78:51:19<15:10:15, 88.81s/it] 84%|████████▍ | 3198/3812 [78:52:47<15:08:12, 88.75s/it] 84%|████████▍ | 3199/3812 [78:54:16<15:06:35, 88.74s/it] 84%|████████▍ | 3200/3812 [78:55:44<15:04:04, 88.63s/it]                                                         {'loss': 1.2573, 'learning_rate': 3.113010674091746e-06, 'epoch': 0.84}
 84%|████████▍ | 3200/3812 [78:55:44<15:04:04, 88.63s/it] 84%|████████▍ | 3201/3812 [78:57:13<15:03:02, 88.68s/it] 84%|████████▍ | 3202/3812 [78:58:42<15:01:51, 88.71s/it] 84%|████████▍ | 3203/3812 [79:00:11<15:01:40, 88.83s/it] 84%|████████▍ | 3204/3812 [79:01:39<14:58:33, 88.67s/it] 84%|████████▍ | 3205/3812 [79:03:08<14:57:42, 88.74s/it] 84%|████████▍ | 3206/3812 [79:04:37<14:55:22, 88.65s/it] 84%|████████▍ | 3207/3812 [79:06:05<14:53:57, 88.66s/it] 84%|████████▍ | 3208/3812 [79:07:34<14:52:16, 88.64s/it] 84%|████████▍ | 3209/3812 [79:09:03<14:50:59, 88.66s/it] 84%|████████▍ | 3210/3812 [79:10:31<14:48:26, 88.55s/it]                                                         {'loss': 1.2587, 'learning_rate': 3.0141885227286003e-06, 'epoch': 0.84}
 84%|████████▍ | 3210/3812 [79:10:31<14:48:26, 88.55s/it] 84%|████████▍ | 3211/3812 [79:12:00<14:48:35, 88.71s/it] 84%|████████▍ | 3212/3812 [79:13:28<14:45:30, 88.55s/it] 84%|████████▍ | 3213/3812 [79:14:57<14:44:22, 88.59s/it] 84%|████████▍ | 3214/3812 [79:16:26<14:43:30, 88.65s/it] 84%|████████▍ | 3215/3812 [79:17:55<14:43:05, 88.75s/it] 84%|████████▍ | 3216/3812 [79:19:23<14:40:48, 88.67s/it] 84%|████████▍ | 3217/3812 [79:20:52<14:40:31, 88.79s/it] 84%|████████▍ | 3218/3812 [79:22:21<14:38:21, 88.72s/it] 84%|████████▍ | 3219/3812 [79:23:50<14:38:27, 88.88s/it] 84%|████████▍ | 3220/3812 [79:25:19<14:37:38, 88.95s/it]                                                         {'loss': 1.2584, 'learning_rate': 2.9168596264529503e-06, 'epoch': 0.84}
 84%|████████▍ | 3220/3812 [79:25:19<14:37:38, 88.95s/it] 84%|████████▍ | 3221/3812 [79:26:48<14:36:55, 89.03s/it] 85%|████████▍ | 3222/3812 [79:28:17<14:34:09, 88.90s/it] 85%|████████▍ | 3223/3812 [79:29:46<14:32:40, 88.90s/it] 85%|████████▍ | 3224/3812 [79:31:14<14:29:35, 88.73s/it] 85%|████████▍ | 3225/3812 [79:32:43<14:28:20, 88.76s/it] 85%|████████▍ | 3226/3812 [79:34:12<14:25:57, 88.66s/it] 85%|████████▍ | 3227/3812 [79:35:40<14:25:14, 88.74s/it] 85%|████████▍ | 3228/3812 [79:37:09<14:22:32, 88.62s/it] 85%|████████▍ | 3229/3812 [79:38:38<14:21:31, 88.66s/it] 85%|████████▍ | 3230/3812 [79:40:06<14:19:30, 88.61s/it]                                                         {'loss': 1.258, 'learning_rate': 2.821030595749441e-06, 'epoch': 0.85}
 85%|████████▍ | 3230/3812 [79:40:06<14:19:30, 88.61s/it] 85%|████████▍ | 3231/3812 [79:41:35<14:18:05, 88.62s/it] 85%|████████▍ | 3232/3812 [79:43:03<14:16:38, 88.62s/it] 85%|████████▍ | 3233/3812 [79:44:32<14:15:52, 88.69s/it] 85%|████████▍ | 3234/3812 [79:46:01<14:13:35, 88.61s/it] 85%|████████▍ | 3235/3812 [79:47:29<14:12:40, 88.67s/it] 85%|████████▍ | 3236/3812 [79:48:58<14:10:30, 88.60s/it] 85%|████████▍ | 3237/3812 [79:50:27<14:09:27, 88.64s/it] 85%|████████▍ | 3238/3812 [79:51:55<14:07:19, 88.57s/it] 85%|████████▍ | 3239/3812 [79:53:24<14:06:42, 88.66s/it] 85%|████████▍ | 3240/3812 [79:54:52<14:04:53, 88.63s/it]                                                         {'loss': 1.2556, 'learning_rate': 2.7267079392333167e-06, 'epoch': 0.85}
 85%|████████▍ | 3240/3812 [79:54:52<14:04:53, 88.63s/it] 85%|████████▌ | 3241/3812 [79:56:21<14:03:57, 88.68s/it] 85%|████████▌ | 3242/3812 [79:57:50<14:01:53, 88.62s/it] 85%|████████▌ | 3243/3812 [79:59:19<14:01:25, 88.73s/it] 85%|████████▌ | 3244/3812 [80:00:47<13:59:58, 88.73s/it] 85%|████████▌ | 3245/3812 [80:02:16<13:59:31, 88.84s/it] 85%|████████▌ | 3246/3812 [80:03:45<13:56:55, 88.72s/it] 85%|████████▌ | 3247/3812 [80:05:14<13:55:33, 88.73s/it] 85%|████████▌ | 3248/3812 [80:06:42<13:53:05, 88.63s/it] 85%|████████▌ | 3249/3812 [80:08:11<13:51:46, 88.64s/it] 85%|████████▌ | 3250/3812 [80:09:39<13:49:49, 88.59s/it]                                                         {'loss': 1.2547, 'learning_rate': 2.6338980632083482e-06, 'epoch': 0.85}
 85%|████████▌ | 3250/3812 [80:09:39<13:49:49, 88.59s/it] 85%|████████▌ | 3251/3812 [80:11:08<13:49:19, 88.70s/it] 85%|████████▌ | 3252/3812 [80:12:37<13:47:28, 88.66s/it] 85%|████████▌ | 3253/3812 [80:14:05<13:46:02, 88.66s/it] 85%|████████▌ | 3254/3812 [80:15:34<13:44:17, 88.63s/it] 85%|████████▌ | 3255/3812 [80:17:03<13:43:36, 88.72s/it] 85%|████████▌ | 3256/3812 [80:18:31<13:41:49, 88.69s/it] 85%|████████▌ | 3257/3812 [80:20:00<13:41:20, 88.79s/it] 85%|████████▌ | 3258/3812 [80:21:29<13:39:58, 88.81s/it] 85%|████████▌ | 3259/3812 [80:22:58<13:39:12, 88.88s/it] 86%|████████▌ | 3260/3812 [80:24:27<13:36:41, 88.77s/it]                                                         {'loss': 1.2506, 'learning_rate': 2.5426072712316866e-06, 'epoch': 0.86}
 86%|████████▌ | 3260/3812 [80:24:27<13:36:41, 88.77s/it] 86%|████████▌ | 3261/3812 [80:25:56<13:35:29, 88.80s/it] 86%|████████▌ | 3262/3812 [80:27:24<13:33:02, 88.70s/it] 86%|████████▌ | 3263/3812 [80:28:53<13:32:13, 88.77s/it] 86%|████████▌ | 3264/3812 [80:30:22<13:29:44, 88.66s/it] 86%|████████▌ | 3265/3812 [80:31:51<13:29:04, 88.75s/it] 86%|████████▌ | 3266/3812 [80:33:19<13:26:43, 88.65s/it] 86%|████████▌ | 3267/3812 [80:34:48<13:25:46, 88.71s/it] 86%|████████▌ | 3268/3812 [80:36:16<13:23:48, 88.66s/it] 86%|████████▌ | 3269/3812 [80:37:45<13:23:01, 88.73s/it] 86%|████████▌ | 3270/3812 [80:39:14<13:20:55, 88.66s/it]                                                         {'loss': 1.2579, 'learning_rate': 2.452841763685795e-06, 'epoch': 0.86}
 86%|████████▌ | 3270/3812 [80:39:14<13:20:55, 88.66s/it] 86%|████████▌ | 3271/3812 [80:40:43<13:19:49, 88.71s/it] 86%|████████▌ | 3272/3812 [80:42:11<13:17:44, 88.64s/it] 86%|████████▌ | 3273/3812 [80:43:40<13:16:12, 88.63s/it] 86%|████████▌ | 3274/3812 [80:45:08<13:14:30, 88.61s/it] 86%|████████▌ | 3275/3812 [80:46:37<13:13:48, 88.69s/it] 86%|████████▌ | 3276/3812 [80:48:06<13:11:59, 88.66s/it] 86%|████████▌ | 3277/3812 [80:49:35<13:11:26, 88.76s/it] 86%|████████▌ | 3278/3812 [80:51:03<13:09:40, 88.73s/it] 86%|████████▌ | 3279/3812 [80:52:32<13:08:56, 88.81s/it] 86%|████████▌ | 3280/3812 [80:54:01<13:07:29, 88.81s/it]                                                         {'loss': 1.2536, 'learning_rate': 2.364607637357258e-06, 'epoch': 0.86}
 86%|████████▌ | 3280/3812 [80:54:01<13:07:29, 88.81s/it] 86%|████████▌ | 3281/3812 [80:55:30<13:06:47, 88.90s/it] 86%|████████▌ | 3282/3812 [80:56:59<13:03:59, 88.75s/it] 86%|████████▌ | 3283/3812 [80:58:28<13:03:14, 88.84s/it] 86%|████████▌ | 3284/3812 [80:59:56<13:01:04, 88.76s/it] 86%|████████▌ | 3285/3812 [81:01:25<13:00:21, 88.84s/it] 86%|████████▌ | 3286/3812 [81:02:54<12:58:19, 88.78s/it] 86%|████████▌ | 3287/3812 [81:04:23<12:57:42, 88.88s/it] 86%|████████▋ | 3288/3812 [81:05:52<12:55:27, 88.79s/it] 86%|████████▋ | 3289/3812 [81:07:21<12:54:16, 88.83s/it] 86%|████████▋ | 3290/3812 [81:08:49<12:52:05, 88.75s/it]                                                         {'loss': 1.2506, 'learning_rate': 2.2779108850227575e-06, 'epoch': 0.86}
 86%|████████▋ | 3290/3812 [81:08:49<12:52:05, 88.75s/it] 86%|████████▋ | 3291/3812 [81:10:18<12:50:48, 88.77s/it] 86%|████████▋ | 3292/3812 [81:11:47<12:49:36, 88.80s/it] 86%|████████▋ | 3293/3812 [81:13:16<12:48:36, 88.86s/it] 86%|████████▋ | 3294/3812 [81:14:44<12:46:13, 88.75s/it] 86%|████████▋ | 3295/3812 [81:16:13<12:45:17, 88.81s/it] 86%|████████▋ | 3296/3812 [81:17:42<12:42:25, 88.65s/it] 86%|████████▋ | 3297/3812 [81:19:10<12:41:04, 88.67s/it] 87%|████████▋ | 3298/3812 [81:20:39<12:39:31, 88.66s/it] 87%|████████▋ | 3299/3812 [81:22:08<12:38:42, 88.74s/it] 87%|████████▋ | 3300/3812 [81:23:36<12:36:02, 88.60s/it]                                                         {'loss': 1.2535, 'learning_rate': 2.192757395041992e-06, 'epoch': 0.87}
 87%|████████▋ | 3300/3812 [81:23:36<12:36:02, 88.60s/it] 87%|████████▋ | 3301/3812 [81:25:05<12:34:56, 88.64s/it] 87%|████████▋ | 3302/3812 [81:26:33<12:32:50, 88.57s/it] 87%|████████▋ | 3303/3812 [81:28:02<12:32:04, 88.65s/it] 87%|████████▋ | 3304/3812 [81:29:31<12:30:32, 88.65s/it] 87%|████████▋ | 3305/3812 [81:31:00<12:29:39, 88.72s/it] 87%|████████▋ | 3306/3812 [81:32:28<12:27:15, 88.61s/it] 87%|████████▋ | 3307/3812 [81:33:57<12:25:57, 88.63s/it] 87%|████████▋ | 3308/3812 [81:35:25<12:24:00, 88.57s/it] 87%|████████▋ | 3309/3812 [81:36:54<12:23:10, 88.65s/it] 87%|████████▋ | 3310/3812 [81:38:23<12:21:52, 88.67s/it]                                                         {'loss': 1.2633, 'learning_rate': 2.109152950957785e-06, 'epoch': 0.87}
 87%|████████▋ | 3310/3812 [81:38:23<12:21:52, 88.67s/it] 87%|████████▋ | 3311/3812 [81:39:51<12:20:52, 88.73s/it] 87%|████████▋ | 3312/3812 [81:41:20<12:18:40, 88.64s/it] 87%|████████▋ | 3313/3812 [81:42:49<12:17:47, 88.71s/it] 87%|████████▋ | 3314/3812 [81:44:17<12:15:31, 88.62s/it] 87%|████████▋ | 3315/3812 [81:45:46<12:14:06, 88.62s/it] 87%|████████▋ | 3316/3812 [81:47:14<12:12:44, 88.64s/it] 87%|████████▋ | 3317/3812 [81:48:43<12:11:40, 88.69s/it] 87%|████████▋ | 3318/3812 [81:50:12<12:09:45, 88.64s/it] 87%|████████▋ | 3319/3812 [81:51:40<12:08:10, 88.62s/it] 87%|████████▋ | 3320/3812 [81:53:09<12:06:02, 88.54s/it]                                                         {'loss': 1.2517, 'learning_rate': 2.027103231103264e-06, 'epoch': 0.87}
 87%|████████▋ | 3320/3812 [81:53:09<12:06:02, 88.54s/it] 87%|████████▋ | 3321/3812 [81:54:37<12:04:52, 88.58s/it] 87%|████████▋ | 3322/3812 [81:56:06<12:03:22, 88.58s/it] 87%|████████▋ | 3323/3812 [81:57:35<12:02:26, 88.64s/it] 87%|████████▋ | 3324/3812 [81:59:04<12:01:19, 88.69s/it] 87%|████████▋ | 3325/3812 [82:00:32<12:00:02, 88.71s/it] 87%|████████▋ | 3326/3812 [82:02:01<11:58:05, 88.65s/it] 87%|████████▋ | 3327/3812 [82:03:29<11:56:34, 88.65s/it] 87%|████████▋ | 3328/3812 [82:04:58<11:55:05, 88.65s/it] 87%|████████▋ | 3329/3812 [82:06:27<11:55:09, 88.84s/it] 87%|████████▋ | 3330/3812 [82:07:56<11:52:42, 88.72s/it]                                                         {'loss': 1.2498, 'learning_rate': 1.946613808216169e-06, 'epoch': 0.87}
 87%|████████▋ | 3330/3812 [82:07:56<11:52:42, 88.72s/it] 87%|████████▋ | 3331/3812 [82:09:25<11:52:08, 88.83s/it] 87%|████████▋ | 3332/3812 [82:10:54<11:50:05, 88.76s/it] 87%|████████▋ | 3333/3812 [82:12:22<11:48:46, 88.78s/it] 87%|████████▋ | 3334/3812 [82:13:51<11:46:53, 88.73s/it] 87%|████████▋ | 3335/3812 [82:15:20<11:45:23, 88.73s/it] 88%|████████▊ | 3336/3812 [82:16:48<11:43:15, 88.65s/it] 88%|████████▊ | 3337/3812 [82:18:17<11:42:23, 88.72s/it] 88%|████████▊ | 3338/3812 [82:19:45<11:39:59, 88.61s/it] 88%|████████▊ | 3339/3812 [82:21:14<11:39:35, 88.74s/it] 88%|████████▊ | 3340/3812 [82:22:43<11:38:03, 88.74s/it]                                                         {'loss': 1.2559, 'learning_rate': 1.8676901490603944e-06, 'epoch': 0.88}
 88%|████████▊ | 3340/3812 [82:22:43<11:38:03, 88.74s/it] 88%|████████▊ | 3341/3812 [82:24:12<11:37:18, 88.83s/it] 88%|████████▊ | 3342/3812 [82:25:41<11:34:59, 88.72s/it] 88%|████████▊ | 3343/3812 [82:27:10<11:34:55, 88.90s/it] 88%|████████▊ | 3344/3812 [82:28:39<11:32:35, 88.79s/it] 88%|████████▊ | 3345/3812 [82:30:07<11:31:11, 88.80s/it] 88%|████████▊ | 3346/3812 [82:31:36<11:29:21, 88.76s/it] 88%|████████▊ | 3347/3812 [82:33:05<11:28:26, 88.83s/it] 88%|████████▊ | 3348/3812 [82:34:33<11:25:51, 88.69s/it] 88%|████████▊ | 3349/3812 [82:36:02<11:25:16, 88.80s/it] 88%|████████▊ | 3350/3812 [82:37:31<11:22:50, 88.68s/it]                                                         {'loss': 1.2495, 'learning_rate': 1.7903376140546602e-06, 'epoch': 0.88}
 88%|████████▊ | 3350/3812 [82:37:31<11:22:50, 88.68s/it] 88%|████████▊ | 3351/3812 [82:39:00<11:21:43, 88.73s/it] 88%|████████▊ | 3352/3812 [82:40:28<11:19:34, 88.64s/it] 88%|████████▊ | 3353/3812 [82:41:57<11:18:40, 88.72s/it] 88%|████████▊ | 3354/3812 [82:43:25<11:16:27, 88.62s/it] 88%|████████▊ | 3355/3812 [82:44:54<11:15:35, 88.70s/it] 88%|████████▊ | 3356/3812 [82:46:23<11:13:39, 88.64s/it] 88%|████████▊ | 3357/3812 [82:47:52<11:12:34, 88.69s/it] 88%|████████▊ | 3358/3812 [82:49:20<11:11:07, 88.69s/it] 88%|████████▊ | 3359/3812 [82:50:49<11:10:17, 88.78s/it] 88%|████████▊ | 3360/3812 [82:52:18<11:07:51, 88.65s/it]                                                         {'loss': 1.251, 'learning_rate': 1.7145614569084672e-06, 'epoch': 0.88}
 88%|████████▊ | 3360/3812 [82:52:18<11:07:51, 88.65s/it] 88%|████████▊ | 3361/3812 [82:53:46<11:06:45, 88.70s/it] 88%|████████▊ | 3362/3812 [82:55:15<11:04:57, 88.66s/it] 88%|████████▊ | 3363/3812 [82:56:44<11:03:31, 88.67s/it] 88%|████████▊ | 3364/3812 [82:58:12<11:02:07, 88.68s/it] 88%|████████▊ | 3365/3812 [82:59:41<11:00:36, 88.67s/it] 88%|████████▊ | 3366/3812 [83:01:10<10:58:47, 88.63s/it] 88%|████████▊ | 3367/3812 [83:02:38<10:57:42, 88.68s/it] 88%|████████▊ | 3368/3812 [83:04:07<10:55:47, 88.62s/it] 88%|████████▊ | 3369/3812 [83:05:36<10:55:11, 88.74s/it] 88%|████████▊ | 3370/3812 [83:07:05<10:53:47, 88.75s/it]                                                         {'loss': 1.2551, 'learning_rate': 1.6403668242652326e-06, 'epoch': 0.88}
 88%|████████▊ | 3370/3812 [83:07:05<10:53:47, 88.75s/it] 88%|████████▊ | 3371/3812 [83:08:34<10:53:06, 88.86s/it] 88%|████████▊ | 3372/3812 [83:10:03<10:52:01, 88.91s/it] 88%|████████▊ | 3373/3812 [83:11:32<10:50:45, 88.94s/it] 89%|████████▊ | 3374/3812 [83:13:01<10:48:55, 88.89s/it] 89%|████████▊ | 3375/3812 [83:14:30<10:47:33, 88.91s/it] 89%|████████▊ | 3376/3812 [83:15:58<10:45:29, 88.83s/it] 89%|████████▊ | 3377/3812 [83:17:27<10:44:00, 88.83s/it] 89%|████████▊ | 3378/3812 [83:18:55<10:41:37, 88.70s/it] 89%|████████▊ | 3379/3812 [83:20:24<10:40:12, 88.71s/it] 89%|████████▊ | 3380/3812 [83:21:53<10:38:04, 88.62s/it]                                                         {'loss': 1.2593, 'learning_rate': 1.5677587553527744e-06, 'epoch': 0.89}
 89%|████████▊ | 3380/3812 [83:21:53<10:38:04, 88.62s/it] 89%|████████▊ | 3381/3812 [83:23:22<10:37:13, 88.71s/it] 89%|████████▊ | 3382/3812 [83:24:50<10:35:39, 88.70s/it] 89%|████████▊ | 3383/3812 [83:26:19<10:35:25, 88.87s/it] 89%|████████▉ | 3384/3812 [83:27:48<10:32:49, 88.71s/it] 89%|████████▉ | 3385/3812 [83:29:17<10:31:52, 88.79s/it] 89%|████████▉ | 3386/3812 [83:30:45<10:29:24, 88.65s/it] 89%|████████▉ | 3387/3812 [83:32:14<10:28:51, 88.78s/it] 89%|████████▉ | 3388/3812 [83:33:43<10:27:29, 88.80s/it] 89%|████████▉ | 3389/3812 [83:35:12<10:26:34, 88.88s/it] 89%|████████▉ | 3390/3812 [83:36:41<10:24:20, 88.77s/it]                                                         {'loss': 1.2548, 'learning_rate': 1.496742181641031e-06, 'epoch': 0.89}
 89%|████████▉ | 3390/3812 [83:36:41<10:24:20, 88.77s/it] 89%|████████▉ | 3391/3812 [83:38:09<10:23:05, 88.80s/it] 89%|████████▉ | 3392/3812 [83:39:38<10:20:53, 88.70s/it] 89%|████████▉ | 3393/3812 [83:41:07<10:19:47, 88.75s/it] 89%|████████▉ | 3394/3812 [83:42:36<10:18:19, 88.75s/it] 89%|████████▉ | 3395/3812 [83:44:05<10:17:22, 88.83s/it] 89%|████████▉ | 3396/3812 [83:45:33<10:14:51, 88.68s/it] 89%|████████▉ | 3397/3812 [83:47:02<10:13:47, 88.74s/it] 89%|████████▉ | 3398/3812 [83:48:30<10:11:49, 88.67s/it] 89%|████████▉ | 3399/3812 [83:49:59<10:10:38, 88.71s/it] 89%|████████▉ | 3400/3812 [83:51:28<10:09:43, 88.79s/it]                                                         {'loss': 1.2525, 'learning_rate': 1.4273219265071135e-06, 'epoch': 0.89}
 89%|████████▉ | 3400/3812 [83:51:28<10:09:43, 88.79s/it] 89%|████████▉ | 3401/3812 [83:52:57<10:08:37, 88.85s/it] 89%|████████▉ | 3402/3812 [83:54:26<10:06:50, 88.81s/it] 89%|████████▉ | 3403/3812 [83:55:55<10:05:35, 88.84s/it] 89%|████████▉ | 3404/3812 [83:57:23<10:03:50, 88.80s/it] 89%|████████▉ | 3405/3812 [83:58:52<10:02:15, 88.78s/it] 89%|████████▉ | 3406/3812 [84:00:21<10:00:51, 88.80s/it] 89%|████████▉ | 3407/3812 [84:01:50<9:59:36, 88.83s/it]  89%|████████▉ | 3408/3812 [84:03:18<9:57:26, 88.73s/it] 89%|████████▉ | 3409/3812 [84:04:47<9:56:26, 88.80s/it] 89%|████████▉ | 3410/3812 [84:06:16<9:54:16, 88.70s/it]                                                        {'loss': 1.2524, 'learning_rate': 1.3595027049077226e-06, 'epoch': 0.89}
 89%|████████▉ | 3410/3812 [84:06:16<9:54:16, 88.70s/it] 89%|████████▉ | 3411/3812 [84:07:45<9:53:11, 88.76s/it] 90%|████████▉ | 3412/3812 [84:09:13<9:51:43, 88.76s/it] 90%|████████▉ | 3413/3812 [84:10:43<9:50:49, 88.85s/it] 90%|████████▉ | 3414/3812 [84:12:11<9:48:37, 88.74s/it] 90%|████████▉ | 3415/3812 [84:13:40<9:47:22, 88.77s/it] 90%|████████▉ | 3416/3812 [84:15:08<9:45:02, 88.64s/it] 90%|████████▉ | 3417/3812 [84:16:37<9:43:38, 88.65s/it] 90%|████████▉ | 3418/3812 [84:18:05<9:41:55, 88.62s/it] 90%|████████▉ | 3419/3812 [84:19:34<9:40:55, 88.69s/it] 90%|████████▉ | 3420/3812 [84:21:03<9:39:05, 88.64s/it]                                                        {'loss': 1.2402, 'learning_rate': 1.2932891230589162e-06, 'epoch': 0.9}
 90%|████████▉ | 3420/3812 [84:21:03<9:39:05, 88.64s/it] 90%|████████▉ | 3421/3812 [84:22:32<9:38:02, 88.70s/it] 90%|████████▉ | 3422/3812 [84:24:00<9:36:12, 88.65s/it] 90%|████████▉ | 3423/3812 [84:25:29<9:35:15, 88.73s/it] 90%|████████▉ | 3424/3812 [84:26:58<9:33:35, 88.70s/it] 90%|████████▉ | 3425/3812 [84:28:27<9:32:23, 88.74s/it] 90%|████████▉ | 3426/3812 [84:29:55<9:30:16, 88.64s/it] 90%|████████▉ | 3427/3812 [84:31:24<9:29:16, 88.72s/it] 90%|████████▉ | 3428/3812 [84:32:52<9:27:09, 88.62s/it] 90%|████████▉ | 3429/3812 [84:34:21<9:26:17, 88.71s/it] 90%|████████▉ | 3430/3812 [84:35:50<9:25:01, 88.75s/it]                                                        {'loss': 1.2498, 'learning_rate': 1.2286856781232313e-06, 'epoch': 0.9}
 90%|████████▉ | 3430/3812 [84:35:50<9:25:01, 88.75s/it] 90%|█████████ | 3431/3812 [84:37:19<9:23:57, 88.81s/it] 90%|█████████ | 3432/3812 [84:38:48<9:22:00, 88.74s/it] 90%|█████████ | 3433/3812 [84:40:16<9:20:40, 88.76s/it] 90%|█████████ | 3434/3812 [84:41:45<9:18:40, 88.68s/it] 90%|█████████ | 3435/3812 [84:43:14<9:17:28, 88.72s/it] 90%|█████████ | 3436/3812 [84:44:42<9:15:51, 88.70s/it] 90%|█████████ | 3437/3812 [84:46:11<9:14:59, 88.80s/it] 90%|█████████ | 3438/3812 [84:47:40<9:12:39, 88.66s/it] 90%|█████████ | 3439/3812 [84:49:09<9:12:16, 88.84s/it] 90%|█████████ | 3440/3812 [84:50:37<9:10:05, 88.72s/it]                                                        {'loss': 1.2482, 'learning_rate': 1.1656967579042804e-06, 'epoch': 0.9}
 90%|█████████ | 3440/3812 [84:50:37<9:10:05, 88.72s/it] 90%|█████████ | 3441/3812 [84:52:06<9:08:54, 88.77s/it] 90%|█████████ | 3442/3812 [84:53:35<9:07:13, 88.74s/it] 90%|█████████ | 3443/3812 [84:55:04<9:05:56, 88.77s/it] 90%|█████████ | 3444/3812 [84:56:32<9:04:01, 88.70s/it] 90%|█████████ | 3445/3812 [84:58:01<9:02:45, 88.73s/it] 90%|█████████ | 3446/3812 [84:59:29<9:00:35, 88.62s/it] 90%|█████████ | 3447/3812 [85:00:59<8:59:50, 88.74s/it] 90%|█████████ | 3448/3812 [85:02:27<8:58:01, 88.69s/it] 90%|█████████ | 3449/3812 [85:03:56<8:57:21, 88.82s/it] 91%|█████████ | 3450/3812 [85:05:25<8:55:39, 88.78s/it]                                                        {'loss': 1.256, 'learning_rate': 1.1043266405487001e-06, 'epoch': 0.9}
 91%|█████████ | 3450/3812 [85:05:25<8:55:39, 88.78s/it] 91%|█████████ | 3451/3812 [85:06:54<8:54:19, 88.81s/it] 91%|█████████ | 3452/3812 [85:08:23<8:52:51, 88.81s/it] 91%|█████████ | 3453/3812 [85:09:51<8:51:22, 88.81s/it] 91%|█████████ | 3454/3812 [85:11:20<8:50:01, 88.83s/it] 91%|█████████ | 3455/3812 [85:12:49<8:48:57, 88.90s/it] 91%|█████████ | 3456/3812 [85:14:18<8:46:27, 88.73s/it] 91%|█████████ | 3457/3812 [85:15:47<8:45:41, 88.85s/it] 91%|█████████ | 3458/3812 [85:17:15<8:43:13, 88.68s/it] 91%|█████████ | 3459/3812 [85:18:44<8:42:01, 88.73s/it] 91%|█████████ | 3460/3812 [85:20:13<8:40:26, 88.71s/it]                                                        {'loss': 1.257, 'learning_rate': 1.0445794942556069e-06, 'epoch': 0.91}
 91%|█████████ | 3460/3812 [85:20:13<8:40:26, 88.71s/it] 91%|█████████ | 3461/3812 [85:21:42<8:39:40, 88.83s/it] 91%|█████████ | 3462/3812 [85:23:10<8:37:38, 88.74s/it] 91%|█████████ | 3463/3812 [85:24:39<8:36:25, 88.78s/it] 91%|█████████ | 3464/3812 [85:26:08<8:34:29, 88.70s/it] 91%|█████████ | 3465/3812 [85:27:37<8:33:37, 88.81s/it] 91%|█████████ | 3466/3812 [85:29:06<8:32:14, 88.83s/it] 91%|█████████ | 3467/3812 [85:30:35<8:31:16, 88.92s/it] 91%|█████████ | 3468/3812 [85:32:03<8:29:16, 88.83s/it] 91%|█████████ | 3469/3812 [85:33:32<8:27:57, 88.86s/it] 91%|█████████ | 3470/3812 [85:35:01<8:25:36, 88.70s/it]                                                        {'loss': 1.2507, 'learning_rate': 9.864593769934953e-07, 'epoch': 0.91}
 91%|█████████ | 3470/3812 [85:35:01<8:25:36, 88.70s/it] 91%|█████████ | 3471/3812 [85:36:29<8:24:07, 88.70s/it] 91%|█████████ | 3472/3812 [85:37:58<8:22:33, 88.69s/it] 91%|█████████ | 3473/3812 [85:39:27<8:21:21, 88.74s/it] 91%|█████████ | 3474/3812 [85:40:55<8:19:11, 88.61s/it] 91%|█████████ | 3475/3812 [85:42:24<8:18:07, 88.69s/it] 91%|█████████ | 3476/3812 [85:43:52<8:16:10, 88.60s/it] 91%|█████████ | 3477/3812 [85:45:21<8:15:02, 88.66s/it] 91%|█████████ | 3478/3812 [85:46:50<8:13:22, 88.63s/it] 91%|█████████▏| 3479/3812 [85:48:19<8:12:18, 88.70s/it] 91%|█████████▏| 3480/3812 [85:49:47<8:10:18, 88.61s/it]                                                        {'loss': 1.249, 'learning_rate': 9.299702362246032e-07, 'epoch': 0.91}
 91%|█████████▏| 3480/3812 [85:49:47<8:10:18, 88.61s/it] 91%|█████████▏| 3481/3812 [85:51:16<8:09:08, 88.67s/it] 91%|█████████▏| 3482/3812 [85:52:44<8:07:01, 88.55s/it] 91%|█████████▏| 3483/3812 [85:54:13<8:05:59, 88.63s/it] 91%|█████████▏| 3484/3812 [85:55:41<8:04:28, 88.62s/it] 91%|█████████▏| 3485/3812 [85:57:10<8:03:22, 88.69s/it] 91%|█████████▏| 3486/3812 [85:58:39<8:01:18, 88.58s/it] 91%|█████████▏| 3487/3812 [86:00:08<8:00:23, 88.69s/it] 92%|█████████▏| 3488/3812 [86:01:36<7:58:12, 88.56s/it] 92%|█████████▏| 3489/3812 [86:03:05<7:57:13, 88.65s/it] 92%|█████████▏| 3490/3812 [86:04:33<7:55:40, 88.63s/it]                                                        {'loss': 1.2477, 'learning_rate': 8.751159086368365e-07, 'epoch': 0.92}
 92%|█████████▏| 3490/3812 [86:04:33<7:55:40, 88.63s/it] 92%|█████████▏| 3491/3812 [86:06:02<7:54:35, 88.71s/it] 92%|█████████▏| 3492/3812 [86:07:31<7:52:32, 88.60s/it] 92%|█████████▏| 3493/3812 [86:08:59<7:51:36, 88.70s/it] 92%|█████████▏| 3494/3812 [86:10:28<7:49:21, 88.56s/it] 92%|█████████▏| 3495/3812 [86:11:57<7:48:32, 88.68s/it] 92%|█████████▏| 3496/3812 [86:13:25<7:47:11, 88.71s/it] 92%|█████████▏| 3497/3812 [86:14:55<7:46:32, 88.86s/it] 92%|█████████▏| 3498/3812 [86:16:23<7:44:39, 88.79s/it] 92%|█████████▏| 3499/3812 [86:17:52<7:43:22, 88.83s/it] 92%|█████████▏| 3500/3812 [86:19:21<7:41:39, 88.78s/it]                                                        {'loss': 1.2469, 'learning_rate': 8.219001198831622e-07, 'epoch': 0.92}
 92%|█████████▏| 3500/3812 [86:19:21<7:41:39, 88.78s/it][INFO|trainer.py:2936] 2024-02-12 13:31:41,137 >> Saving model checkpoint to /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3500
[INFO|configuration_utils.py:473] 2024-02-12 13:31:41,139 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3500/config.json
[INFO|configuration_utils.py:594] 2024-02-12 13:31:41,139 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3500/generation_config.json
[INFO|modeling_utils.py:2501] 2024-02-12 13:31:59,521 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2433] 2024-02-12 13:31:59,522 >> tokenizer config file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-12 13:31:59,522 >> Special tokens file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tmp-checkpoint-3500/special_tokens_map.json
[INFO|trainer.py:3028] 2024-02-12 13:32:00,334 >> Deleting older checkpoint [/home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/checkpoint-2000] due to args.save_total_limit
 92%|█████████▏| 3501/3812 [86:21:19<8:26:00, 97.62s/it] 92%|█████████▏| 3502/3812 [86:22:48<8:10:14, 94.88s/it] 92%|█████████▏| 3503/3812 [86:24:17<8:00:08, 93.23s/it] 92%|█████████▏| 3504/3812 [86:25:46<7:51:29, 91.85s/it] 92%|█████████▏| 3505/3812 [86:27:15<7:45:29, 90.98s/it] 92%|█████████▏| 3506/3812 [86:28:43<7:39:58, 90.19s/it] 92%|█████████▏| 3507/3812 [86:30:12<7:36:09, 89.74s/it] 92%|█████████▏| 3508/3812 [86:31:40<7:32:27, 89.30s/it] 92%|█████████▏| 3509/3812 [86:33:09<7:30:14, 89.16s/it] 92%|█████████▏| 3510/3812 [86:34:38<7:28:13, 89.05s/it]                                                        {'loss': 1.2525, 'learning_rate': 7.703264843285624e-07, 'epoch': 0.92}
 92%|█████████▏| 3510/3812 [86:34:38<7:28:13, 89.05s/it] 92%|█████████▏| 3511/3812 [86:36:06<7:26:25, 88.99s/it] 92%|█████████▏| 3512/3812 [86:37:35<7:24:15, 88.85s/it] 92%|█████████▏| 3513/3812 [86:39:04<7:22:53, 88.87s/it] 92%|█████████▏| 3514/3812 [86:40:32<7:20:51, 88.76s/it] 92%|█████████▏| 3515/3812 [86:42:01<7:19:42, 88.83s/it] 92%|█████████▏| 3516/3812 [86:43:30<7:17:59, 88.78s/it] 92%|█████████▏| 3517/3812 [86:44:59<7:17:09, 88.91s/it] 92%|█████████▏| 3518/3812 [86:46:28<7:14:54, 88.76s/it] 92%|█████████▏| 3519/3812 [86:47:57<7:13:52, 88.85s/it] 92%|█████████▏| 3520/3812 [86:49:25<7:11:55, 88.75s/it]                                                        {'loss': 1.2553, 'learning_rate': 7.203985048045758e-07, 'epoch': 0.92}
 92%|█████████▏| 3520/3812 [86:49:25<7:11:55, 88.75s/it] 92%|█████████▏| 3521/3812 [86:50:54<7:10:37, 88.79s/it] 92%|█████████▏| 3522/3812 [86:52:23<7:09:04, 88.77s/it] 92%|█████████▏| 3523/3812 [86:53:52<7:07:53, 88.84s/it] 92%|█████████▏| 3524/3812 [86:55:20<7:05:37, 88.67s/it] 92%|█████████▏| 3525/3812 [86:56:49<7:04:23, 88.72s/it] 92%|█████████▏| 3526/3812 [86:58:17<7:02:25, 88.62s/it] 93%|█████████▎| 3527/3812 [86:59:46<7:01:20, 88.70s/it] 93%|█████████▎| 3528/3812 [87:01:15<6:59:49, 88.69s/it] 93%|█████████▎| 3529/3812 [87:02:44<6:58:45, 88.78s/it] 93%|█████████▎| 3530/3812 [87:04:12<6:56:48, 88.68s/it]                                                        {'loss': 1.2563, 'learning_rate': 6.721195723713658e-07, 'epoch': 0.93}
 93%|█████████▎| 3530/3812 [87:04:12<6:56:48, 88.68s/it] 93%|█████████▎| 3531/3812 [87:05:41<6:55:51, 88.79s/it] 93%|█████████▎| 3532/3812 [87:07:10<6:54:20, 88.79s/it] 93%|█████████▎| 3533/3812 [87:08:39<6:53:22, 88.90s/it] 93%|█████████▎| 3534/3812 [87:10:08<6:51:23, 88.79s/it] 93%|█████████▎| 3535/3812 [87:11:37<6:50:05, 88.83s/it] 93%|█████████▎| 3536/3812 [87:13:05<6:48:10, 88.73s/it] 93%|█████████▎| 3537/3812 [87:14:34<6:47:02, 88.81s/it] 93%|█████████▎| 3538/3812 [87:16:03<6:45:06, 88.71s/it] 93%|█████████▎| 3539/3812 [87:17:31<6:43:39, 88.72s/it] 93%|█████████▎| 3540/3812 [87:19:00<6:42:03, 88.69s/it]                                                        {'loss': 1.2524, 'learning_rate': 6.254929660874015e-07, 'epoch': 0.93}
 93%|█████████▎| 3540/3812 [87:19:00<6:42:03, 88.69s/it] 93%|█████████▎| 3541/3812 [87:20:29<6:40:50, 88.75s/it] 93%|█████████▎| 3542/3812 [87:21:57<6:38:39, 88.59s/it] 93%|█████████▎| 3543/3812 [87:23:26<6:37:40, 88.70s/it] 93%|█████████▎| 3544/3812 [87:24:55<6:35:52, 88.63s/it] 93%|█████████▎| 3545/3812 [87:26:24<6:35:07, 88.79s/it] 93%|█████████▎| 3546/3812 [87:27:52<6:33:24, 88.74s/it] 93%|█████████▎| 3547/3812 [87:29:21<6:32:09, 88.79s/it] 93%|█████████▎| 3548/3812 [87:30:50<6:30:11, 88.68s/it] 93%|█████████▎| 3549/3812 [87:32:19<6:29:13, 88.80s/it] 93%|█████████▎| 3550/3812 [87:33:47<6:27:22, 88.71s/it]                                                        {'loss': 1.2544, 'learning_rate': 5.805218527867667e-07, 'epoch': 0.93}
 93%|█████████▎| 3550/3812 [87:33:47<6:27:22, 88.71s/it] 93%|█████████▎| 3551/3812 [87:35:16<6:26:04, 88.75s/it] 93%|█████████▎| 3552/3812 [87:36:45<6:24:31, 88.74s/it] 93%|█████████▎| 3553/3812 [87:38:14<6:23:14, 88.78s/it] 93%|█████████▎| 3554/3812 [87:39:42<6:21:12, 88.65s/it] 93%|█████████▎| 3555/3812 [87:41:11<6:19:51, 88.68s/it] 93%|█████████▎| 3556/3812 [87:42:39<6:17:53, 88.57s/it] 93%|█████████▎| 3557/3812 [87:44:08<6:16:47, 88.66s/it] 93%|█████████▎| 3558/3812 [87:45:37<6:15:08, 88.62s/it] 93%|█████████▎| 3559/3812 [87:47:05<6:14:00, 88.70s/it] 93%|█████████▎| 3560/3812 [87:48:34<6:12:18, 88.64s/it]                                                        {'loss': 1.2557, 'learning_rate': 5.372092868640516e-07, 'epoch': 0.93}
 93%|█████████▎| 3560/3812 [87:48:34<6:12:18, 88.64s/it] 93%|█████████▎| 3561/3812 [87:50:03<6:11:14, 88.74s/it] 93%|█████████▎| 3562/3812 [87:51:32<6:09:35, 88.70s/it] 93%|█████████▎| 3563/3812 [87:53:00<6:08:16, 88.74s/it] 93%|█████████▎| 3564/3812 [87:54:29<6:06:56, 88.78s/it] 94%|█████████▎| 3565/3812 [87:55:58<6:05:39, 88.82s/it] 94%|█████████▎| 3566/3812 [87:57:27<6:03:39, 88.70s/it] 94%|█████████▎| 3567/3812 [87:58:55<6:02:30, 88.78s/it] 94%|█████████▎| 3568/3812 [88:00:24<6:00:31, 88.65s/it] 94%|█████████▎| 3569/3812 [88:01:53<5:59:26, 88.75s/it] 94%|█████████▎| 3570/3812 [88:03:22<5:57:58, 88.75s/it]                                                        {'loss': 1.2554, 'learning_rate': 4.955582100669154e-07, 'epoch': 0.94}
 94%|█████████▎| 3570/3812 [88:03:22<5:57:58, 88.75s/it] 94%|█████████▎| 3571/3812 [88:04:51<5:57:01, 88.89s/it] 94%|█████████▎| 3572/3812 [88:06:19<5:55:10, 88.79s/it] 94%|█████████▎| 3573/3812 [88:07:48<5:53:51, 88.84s/it] 94%|█████████▍| 3574/3812 [88:09:17<5:51:44, 88.67s/it] 94%|█████████▍| 3575/3812 [88:10:45<5:50:31, 88.74s/it] 94%|█████████▍| 3576/3812 [88:12:14<5:48:50, 88.69s/it] 94%|█████████▍| 3577/3812 [88:13:43<5:47:55, 88.83s/it] 94%|█████████▍| 3578/3812 [88:15:12<5:46:05, 88.74s/it] 94%|█████████▍| 3579/3812 [88:16:41<5:45:03, 88.86s/it] 94%|█████████▍| 3580/3812 [88:18:09<5:43:08, 88.74s/it]                                                        {'loss': 1.2526, 'learning_rate': 4.555714512962855e-07, 'epoch': 0.94}
 94%|█████████▍| 3580/3812 [88:18:09<5:43:08, 88.74s/it] 94%|█████████▍| 3581/3812 [88:19:38<5:41:44, 88.76s/it] 94%|█████████▍| 3582/3812 [88:21:07<5:40:28, 88.82s/it] 94%|█████████▍| 3583/3812 [88:22:36<5:39:04, 88.84s/it] 94%|█████████▍| 3584/3812 [88:24:05<5:37:18, 88.77s/it] 94%|█████████▍| 3585/3812 [88:25:34<5:36:02, 88.82s/it] 94%|█████████▍| 3586/3812 [88:27:02<5:34:16, 88.74s/it] 94%|█████████▍| 3587/3812 [88:28:31<5:33:09, 88.84s/it] 94%|█████████▍| 3588/3812 [88:30:00<5:31:27, 88.78s/it] 94%|█████████▍| 3589/3812 [88:31:29<5:30:07, 88.82s/it] 94%|█████████▍| 3590/3812 [88:32:57<5:28:15, 88.72s/it]                                                        {'loss': 1.2456, 'learning_rate': 4.172517264141973e-07, 'epoch': 0.94}
 94%|█████████▍| 3590/3812 [88:32:57<5:28:15, 88.72s/it] 94%|█████████▍| 3591/3812 [88:34:26<5:27:08, 88.82s/it] 94%|█████████▍| 3592/3812 [88:35:55<5:25:14, 88.70s/it] 94%|█████████▍| 3593/3812 [88:37:24<5:23:54, 88.74s/it] 94%|█████████▍| 3594/3812 [88:38:52<5:22:16, 88.70s/it] 94%|█████████▍| 3595/3812 [88:40:21<5:21:01, 88.76s/it] 94%|█████████▍| 3596/3812 [88:41:49<5:19:11, 88.66s/it] 94%|█████████▍| 3597/3812 [88:43:19<5:18:24, 88.86s/it] 94%|█████████▍| 3598/3812 [88:44:47<5:16:26, 88.72s/it] 94%|█████████▍| 3599/3812 [88:46:16<5:15:10, 88.78s/it] 94%|█████████▍| 3600/3812 [88:47:45<5:13:38, 88.77s/it]                                                        {'loss': 1.2548, 'learning_rate': 3.8060163805937465e-07, 'epoch': 0.94}
 94%|█████████▍| 3600/3812 [88:47:45<5:13:38, 88.77s/it] 94%|█████████▍| 3601/3812 [88:49:14<5:12:11, 88.77s/it] 94%|█████████▍| 3602/3812 [88:50:42<5:10:27, 88.70s/it] 95%|█████████▍| 3603/3812 [88:52:11<5:08:58, 88.70s/it] 95%|█████████▍| 3604/3812 [88:53:39<5:07:15, 88.63s/it] 95%|█████████▍| 3605/3812 [88:55:08<5:05:58, 88.69s/it] 95%|█████████▍| 3606/3812 [88:56:37<5:04:32, 88.70s/it] 95%|█████████▍| 3607/3812 [88:58:06<5:03:14, 88.76s/it] 95%|█████████▍| 3608/3812 [88:59:34<5:01:30, 88.68s/it] 95%|█████████▍| 3609/3812 [89:01:03<5:00:08, 88.71s/it] 95%|█████████▍| 3610/3812 [89:02:31<4:58:14, 88.58s/it]                                                        {'loss': 1.2533, 'learning_rate': 3.4562367547043007e-07, 'epoch': 0.95}
 95%|█████████▍| 3610/3812 [89:02:31<4:58:14, 88.58s/it] 95%|█████████▍| 3611/3812 [89:04:00<4:57:01, 88.67s/it] 95%|█████████▍| 3612/3812 [89:05:29<4:55:30, 88.65s/it] 95%|█████████▍| 3613/3812 [89:06:58<4:54:16, 88.73s/it] 95%|█████████▍| 3614/3812 [89:08:26<4:52:29, 88.63s/it] 95%|█████████▍| 3615/3812 [89:09:55<4:51:19, 88.73s/it] 95%|█████████▍| 3616/3812 [89:11:24<4:49:33, 88.64s/it] 95%|█████████▍| 3617/3812 [89:12:52<4:48:15, 88.69s/it] 95%|█████████▍| 3618/3812 [89:14:21<4:46:57, 88.75s/it] 95%|█████████▍| 3619/3812 [89:15:50<4:45:44, 88.83s/it] 95%|█████████▍| 3620/3812 [89:17:19<4:44:22, 88.87s/it]                                                        {'loss': 1.2549, 'learning_rate': 3.1232021431680527e-07, 'epoch': 0.95}
 95%|█████████▍| 3620/3812 [89:17:19<4:44:22, 88.87s/it] 95%|█████████▍| 3621/3812 [89:18:48<4:42:50, 88.85s/it] 95%|█████████▌| 3622/3812 [89:20:17<4:41:06, 88.77s/it] 95%|█████████▌| 3623/3812 [89:21:46<4:39:51, 88.84s/it] 95%|█████████▌| 3624/3812 [89:23:14<4:38:13, 88.80s/it] 95%|█████████▌| 3625/3812 [89:24:43<4:36:44, 88.79s/it] 95%|█████████▌| 3626/3812 [89:26:11<4:34:52, 88.67s/it] 95%|█████████▌| 3627/3812 [89:27:40<4:33:31, 88.71s/it] 95%|█████████▌| 3628/3812 [89:29:09<4:31:41, 88.60s/it] 95%|█████████▌| 3629/3812 [89:30:37<4:30:20, 88.64s/it] 95%|█████████▌| 3630/3812 [89:32:06<4:28:55, 88.65s/it]                                                        {'loss': 1.2534, 'learning_rate': 2.806935165374308e-07, 'epoch': 0.95}
 95%|█████████▌| 3630/3812 [89:32:06<4:28:55, 88.65s/it] 95%|█████████▌| 3631/3812 [89:33:35<4:27:41, 88.74s/it] 95%|█████████▌| 3632/3812 [89:35:03<4:25:52, 88.62s/it] 95%|█████████▌| 3633/3812 [89:36:32<4:24:26, 88.64s/it] 95%|█████████▌| 3634/3812 [89:38:00<4:22:41, 88.55s/it] 95%|█████████▌| 3635/3812 [89:39:29<4:21:22, 88.60s/it] 95%|█████████▌| 3636/3812 [89:40:58<4:19:47, 88.57s/it] 95%|█████████▌| 3637/3812 [89:42:26<4:18:38, 88.68s/it] 95%|█████████▌| 3638/3812 [89:43:55<4:16:54, 88.59s/it] 95%|█████████▌| 3639/3812 [89:45:24<4:15:40, 88.68s/it] 95%|█████████▌| 3640/3812 [89:46:52<4:14:04, 88.63s/it]                                                        {'loss': 1.2588, 'learning_rate': 2.5074573018708524e-07, 'epoch': 0.95}
 95%|█████████▌| 3640/3812 [89:46:52<4:14:04, 88.63s/it] 96%|█████████▌| 3641/3812 [89:48:21<4:12:59, 88.77s/it] 96%|█████████▌| 3642/3812 [89:49:50<4:11:23, 88.72s/it] 96%|█████████▌| 3643/3812 [89:51:19<4:10:14, 88.84s/it] 96%|█████████▌| 3644/3812 [89:52:47<4:08:22, 88.70s/it] 96%|█████████▌| 3645/3812 [89:54:16<4:06:56, 88.72s/it] 96%|█████████▌| 3646/3812 [89:55:45<4:05:20, 88.68s/it] 96%|█████████▌| 3647/3812 [89:57:14<4:04:10, 88.79s/it] 96%|█████████▌| 3648/3812 [89:58:43<4:02:40, 88.78s/it] 96%|█████████▌| 3649/3812 [90:00:12<4:01:15, 88.81s/it] 96%|█████████▌| 3650/3812 [90:01:40<3:59:27, 88.69s/it]                                                        {'loss': 1.2502, 'learning_rate': 2.2247888929049777e-07, 'epoch': 0.96}
 96%|█████████▌| 3650/3812 [90:01:40<3:59:27, 88.69s/it] 96%|█████████▌| 3651/3812 [90:03:09<3:58:06, 88.74s/it] 96%|█████████▌| 3652/3812 [90:04:38<3:56:38, 88.74s/it] 96%|█████████▌| 3653/3812 [90:06:06<3:55:15, 88.78s/it] 96%|█████████▌| 3654/3812 [90:07:35<3:53:46, 88.77s/it] 96%|█████████▌| 3655/3812 [90:09:04<3:52:24, 88.82s/it] 96%|█████████▌| 3656/3812 [90:10:33<3:50:42, 88.73s/it] 96%|█████████▌| 3657/3812 [90:12:02<3:49:22, 88.79s/it] 96%|█████████▌| 3658/3812 [90:13:31<3:48:04, 88.86s/it] 96%|█████████▌| 3659/3812 [90:15:00<3:46:41, 88.90s/it] 96%|█████████▌| 3660/3812 [90:16:29<3:45:17, 88.93s/it]                                                        {'loss': 1.2569, 'learning_rate': 1.9589491370420876e-07, 'epoch': 0.96}
 96%|█████████▌| 3660/3812 [90:16:29<3:45:17, 88.93s/it] 96%|█████████▌| 3661/3812 [90:17:58<3:43:57, 88.99s/it] 96%|█████████▌| 3662/3812 [90:19:26<3:42:05, 88.84s/it] 96%|█████████▌| 3663/3812 [90:20:55<3:40:40, 88.86s/it] 96%|█████████▌| 3664/3812 [90:22:24<3:39:05, 88.82s/it] 96%|█████████▌| 3665/3812 [90:23:53<3:37:49, 88.91s/it] 96%|█████████▌| 3666/3812 [90:25:22<3:36:11, 88.84s/it] 96%|█████████▌| 3667/3812 [90:26:50<3:34:43, 88.85s/it] 96%|█████████▌| 3668/3812 [90:28:19<3:32:50, 88.68s/it] 96%|█████████▌| 3669/3812 [90:29:48<3:31:26, 88.72s/it] 96%|█████████▋| 3670/3812 [90:31:16<3:29:42, 88.61s/it]                                                        {'loss': 1.2571, 'learning_rate': 1.7099560898617416e-07, 'epoch': 0.96}
 96%|█████████▋| 3670/3812 [90:31:16<3:29:42, 88.61s/it] 96%|█████████▋| 3671/3812 [90:32:45<3:28:25, 88.69s/it] 96%|█████████▋| 3672/3812 [90:34:14<3:27:01, 88.73s/it] 96%|█████████▋| 3673/3812 [90:35:43<3:25:42, 88.80s/it] 96%|█████████▋| 3674/3812 [90:37:11<3:24:01, 88.70s/it] 96%|█████████▋| 3675/3812 [90:38:40<3:22:37, 88.74s/it] 96%|█████████▋| 3676/3812 [90:40:08<3:21:03, 88.70s/it] 96%|█████████▋| 3677/3812 [90:41:37<3:19:35, 88.71s/it] 96%|█████████▋| 3678/3812 [90:43:06<3:18:12, 88.75s/it] 97%|█████████▋| 3679/3812 [90:44:35<3:16:42, 88.74s/it] 97%|█████████▋| 3680/3812 [90:46:03<3:15:10, 88.72s/it]                                                        {'loss': 1.2622, 'learning_rate': 1.477826662731302e-07, 'epoch': 0.97}
 97%|█████████▋| 3680/3812 [90:46:03<3:15:10, 88.72s/it] 97%|█████████▋| 3681/3812 [90:47:32<3:13:47, 88.76s/it] 97%|█████████▋| 3682/3812 [90:49:01<3:12:10, 88.70s/it] 97%|█████████▋| 3683/3812 [90:50:30<3:10:49, 88.75s/it] 97%|█████████▋| 3684/3812 [90:51:59<3:09:21, 88.76s/it] 97%|█████████▋| 3685/3812 [90:53:28<3:08:08, 88.88s/it] 97%|█████████▋| 3686/3812 [90:54:56<3:06:26, 88.78s/it] 97%|█████████▋| 3687/3812 [90:56:25<3:05:08, 88.87s/it] 97%|█████████▋| 3688/3812 [90:57:54<3:03:22, 88.73s/it] 97%|█████████▋| 3689/3812 [90:59:23<3:01:58, 88.77s/it] 97%|█████████▋| 3690/3812 [91:00:51<3:00:30, 88.78s/it]                                                        {'loss': 1.2539, 'learning_rate': 1.2625766216573243e-07, 'epoch': 0.97}
 97%|█████████▋| 3690/3812 [91:00:51<3:00:30, 88.78s/it] 97%|█████████▋| 3691/3812 [91:02:20<2:59:06, 88.81s/it] 97%|█████████▋| 3692/3812 [91:03:49<2:57:28, 88.74s/it] 97%|█████████▋| 3693/3812 [91:05:18<2:55:59, 88.74s/it] 97%|█████████▋| 3694/3812 [91:06:46<2:54:17, 88.62s/it] 97%|█████████▋| 3695/3812 [91:08:15<2:52:58, 88.70s/it] 97%|█████████▋| 3696/3812 [91:09:43<2:51:29, 88.70s/it] 97%|█████████▋| 3697/3812 [91:11:12<2:50:09, 88.77s/it] 97%|█████████▋| 3698/3812 [91:12:41<2:48:29, 88.68s/it] 97%|█████████▋| 3699/3812 [91:14:10<2:47:23, 88.88s/it] 97%|█████████▋| 3700/3812 [91:15:39<2:45:49, 88.83s/it]                                                        {'loss': 1.2618, 'learning_rate': 1.0642205862147759e-07, 'epoch': 0.97}
 97%|█████████▋| 3700/3812 [91:15:39<2:45:49, 88.83s/it] 97%|█████████▋| 3701/3812 [91:17:08<2:44:34, 88.96s/it] 97%|█████████▋| 3702/3812 [91:18:37<2:42:54, 88.86s/it] 97%|█████████▋| 3703/3812 [91:20:06<2:41:40, 89.00s/it] 97%|█████████▋| 3704/3812 [91:21:35<2:39:59, 88.88s/it] 97%|█████████▋| 3705/3812 [91:23:04<2:38:33, 88.91s/it] 97%|█████████▋| 3706/3812 [91:24:32<2:36:52, 88.80s/it] 97%|█████████▋| 3707/3812 [91:26:01<2:35:28, 88.84s/it] 97%|█████████▋| 3708/3812 [91:27:30<2:33:55, 88.80s/it] 97%|█████████▋| 3709/3812 [91:28:59<2:32:30, 88.84s/it] 97%|█████████▋| 3710/3812 [91:30:28<2:30:57, 88.79s/it]                                                        {'loss': 1.2497, 'learning_rate': 8.827720285540242e-08, 'epoch': 0.97}
 97%|█████████▋| 3710/3812 [91:30:28<2:30:57, 88.79s/it] 97%|█████████▋| 3711/3812 [91:31:57<2:29:36, 88.88s/it] 97%|█████████▋| 3712/3812 [91:33:25<2:27:54, 88.75s/it] 97%|█████████▋| 3713/3812 [91:34:54<2:26:35, 88.85s/it] 97%|█████████▋| 3714/3812 [91:36:23<2:24:54, 88.72s/it] 97%|█████████▋| 3715/3812 [91:37:52<2:23:35, 88.82s/it] 97%|█████████▋| 3716/3812 [91:39:20<2:22:04, 88.79s/it] 98%|█████████▊| 3717/3812 [91:40:49<2:20:39, 88.84s/it] 98%|█████████▊| 3718/3812 [91:42:18<2:18:57, 88.70s/it] 98%|█████████▊| 3719/3812 [91:43:47<2:17:43, 88.86s/it] 98%|█████████▊| 3720/3812 [91:45:15<2:16:04, 88.75s/it]                                                        {'loss': 1.2435, 'learning_rate': 7.182432724859578e-08, 'epoch': 0.98}
 98%|█████████▊| 3720/3812 [91:45:15<2:16:04, 88.75s/it] 98%|█████████▊| 3721/3812 [91:46:44<2:14:42, 88.82s/it] 98%|█████████▊| 3722/3812 [91:48:13<2:13:17, 88.86s/it] 98%|█████████▊| 3723/3812 [91:49:42<2:11:54, 88.93s/it] 98%|█████████▊| 3724/3812 [91:51:11<2:10:15, 88.81s/it] 98%|█████████▊| 3725/3812 [91:52:40<2:08:52, 88.88s/it] 98%|█████████▊| 3726/3812 [91:54:09<2:07:14, 88.77s/it] 98%|█████████▊| 3727/3812 [91:55:37<2:05:49, 88.82s/it] 98%|█████████▊| 3728/3812 [91:57:06<2:04:14, 88.75s/it] 98%|█████████▊| 3729/3812 [91:58:35<2:02:58, 88.90s/it] 98%|█████████▊| 3730/3812 [92:00:04<2:01:18, 88.76s/it]                                                        {'loss': 1.2451, 'learning_rate': 5.706454926448224e-08, 'epoch': 0.98}
 98%|█████████▊| 3730/3812 [92:00:04<2:01:18, 88.76s/it] 98%|█████████▊| 3731/3812 [92:01:33<1:59:54, 88.82s/it] 98%|█████████▊| 3732/3812 [92:03:01<1:58:18, 88.73s/it] 98%|█████████▊| 3733/3812 [92:04:30<1:56:54, 88.79s/it] 98%|█████████▊| 3734/3812 [92:05:59<1:55:27, 88.81s/it] 98%|█████████▊| 3735/3812 [92:07:28<1:54:05, 88.90s/it] 98%|█████████▊| 3736/3812 [92:08:57<1:52:25, 88.75s/it] 98%|█████████▊| 3737/3812 [92:10:26<1:51:02, 88.84s/it] 98%|█████████▊| 3738/3812 [92:11:54<1:49:27, 88.76s/it] 98%|█████████▊| 3739/3812 [92:13:23<1:48:10, 88.91s/it] 98%|█████████▊| 3740/3812 [92:14:52<1:46:35, 88.83s/it]                                                        {'loss': 1.2448, 'learning_rate': 4.399887137292724e-08, 'epoch': 0.98}
 98%|█████████▊| 3740/3812 [92:14:52<1:46:35, 88.83s/it] 98%|█████████▊| 3741/3812 [92:16:21<1:45:10, 88.88s/it] 98%|█████████▊| 3742/3812 [92:17:49<1:43:31, 88.73s/it] 98%|█████████▊| 3743/3812 [92:19:18<1:42:04, 88.75s/it] 98%|█████████▊| 3744/3812 [92:20:47<1:40:26, 88.63s/it] 98%|█████████▊| 3745/3812 [92:22:15<1:38:56, 88.61s/it] 98%|█████████▊| 3746/3812 [92:23:44<1:37:31, 88.66s/it] 98%|█████████▊| 3747/3812 [92:25:13<1:36:05, 88.70s/it] 98%|█████████▊| 3748/3812 [92:26:41<1:34:27, 88.55s/it] 98%|█████████▊| 3749/3812 [92:28:10<1:33:01, 88.59s/it] 98%|█████████▊| 3750/3812 [92:29:38<1:31:27, 88.51s/it]                                                        {'loss': 1.2529, 'learning_rate': 3.26281809821638e-08, 'epoch': 0.98}
 98%|█████████▊| 3750/3812 [92:29:38<1:31:27, 88.51s/it] 98%|█████████▊| 3751/3812 [92:31:07<1:30:01, 88.55s/it] 98%|█████████▊| 3752/3812 [92:32:35<1:28:30, 88.51s/it] 98%|█████████▊| 3753/3812 [92:34:04<1:27:05, 88.56s/it] 98%|█████████▊| 3754/3812 [92:35:32<1:25:33, 88.51s/it] 99%|█████████▊| 3755/3812 [92:37:01<1:24:10, 88.61s/it] 99%|█████████▊| 3756/3812 [92:38:30<1:22:43, 88.63s/it] 99%|█████████▊| 3757/3812 [92:39:58<1:21:14, 88.63s/it] 99%|█████████▊| 3758/3812 [92:41:27<1:19:48, 88.67s/it] 99%|█████████▊| 3759/3812 [92:42:56<1:18:21, 88.71s/it] 99%|█████████▊| 3760/3812 [92:44:24<1:16:51, 88.68s/it]                                                        {'loss': 1.2608, 'learning_rate': 2.2953250378499046e-08, 'epoch': 0.99}
 99%|█████████▊| 3760/3812 [92:44:24<1:16:51, 88.68s/it] 99%|█████████▊| 3761/3812 [92:45:53<1:15:24, 88.71s/it] 99%|█████████▊| 3762/3812 [92:47:22<1:13:58, 88.78s/it] 99%|█████████▊| 3763/3812 [92:48:51<1:12:29, 88.76s/it] 99%|█████████▊| 3764/3812 [92:50:20<1:11:00, 88.77s/it] 99%|█████████▉| 3765/3812 [92:51:49<1:09:34, 88.81s/it] 99%|█████████▉| 3766/3812 [92:53:17<1:07:59, 88.69s/it] 99%|█████████▉| 3767/3812 [92:54:46<1:06:33, 88.74s/it] 99%|█████████▉| 3768/3812 [92:56:14<1:05:01, 88.67s/it] 99%|█████████▉| 3769/3812 [92:57:43<1:03:34, 88.71s/it] 99%|█████████▉| 3770/3812 [92:59:12<1:02:05, 88.70s/it]                                                        {'loss': 1.25, 'learning_rate': 1.4974736673883937e-08, 'epoch': 0.99}
 99%|█████████▉| 3770/3812 [92:59:12<1:02:05, 88.70s/it] 99%|█████████▉| 3771/3812 [93:00:41<1:00:41, 88.81s/it] 99%|█████████▉| 3772/3812 [93:02:09<59:08, 88.72s/it]   99%|█████████▉| 3773/3812 [93:03:38<57:41, 88.76s/it] 99%|█████████▉| 3774/3812 [93:05:07<56:10, 88.70s/it] 99%|█████████▉| 3775/3812 [93:06:35<54:41, 88.69s/it] 99%|█████████▉| 3776/3812 [93:08:04<53:11, 88.65s/it] 99%|█████████▉| 3777/3812 [93:09:33<51:43, 88.67s/it] 99%|█████████▉| 3778/3812 [93:11:01<50:11, 88.57s/it] 99%|█████████▉| 3779/3812 [93:12:30<48:45, 88.64s/it] 99%|█████████▉| 3780/3812 [93:13:58<47:15, 88.61s/it]                                                      {'loss': 1.2514, 'learning_rate': 8.693181761260105e-09, 'epoch': 0.99}
 99%|█████████▉| 3780/3812 [93:13:58<47:15, 88.61s/it] 99%|█████████▉| 3781/3812 [93:15:27<45:46, 88.61s/it] 99%|█████████▉| 3782/3812 [93:16:56<44:20, 88.68s/it] 99%|█████████▉| 3783/3812 [93:18:25<42:53, 88.75s/it] 99%|█████████▉| 3784/3812 [93:19:53<41:22, 88.66s/it] 99%|█████████▉| 3785/3812 [93:21:22<39:55, 88.72s/it] 99%|█████████▉| 3786/3812 [93:22:51<38:25, 88.66s/it] 99%|█████████▉| 3787/3812 [93:24:19<36:57, 88.71s/it] 99%|█████████▉| 3788/3812 [93:25:48<35:27, 88.65s/it] 99%|█████████▉| 3789/3812 [93:27:17<34:01, 88.76s/it] 99%|█████████▉| 3790/3812 [93:28:45<32:30, 88.66s/it]                                                      {'loss': 1.2523, 'learning_rate': 4.109012277775381e-09, 'epoch': 0.99}
 99%|█████████▉| 3790/3812 [93:28:45<32:30, 88.66s/it] 99%|█████████▉| 3791/3812 [93:30:14<31:03, 88.73s/it] 99%|█████████▉| 3792/3812 [93:31:43<29:32, 88.65s/it]100%|█████████▉| 3793/3812 [93:33:11<28:05, 88.70s/it]100%|█████████▉| 3794/3812 [93:34:40<26:36, 88.68s/it]100%|█████████▉| 3795/3812 [93:36:09<25:07, 88.70s/it]100%|█████████▉| 3796/3812 [93:37:37<23:37, 88.62s/it]100%|█████████▉| 3797/3812 [93:39:06<22:09, 88.65s/it]100%|█████████▉| 3798/3812 [93:40:35<20:40, 88.63s/it]100%|█████████▉| 3799/3812 [93:42:04<19:13, 88.73s/it]100%|█████████▉| 3800/3812 [93:43:32<17:44, 88.70s/it]                                                      {'loss': 1.2501, 'learning_rate': 1.2225395757931025e-09, 'epoch': 1.0}
100%|█████████▉| 3800/3812 [93:43:32<17:44, 88.70s/it]100%|█████████▉| 3801/3812 [93:45:01<16:16, 88.81s/it]100%|█████████▉| 3802/3812 [93:46:30<14:47, 88.70s/it]100%|█████████▉| 3803/3812 [93:47:59<13:19, 88.78s/it]100%|█████████▉| 3804/3812 [93:49:27<11:49, 88.73s/it]100%|█████████▉| 3805/3812 [93:50:56<10:21, 88.77s/it]100%|█████████▉| 3806/3812 [93:52:25<08:52, 88.80s/it]100%|█████████▉| 3807/3812 [93:53:54<07:24, 88.85s/it]100%|█████████▉| 3808/3812 [93:55:23<05:55, 88.76s/it]100%|█████████▉| 3809/3812 [93:56:51<04:26, 88.76s/it]100%|█████████▉| 3810/3812 [93:58:20<02:57, 88.67s/it]                                                      {'loss': 1.2375, 'learning_rate': 3.395970175346097e-11, 'epoch': 1.0}
100%|█████████▉| 3810/3812 [93:58:20<02:57, 88.67s/it]100%|█████████▉| 3811/3812 [93:59:49<01:28, 88.77s/it]100%|██████████| 3812/3812 [94:01:18<00:00, 88.77s/it][INFO|trainer.py:1962] 2024-02-12 21:13:28,716 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                      {'train_runtime': 338478.0474, 'train_samples_per_second': 2.883, 'train_steps_per_second': 0.011, 'train_loss': 1.3330535017430845, 'epoch': 1.0}
100%|██████████| 3812/3812 [94:01:18<00:00, 88.77s/it]100%|██████████| 3812/3812 [94:01:18<00:00, 88.79s/it]
[INFO|trainer.py:2936] 2024-02-12 21:13:37,245 >> Saving model checkpoint to /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis
[INFO|configuration_utils.py:473] 2024-02-12 21:13:37,246 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/config.json
[INFO|configuration_utils.py:594] 2024-02-12 21:13:37,247 >> Configuration saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/generation_config.json
[2024-02-12 21:13:45,323] [INFO] [launch.py:347:main] Process 995996 exits successfully.
[2024-02-12 21:13:46,325] [INFO] [launch.py:347:main] Process 995995 exits successfully.
[2024-02-12 21:13:48,328] [INFO] [launch.py:347:main] Process 995994 exits successfully.
[INFO|modeling_utils.py:2501] 2024-02-12 21:13:55,238 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2433] 2024-02-12 21:13:55,239 >> tokenizer config file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-12 21:13:55,239 >> Special tokens file saved in /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/special_tokens_map.json
***** train metrics *****
  epoch                    =                 1.0
  train_loss               =              1.3331
  train_runtime            = 3 days, 22:01:18.04
  train_samples_per_second =               2.883
  train_steps_per_second   =               0.011
Figure saved: /home/yangdezhao/zhouh_temp/LLaMA-Factory/llama-pt/ckpts/full-zhis/training_loss.png
02/12/2024 21:13:56 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-02-12 21:13:56,023 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-02-12 21:14:03,345] [INFO] [launch.py:347:main] Process 995993 exits successfully.
