[2024-03-09 14:47:45,897] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-09 14:47:49,173] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-03-09 14:47:49,222] [INFO] [runner.py:570:main] cmd = /home/nfs02/anaconda3/envs/wzjsz/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=9901 --enable_each_rank_log=None src/train_bash.py --deepspeed /home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json --stage pt --model_name_or_path /home/nfs02/model/llama2/hf/Llama-2-7b-hf --flash_attn --do_train --dataset skypile_1b,slimpajam_1b,is_1b --en_max_samples 10000 --preprocessing_num_workers 20 --mix_strategy concat --cutoff_len 2048 --finetuning_type moe --moe_every_k_layers 1 --moe_router_type top1 --moe_num_experts 2 --output_dir /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1 --overwrite_output_dir --per_device_train_batch_size 8 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 10 --save_total_limit 1 --save_only_model --save_steps 1000 --learning_rate 5e-5 --num_train_epochs 1.0 --plot_loss --bf16
[2024-03-09 14:47:50,762] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-09 14:47:52,917] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-03-09 14:47:52,917] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-03-09 14:47:52,917] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-03-09 14:47:52,917] [INFO] [launch.py:163:main] dist_world_size=4
[2024-03-09 14:47:52,917] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-03-09 14:47:58,161] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-09 14:47:58,162] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-09 14:47:58,163] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-09 14:47:58,166] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-09 14:48:05,700] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-09 14:48:05,700] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-09 14:48:05,700] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-09 14:48:05,701] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-09 14:48:05,701] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
03/09/2024 14:48:06 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/09/2024 14:48:06 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/09/2024 14:48:06 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/09/2024 14:48:06 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1/runs/Mar09_14-48-05_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/09/2024 14:48:06 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1/runs/Mar09_14-48-05_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/09/2024 14:48:06 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1/runs/Mar09_14-48-05_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2027] 2024-03-09 14:48:06,731 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2027] 2024-03-09 14:48:06,731 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-03-09 14:48:06,731 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-03-09 14:48:06,731 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2027] 2024-03-09 14:48:06,731 >> loading file tokenizer.json
03/09/2024 14:48:06 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.bfloat16
03/09/2024 14:48:06 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=/home/wangzj/LLaMA-Factory/llama-pt/config/ds_config_cpu.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1/runs/Mar09_14-48-05_2080ti-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=1,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/09/2024 14:48:06 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
03/09/2024 14:48:06 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
03/09/2024 14:48:06 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
[INFO|configuration_utils.py:727] 2024-03-09 14:48:06,933 >> loading configuration file /home/nfs02/model/llama2/hf/Llama-2-7b-hf/config.json
[INFO|configuration_utils.py:792] 2024-03-09 14:48:06,935 >> Model config LlamaConfig {
  "_name_or_path": "/home/nfs02/model/llama2/hf/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

03/09/2024 14:48:06 - INFO - llmtuner.model.patcher - Using FlashAttention-2 for faster training and inference.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|modeling_utils.py:3334] 2024-03-09 14:48:07,008 >> loading weights file /home/nfs02/model/llama2/hf/Llama-2-7b-hf/model.safetensors.index.json
[INFO|modeling_utils.py:1459] 2024-03-09 14:48:07,009 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[WARNING|logging.py:329] 2024-03-09 14:48:07,009 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[WARNING|logging.py:329] 2024-03-09 14:48:07,012 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:827] 2024-03-09 14:48:07,015 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.50s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.46s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.54s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.00s/it]
03/09/2024 14:48:16 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/09/2024 14:48:16 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.04s/it]
[INFO|modeling_utils.py:4070] 2024-03-09 14:48:16,516 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4078] 2024-03-09 14:48:16,516 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/nfs02/model/llama2/hf/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:780] 2024-03-09 14:48:16,520 >> loading configuration file /home/nfs02/model/llama2/hf/Llama-2-7b-hf/generation_config.json
[INFO|configuration_utils.py:827] 2024-03-09 14:48:16,520 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

03/09/2024 14:48:16 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/09/2024 14:48:16 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.10s/it]
03/09/2024 14:48:16 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/09/2024 14:48:16 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]
03/09/2024 14:48:16 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
03/09/2024 14:48:16 - INFO - llmtuner.model.adapter - Fine-tuning method: MOE
03/09/2024 14:48:18 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
03/09/2024 14:48:18 - INFO - llmtuner.data.template - Add pad token: </s>
03/09/2024 14:48:19 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
03/09/2024 14:48:19 - INFO - llmtuner.data.template - Add pad token: </s>
03/09/2024 14:48:19 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
03/09/2024 14:48:19 - INFO - llmtuner.data.template - Add pad token: </s>
03/09/2024 14:48:19 - INFO - llmtuner.model.loader - trainable params: 4328783872 || all params: 11067199488 || trainable%: 39.1136
03/09/2024 14:48:19 - INFO - llmtuner.data.template - Add pad token: </s>
03/09/2024 14:48:26 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
Using custom data configuration default-e04fdd9113403d69
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
Found cached dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
Dataset({
    features: ['text'],
    num_rows: 661595
})
{'text': '一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁\n所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！\n隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。\n'}
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00000_of_00020.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00001_of_00020.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00002_of_00020.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00003_of_00020.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00004_of_00020.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00005_of_00020.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00006_of_00020.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00007_of_00020.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00008_of_00020.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00009_of_00020.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00010_of_00020.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00011_of_00020.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00012_of_00020.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00013_of_00020.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00014_of_00020.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00015_of_00020.arrow
Process #16 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00016_of_00020.arrow
Process #17 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00017_of_00020.arrow
Process #18 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00018_of_00020.arrow
Process #19 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_00019_of_00020.arrow
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5528d6252f04d9a4_*_of_00020.arrow
Concatenating 20 shards
03/09/2024 14:48:43 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Using custom data configuration default-fd8d5f83c6d4fc15
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
Found cached dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Loading Dataset info from /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
Sample en data 10000 lines
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 10000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00000_of_00020.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00001_of_00020.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00002_of_00020.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00003_of_00020.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00004_of_00020.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00005_of_00020.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00006_of_00020.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00007_of_00020.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00008_of_00020.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00009_of_00020.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00010_of_00020.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00011_of_00020.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00012_of_00020.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00013_of_00020.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00014_of_00020.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00015_of_00020.arrow
Process #16 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00016_of_00020.arrow
Process #17 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00017_of_00020.arrow
Process #18 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00018_of_00020.arrow
Process #19 will write at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_00019_of_00020.arrow
Loading cached processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-fd8d5f83c6d4fc15/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d5178b06c9d25346_*_of_00020.arrow
Concatenating 20 shards
03/09/2024 14:49:25 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
Using custom data configuration default-b019e4271eec4c56
Loading Dataset Infos from /home/nfs02/anaconda3/envs/wzjsz/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Downloading and preparing dataset json/default to /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 3869.28it/s]
Downloading took 0.0 min
Checksum Computation took 0.0 min
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 209.72it/s]
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 6079 examples [00:00, 42612.26 examples/s]Generating train split: 14286 examples [00:00, 58169.85 examples/s]Generating train split: 22586 examples [00:00, 64331.64 examples/s]Generating train split: 30705 examples [00:00, 67308.76 examples/s]Generating train split: 39013 examples [00:00, 70148.29 examples/s]Generating train split: 47412 examples [00:00, 73834.23 examples/s]Generating train split: 55790 examples [00:00, 74574.60 examples/s]Generating train split: 63858 examples [00:00, 73883.50 examples/s]Generating train split: 71828 examples [00:01, 71973.07 examples/s]Generating train split: 80015 examples [00:01, 70970.91 examples/s]Generating train split: 88233 examples [00:01, 69061.12 examples/s]Generating train split: 96323 examples [00:01, 67407.30 examples/s]Generating train split: 104680 examples [00:01, 69057.88 examples/s]Generating train split: 114850 examples [00:01, 72407.90 examples/s]Generating train split: 126970 examples [00:01, 75452.95 examples/s]Generating train split: 135072 examples [00:01, 75131.87 examples/s]Generating train split: 143305 examples [00:02, 74499.75 examples/s]Generating train split: 151203 examples [00:02, 73645.04 examples/s]Generating train split: 159215 examples [00:02, 73081.78 examples/s]Generating train split: 167048 examples [00:02, 73381.97 examples/s]Generating train split: 175145 examples [00:02, 74110.36 examples/s]Generating train split: 185347 examples [00:02, 75280.59 examples/s]Generating train split: 193716 examples [00:02, 76332.29 examples/s]Generating train split: 201625 examples [00:02, 72804.95 examples/s]Generating train split: 209916 examples [00:02, 72574.28 examples/s]Generating train split: 218183 examples [00:03, 71409.29 examples/s]Generating train split: 226069 examples [00:03, 69183.23 examples/s]Generating train split: 234185 examples [00:03, 69135.17 examples/s]Generating train split: 242183 examples [00:03, 68861.60 examples/s]Generating train split: 250537 examples [00:03, 68382.51 examples/s]Generating train split: 258807 examples [00:03, 69345.86 examples/s]Generating train split: 266858 examples [00:03, 69148.92 examples/s]Generating train split: 275075 examples [00:03, 68964.07 examples/s]Generating train split: 283309 examples [00:04, 64927.31 examples/s]Generating train split: 291359 examples [00:04, 66211.59 examples/s]Generating train split: 299603 examples [00:04, 67125.78 examples/s]Generating train split: 307938 examples [00:04, 68029.30 examples/s]Generating train split: 316089 examples [00:04, 68379.93 examples/s]Generating train split: 324384 examples [00:04, 67416.07 examples/s]Generating train split: 332486 examples [00:04, 67338.29 examples/s]Generating train split: 340589 examples [00:04, 68143.20 examples/s]Generating train split: 348519 examples [00:04, 67858.62 examples/s]Generating train split: 356708 examples [00:05, 68961.32 examples/s]Generating train split: 364834 examples [00:05, 67827.65 examples/s]Generating train split: 372773 examples [00:05, 67280.80 examples/s]Generating train split: 380749 examples [00:05, 67264.42 examples/s]Generating train split: 388555 examples [00:05, 67086.39 examples/s]Generating train split: 396732 examples [00:05, 67822.97 examples/s]Generating train split: 404862 examples [00:05, 68427.02 examples/s]Generating train split: 412811 examples [00:05, 68329.79 examples/s]Generating train split: 420865 examples [00:06, 68215.06 examples/s]Generating train split: 429209 examples [00:06, 68078.90 examples/s]Generating train split: 437231 examples [00:06, 68537.15 examples/s]Generating train split: 445253 examples [00:06, 68786.71 examples/s]Generating train split: 453547 examples [00:06, 69052.99 examples/s]Generating train split: 461627 examples [00:06, 68973.39 examples/s]Generating train split: 469676 examples [00:06, 67138.92 examples/s]Generating train split: 477725 examples [00:06, 67314.54 examples/s]Generating train split: 485915 examples [00:06, 67875.75 examples/s]Generating train split: 494396 examples [00:07, 69366.70 examples/s]Generating train split: 502373 examples [00:07, 67121.20 examples/s]Generating train split: 510506 examples [00:07, 67776.16 examples/s]Generating train split: 518507 examples [00:07, 66869.99 examples/s]Generating train split: 526618 examples [00:07, 66687.17 examples/s]Generating train split: 534762 examples [00:07, 66084.74 examples/s]Generating train split: 542705 examples [00:07, 64162.91 examples/s]Generating train split: 550700 examples [00:07, 64658.09 examples/s]Generating train split: 558842 examples [00:08, 66085.34 examples/s]Generating train split: 566808 examples [00:08, 63505.96 examples/s]Generating train split: 575100 examples [00:08, 65625.80 examples/s]Generating train split: 583267 examples [00:08, 65721.05 examples/s]Generating train split: 591245 examples [00:08, 65727.71 examples/s]Generating train split: 599463 examples [00:08, 66141.67 examples/s]Generating train split: 607391 examples [00:08, 66016.90 examples/s]Generating train split: 615831 examples [00:08, 68818.60 examples/s]Generating train split: 619741 examples [00:09, 68747.32 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
Dataset({
    features: ['text'],
    num_rows: 619741
})
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00000_of_00020.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00001_of_00020.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00002_of_00020.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00003_of_00020.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00004_of_00020.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00005_of_00020.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00006_of_00020.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00007_of_00020.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00008_of_00020.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00009_of_00020.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00010_of_00020.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00011_of_00020.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00012_of_00020.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00013_of_00020.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00014_of_00020.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00015_of_00020.arrow
Process #16 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00016_of_00020.arrow
Process #17 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00017_of_00020.arrow
Process #18 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00018_of_00020.arrow
Process #19 will write at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00019_of_00020.arrow
Spawning 20 processes
Converting format of dataset (num_proc=20):   0%|          | 0/619741 [00:00<?, ? examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00009_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00000_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00015_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00003_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00006_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00007_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00004_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00001_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00010_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00005_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00014_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00002_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00008_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00011_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00019_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00013_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00017_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00012_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00016_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-b019e4271eec4c56/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-92ed5715cd538b3e_00018_of_00020.arrow
Converting format of dataset (num_proc=20):   0%|          | 1000/619741 [00:00<03:52, 2660.83 examples/s]Converting format of dataset (num_proc=20):   6%|▌         | 35000/619741 [00:00<00:06, 92797.21 examples/s]Converting format of dataset (num_proc=20):  13%|█▎        | 78000/619741 [00:00<00:02, 184059.44 examples/s]Converting format of dataset (num_proc=20):  20%|██        | 124000/619741 [00:00<00:01, 261043.28 examples/s]Converting format of dataset (num_proc=20):  28%|██▊       | 176000/619741 [00:00<00:01, 328488.27 examples/s]Converting format of dataset (num_proc=20):  36%|███▋      | 226000/619741 [00:00<00:01, 376655.26 examples/s]Converting format of dataset (num_proc=20):  44%|████▍     | 274000/619741 [00:01<00:00, 400942.38 examples/s]Converting format of dataset (num_proc=20):  53%|█████▎    | 330000/619741 [00:01<00:00, 440978.00 examples/s]Converting format of dataset (num_proc=20):  62%|██████▏   | 386000/619741 [00:01<00:00, 467614.87 examples/s]Converting format of dataset (num_proc=20):  70%|███████   | 435000/619741 [00:01<00:00, 468044.58 examples/s]Converting format of dataset (num_proc=20):  79%|███████▊  | 487000/619741 [00:01<00:00, 479293.51 examples/s]Converting format of dataset (num_proc=20):  87%|████████▋ | 539000/619741 [00:01<00:00, 487957.80 examples/s]Converting format of dataset (num_proc=20):  95%|█████████▌| 588949/619741 [00:01<00:00, 402563.56 examples/s]Converting format of dataset (num_proc=20):  98%|█████████▊| 606910/619741 [00:21<00:00, 402563.56 examples/s]Converting format of dataset (num_proc=20):  98%|█████████▊| 607897/619741 [00:29<00:02, 4928.16 examples/s]  Converting format of dataset (num_proc=20):  98%|█████████▊| 608884/619741 [00:29<00:02, 4898.98 examples/s]Converting format of dataset (num_proc=20): 100%|██████████| 619741/619741 [00:31<00:00, 19824.21 examples/s]
Concatenating 20 shards
Process #0 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00000_of_00020.arrow
Process #1 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00001_of_00020.arrow
Process #2 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00002_of_00020.arrow
Process #3 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00003_of_00020.arrow
Process #4 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00004_of_00020.arrow
Process #5 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00005_of_00020.arrow
Process #6 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00006_of_00020.arrow
Process #7 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00007_of_00020.arrow
Process #8 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00008_of_00020.arrow
Process #9 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00009_of_00020.arrow
Process #10 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00010_of_00020.arrow
Process #11 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00011_of_00020.arrow
Process #12 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00012_of_00020.arrow
Process #13 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00013_of_00020.arrow
Process #14 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00014_of_00020.arrow
Process #15 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00015_of_00020.arrow
Process #16 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00016_of_00020.arrow
Process #17 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00017_of_00020.arrow
Process #18 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00018_of_00020.arrow
Process #19 will write at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00019_of_00020.arrow
Spawning 20 processes
Running tokenizer on dataset (num_proc=20):   0%|          | 0/1291336 [00:00<?, ? examples/s]03/09/2024 14:50:30 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
03/09/2024 14:50:30 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
03/09/2024 14:50:30 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/skypile_1b.jsonl.
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00006_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00001_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00009_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00008_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00000_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   0%|          | 1000/1291336 [00:06<2:09:16, 166.35 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00007_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   0%|          | 5000/1291336 [00:06<20:33, 1042.63 examples/s] Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00004_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   0%|          | 6000/1291336 [00:06<17:34, 1219.42 examples/s]Running tokenizer on dataset (num_proc=20):   1%|          | 7000/1291336 [00:06<13:52, 1541.84 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00005_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00002_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   1%|          | 8000/1291336 [00:07<12:49, 1668.27 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00003_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   1%|          | 10000/1291336 [00:07<10:03, 2122.24 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00010_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00012_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   1%|          | 11000/1291336 [00:08<13:04, 1631.00 examples/s]Running tokenizer on dataset (num_proc=20):   1%|          | 12000/1291336 [00:09<11:17, 1889.02 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00013_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00011_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   1%|          | 13000/1291336 [00:09<11:31, 1848.58 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00019_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00018_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00014_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   1%|          | 15000/1291336 [00:10<09:01, 2354.98 examples/s]Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00015_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00016_of_00020.arrow
Caching processed dataset at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fb8a7459f9e387b9_00017_of_00020.arrow
Running tokenizer on dataset (num_proc=20):   1%|▏         | 17000/1291336 [00:10<06:46, 3135.77 examples/s]Running tokenizer on dataset (num_proc=20):   1%|▏         | 18000/1291336 [00:10<05:53, 3604.87 examples/s]Running tokenizer on dataset (num_proc=20):   1%|▏         | 19000/1291336 [00:10<05:16, 4015.17 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 21000/1291336 [00:11<06:21, 3334.13 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 22000/1291336 [00:11<05:28, 3859.74 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 23000/1291336 [00:11<04:46, 4428.83 examples/s]Dataset({
    features: ['text'],
    num_rows: 661595
})
{'text': '一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁\n所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！\n隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。\n'}
Dataset({
    features: ['text'],
    num_rows: 661595
})
{'text': '一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁\n所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！\n隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。\n'}
Dataset({
    features: ['text'],
    num_rows: 661595
})
{'text': '一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁\n所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！\n隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。\n'}
Running tokenizer on dataset (num_proc=20):   2%|▏         | 24000/1291336 [00:12<05:39, 3730.35 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 25000/1291336 [00:12<05:12, 4057.22 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 27000/1291336 [00:12<03:41, 5710.92 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 29000/1291336 [00:13<06:25, 3272.19 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 30000/1291336 [00:14<08:57, 2347.85 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 31000/1291336 [00:15<09:05, 2309.75 examples/s]Running tokenizer on dataset (num_proc=20):   2%|▏         | 32000/1291336 [00:17<17:27, 1202.16 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 33000/1291336 [00:17<14:06, 1486.58 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 34000/1291336 [00:17<14:31, 1442.96 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 35000/1291336 [00:18<12:47, 1635.86 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 36000/1291336 [00:18<09:55, 2107.57 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 37000/1291336 [00:18<08:04, 2586.61 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 38000/1291336 [00:19<09:31, 2192.28 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 39000/1291336 [00:19<10:41, 1950.96 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 41000/1291336 [00:20<06:31, 3190.79 examples/s]03/09/2024 14:50:48 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Running tokenizer on dataset (num_proc=20):   3%|▎         | 43000/1291336 [00:20<04:29, 4639.08 examples/s]03/09/2024 14:50:48 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Running tokenizer on dataset (num_proc=20):   3%|▎         | 44000/1291336 [00:20<04:32, 4569.11 examples/s]Running tokenizer on dataset (num_proc=20):   3%|▎         | 45000/1291336 [00:20<04:15, 4877.33 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▎         | 46000/1291336 [00:20<04:24, 4709.44 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▎         | 47000/1291336 [00:21<04:07, 5018.56 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▎         | 48000/1291336 [00:21<03:36, 5754.73 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 49000/1291336 [00:21<06:01, 3435.30 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 50000/1291336 [00:22<08:25, 2456.98 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 52000/1291336 [00:22<05:21, 3857.42 examples/s]03/09/2024 14:50:51 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/slimpajam_1b.jsonl.
Running tokenizer on dataset (num_proc=20):   4%|▍         | 54000/1291336 [00:23<06:37, 3116.57 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 55000/1291336 [00:23<06:48, 3022.87 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 56000/1291336 [00:23<05:42, 3609.42 examples/s]Running tokenizer on dataset (num_proc=20):   4%|▍         | 57000/1291336 [00:26<16:36, 1238.87 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 59000/1291336 [00:26<10:43, 1914.08 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 60000/1291336 [00:27<11:01, 1861.63 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 61000/1291336 [00:27<11:29, 1784.55 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 62000/1291336 [00:27<09:37, 2128.43 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▍         | 64000/1291336 [00:28<06:01, 3391.66 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 66000/1291336 [00:28<06:47, 3009.38 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 68000/1291336 [00:28<04:49, 4218.56 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 69000/1291336 [00:29<05:29, 3714.41 examples/s]Running tokenizer on dataset (num_proc=20):   5%|▌         | 71000/1291336 [00:29<04:01, 5050.02 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 73000/1291336 [00:29<03:08, 6448.21 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 75000/1291336 [00:30<03:32, 5733.37 examples/s]Sample en data 10000 lines
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 10000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Sample en data 10000 lines
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 10000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Running tokenizer on dataset (num_proc=20):   6%|▌         | 76000/1291336 [00:31<09:16, 2182.59 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 78000/1291336 [00:32<07:03, 2867.45 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 79000/1291336 [00:32<06:30, 3102.59 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▌         | 80000/1291336 [00:32<05:56, 3402.07 examples/s]Sample en data 10000 lines
Dataset({
    features: ['text', 'meta', '__index_level_0__'],
    num_rows: 10000
})
{'text': '@interface PodsDummy_XCDLumberjackNSLogger_OSX : NSObject\n@end\n@implementation PodsDummy_XCDLumberjackNSLogger_OSX\n@end\n', 'meta': {'redpajama_set_name': 'RedPajamaGithub'}, '__index_level_0__': 3706}
Running tokenizer on dataset (num_proc=20):   6%|▋         | 81000/1291336 [00:33<09:02, 2230.71 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▋         | 82000/1291336 [00:33<07:56, 2536.95 examples/s]Running tokenizer on dataset (num_proc=20):   6%|▋         | 83000/1291336 [00:34<07:42, 2611.74 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 84000/1291336 [00:34<07:11, 2798.44 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 86000/1291336 [00:34<06:16, 3204.23 examples/s]03/09/2024 14:51:04 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
Running tokenizer on dataset (num_proc=20):   7%|▋         | 87000/1291336 [00:37<19:04, 1052.11 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 88000/1291336 [00:37<14:43, 1361.79 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 89000/1291336 [00:38<13:20, 1501.35 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 90000/1291336 [00:38<11:41, 1711.60 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 92000/1291336 [00:38<07:15, 2756.11 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 93000/1291336 [00:39<06:30, 3071.77 examples/s]Running tokenizer on dataset (num_proc=20):   7%|▋         | 95000/1291336 [00:39<04:36, 4320.92 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 97000/1291336 [00:39<03:16, 6065.00 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 100000/1291336 [00:39<02:32, 7831.99 examples/s]03/09/2024 14:51:08 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
Running tokenizer on dataset (num_proc=20):   8%|▊         | 102000/1291336 [00:40<03:34, 5534.93 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 103000/1291336 [00:40<03:41, 5352.97 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 104000/1291336 [00:40<03:26, 5743.96 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 105000/1291336 [00:40<03:49, 5159.41 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 106000/1291336 [00:41<08:01, 2461.29 examples/s]03/09/2024 14:51:10 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/is_1b.jsonl.
Running tokenizer on dataset (num_proc=20):   8%|▊         | 107000/1291336 [00:43<12:48, 1540.78 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 108000/1291336 [00:44<14:14, 1385.63 examples/s]Running tokenizer on dataset (num_proc=20):   8%|▊         | 109000/1291336 [00:44<11:35, 1699.84 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▊         | 111000/1291336 [00:45<09:38, 2039.17 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▊         | 112000/1291336 [00:45<08:09, 2407.36 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 113000/1291336 [00:46<09:49, 1999.68 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 114000/1291336 [00:46<07:44, 2533.06 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 116000/1291336 [00:46<04:51, 4033.93 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 117000/1291336 [00:46<04:13, 4634.91 examples/s]Dataset({
    features: ['text'],
    num_rows: 619741
})
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
Running tokenizer on dataset (num_proc=20):   9%|▉         | 118000/1291336 [00:48<11:15, 1738.16 examples/s]Running tokenizer on dataset (num_proc=20):   9%|▉         | 120000/1291336 [00:48<08:03, 2420.74 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 123000/1291336 [00:48<05:15, 3697.90 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 124000/1291336 [00:49<05:42, 3407.40 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 125000/1291336 [00:49<05:57, 3259.53 examples/s]Running tokenizer on dataset (num_proc=20):  10%|▉         | 126000/1291336 [00:49<05:32, 3510.04 examples/s]Dataset({
    features: ['text'],
    num_rows: 619741
})
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
Running tokenizer on dataset (num_proc=20):  10%|▉         | 128000/1291336 [00:49<03:51, 5022.23 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 130000/1291336 [00:50<05:18, 3643.70 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 132000/1291336 [00:51<06:06, 3161.42 examples/s]Running tokenizer on dataset (num_proc=20):  10%|█         | 135000/1291336 [00:52<05:03, 3811.06 examples/s]Dataset({
    features: ['text'],
    num_rows: 619741
})
{'text': 'JÓLAGJÖF MÍN OG KÆRÓ TIL HVERS ANNARS - Trendnet\n66°NorðurDANMÖRKMEN\'S STYLENEW INPERSONALSTYLE\nÆtli við séum ekki bara komnir á þennan stað, búnir að vera saman í 5 ár og þá förum við að segja "Æ eigum við ekki bara að gefa hver öðrum sófa í jólagjöf, eða nýja ryksugu, blalala" – jú eða bara fara í lúxusfrí. Sem er jú alveg praktískt, og mér finnst ekkert asnalegt þegar aðrir gera það. Ég lofaði bara sjálfum mér að ég yrði ekki svoleiðis, ég elska að gefa, ég elska að pæla og svona, æ þið vitið, allt í kringum gjafagleðina um jólin, en hér er ég. Ég gerði þetta í ár. Þetta var reyndar hvorki ryksuga, eða mubla, heldur jakki. Ekki misskilja heldur, ég er yfir mig ánægður með þetta og ég meira segja stakk uppá þessu. Við gáfum hver öðrum Tindur úlpuna frá 66°Norður – en fyrst var keypt rauð-appelsínugula og það kom fljótt í ljós að það var ekki séns að við gætum deilt úlpunni. Við vorum farnir í keppni hver vaknaði fyrr á morgnana til að ná honum í vinnuna, en við mætum á sama tíma semsagt.\nSvo samankomulagið var að við mundum deila kostnaði á nýrri úlpu, eða hann í rauninni "keypti sig úr" appelsínugulu og ég nýtti þann pening uppí svörtu. Svo nú verðum við hamingjusamir til æviloka. En úlpan er meira og minna uppseld ég, en þau áttu eina á Sværtegade hér í Köben, sem var líka bara skilaboð frá alheiminum að ég átti að grípa hana. Ég er himinnlifandi með hana, og jakkaperrinn sem ég er, þá líður mér eins og ég hef náð einhverjum topp. Ég gjörsamlega elska þessa úlpu.\nDrulluferskir á sunnudagsmorgni – kannski gaman að deila því með ykkur að þessar buxur eru nýkeyptar og þær voru svo þröngar að ég hélt þær ætluðu að sprengja á mér kúlurnar þegar ég keypti þær, svo gáfu þær sig það mikið að ég gæti orðið óléttur og notað þær á meðgöngu. Hvernig þrengi ég buxur? Sjóða þær í 90°? Help y\'all.\nJÓLAÓSKALISTINN MINN 2017 -\nÓKEEEEI GLEÐILEGAN FYRSTA DESEMBER!!!! ER LÍFIÐ EKKI DÁSAMLEGT KRAKKAR!! Ég er búinn að kaupa alltof margar jólagjafir og mér finnst ég samt eiga fullt eftir. Djöfull elska ég þennan tíma árs. Ég er bókstaflega byrjaður að telja niður klukkutímana, og er búinn að setja fullt í kalenderið mitt svo ég…'}
Running tokenizer on dataset (num_proc=20):  11%|█         | 137000/1291336 [00:53<08:15, 2328.57 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 138000/1291336 [00:54<08:22, 2295.97 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 139000/1291336 [00:55<10:13, 1877.58 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 140000/1291336 [00:55<09:32, 2011.83 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 141000/1291336 [00:56<09:55, 1933.14 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 142000/1291336 [00:56<08:17, 2310.26 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 143000/1291336 [00:56<08:51, 2161.65 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█         | 145000/1291336 [00:57<06:10, 3097.65 examples/s]Running tokenizer on dataset (num_proc=20):  11%|█▏        | 147000/1291336 [00:57<04:19, 4412.06 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 149000/1291336 [00:57<03:12, 5927.66 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 150000/1291336 [00:58<05:10, 3673.21 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 152000/1291336 [00:58<04:32, 4175.95 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 153000/1291336 [00:58<04:00, 4730.30 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 154000/1291336 [00:58<03:58, 4770.35 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 155000/1291336 [00:58<03:40, 5162.72 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 156000/1291336 [00:59<07:06, 2664.17 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 157000/1291336 [01:00<09:56, 1901.62 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 158000/1291336 [01:00<08:21, 2258.94 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 159000/1291336 [01:01<09:20, 2019.48 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 160000/1291336 [01:01<08:15, 2283.27 examples/s]Running tokenizer on dataset (num_proc=20):  12%|█▏        | 161000/1291336 [01:02<08:12, 2294.91 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 162000/1291336 [01:02<06:36, 2847.73 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 163000/1291336 [01:02<05:47, 3243.49 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 164000/1291336 [01:05<17:52, 1051.48 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 166000/1291336 [01:05<10:44, 1746.25 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 167000/1291336 [01:06<11:41, 1603.15 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 168000/1291336 [01:06<09:23, 1993.51 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 169000/1291336 [01:06<07:51, 2380.47 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 171000/1291336 [01:06<04:55, 3789.87 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 172000/1291336 [01:06<04:26, 4203.86 examples/s]Running tokenizer on dataset (num_proc=20):  13%|█▎        | 173000/1291336 [01:06<04:13, 4410.93 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▎        | 176000/1291336 [01:07<03:15, 5693.87 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▎        | 177000/1291336 [01:07<03:07, 5940.38 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 178000/1291336 [01:07<03:32, 5229.14 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 180000/1291336 [01:08<04:07, 4485.77 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 182000/1291336 [01:10<10:02, 1840.03 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 183000/1291336 [01:11<09:59, 1850.20 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 184000/1291336 [01:11<09:13, 2002.37 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 185000/1291336 [01:11<08:16, 2230.07 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 186000/1291336 [01:11<07:10, 2567.21 examples/s]Running tokenizer on dataset (num_proc=20):  14%|█▍        | 187000/1291336 [01:12<06:45, 2722.79 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 188000/1291336 [01:12<06:24, 2866.65 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 190000/1291336 [01:12<04:25, 4152.28 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 191000/1291336 [01:13<04:30, 4073.83 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 192000/1291336 [01:13<04:18, 4249.43 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▍        | 193000/1291336 [01:13<06:49, 2679.36 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 194000/1291336 [01:15<11:12, 1632.50 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 195000/1291336 [01:15<09:46, 1868.62 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 197000/1291336 [01:16<09:13, 1978.14 examples/s]Running tokenizer on dataset (num_proc=20):  15%|█▌        | 198000/1291336 [01:16<07:39, 2378.09 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 201000/1291336 [01:16<04:13, 4297.33 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 202000/1291336 [01:17<04:32, 3993.33 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 203000/1291336 [01:17<05:09, 3519.52 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 204000/1291336 [01:17<04:50, 3745.38 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 205000/1291336 [01:18<06:00, 3015.35 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▌        | 208000/1291336 [01:18<03:31, 5118.69 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▋        | 210000/1291336 [01:20<08:39, 2080.21 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▋        | 211000/1291336 [01:21<11:19, 1588.86 examples/s]Running tokenizer on dataset (num_proc=20):  16%|█▋        | 212000/1291336 [01:22<11:07, 1616.57 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 214000/1291336 [01:23<09:30, 1889.81 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 216000/1291336 [01:23<07:12, 2484.27 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 219000/1291336 [01:23<04:40, 3827.52 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 220000/1291336 [01:24<05:06, 3496.18 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 221000/1291336 [01:24<04:44, 3758.81 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 222000/1291336 [01:24<04:23, 4057.62 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 223000/1291336 [01:24<04:51, 3662.55 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 224000/1291336 [01:26<08:39, 2055.97 examples/s]Running tokenizer on dataset (num_proc=20):  17%|█▋        | 225000/1291336 [01:26<07:08, 2486.19 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 226000/1291336 [01:26<06:30, 2729.95 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 227000/1291336 [01:26<05:23, 3286.49 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 228000/1291336 [01:26<05:22, 3300.51 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 229000/1291336 [01:27<05:51, 3022.56 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 230000/1291336 [01:27<06:41, 2646.52 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 231000/1291336 [01:28<09:32, 1850.77 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 232000/1291336 [01:28<08:03, 2191.16 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 233000/1291336 [01:29<07:12, 2448.66 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 235000/1291336 [01:29<05:01, 3504.97 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 236000/1291336 [01:30<06:15, 2812.48 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 237000/1291336 [01:30<06:50, 2569.96 examples/s]Running tokenizer on dataset (num_proc=20):  18%|█▊        | 238000/1291336 [01:32<13:18, 1319.22 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 239000/1291336 [01:32<11:30, 1523.98 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 240000/1291336 [01:32<08:46, 1995.70 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▊        | 241000/1291336 [01:33<07:11, 2435.40 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 243000/1291336 [01:33<05:57, 2928.98 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 245000/1291336 [01:34<06:37, 2635.07 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 246000/1291336 [01:34<06:12, 2806.93 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 247000/1291336 [01:34<05:39, 3080.20 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 249000/1291336 [01:35<03:42, 4679.10 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 250000/1291336 [01:35<03:57, 4393.04 examples/s]Running tokenizer on dataset (num_proc=20):  19%|█▉        | 251000/1291336 [01:35<04:14, 4081.24 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 252000/1291336 [01:36<07:14, 2391.81 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 253000/1291336 [01:36<07:02, 2458.46 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 254000/1291336 [01:37<09:25, 1835.25 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 255000/1291336 [01:38<08:18, 2079.88 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 257000/1291336 [01:38<05:47, 2972.49 examples/s]Running tokenizer on dataset (num_proc=20):  20%|█▉        | 258000/1291336 [01:38<06:40, 2578.16 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 259000/1291336 [01:39<06:43, 2559.22 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 260000/1291336 [01:39<05:35, 3078.40 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 261000/1291336 [01:40<06:25, 2675.36 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 262000/1291336 [01:40<05:14, 3276.25 examples/s]Running tokenizer on dataset (num_proc=20):  20%|██        | 264000/1291336 [01:40<03:16, 5232.51 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 266000/1291336 [01:42<08:11, 2087.08 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 267000/1291336 [01:42<08:13, 2076.80 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 268000/1291336 [01:43<09:11, 1855.59 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 269000/1291336 [01:44<09:52, 1724.02 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 270000/1291336 [01:44<08:42, 1956.10 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 271000/1291336 [01:44<06:46, 2510.82 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 272000/1291336 [01:44<05:44, 2955.35 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 273000/1291336 [01:44<04:37, 3672.50 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██        | 274000/1291336 [01:44<04:28, 3782.99 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██▏       | 275000/1291336 [01:45<03:49, 4420.94 examples/s]Running tokenizer on dataset (num_proc=20):  21%|██▏       | 276000/1291336 [01:45<04:41, 3602.30 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 278000/1291336 [01:45<02:57, 5702.73 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 279000/1291336 [01:46<03:44, 4516.74 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 280000/1291336 [01:46<03:46, 4472.61 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 281000/1291336 [01:46<04:56, 3410.72 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 282000/1291336 [01:47<08:49, 1904.64 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 284000/1291336 [01:48<08:47, 1908.99 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 285000/1291336 [01:50<11:14, 1491.17 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 286000/1291336 [01:50<10:07, 1655.85 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 287000/1291336 [01:50<08:04, 2074.24 examples/s]Running tokenizer on dataset (num_proc=20):  22%|██▏       | 289000/1291336 [01:50<05:53, 2838.01 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 292000/1291336 [01:51<03:29, 4762.08 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 293000/1291336 [01:51<03:09, 5259.44 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 294000/1291336 [01:51<02:51, 5809.25 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 295000/1291336 [01:51<03:47, 4378.90 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 296000/1291336 [01:52<07:08, 2320.67 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 297000/1291336 [01:53<07:38, 2169.27 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 298000/1291336 [01:54<09:54, 1671.13 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 299000/1291336 [01:55<10:52, 1520.13 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 300000/1291336 [01:55<09:06, 1814.09 examples/s]Running tokenizer on dataset (num_proc=20):  23%|██▎       | 301000/1291336 [01:55<07:56, 2078.28 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▎       | 304000/1291336 [01:55<03:52, 4249.93 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▎       | 306000/1291336 [01:56<05:35, 2934.62 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 307000/1291336 [01:57<05:21, 3062.96 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 308000/1291336 [01:57<07:04, 2313.79 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 309000/1291336 [01:58<06:03, 2705.50 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 310000/1291336 [01:58<07:20, 2229.87 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 312000/1291336 [01:59<07:06, 2294.69 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 313000/1291336 [02:00<10:09, 1606.02 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 315000/1291336 [02:01<06:31, 2496.24 examples/s]Running tokenizer on dataset (num_proc=20):  24%|██▍       | 316000/1291336 [02:01<06:10, 2633.16 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 317000/1291336 [02:01<05:39, 2870.48 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 318000/1291336 [02:01<05:03, 3212.11 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 320000/1291336 [02:02<03:53, 4167.86 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 321000/1291336 [02:02<03:49, 4231.55 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▍       | 322000/1291336 [02:03<07:04, 2284.06 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 325000/1291336 [02:03<04:19, 3730.13 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 326000/1291336 [02:04<05:44, 2801.88 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 327000/1291336 [02:04<06:01, 2664.83 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 328000/1291336 [02:05<05:24, 2972.18 examples/s]Running tokenizer on dataset (num_proc=20):  25%|██▌       | 329000/1291336 [02:06<08:08, 1968.96 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 330000/1291336 [02:06<07:35, 2112.79 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 331000/1291336 [02:06<07:56, 2015.62 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 332000/1291336 [02:07<06:34, 2434.59 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 334000/1291336 [02:08<07:07, 2239.91 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 335000/1291336 [02:08<07:25, 2147.41 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 336000/1291336 [02:09<08:15, 1929.28 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 337000/1291336 [02:10<10:42, 1484.49 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▌       | 338000/1291336 [02:10<10:04, 1576.77 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▋       | 340000/1291336 [02:11<06:20, 2497.78 examples/s]Running tokenizer on dataset (num_proc=20):  26%|██▋       | 342000/1291336 [02:11<05:23, 2934.00 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 344000/1291336 [02:12<04:39, 3384.86 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 346000/1291336 [02:12<03:28, 4527.68 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 348000/1291336 [02:12<02:36, 6032.14 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 350000/1291336 [02:13<04:18, 3638.08 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 351000/1291336 [02:14<05:53, 2662.60 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 352000/1291336 [02:14<07:05, 2209.50 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 353000/1291336 [02:15<07:46, 2009.57 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 354000/1291336 [02:16<08:04, 1934.76 examples/s]Running tokenizer on dataset (num_proc=20):  27%|██▋       | 355000/1291336 [02:16<07:43, 2020.00 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 356000/1291336 [02:16<07:15, 2145.41 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 358000/1291336 [02:17<06:21, 2443.82 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 361000/1291336 [02:17<03:46, 4113.09 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 363000/1291336 [02:18<05:17, 2924.33 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 364000/1291336 [02:19<05:50, 2643.15 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 365000/1291336 [02:20<06:53, 2239.47 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 366000/1291336 [02:20<05:53, 2614.64 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 367000/1291336 [02:20<06:18, 2444.77 examples/s]Running tokenizer on dataset (num_proc=20):  28%|██▊       | 368000/1291336 [02:20<05:05, 3019.63 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 369000/1291336 [02:21<06:11, 2481.69 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 370000/1291336 [02:21<04:55, 3112.77 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▊       | 371000/1291336 [02:22<05:23, 2841.43 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 373000/1291336 [02:22<03:40, 4171.79 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 374000/1291336 [02:23<05:30, 2776.84 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 375000/1291336 [02:23<05:41, 2682.83 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 376000/1291336 [02:24<08:04, 1889.46 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 378000/1291336 [02:24<05:55, 2568.49 examples/s]Running tokenizer on dataset (num_proc=20):  29%|██▉       | 380000/1291336 [02:26<08:31, 1783.02 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 381000/1291336 [02:27<10:06, 1500.52 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 383000/1291336 [02:27<07:17, 2077.74 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 384000/1291336 [02:28<06:19, 2389.82 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 385000/1291336 [02:28<05:25, 2781.40 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 386000/1291336 [02:28<04:43, 3196.25 examples/s]Running tokenizer on dataset (num_proc=20):  30%|██▉       | 387000/1291336 [02:28<04:32, 3313.82 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 388000/1291336 [02:28<03:58, 3787.83 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 389000/1291336 [02:29<03:47, 3967.26 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 390000/1291336 [02:29<04:51, 3092.39 examples/s]Running tokenizer on dataset (num_proc=20):  30%|███       | 392000/1291336 [02:30<04:35, 3260.26 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 394000/1291336 [02:30<04:16, 3495.87 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 395000/1291336 [02:31<07:05, 2108.48 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 396000/1291336 [02:31<05:54, 2522.46 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 398000/1291336 [02:32<04:25, 3359.51 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 399000/1291336 [02:32<05:37, 2644.14 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 400000/1291336 [02:33<04:42, 3158.85 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 401000/1291336 [02:34<07:13, 2053.21 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███       | 402000/1291336 [02:34<06:00, 2465.36 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███▏      | 404000/1291336 [02:35<06:06, 2418.75 examples/s]Running tokenizer on dataset (num_proc=20):  31%|███▏      | 406000/1291336 [02:37<09:28, 1556.90 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 407000/1291336 [02:37<08:42, 1693.83 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 409000/1291336 [02:37<06:11, 2376.34 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 410000/1291336 [02:37<05:25, 2711.20 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 411000/1291336 [02:38<05:25, 2703.32 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 412000/1291336 [02:38<05:38, 2596.54 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 414000/1291336 [02:39<05:23, 2715.22 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 416000/1291336 [02:39<03:48, 3826.98 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 417000/1291336 [02:39<04:07, 3536.40 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 418000/1291336 [02:40<05:08, 2827.92 examples/s]Running tokenizer on dataset (num_proc=20):  32%|███▏      | 419000/1291336 [02:41<06:27, 2252.60 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 420000/1291336 [02:41<05:14, 2771.22 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 422000/1291336 [02:42<04:57, 2924.67 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 423000/1291336 [02:42<05:32, 2614.12 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 425000/1291336 [02:42<04:12, 3429.31 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 427000/1291336 [02:43<05:09, 2795.63 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 429000/1291336 [02:44<05:55, 2426.55 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 430000/1291336 [02:44<05:04, 2829.37 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 431000/1291336 [02:46<07:22, 1944.26 examples/s]Running tokenizer on dataset (num_proc=20):  33%|███▎      | 432000/1291336 [02:46<08:15, 1733.39 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 434000/1291336 [02:47<06:48, 2098.79 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▎      | 435000/1291336 [02:48<06:58, 2048.41 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 436000/1291336 [02:48<05:58, 2388.64 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 438000/1291336 [02:48<04:36, 3091.65 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 439000/1291336 [02:48<04:11, 3389.56 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 440000/1291336 [02:49<04:24, 3214.68 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 441000/1291336 [02:50<06:38, 2136.15 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 443000/1291336 [02:50<04:30, 3136.11 examples/s]Running tokenizer on dataset (num_proc=20):  34%|███▍      | 445000/1291336 [02:51<04:41, 3006.31 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 446000/1291336 [02:51<04:24, 3190.46 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 447000/1291336 [02:51<03:55, 3589.41 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 448000/1291336 [02:52<07:02, 1995.91 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 449000/1291336 [02:52<06:21, 2210.63 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▍      | 450000/1291336 [02:53<06:56, 2021.15 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 452000/1291336 [02:53<04:24, 3168.06 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 453000/1291336 [02:54<06:12, 2252.85 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 454000/1291336 [02:54<05:11, 2686.62 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 455000/1291336 [02:55<05:05, 2740.16 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 456000/1291336 [02:55<05:31, 2518.55 examples/s]Running tokenizer on dataset (num_proc=20):  35%|███▌      | 457000/1291336 [02:55<04:32, 3059.89 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 459000/1291336 [02:56<05:38, 2462.53 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 460000/1291336 [02:57<05:40, 2438.48 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 461000/1291336 [02:57<04:53, 2827.96 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 462000/1291336 [02:58<08:03, 1715.22 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 463000/1291336 [02:58<06:59, 1975.36 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 464000/1291336 [02:59<06:22, 2164.95 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 465000/1291336 [02:59<05:33, 2479.24 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 466000/1291336 [02:59<04:50, 2839.80 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 467000/1291336 [03:00<05:08, 2673.60 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▌      | 468000/1291336 [03:00<05:20, 2572.38 examples/s]Running tokenizer on dataset (num_proc=20):  36%|███▋      | 469000/1291336 [03:01<06:39, 2058.55 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 472000/1291336 [03:01<03:39, 3728.44 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 473000/1291336 [03:01<04:01, 3384.97 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 474000/1291336 [03:02<04:23, 3103.18 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 476000/1291336 [03:02<03:17, 4121.29 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 477000/1291336 [03:03<05:14, 2587.67 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 478000/1291336 [03:03<05:12, 2602.41 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 479000/1291336 [03:04<05:33, 2438.04 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 481000/1291336 [03:04<03:51, 3502.28 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 482000/1291336 [03:05<05:35, 2413.80 examples/s]Running tokenizer on dataset (num_proc=20):  37%|███▋      | 484000/1291336 [03:06<06:28, 2080.66 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 485000/1291336 [03:06<06:06, 2198.31 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 487000/1291336 [03:07<05:52, 2284.27 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 488000/1291336 [03:07<04:57, 2700.85 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 490000/1291336 [03:08<05:24, 2468.29 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 491000/1291336 [03:09<05:32, 2404.42 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 492000/1291336 [03:10<06:47, 1962.79 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 495000/1291336 [03:10<04:18, 3085.96 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 496000/1291336 [03:11<05:14, 2527.24 examples/s]Running tokenizer on dataset (num_proc=20):  38%|███▊      | 497000/1291336 [03:11<04:47, 2759.62 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▊      | 498000/1291336 [03:11<05:22, 2458.19 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▊      | 500000/1291336 [03:12<04:09, 3175.41 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 501000/1291336 [03:12<04:33, 2893.86 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 502000/1291336 [03:13<05:07, 2568.94 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 503000/1291336 [03:13<04:42, 2790.47 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 504000/1291336 [03:13<04:14, 3096.73 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 505000/1291336 [03:14<05:27, 2404.45 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 506000/1291336 [03:15<05:57, 2199.70 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 507000/1291336 [03:15<05:37, 2326.02 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 508000/1291336 [03:15<06:06, 2138.90 examples/s]Running tokenizer on dataset (num_proc=20):  39%|███▉      | 509000/1291336 [03:16<06:25, 2028.69 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 512000/1291336 [03:16<03:59, 3257.55 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 513000/1291336 [03:17<04:05, 3167.81 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 514000/1291336 [03:17<04:33, 2840.82 examples/s]Running tokenizer on dataset (num_proc=20):  40%|███▉      | 515000/1291336 [03:18<04:29, 2877.72 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 517000/1291336 [03:18<03:18, 3906.21 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 518000/1291336 [03:18<03:14, 3970.93 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 519000/1291336 [03:20<09:17, 1384.88 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 520000/1291336 [03:21<08:55, 1440.87 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 521000/1291336 [03:21<07:14, 1772.52 examples/s]Running tokenizer on dataset (num_proc=20):  40%|████      | 522000/1291336 [03:22<07:26, 1722.27 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 523000/1291336 [03:22<05:59, 2138.67 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 524000/1291336 [03:22<05:32, 2308.26 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 526000/1291336 [03:23<04:25, 2887.81 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 527000/1291336 [03:23<03:55, 3249.39 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 528000/1291336 [03:23<03:19, 3821.82 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 529000/1291336 [03:24<04:21, 2917.53 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 530000/1291336 [03:24<03:50, 3304.56 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████      | 532000/1291336 [03:26<06:59, 1812.21 examples/s]Running tokenizer on dataset (num_proc=20):  41%|████▏     | 535000/1291336 [03:26<03:53, 3235.85 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 537000/1291336 [03:27<05:03, 2487.21 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 538000/1291336 [03:27<05:18, 2367.43 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 540000/1291336 [03:28<04:10, 3004.79 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 541000/1291336 [03:29<05:20, 2342.84 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 543000/1291336 [03:29<04:39, 2676.24 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 545000/1291336 [03:29<03:43, 3344.18 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 546000/1291336 [03:30<05:11, 2393.22 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 547000/1291336 [03:30<04:26, 2796.31 examples/s]Running tokenizer on dataset (num_proc=20):  42%|████▏     | 548000/1291336 [03:31<05:31, 2240.83 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 551000/1291336 [03:33<05:30, 2237.02 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 552000/1291336 [03:33<05:26, 2264.12 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 553000/1291336 [03:33<04:33, 2699.31 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 554000/1291336 [03:34<04:56, 2485.60 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 555000/1291336 [03:34<05:15, 2333.60 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 556000/1291336 [03:34<04:49, 2541.74 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 557000/1291336 [03:35<04:00, 3058.99 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 558000/1291336 [03:35<05:09, 2369.57 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 559000/1291336 [03:36<04:47, 2544.87 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 560000/1291336 [03:36<04:07, 2956.27 examples/s]Running tokenizer on dataset (num_proc=20):  43%|████▎     | 561000/1291336 [03:36<03:47, 3212.99 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▎     | 562000/1291336 [03:36<04:13, 2873.76 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▎     | 563000/1291336 [03:37<03:58, 3052.15 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▎     | 564000/1291336 [03:37<03:30, 3459.28 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 565000/1291336 [03:38<06:17, 1921.53 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 566000/1291336 [03:38<05:36, 2158.20 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 568000/1291336 [03:38<03:29, 3451.08 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 569000/1291336 [03:39<05:21, 2249.21 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 570000/1291336 [03:40<04:24, 2724.42 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 572000/1291336 [03:40<04:38, 2579.76 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 573000/1291336 [03:41<05:17, 2265.64 examples/s]Running tokenizer on dataset (num_proc=20):  44%|████▍     | 574000/1291336 [03:41<05:21, 2230.89 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 575000/1291336 [03:42<05:28, 2178.41 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 576000/1291336 [03:42<04:49, 2470.36 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 577000/1291336 [03:43<06:28, 1840.00 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 578000/1291336 [03:44<06:10, 1927.81 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 579000/1291336 [03:44<05:04, 2340.97 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▍     | 581000/1291336 [03:44<03:49, 3088.57 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 582000/1291336 [03:44<03:34, 3304.71 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 584000/1291336 [03:45<02:23, 4926.17 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 586000/1291336 [03:45<02:08, 5509.65 examples/s]Running tokenizer on dataset (num_proc=20):  45%|████▌     | 587000/1291336 [03:45<02:00, 5858.36 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 588000/1291336 [03:45<02:48, 4177.87 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 589000/1291336 [03:47<05:26, 2153.55 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 590000/1291336 [03:47<05:11, 2250.74 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 591000/1291336 [03:47<04:36, 2529.79 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 593000/1291336 [03:48<03:53, 2985.29 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 594000/1291336 [03:49<06:02, 1923.95 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 595000/1291336 [03:49<05:28, 2120.71 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▌     | 597000/1291336 [03:50<04:52, 2371.36 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 598000/1291336 [03:50<04:29, 2571.29 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 599000/1291336 [03:51<06:13, 1852.04 examples/s]Running tokenizer on dataset (num_proc=20):  46%|████▋     | 600000/1291336 [03:52<06:35, 1746.27 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 601000/1291336 [03:52<05:25, 2118.32 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 602000/1291336 [03:52<04:58, 2307.78 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 603000/1291336 [03:53<04:51, 2362.18 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 604000/1291336 [03:53<05:09, 2217.97 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 606000/1291336 [03:53<03:24, 3348.81 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 607000/1291336 [03:54<03:31, 3242.14 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 608000/1291336 [03:54<03:08, 3629.16 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 609000/1291336 [03:54<02:39, 4275.97 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 610000/1291336 [03:54<02:40, 4254.45 examples/s]Running tokenizer on dataset (num_proc=20):  47%|████▋     | 611000/1291336 [03:55<05:33, 2040.24 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 614000/1291336 [03:56<04:24, 2557.12 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 615000/1291336 [03:57<04:37, 2433.40 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 617000/1291336 [03:57<03:28, 3228.81 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 618000/1291336 [03:58<04:40, 2398.29 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 619000/1291336 [03:58<03:54, 2871.33 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 620000/1291336 [03:58<03:29, 3203.71 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 621000/1291336 [03:59<03:41, 3032.87 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 623000/1291336 [04:00<04:25, 2519.97 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 624000/1291336 [04:01<06:27, 1722.84 examples/s]Running tokenizer on dataset (num_proc=20):  48%|████▊     | 625000/1291336 [04:01<05:37, 1973.33 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▊     | 627000/1291336 [04:02<05:00, 2207.33 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▊     | 628000/1291336 [04:02<05:04, 2180.85 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▊     | 629000/1291336 [04:02<04:05, 2694.83 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 630000/1291336 [04:03<03:56, 2793.81 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 631000/1291336 [04:03<03:55, 2806.81 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 633000/1291336 [04:04<03:29, 3136.28 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 634000/1291336 [04:04<03:05, 3552.67 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 635000/1291336 [04:04<02:47, 3915.88 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 636000/1291336 [04:04<03:20, 3270.33 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 637000/1291336 [04:05<02:51, 3809.51 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 638000/1291336 [04:05<04:35, 2370.15 examples/s]Running tokenizer on dataset (num_proc=20):  49%|████▉     | 639000/1291336 [04:06<06:01, 1805.22 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 640000/1291336 [04:07<06:06, 1778.75 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 641000/1291336 [04:07<05:01, 2159.47 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 643000/1291336 [04:08<05:02, 2145.12 examples/s]Running tokenizer on dataset (num_proc=20):  50%|████▉     | 644000/1291336 [04:08<04:34, 2362.31 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 647000/1291336 [04:09<02:26, 4391.02 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 648000/1291336 [04:09<02:27, 4365.83 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 649000/1291336 [04:09<03:29, 3068.86 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 650000/1291336 [04:10<03:43, 2875.61 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 651000/1291336 [04:11<05:44, 1857.72 examples/s]Running tokenizer on dataset (num_proc=20):  50%|█████     | 652000/1291336 [04:11<04:53, 2174.79 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 653000/1291336 [04:11<04:15, 2494.08 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 654000/1291336 [04:12<04:14, 2505.55 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 655000/1291336 [04:12<03:21, 3153.03 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 656000/1291336 [04:13<06:07, 1731.14 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 657000/1291336 [04:14<05:44, 1838.67 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 658000/1291336 [04:14<04:50, 2178.86 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████     | 661000/1291336 [04:14<02:37, 4007.52 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 662000/1291336 [04:15<03:53, 2690.97 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 664000/1291336 [04:15<02:49, 3692.06 examples/s]Running tokenizer on dataset (num_proc=20):  51%|█████▏    | 665000/1291336 [04:16<04:13, 2472.86 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 666000/1291336 [04:16<03:43, 2796.79 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 668000/1291336 [04:16<02:29, 4161.44 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 669000/1291336 [04:17<03:34, 2900.59 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 670000/1291336 [04:18<03:46, 2741.27 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 672000/1291336 [04:19<04:58, 2071.49 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 673000/1291336 [04:19<04:44, 2171.20 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 674000/1291336 [04:19<04:11, 2453.83 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 675000/1291336 [04:20<05:19, 1930.04 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 676000/1291336 [04:20<04:20, 2362.42 examples/s]Running tokenizer on dataset (num_proc=20):  52%|█████▏    | 677000/1291336 [04:21<03:46, 2715.54 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 678000/1291336 [04:21<03:16, 3124.34 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 679000/1291336 [04:22<04:33, 2237.50 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 681000/1291336 [04:23<04:45, 2136.84 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 682000/1291336 [04:23<04:13, 2400.10 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 684000/1291336 [04:23<02:49, 3593.01 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 685000/1291336 [04:24<03:16, 3081.65 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 686000/1291336 [04:24<03:03, 3295.53 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 687000/1291336 [04:24<03:24, 2961.01 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 688000/1291336 [04:25<04:55, 2040.53 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 689000/1291336 [04:26<06:13, 1613.97 examples/s]Running tokenizer on dataset (num_proc=20):  53%|█████▎    | 690000/1291336 [04:26<05:18, 1889.42 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 691000/1291336 [04:27<04:13, 2372.40 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 692000/1291336 [04:27<03:37, 2759.50 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▎    | 693000/1291336 [04:27<04:01, 2479.21 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 695000/1291336 [04:28<03:16, 3038.57 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 696000/1291336 [04:28<03:27, 2865.28 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 697000/1291336 [04:28<02:58, 3334.75 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 698000/1291336 [04:29<02:43, 3638.24 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 699000/1291336 [04:30<04:46, 2069.87 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 701000/1291336 [04:30<03:01, 3255.95 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 702000/1291336 [04:30<03:23, 2902.86 examples/s]Running tokenizer on dataset (num_proc=20):  54%|█████▍    | 703000/1291336 [04:30<02:55, 3360.07 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 705000/1291336 [04:31<03:02, 3205.67 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 706000/1291336 [04:31<03:24, 2862.69 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 707000/1291336 [04:32<03:47, 2573.00 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 708000/1291336 [04:33<04:42, 2062.22 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 709000/1291336 [04:33<04:09, 2335.91 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▍    | 710000/1291336 [04:33<03:34, 2715.04 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 711000/1291336 [04:34<04:13, 2290.43 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 712000/1291336 [04:35<05:02, 1917.77 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 714000/1291336 [04:35<02:59, 3218.11 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 715000/1291336 [04:36<05:48, 1653.92 examples/s]Running tokenizer on dataset (num_proc=20):  55%|█████▌    | 716000/1291336 [04:37<05:55, 1619.57 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 717000/1291336 [04:37<05:03, 1895.06 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 718000/1291336 [04:37<04:14, 2251.33 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 719000/1291336 [04:38<04:57, 1924.83 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 720000/1291336 [04:38<04:09, 2288.73 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 721000/1291336 [04:39<04:48, 1975.68 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 722000/1291336 [04:39<03:43, 2546.35 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 724000/1291336 [04:39<02:22, 3973.87 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▌    | 725000/1291336 [04:40<03:53, 2426.71 examples/s]Running tokenizer on dataset (num_proc=20):  56%|█████▋    | 729000/1291336 [04:41<02:11, 4285.60 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 730000/1291336 [04:41<02:06, 4438.02 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 731000/1291336 [04:41<02:34, 3620.93 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 732000/1291336 [04:42<04:22, 2133.45 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 733000/1291336 [04:43<05:46, 1611.23 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 734000/1291336 [04:44<05:11, 1790.76 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 735000/1291336 [04:44<04:51, 1910.67 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 736000/1291336 [04:44<03:47, 2437.13 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 737000/1291336 [04:45<03:37, 2554.50 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 738000/1291336 [04:45<03:36, 2557.16 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 740000/1291336 [04:45<02:13, 4131.24 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 741000/1291336 [04:45<02:02, 4508.98 examples/s]Running tokenizer on dataset (num_proc=20):  57%|█████▋    | 742000/1291336 [04:46<01:56, 4711.88 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 743000/1291336 [04:46<03:10, 2882.31 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 744000/1291336 [04:47<03:41, 2475.34 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 746000/1291336 [04:48<04:11, 2166.71 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 747000/1291336 [04:49<04:55, 1839.37 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 748000/1291336 [04:49<04:32, 1992.36 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 749000/1291336 [04:49<03:42, 2440.32 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 750000/1291336 [04:49<03:04, 2937.40 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 751000/1291336 [04:50<02:30, 3582.39 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 752000/1291336 [04:50<03:23, 2651.95 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 753000/1291336 [04:50<03:04, 2916.43 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 754000/1291336 [04:51<02:31, 3545.59 examples/s]Running tokenizer on dataset (num_proc=20):  58%|█████▊    | 755000/1291336 [04:51<03:16, 2725.90 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 756000/1291336 [04:52<03:27, 2574.23 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▊    | 757000/1291336 [04:52<04:31, 1965.38 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 759000/1291336 [04:53<03:51, 2294.85 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 760000/1291336 [04:53<03:36, 2453.36 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 761000/1291336 [04:54<04:43, 1871.02 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 762000/1291336 [04:55<04:00, 2201.88 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 763000/1291336 [04:55<03:29, 2526.77 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 764000/1291336 [04:55<02:45, 3195.75 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 766000/1291336 [04:55<01:52, 4685.44 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 767000/1291336 [04:56<02:44, 3194.09 examples/s]Running tokenizer on dataset (num_proc=20):  59%|█████▉    | 768000/1291336 [04:56<02:36, 3353.69 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 769000/1291336 [04:56<02:14, 3885.55 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 770000/1291336 [04:57<03:02, 2854.50 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 771000/1291336 [04:58<04:20, 1998.13 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 772000/1291336 [04:58<03:45, 2302.76 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 773000/1291336 [04:58<03:11, 2703.39 examples/s]Running tokenizer on dataset (num_proc=20):  60%|█████▉    | 774000/1291336 [04:58<02:43, 3168.63 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 775000/1291336 [04:59<04:15, 2017.64 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 776000/1291336 [04:59<03:35, 2390.45 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 777000/1291336 [05:00<03:38, 2349.31 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 778000/1291336 [05:00<03:30, 2439.06 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 779000/1291336 [05:00<02:56, 2898.30 examples/s]Running tokenizer on dataset (num_proc=20):  60%|██████    | 781000/1291336 [05:01<03:24, 2494.17 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 782000/1291336 [05:02<03:17, 2578.15 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 783000/1291336 [05:02<02:51, 2967.96 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 784000/1291336 [05:02<03:11, 2652.98 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 785000/1291336 [05:03<04:41, 1796.51 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 786000/1291336 [05:04<03:48, 2215.18 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 787000/1291336 [05:04<03:21, 2504.87 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████    | 789000/1291336 [05:05<04:13, 1978.60 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 791000/1291336 [05:05<03:05, 2700.77 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 792000/1291336 [05:06<02:47, 2978.72 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 793000/1291336 [05:06<02:36, 3194.33 examples/s]Running tokenizer on dataset (num_proc=20):  61%|██████▏   | 794000/1291336 [05:07<03:40, 2257.53 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 795000/1291336 [05:07<04:08, 1999.43 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 796000/1291336 [05:08<03:25, 2411.85 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 797000/1291336 [05:08<04:19, 1904.45 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 798000/1291336 [05:09<03:45, 2186.75 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 799000/1291336 [05:09<03:10, 2580.76 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 800000/1291336 [05:09<02:41, 3037.74 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 801000/1291336 [05:09<02:11, 3735.73 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 802000/1291336 [05:09<01:51, 4377.51 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 803000/1291336 [05:10<02:14, 3631.54 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 804000/1291336 [05:10<03:11, 2540.29 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 806000/1291336 [05:11<02:00, 4039.44 examples/s]Running tokenizer on dataset (num_proc=20):  62%|██████▏   | 807000/1291336 [05:11<02:19, 3481.43 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 808000/1291336 [05:11<02:07, 3789.12 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 809000/1291336 [05:11<02:11, 3668.46 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 810000/1291336 [05:12<03:05, 2601.69 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 811000/1291336 [05:13<03:58, 2017.81 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 812000/1291336 [05:13<03:05, 2586.30 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 813000/1291336 [05:13<02:31, 3157.15 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 814000/1291336 [05:14<04:08, 1917.81 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 815000/1291336 [05:15<03:57, 2006.50 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 817000/1291336 [05:15<02:21, 3352.68 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 818000/1291336 [05:16<03:57, 1993.34 examples/s]Running tokenizer on dataset (num_proc=20):  63%|██████▎   | 819000/1291336 [05:16<03:07, 2517.65 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 820000/1291336 [05:16<02:37, 2984.30 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 821000/1291336 [05:16<02:44, 2867.49 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 822000/1291336 [05:17<03:25, 2287.56 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▎   | 823000/1291336 [05:18<03:14, 2408.79 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 824000/1291336 [05:18<03:51, 2014.92 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 825000/1291336 [05:19<04:35, 1694.61 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 826000/1291336 [05:20<04:30, 1717.97 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 828000/1291336 [05:20<03:24, 2269.12 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 829000/1291336 [05:20<02:53, 2664.61 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 830000/1291336 [05:21<02:58, 2579.30 examples/s]Running tokenizer on dataset (num_proc=20):  64%|██████▍   | 832000/1291336 [05:21<02:28, 3097.54 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 834000/1291336 [05:22<02:21, 3230.19 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 835000/1291336 [05:22<02:24, 3153.73 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 836000/1291336 [05:23<03:31, 2155.43 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 837000/1291336 [05:23<03:11, 2366.75 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 838000/1291336 [05:23<02:34, 2939.80 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▍   | 839000/1291336 [05:24<02:37, 2866.98 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 840000/1291336 [05:24<02:35, 2897.79 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 842000/1291336 [05:25<02:27, 3040.78 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 843000/1291336 [05:25<02:17, 3252.92 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 844000/1291336 [05:25<02:29, 3000.91 examples/s]Running tokenizer on dataset (num_proc=20):  65%|██████▌   | 845000/1291336 [05:26<02:15, 3301.36 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 846000/1291336 [05:26<02:41, 2751.38 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 847000/1291336 [05:27<03:02, 2434.54 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 848000/1291336 [05:27<03:35, 2058.63 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 849000/1291336 [05:28<04:33, 1618.05 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 850000/1291336 [05:29<04:17, 1712.12 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 851000/1291336 [05:29<03:23, 2168.01 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 852000/1291336 [05:30<03:57, 1848.51 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▌   | 853000/1291336 [05:30<03:52, 1886.65 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 856000/1291336 [05:31<02:20, 3087.84 examples/s]Running tokenizer on dataset (num_proc=20):  66%|██████▋   | 857000/1291336 [05:31<02:30, 2880.32 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 859000/1291336 [05:32<02:10, 3305.89 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 860000/1291336 [05:33<03:07, 2295.12 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 861000/1291336 [05:33<02:35, 2769.99 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 862000/1291336 [05:33<02:23, 2991.30 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 866000/1291336 [05:34<01:50, 3859.78 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 867000/1291336 [05:34<02:17, 3087.52 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 868000/1291336 [05:36<03:46, 1870.17 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 870000/1291336 [05:36<02:53, 2427.61 examples/s]Running tokenizer on dataset (num_proc=20):  67%|██████▋   | 871000/1291336 [05:37<03:04, 2273.12 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 872000/1291336 [05:38<04:00, 1743.86 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 873000/1291336 [05:38<03:29, 1997.24 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 874000/1291336 [05:38<03:09, 2203.13 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 877000/1291336 [05:39<01:57, 3536.21 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 878000/1291336 [05:39<01:56, 3544.03 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 879000/1291336 [05:40<03:38, 1883.41 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 880000/1291336 [05:41<03:28, 1971.28 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 881000/1291336 [05:41<03:21, 2036.08 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 883000/1291336 [05:41<02:10, 3123.06 examples/s]Running tokenizer on dataset (num_proc=20):  68%|██████▊   | 884000/1291336 [05:42<02:06, 3231.56 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 885000/1291336 [05:42<01:57, 3449.20 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 886000/1291336 [05:42<01:52, 3596.97 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▊   | 887000/1291336 [05:43<02:15, 2982.37 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 889000/1291336 [05:43<01:46, 3773.36 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 890000/1291336 [05:43<01:38, 4083.83 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 891000/1291336 [05:43<01:38, 4048.19 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 892000/1291336 [05:44<01:42, 3897.94 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 893000/1291336 [05:44<01:49, 3649.46 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 894000/1291336 [05:44<01:41, 3919.75 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 895000/1291336 [05:47<05:33, 1188.10 examples/s]Running tokenizer on dataset (num_proc=20):  69%|██████▉   | 896000/1291336 [05:47<05:16, 1247.27 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 898000/1291336 [05:47<03:10, 2061.72 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 899000/1291336 [05:48<03:05, 2118.66 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 901000/1291336 [05:48<02:06, 3083.88 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 902000/1291336 [05:48<02:03, 3162.82 examples/s]Running tokenizer on dataset (num_proc=20):  70%|██████▉   | 903000/1291336 [05:49<02:48, 2308.97 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 904000/1291336 [05:50<02:40, 2407.74 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 905000/1291336 [05:51<04:03, 1588.32 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 906000/1291336 [05:51<03:17, 1947.97 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 907000/1291336 [05:52<03:34, 1789.24 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 908000/1291336 [05:52<02:50, 2250.28 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 909000/1291336 [05:52<02:34, 2467.96 examples/s]Running tokenizer on dataset (num_proc=20):  70%|███████   | 910000/1291336 [05:53<02:33, 2486.03 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 911000/1291336 [05:53<02:14, 2835.11 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 912000/1291336 [05:53<01:47, 3518.16 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 916000/1291336 [05:53<00:55, 6742.14 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 917000/1291336 [05:54<01:08, 5499.49 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 918000/1291336 [05:54<01:22, 4531.31 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 919000/1291336 [05:55<02:25, 2556.57 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████   | 920000/1291336 [05:56<04:00, 1543.64 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████▏  | 921000/1291336 [05:57<03:28, 1773.81 examples/s]Running tokenizer on dataset (num_proc=20):  71%|███████▏  | 923000/1291336 [05:57<02:25, 2527.06 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 924000/1291336 [05:57<02:17, 2668.01 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 926000/1291336 [05:58<02:26, 2492.93 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 927000/1291336 [05:59<02:27, 2465.27 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 928000/1291336 [05:59<02:28, 2440.67 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 929000/1291336 [05:59<02:09, 2796.37 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 930000/1291336 [05:59<01:56, 3092.52 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 931000/1291336 [06:00<02:33, 2346.24 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 932000/1291336 [06:00<02:23, 2512.67 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 933000/1291336 [06:01<02:01, 2948.81 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 934000/1291336 [06:02<03:59, 1492.70 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 935000/1291336 [06:02<03:07, 1898.65 examples/s]Running tokenizer on dataset (num_proc=20):  72%|███████▏  | 936000/1291336 [06:02<02:32, 2322.48 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 937000/1291336 [06:03<02:01, 2919.05 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 938000/1291336 [06:03<01:54, 3098.24 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 939000/1291336 [06:03<01:48, 3254.39 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 940000/1291336 [06:04<02:11, 2662.15 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 941567/1291336 [06:04<01:32, 3773.78 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 942567/1291336 [06:05<02:38, 2197.21 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 943567/1291336 [06:05<02:42, 2137.48 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 945567/1291336 [06:05<01:42, 3362.74 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 946567/1291336 [06:06<01:34, 3650.50 examples/s]Running tokenizer on dataset (num_proc=20):  73%|███████▎  | 948567/1291336 [06:06<01:45, 3242.59 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 949567/1291336 [06:07<02:39, 2145.67 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 950567/1291336 [06:08<03:17, 1727.79 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 951134/1291336 [06:09<03:06, 1826.05 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▎  | 952134/1291336 [06:09<02:41, 2103.44 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 953134/1291336 [06:09<02:06, 2674.30 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 955134/1291336 [06:09<01:21, 4115.05 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 957134/1291336 [06:10<01:47, 3114.30 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 959134/1291336 [06:11<01:38, 3385.50 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 960134/1291336 [06:11<01:41, 3253.67 examples/s]Running tokenizer on dataset (num_proc=20):  74%|███████▍  | 961134/1291336 [06:11<01:42, 3210.94 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 962134/1291336 [06:12<01:47, 3054.74 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 963134/1291336 [06:12<01:29, 3654.05 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 964701/1291336 [06:13<02:34, 2112.08 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 965701/1291336 [06:14<03:47, 1433.10 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 966701/1291336 [06:15<02:57, 1832.30 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▍  | 967701/1291336 [06:15<02:24, 2239.37 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 968701/1291336 [06:15<02:03, 2602.85 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 969701/1291336 [06:16<02:36, 2056.31 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 971701/1291336 [06:16<01:36, 3307.27 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 972701/1291336 [06:16<01:26, 3662.71 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 973701/1291336 [06:16<01:21, 3915.88 examples/s]Running tokenizer on dataset (num_proc=20):  75%|███████▌  | 974701/1291336 [06:17<01:56, 2708.48 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 975701/1291336 [06:17<01:44, 3032.22 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 976701/1291336 [06:17<01:32, 3404.40 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 977701/1291336 [06:19<04:02, 1290.89 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 980701/1291336 [06:19<02:00, 2585.76 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 981701/1291336 [06:20<02:16, 2273.98 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 982701/1291336 [06:20<01:54, 2694.56 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▌  | 983701/1291336 [06:21<02:15, 2268.56 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 984701/1291336 [06:21<02:03, 2481.32 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 985701/1291336 [06:21<01:40, 3050.11 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 986701/1291336 [06:22<01:54, 2670.83 examples/s]Running tokenizer on dataset (num_proc=20):  76%|███████▋  | 987701/1291336 [06:22<01:59, 2537.73 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 988701/1291336 [06:23<02:18, 2192.02 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 989701/1291336 [06:24<03:01, 1662.48 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 990701/1291336 [06:25<03:47, 1321.58 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 992701/1291336 [06:25<02:18, 2155.10 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 994701/1291336 [06:26<01:47, 2749.78 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 995701/1291336 [06:26<01:36, 3060.44 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 996701/1291336 [06:26<01:29, 3285.07 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 997701/1291336 [06:27<02:12, 2215.64 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 998701/1291336 [06:27<01:52, 2611.08 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 999701/1291336 [06:28<01:51, 2614.74 examples/s]Running tokenizer on dataset (num_proc=20):  77%|███████▋  | 1000701/1291336 [06:28<01:58, 2447.36 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1001701/1291336 [06:29<02:37, 1839.64 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1003701/1291336 [06:29<01:43, 2791.75 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1004268/1291336 [06:30<02:38, 1810.23 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1004835/1291336 [06:30<02:33, 1865.67 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1005835/1291336 [06:30<01:55, 2479.39 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1006835/1291336 [06:31<01:40, 2825.06 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1007835/1291336 [06:31<01:34, 2985.19 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1008835/1291336 [06:31<01:15, 3720.65 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1009835/1291336 [06:31<01:23, 3354.46 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1010835/1291336 [06:33<02:58, 1572.55 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1011835/1291336 [06:33<02:33, 1823.33 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1012402/1291336 [06:33<02:26, 1905.54 examples/s]Running tokenizer on dataset (num_proc=20):  78%|███████▊  | 1013402/1291336 [06:34<01:49, 2532.92 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▊  | 1014402/1291336 [06:34<01:46, 2611.50 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▊  | 1014969/1291336 [06:34<01:41, 2726.09 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▊  | 1015969/1291336 [06:35<02:46, 1655.62 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 1016969/1291336 [06:35<02:02, 2231.34 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 1017969/1291336 [06:36<02:30, 1813.01 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 1018969/1291336 [06:36<01:58, 2300.04 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 1019969/1291336 [06:37<02:21, 1916.63 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 1020969/1291336 [06:38<03:09, 1428.09 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 1021969/1291336 [06:39<02:54, 1546.01 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 1023969/1291336 [06:39<01:59, 2242.24 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 1024969/1291336 [06:40<02:26, 1815.54 examples/s]Running tokenizer on dataset (num_proc=20):  79%|███████▉  | 1025969/1291336 [06:41<02:31, 1753.17 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 1026969/1291336 [06:41<02:19, 1896.84 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 1027969/1291336 [06:41<02:16, 1933.89 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 1028969/1291336 [06:42<02:43, 1601.46 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 1029969/1291336 [06:43<02:21, 1846.46 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 1030969/1291336 [06:44<03:07, 1388.98 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 1031969/1291336 [06:44<02:40, 1613.03 examples/s]Running tokenizer on dataset (num_proc=20):  80%|███████▉  | 1032969/1291336 [06:45<02:47, 1544.38 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 1033969/1291336 [06:45<02:19, 1850.55 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 1035969/1291336 [06:46<01:57, 2165.51 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 1036969/1291336 [06:47<02:31, 1684.32 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 1037969/1291336 [06:48<03:18, 1273.52 examples/s]Running tokenizer on dataset (num_proc=20):  80%|████████  | 1038969/1291336 [06:48<02:39, 1579.86 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 1039969/1291336 [06:49<02:17, 1822.85 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 1041969/1291336 [06:49<01:44, 2393.50 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 1042969/1291336 [06:50<02:06, 1969.14 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 1043969/1291336 [06:50<01:41, 2426.37 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 1044969/1291336 [06:51<02:20, 1749.54 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 1045969/1291336 [06:52<02:28, 1652.49 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 1046969/1291336 [06:53<03:19, 1226.05 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 1047969/1291336 [06:54<02:42, 1495.66 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████  | 1048536/1291336 [06:54<02:47, 1453.64 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████▏ | 1050536/1291336 [06:54<01:36, 2497.14 examples/s]Running tokenizer on dataset (num_proc=20):  81%|████████▏ | 1051536/1291336 [06:54<01:25, 2808.29 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1052536/1291336 [06:56<02:29, 1595.74 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1053536/1291336 [06:56<02:33, 1553.57 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1054536/1291336 [06:57<02:09, 1835.41 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1055536/1291336 [06:58<02:24, 1637.35 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1056536/1291336 [06:58<02:13, 1761.78 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1057536/1291336 [06:59<02:36, 1493.11 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1058536/1291336 [06:59<02:07, 1831.94 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1059536/1291336 [07:00<02:26, 1577.35 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1061536/1291336 [07:02<02:57, 1296.71 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1062536/1291336 [07:02<02:38, 1445.50 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1063536/1291336 [07:03<02:28, 1533.87 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1064103/1291336 [07:03<02:12, 1708.52 examples/s]Running tokenizer on dataset (num_proc=20):  82%|████████▏ | 1065103/1291336 [07:04<03:04, 1224.37 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1067103/1291336 [07:05<02:13, 1676.03 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1068103/1291336 [07:06<02:29, 1489.78 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1069103/1291336 [07:06<02:12, 1680.40 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1070103/1291336 [07:07<01:56, 1891.78 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1071103/1291336 [07:07<01:48, 2025.30 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1071670/1291336 [07:07<01:50, 1981.97 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1072670/1291336 [07:08<02:05, 1747.92 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1073670/1291336 [07:10<03:28, 1042.52 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1074670/1291336 [07:11<03:16, 1103.58 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1075670/1291336 [07:11<02:34, 1391.43 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1076670/1291336 [07:12<03:10, 1126.16 examples/s]Running tokenizer on dataset (num_proc=20):  83%|████████▎ | 1077670/1291336 [07:13<02:46, 1286.08 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 1078670/1291336 [07:14<03:14, 1092.77 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 1079670/1291336 [07:14<02:39, 1327.26 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▎ | 1080670/1291336 [07:15<02:13, 1574.76 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 1081670/1291336 [07:16<02:34, 1360.81 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 1082670/1291336 [07:17<03:12, 1083.09 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 1083670/1291336 [07:18<02:53, 1194.01 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 1084670/1291336 [07:19<02:51, 1206.22 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 1085670/1291336 [07:19<02:25, 1409.84 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 1086670/1291336 [07:21<03:35, 950.95 examples/s] Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 1087670/1291336 [07:22<03:29, 970.75 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 1089670/1291336 [07:23<02:28, 1356.74 examples/s]Running tokenizer on dataset (num_proc=20):  84%|████████▍ | 1090670/1291336 [07:23<02:34, 1296.74 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 1091670/1291336 [07:24<01:59, 1664.23 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 1092670/1291336 [07:26<03:29, 946.86 examples/s] Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 1093670/1291336 [07:26<02:52, 1144.57 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 1095670/1291336 [07:27<01:49, 1785.61 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▍ | 1096670/1291336 [07:29<03:19, 977.46 examples/s] Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 1097670/1291336 [07:30<03:31, 913.70 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 1098670/1291336 [07:31<03:21, 956.79 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 1099670/1291336 [07:32<02:37, 1219.45 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 1100670/1291336 [07:32<02:22, 1338.95 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 1101670/1291336 [07:33<02:05, 1516.62 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 1102670/1291336 [07:34<02:21, 1332.85 examples/s]Running tokenizer on dataset (num_proc=20):  85%|████████▌ | 1103670/1291336 [07:34<02:27, 1274.88 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 1104670/1291336 [07:35<02:17, 1358.20 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 1105670/1291336 [07:35<01:46, 1738.16 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 1106670/1291336 [07:38<03:40, 836.28 examples/s] Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 1107670/1291336 [07:39<03:13, 949.26 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 1108670/1291336 [07:40<03:21, 907.25 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 1109670/1291336 [07:40<02:42, 1117.40 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 1110670/1291336 [07:40<02:05, 1443.14 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 1111670/1291336 [07:42<02:28, 1211.80 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 1112670/1291336 [07:42<01:53, 1573.72 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▌ | 1113670/1291336 [07:42<01:47, 1651.38 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▋ | 1114670/1291336 [07:43<01:43, 1714.46 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▋ | 1115670/1291336 [07:44<02:20, 1251.10 examples/s]Running tokenizer on dataset (num_proc=20):  86%|████████▋ | 1116670/1291336 [07:47<03:59, 730.65 examples/s] Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1117670/1291336 [07:47<02:55, 987.95 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1118670/1291336 [07:48<03:14, 888.90 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1120670/1291336 [07:49<01:50, 1547.16 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1121670/1291336 [07:50<02:08, 1317.43 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1122670/1291336 [07:51<02:11, 1282.17 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1123670/1291336 [07:51<01:54, 1460.03 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1124670/1291336 [07:53<02:35, 1075.14 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1125670/1291336 [07:53<02:01, 1358.36 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1126670/1291336 [07:55<03:29, 784.17 examples/s] Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1127670/1291336 [07:56<02:39, 1027.77 examples/s]Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1128670/1291336 [07:57<02:52, 941.84 examples/s] Running tokenizer on dataset (num_proc=20):  87%|████████▋ | 1129670/1291336 [07:57<02:25, 1114.69 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 1130670/1291336 [07:58<02:01, 1324.33 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 1131670/1291336 [07:59<02:32, 1046.72 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 1134670/1291336 [08:00<01:41, 1541.11 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 1135670/1291336 [08:02<02:16, 1137.24 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 1136670/1291336 [08:04<02:54, 885.13 examples/s] Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 1137670/1291336 [08:05<02:28, 1034.47 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 1138670/1291336 [08:06<02:29, 1022.22 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 1139670/1291336 [08:06<02:17, 1101.97 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 1141670/1291336 [08:06<01:20, 1858.56 examples/s]Running tokenizer on dataset (num_proc=20):  88%|████████▊ | 1142670/1291336 [08:08<02:01, 1225.42 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 1143670/1291336 [08:08<01:35, 1541.98 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 1144670/1291336 [08:09<01:50, 1321.80 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▊ | 1145670/1291336 [08:11<02:20, 1040.26 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 1146670/1291336 [08:12<02:24, 1002.50 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 1147670/1291336 [08:14<02:45, 870.47 examples/s] Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 1148670/1291336 [08:14<02:10, 1089.85 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 1149670/1291336 [08:15<02:01, 1169.12 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 1150670/1291336 [08:15<01:38, 1424.99 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 1151670/1291336 [08:15<01:13, 1911.19 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 1152670/1291336 [08:17<02:08, 1082.55 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 1154670/1291336 [08:18<01:31, 1493.27 examples/s]Running tokenizer on dataset (num_proc=20):  89%|████████▉ | 1155670/1291336 [08:20<02:25, 933.22 examples/s] Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 1156670/1291336 [08:20<02:04, 1079.31 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 1157670/1291336 [08:22<02:12, 1007.13 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 1158670/1291336 [08:22<01:55, 1150.68 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 1159670/1291336 [08:23<01:43, 1276.29 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 1160670/1291336 [08:24<01:52, 1161.60 examples/s]Running tokenizer on dataset (num_proc=20):  90%|████████▉ | 1161670/1291336 [08:24<01:31, 1424.71 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 1162670/1291336 [08:25<01:43, 1238.97 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 1163670/1291336 [08:25<01:18, 1635.14 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 1164670/1291336 [08:26<01:11, 1761.68 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 1165670/1291336 [08:28<02:30, 836.80 examples/s] Running tokenizer on dataset (num_proc=20):  90%|█████████ | 1166670/1291336 [08:29<01:56, 1065.59 examples/s]Running tokenizer on dataset (num_proc=20):  90%|█████████ | 1167670/1291336 [08:31<02:25, 848.85 examples/s] Running tokenizer on dataset (num_proc=20):  91%|█████████ | 1168670/1291336 [08:31<02:01, 1010.51 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 1169670/1291336 [08:32<01:42, 1182.26 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 1170670/1291336 [08:32<01:37, 1237.06 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 1171670/1291336 [08:33<01:23, 1431.73 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 1172670/1291336 [08:33<01:20, 1466.36 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 1173670/1291336 [08:34<01:32, 1269.03 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 1174670/1291336 [08:35<01:11, 1624.21 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 1175670/1291336 [08:37<02:03, 937.67 examples/s] Running tokenizer on dataset (num_proc=20):  91%|█████████ | 1176670/1291336 [08:37<01:42, 1123.12 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████ | 1177670/1291336 [08:39<01:59, 947.31 examples/s] Running tokenizer on dataset (num_proc=20):  91%|█████████▏| 1178670/1291336 [08:40<02:09, 873.24 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████▏| 1179670/1291336 [08:40<01:37, 1144.49 examples/s]Running tokenizer on dataset (num_proc=20):  91%|█████████▏| 1180670/1291336 [08:41<01:35, 1160.87 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1181670/1291336 [08:42<01:25, 1275.60 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1182670/1291336 [08:42<01:13, 1478.62 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1183670/1291336 [08:43<01:08, 1560.55 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1184670/1291336 [08:43<00:58, 1825.54 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1185670/1291336 [08:45<01:35, 1105.76 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1186670/1291336 [08:46<01:35, 1098.48 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1187670/1291336 [08:48<02:20, 738.93 examples/s] Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1189670/1291336 [08:48<01:18, 1301.47 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1190670/1291336 [08:50<01:34, 1060.22 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1191670/1291336 [08:50<01:17, 1278.20 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1192670/1291336 [08:51<01:12, 1363.94 examples/s]Running tokenizer on dataset (num_proc=20):  92%|█████████▏| 1193670/1291336 [08:51<00:58, 1656.50 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 1194670/1291336 [08:52<01:02, 1536.44 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 1195670/1291336 [08:53<01:23, 1147.81 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 1196670/1291336 [08:54<01:22, 1150.99 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 1197670/1291336 [08:57<02:09, 721.57 examples/s] Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 1198670/1291336 [08:57<01:46, 868.63 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 1200670/1291336 [08:58<01:04, 1414.52 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 1202670/1291336 [08:59<00:58, 1510.41 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 1203670/1291336 [09:00<01:07, 1300.66 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 1204670/1291336 [09:01<01:08, 1270.58 examples/s]Running tokenizer on dataset (num_proc=20):  93%|█████████▎| 1206670/1291336 [09:03<01:20, 1055.32 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▎| 1207670/1291336 [09:05<01:29, 936.96 examples/s] Running tokenizer on dataset (num_proc=20):  94%|█████████▎| 1208670/1291336 [09:05<01:22, 996.46 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▎| 1209670/1291336 [09:05<01:03, 1292.85 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 1210670/1291336 [09:06<00:48, 1650.79 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 1211670/1291336 [09:06<00:37, 2098.84 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 1212670/1291336 [09:07<00:59, 1329.52 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 1213670/1291336 [09:07<00:44, 1742.55 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 1214670/1291336 [09:09<01:07, 1137.06 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 1215670/1291336 [09:09<00:52, 1434.72 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 1216670/1291336 [09:11<01:13, 1012.24 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 1217670/1291336 [09:12<01:23, 883.85 examples/s] Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 1218670/1291336 [09:14<01:23, 871.25 examples/s]Running tokenizer on dataset (num_proc=20):  94%|█████████▍| 1219670/1291336 [09:14<00:59, 1196.84 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 1220670/1291336 [09:14<00:45, 1559.78 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 1221670/1291336 [09:14<00:34, 2020.40 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 1222670/1291336 [09:16<00:59, 1156.38 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 1223670/1291336 [09:17<01:02, 1076.85 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 1224670/1291336 [09:17<00:51, 1292.10 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 1225670/1291336 [09:18<00:55, 1188.79 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▍| 1226670/1291336 [09:20<01:07, 953.86 examples/s] Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 1227670/1291336 [09:21<01:05, 973.90 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 1228670/1291336 [09:21<00:54, 1159.48 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 1230670/1291336 [09:22<00:40, 1515.83 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 1231670/1291336 [09:22<00:32, 1825.61 examples/s]Running tokenizer on dataset (num_proc=20):  95%|█████████▌| 1232670/1291336 [09:24<00:52, 1117.50 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 1233670/1291336 [09:25<00:54, 1057.38 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 1234670/1291336 [09:26<00:50, 1129.70 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 1235670/1291336 [09:26<00:37, 1493.21 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 1236670/1291336 [09:28<00:54, 1000.65 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 1237670/1291336 [09:28<00:46, 1154.66 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 1238670/1291336 [09:30<00:49, 1071.51 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 1239670/1291336 [09:30<00:35, 1439.09 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 1240670/1291336 [09:30<00:34, 1461.00 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 1241670/1291336 [09:30<00:26, 1863.35 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▌| 1242670/1291336 [09:32<00:41, 1181.16 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 1243670/1291336 [09:33<00:47, 1000.68 examples/s]Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 1244670/1291336 [09:35<00:49, 945.47 examples/s] Running tokenizer on dataset (num_proc=20):  96%|█████████▋| 1245670/1291336 [09:35<00:40, 1132.32 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1246670/1291336 [09:36<00:40, 1106.29 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1247670/1291336 [09:37<00:34, 1280.54 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1248670/1291336 [09:37<00:26, 1597.95 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1249670/1291336 [09:38<00:30, 1377.16 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1250670/1291336 [09:39<00:30, 1342.09 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1251670/1291336 [09:39<00:27, 1463.32 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1252670/1291336 [09:41<00:35, 1088.85 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1253670/1291336 [09:43<00:46, 801.67 examples/s] Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1254670/1291336 [09:43<00:39, 922.05 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1255237/1291336 [09:44<00:36, 980.94 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1256237/1291336 [09:44<00:28, 1242.28 examples/s]Running tokenizer on dataset (num_proc=20):  97%|█████████▋| 1257237/1291336 [09:44<00:21, 1581.50 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1259237/1291336 [09:46<00:21, 1464.94 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1260237/1291336 [09:47<00:22, 1412.13 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1261237/1291336 [09:47<00:18, 1624.71 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1261803/1291336 [09:49<00:32, 917.33 examples/s] Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1262803/1291336 [09:49<00:24, 1187.67 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1263803/1291336 [09:51<00:34, 787.60 examples/s] Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1264803/1291336 [09:51<00:24, 1094.17 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1265803/1291336 [09:52<00:20, 1237.60 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1266803/1291336 [09:52<00:14, 1650.54 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1267370/1291336 [09:54<00:27, 884.63 examples/s] Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1268370/1291336 [09:54<00:20, 1141.96 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1269370/1291336 [09:55<00:16, 1347.16 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1270370/1291336 [09:56<00:18, 1151.32 examples/s]Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1271370/1291336 [09:59<00:30, 662.14 examples/s] Running tokenizer on dataset (num_proc=20):  98%|█████████▊| 1271937/1291336 [09:59<00:25, 775.25 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▊| 1272937/1291336 [09:59<00:17, 1057.55 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▊| 1273937/1291336 [10:00<00:12, 1377.31 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▊| 1274937/1291336 [10:00<00:09, 1699.57 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 1275937/1291336 [10:02<00:17, 905.15 examples/s] Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 1276937/1291336 [10:03<00:15, 939.62 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 1277937/1291336 [10:07<00:26, 496.47 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 1279937/1291336 [10:08<00:13, 846.81 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 1280937/1291336 [10:09<00:11, 909.10 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 1281937/1291336 [10:10<00:11, 826.36 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 1282937/1291336 [10:11<00:10, 813.28 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 1283504/1291336 [10:12<00:09, 860.58 examples/s]Running tokenizer on dataset (num_proc=20):  99%|█████████▉| 1284071/1291336 [10:13<00:09, 746.93 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 1285071/1291336 [10:15<00:10, 598.07 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 1286071/1291336 [10:16<00:07, 729.85 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 1287071/1291336 [10:19<00:07, 596.15 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 1288071/1291336 [10:20<00:05, 640.75 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 1288637/1291336 [10:20<00:03, 745.21 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 1289637/1291336 [10:22<00:02, 621.86 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 1290203/1291336 [10:23<00:01, 603.44 examples/s]Running tokenizer on dataset (num_proc=20): 100%|█████████▉| 1290769/1291336 [10:24<00:00, 637.17 examples/s]Running tokenizer on dataset (num_proc=20): 100%|██████████| 1291336/1291336 [10:26<00:00, 458.43 examples/s]Running tokenizer on dataset (num_proc=20): 100%|██████████| 1291336/1291336 [10:27<00:00, 2057.54 examples/s]
Concatenating 20 shards
input_ids:
[29871, 30287, 31450, 235, 192, 169, 30888, 30214, 31530, 232, 179, 191, 233, 140, 155, 31885, 31600, 30346, 30533, 235, 192, 169, 30214, 29896, 29889, 29947, 232, 144, 138, 29902, 29899, 29946, 30910, 30846, 31429, 30214, 232, 187, 169, 31462, 31859, 234, 177, 180, 30210, 29896, 31859, 15633, 29911, 30214, 231, 192, 145, 231, 190, 186, 30544, 232, 151, 177, 30584, 29871, 30888, 30698, 31141, 232, 193, 132, 30383, 29899, 30666, 234, 134, 176, 30607, 30658, 233, 145, 149, 31780, 233, 167, 136, 29899, 31352, 236, 149, 168, 232, 143, 156, 31174, 30752, 29899, 235, 150, 160, 234, 140, 156, 31903, 31092, 29899, 31676, 30815, 30872, 232, 167, 138, 30893, 30494, 29899, 235, 194, 159, 31101, 232, 147, 178, 30846, 29899, 31679, 30846, 30408, 234, 173, 154, 29899, 232, 171, 180, 31616, 30689, 31021, 31542, 30858, 29899, 30486, 31613, 31382, 30607, 317, 5098, 4330, 29979, 847, 27827, 29899, 1164, 29903, 30383, 29899, 232, 131, 149, 235, 192, 169, 233, 148, 135, 31551, 31584, 29899, 232, 139, 188, 235, 192, 169, 235, 193, 136, 31931, 29899, 31679, 30846, 31631, 31649, 29899, 232, 135, 194, 234, 174, 168, 30670, 30753, 236, 151, 132, 13, 30744, 30417, 235, 192, 169, 235, 193, 137, 232, 160, 138, 30768, 31138, 21576, 30419, 30768, 30406, 233, 180, 192, 235, 192, 169, 30409, 29896, 29945, 29945, 31888, 29871, 31439, 235, 178, 132, 233, 166, 131, 31851, 30214, 31666, 231, 187, 151, 232, 136, 144, 235, 183, 188, 31302, 231, 193, 158, 29945, 29900, 29900, 29900, 8848, 31391, 29929, 29900, 30408, 30753, 235, 192, 169, 30982, 31273, 30584, 232, 136, 144, 235, 183, 188, 31545, 30287, 234, 177, 180, 233, 181, 188, 29974, 29906, 30936, 31640, 31429, 233, 181, 188, 29974, 30287, 30936, 232, 136, 144, 235, 183, 188, 30928, 235, 192, 177, 30495, 30956, 31520, 31358, 30584, 13, 236, 157, 185, 31360, 30909, 12300, 6028, 1114, 30893, 232, 158, 165, 30419, 30666, 233, 142, 194, 30257, 30878, 30257, 233, 180, 192, 235, 192, 169, 30893, 232, 158, 165, 30409, 30888, 235, 147, 168, 21576, 30419, 30768, 30406, 233, 180, 192, 235, 192, 169, 30409, 233, 154, 154, 30557, 31399, 234, 140, 143, 30214, 233, 142, 168, 30417, 30753, 232, 168, 154, 233, 180, 192, 235, 192, 169, 31520, 31358, 30584, 233, 175, 165, 235, 194, 145, 30805, 234, 145, 172, 30267, 13, 2, 306, 29950, 30705, 31041, 234, 169, 190, 30869, 233, 182, 184, 31573, 31545, 31633, 235, 183, 171, 233, 187, 172, 30898, 30573, 29899, 29906, 29900, 229, 135, 134, 30739, 29896, 29900, 29945, 229, 135, 134, 30214, 31383, 30698, 30594, 236, 138, 138, 30406, 232, 137, 186, 232, 144, 183, 233, 145, 173, 233, 153, 192, 30682, 31573, 31545, 31100, 30528, 233, 187, 172, 30898, 30210, 31633, 235, 183, 171, 30214, 236, 131, 133, 30406, 30909, 30705, 31041, 30330, 30814, 233, 181, 188, 30330, 232, 137, 185, 30659, 30330, 31679, 31074, 30330, 31420, 234, 189, 187, 30330, 31855, 31399, 30330, 31072, 235, 144, 178, 30330, 234, 145, 178, 30982, 30330, 232, 189, 162, 30716, 31548, 30687, 30503, 30733, 30494, 234, 189, 167, 234, 190, 183, 31184, 30448, 31729, 30406, 30909, 31573, 31545, 232, 147, 135, 31893, 235, 136, 147, 235, 157, 131, 30210, 31391, 30413, 232, 136, 132, 235, 177, 187, 233, 180, 164, 233, 162, 150, 30210, 30832, 231, 191, 191, 30909, 30716, 30210, 31633, 235, 183, 171, 30267, 13, 30287, 30330, 29902, 29950, 30883, 232, 144, 170, 30607, 30705, 31041, 234, 169, 190, 30869, 233, 182, 184, 231, 189, 170, 31399, 233, 169, 133, 235, 194, 179, 30383, 13, 29902, 29950, 30883, 30705, 31041, 233, 182, 184, 30392, 31166, 234, 189, 170, 31166, 232, 147, 187, 30419, 235, 192, 183, 31331, 232, 147, 187, 30752, 30409, 233, 133, 175, 235, 138, 133, 30607, 234, 169, 190, 30869, 233, 182, 184, 30214, 231, 193, 158, 31573, 31545, 30413, 232, 147, 174, 232, 158, 189, 30988, 236, 165, 154, 234, 181, 149, 232, 136, 186, 30417, 235, 136, 147, 235, 157, 131, 30952, 30330, 234, 181, 155, 30898, 30832, 231, 191, 191, 30716, 30210, 233, 185, 181, 30988, 30267, 31149, 31062, 31410, 30330, 236, 165, 160, 30495, 30952, 30815, 30503, 232, 179, 189, 232, 178, 187, 31184, 31944, 236, 138, 138, 30406, 31062, 232, 138, 137, 29096, 29906, 29947, 29945, 29947, 30214, 232, 136, 186, 30417, 30952, 30815, 235, 143, 134, 232, 158, 183, 31566, 30330, 31944, 234, 145, 138, 30528, 30330, 30015, 30457, 30705, 30024, 30716, 30606, 30528, 30503, 234, 190, 183, 31273, 30525, 231, 193, 194, 31184, 31141, 30940, 30214, 31149, 31944, 234, 145, 138, 31419, 29943, 30883, 233, 182, 184, 30606, 232, 160, 138, 31302, 30528, 29945, 242, 191, 136, 30214, 30392, 30356, 30613, 233, 145, 171, 31566, 30210, 31669, 30815, 231, 189, 170, 31399, 30267, 13, 29902, 29950, 30883, 30705, 31041, 233, 182, 184, 31573, 31545, 31633, 235, 183, 171, 233, 187, 172, 30898, 30573, 29899, 29906, 29900, 229, 135, 134, 30739, 29896, 29900, 29945, 229, 135, 134, 30214, 31383, 30698, 30594, 236, 138, 138, 30406, 232, 137, 186, 232, 144, 183, 233, 145, 173, 233, 153, 192, 30682, 31573, 31545, 31100, 30528, 233, 187, 172, 30898, 30210, 31633, 235, 183, 171, 30214, 236, 131, 133, 30406, 30909, 30705, 31041, 30330, 30814, 233, 181, 188, 30330, 232, 137, 185, 30659, 30330, 31679, 31074, 30330, 31420, 234, 189, 187, 30330, 31855, 31399, 30330, 31072, 235, 144, 178, 30330, 234, 145, 178, 30982, 30330, 232, 189, 162, 30716, 31548, 30687, 30503, 30733, 30494, 234, 189, 167, 234, 190, 183, 31184, 30448, 31729, 30406, 30909, 31573, 31545, 232, 147, 135, 31893, 235, 136, 147, 235, 157, 131, 30210, 31391, 30413, 232, 136, 132, 235, 177, 187, 233, 180, 164, 233, 162, 150, 30210, 30832, 231, 191, 191, 30909, 30716, 30210, 31633, 235, 183, 171, 30267, 13, 30654, 30606, 31534, 233, 182, 184, 31729, 30893, 232, 158, 165, 30417, 31175, 30539, 30931, 31756, 31729, 31302, 231, 193, 158, 29902, 29950, 30705, 31041, 234, 169, 190, 30869, 233, 182, 184, 31184, 231, 189, 170, 31399, 30689, 31021, 30214, 233, 175, 165, 235, 194, 145, 30805, 31679, 232, 149, 171, 235, 178, 165, 30584, 2, 29871, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 30698, 31522, 31032, 234, 153, 154, 234, 153, 193, 234, 154, 136, 30214, 31333, 233, 142, 172, 30724, 31835, 30210, 232, 143, 190, 30963, 30392, 31838, 31190, 30908, 30698, 30210, 30214, 30868, 234, 156, 159, 236, 166, 145, 30392, 30287, 31893, 31838, 31190, 236, 164, 192, 232, 158, 189, 30210, 234, 153, 193, 234, 154, 136, 30214, 31032, 234, 153, 154, 31558, 30805, 30953, 30392, 31419, 235, 193, 134, 232, 158, 179, 236, 157, 193, 30210, 30214, 30744, 30651, 233, 133, 166, 30767, 30847, 30801, 31423, 30417, 31333, 233, 142, 172, 31076, 30724, 235, 170, 135, 30210, 232, 143, 190, 30963, 30210, 31852, 30214, 234, 153, 193, 234, 154, 136, 30953, 30392, 232, 193, 139, 236, 157, 193, 31032, 234, 153, 154, 30210, 30267, 31356, 31882, 30214, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 30557, 30806, 31238, 235, 177, 172, 232, 136, 179, 30765, 30275, 31367, 30868, 234, 156, 159, 236, 166, 145, 232, 143, 190, 30963, 30210, 30805, 31999, 30257, 30613, 31633, 234, 190, 144, 30287, 30557, 30267, 13, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 13, 29896, 30330, 30868, 234, 156, 159, 236, 166, 145, 30287, 235, 139, 175, 30910, 30486, 30505, 235, 166, 187, 236, 159, 181, 30210, 234, 157, 177, 235, 133, 167, 30429, 30806, 30214, 30910, 234, 154, 136, 31120, 31117, 30868, 233, 153, 148, 30806, 234, 170, 178, 30446, 30214, 30354, 31180, 31022, 30214, 30768, 31190, 31557, 30417, 232, 138, 163, 232, 160, 154, 31391, 30767, 30392, 232, 138, 163, 30940, 30267, 30847, 30413, 234, 152, 156, 31474, 30210, 31852, 30214, 30868, 233, 153, 148, 234, 154, 136, 30910, 31599, 31238, 30437, 31305, 30494, 31022, 30354, 30210, 233, 153, 148, 232, 160, 154, 30910, 31599, 30780, 30287, 31558, 30214, 31943, 235, 138, 183, 30257, 30806, 234, 170, 178, 30210, 30868, 233, 153, 148, 234, 154, 136, 30267, 13, 29906, 30330, 30868, 234, 156, 159, 236, 166, 145, 30658, 31117, 30868, 233, 153, 148, 234, 154, 136, 30910, 234, 154, 136, 31352, 30688, 235, 170, 140, 30952, 30214, 233, 133, 166, 31548, 30636, 30956, 234, 154, 149, 233, 135, 162, 30214, 31325, 231, 187, 151, 233, 133, 166, 31548, 30210, 235, 133, 143, 235, 133, 167, 31066, 30746, 233, 185, 169, 233, 190, 148, 30214, 31352, 236, 182, 161, 232, 180, 148, 31423, 30417, 235, 147, 145, 234, 191, 172, 30210, 30746, 31133, 30544, 31424, 30267, 30810, 31238, 235, 138, 183, 30785, 232, 193, 139, 30923, 234, 154, 136, 30313, 31352, 30545, 31436, 30594, 30210, 30910, 31424, 30868, 233, 153, 148, 234, 154, 136, 30214, 31979, 31420, 30494, 30743, 30868, 233, 153, 148, 234, 154, 136, 30210, 31174, 30287, 233, 176, 168, 30748, 233, 152, 166, 30267, 13, 29941, 30330, 30868, 233, 153, 148, 30437, 236, 157, 146, 234, 160, 131, 234, 153, 193, 234, 154, 136, 30210, 30666, 30908, 236, 165, 159, 31085, 236, 131, 147, 233, 187, 147, 31462, 31947, 30214, 31267, 31149, 31221, 234, 157, 177, 235, 133, 167, 30210, 31993, 30967, 30953, 30437, 31844, 30805, 30267, 30868, 234, 156, 159, 236, 166, 145, 31120, 31117, 30392, 30417, 30287, 31959, 31141, 30210, 31141, 232, 193, 132, 30210, 30214, 30868, 234, 156, 159, 236, 166, 145, 234, 157, 177, 233, 144, 162, 30467, 30505, 31120, 31117, 30287, 235, 139, 175, 30392, 233, 184, 136, 30868, 31085, 30210, 30214, 30868, 234, 156, 159, 236, 166, 145, 30868, 233, 153, 148, 30210, 31109, 232, 158, 183, 30437, 30544, 31424, 30287, 31217, 31935, 31935, 232, 138, 187, 31558, 30210, 234, 133, 145, 234, 154, 138, 30952, 233, 157, 154, 31869, 31085, 31217, 234, 189, 188, 30214, 30287, 235, 139, 175, 30769, 30437, 31695, 234, 190, 176, 30544, 31424, 232, 138, 163, 30502, 30900, 31117, 30267, 13, 31548, 30505, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30847, 31502, 31032, 234, 153, 154, 29973, 30768, 31138, 30429, 30806, 232, 136, 179, 30765, 30275, 31367, 30868, 234, 156, 159, 236, 166, 145, 232, 143, 190, 30963, 30210, 30210, 31633, 234, 190, 144, 30214, 30257, 30613, 30783, 30910, 31599, 31117, 30210, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 30417, 30743, 232, 193, 139, 30257, 30210, 30743, 31201, 30267, 31424, 30505, 30672, 31381, 31290, 31412, 30743, 31201, 31050, 30743, 30868, 234, 156, 159, 236, 166, 145, 233, 131, 145, 31882, 232, 141, 161, 30214, 31356, 31882, 30672, 31381, 30953, 31370, 31751, 31043, 30397, 30868, 234, 156, 159, 236, 166, 145, 31370, 31751, 233, 131, 145, 31882, 31475, 236, 165, 135, 236, 155, 181, 30214, 30672, 31381, 31370, 31751, 30505, 30486, 31704, 30275, 234, 170, 178, 233, 161, 132, 236, 165, 135, 236, 155, 181, 234, 153, 193, 234, 154, 136, 30214, 30810, 31819, 30672, 31381, 31979, 30815, 30210, 31617, 31072, 234, 153, 193, 234, 154, 136, 30214, 232, 138, 146, 31022, 234, 153, 193, 234, 154, 136, 232, 187, 169, 30805, 30210, 232, 144, 180, 232, 177, 182, 30267, 13, 2, 29871, 30287, 30470, 30287, 30898, 233, 157, 148, 232, 132, 138, 232, 144, 182, 30998, 31026, 31020, 30214, 30573, 30446, 233, 159, 142, 31373, 232, 138, 137, 232, 167, 138, 30670, 233, 145, 149, 31704, 30846, 30210, 30613, 31143, 30682, 31424, 30505, 233, 141, 168, 235, 178, 190, 233, 157, 148, 31117, 30395, 232, 193, 135, 234, 146, 176, 30214, 30573, 30446, 233, 159, 142, 31373, 31138, 30287, 30502, 232, 136, 136, 233, 190, 164, 31704, 31074, 30210, 232, 132, 138, 31117, 29871, 30429, 31338, 30533, 30940, 30383, 31502, 30333, 30395, 31885, 31730, 30813, 30330, 31947, 30716, 232, 162, 154, 30330, 31476, 30395, 29871, 30986, 235, 164, 166, 30330, 232, 180, 178, 31649, 30330, 236, 150, 159, 236, 151, 166, 233, 188, 193, 30330, 30998, 31867, 233, 193, 182, 29871, 30257, 232, 162, 151, 31169, 235, 147, 134, 30415, 31071, 30330, 31869, 234, 166, 164, 31169, 235, 147, 134, 232, 188, 191, 234, 171, 157, 232, 158, 176, 29871, 30325, 31117, 30383, 29955, 30534, 29896, 29953, 30325, 235, 138, 182, 29947, 30534, 29906, 29955, 30325, 31117, 31016, 29871, 30783]
inputs:
一任车主，马尼托巴省本地车，1.8升I-4发动机，带变速箱的1速CVT，低价出售！ 主要特征：-加热式前排座椅-无钥匙进入-蓝牙连接-智能设备集成-远程启动-电动天窗-娱乐信息显示-生态模式 SAFTEY / ADD-ONS：-倒车摄像头-刹车辅助-电动尾门-儿童安全锁
所有车辆均通过GM（通用汽车）155项 认证检测，并且免费提供5000km或90天全车保修！免费送一箱油+2次换机油+一次免费四轮定位服务！
隶属于AutoCanada集团（加拿大最大汽车集团）主营GM（通用汽车）旗下品牌，拥有全套汽车服务！欢迎来玩。
</s> IH化工离心泵输送介质温度为-20℃～105℃，需要时采用冷却措施可输送更高温度的介质，适用于化工、石油、冶金、电力、造纸、食品、制药、环保、废水处理和合成纤维等行业用于输送各种腐蚀的或不允许污染的类似于水的介质。
一、IH型卧式化工离心泵产品概述：
IH型化工泵是单级单吸（轴向吸入）悬臂式离心泵，供输送不含固体颗粒具有腐蚀性、粘度类似水的液体。其标记、额定性能和尺寸等效采用标准ISO2858，具有性能范围广、效率高、“三化”水平高和维修方便等特点，其效率比F型泵平均提高5％，是国家推广的节能产品。
IH型化工泵输送介质温度为-20℃～105℃，需要时采用冷却措施可输送更高温度的介质，适用于化工、石油、冶金、电力、造纸、食品、制药、环保、废水处理和合成纤维等行业用于输送各种腐蚀的或不允许污染的类似于水的介质。
太平洋泵业集团有限公司专业提供IH化工离心泵等产品信息，欢迎来电咨询！</s> 处在发展期的白癜风应该如何治疗?要想治疗疾病，选择正确的医院是非常重要的，白癜风是一种非常顽固的疾病，治疗起来也是比较困难的，所以患者如果没有选择好正规的医院的话，疾病也是很难治疗的。那么，处在发展期的白癜风应该如何治疗?下面就让兰州中研白癜风医院的来给大家介绍一下。
处在发展期的白癜风应该如何治疗?
1、白癜风一般发生在裸露的皮肤上面，发病初期白斑面积小，数量少，通常只有几块或者是几点。如不留意的话，白斑病发展就会形成少数的斑块发展到一起，导致大面积的白斑病。
2、白癜风前期白斑病发病无自觉性，患处部位痒感，而且患处的肌肤外表润滑，无鳞屑没有萎缩的表象出现。这就致使很多病人无法及时的发现白斑病，才造成了白斑病的进一步分散。
3、白斑会随着疾病的加重颜色逐渐变深，与其他皮肤的边界也会越来。白癜风初期是有一些特的特征的，白癜风皮损区在初期一般是浅白色的，白癜风白斑的周围会出现一条微微凸起的炎症性暗红色条纹，一般都会持续出现几个星期。
处在发展期的白癜风应该如何治疗?通过上面兰州中研白癜风医院的的介绍，大家对发展期的白癜风应该有了很大的了解。现在我们已经了解得了白癜风怎么办，那么我们也应该知道白癜风应该怎么去预防，我们应该在生活中积极预防疾病，这样我们才能的控制疾病，减少疾病带来的危害。
</s> 一年一度暑假即将开始，为小朋友准备安排活动的家长可现在报读暑期田径班，为小朋友过一个充满活力的假期 上堂地点：何文田巴富街、深水埗、沙田 青衣、屯门、铜锣湾、将军澳 大埔德萃学校、红磡德萃幼稚园 日期：7月16日至8月27日期间 对
Caching indices mapping at /home/wangzj/.cache/huggingface/datasets/json/default-e04fdd9113403d69/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5644c8a4f7ef5cf1.arrow
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 981401
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 981401
})
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 981401
})
[INFO|trainer.py:586] 2024-03-09 15:00:57,142 >> Using auto half precision backend
Dataset({
    features: ['input_ids', 'attention_mask'],
    num_rows: 981401
})
[INFO|deepspeed.py:325] 2024-03-09 15:00:57,464 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/wangzj/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/wangzj/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.914942741394043 seconds
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.966256618499756 seconds
Time to load cpu_adam op: 2.965484857559204 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 3.0020833015441895 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-03-09 15:01:03,495] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3, git-hash=unknown, git-branch=unknown
[2024-03-09 15:01:22,338] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-03-09 15:01:22,341] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-03-09 15:01:22,341] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-03-09 15:01:22,350] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-03-09 15:01:22,350] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-03-09 15:01:22,350] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-03-09 15:01:22,350] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 500000000
[2024-03-09 15:01:22,350] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500000000
[2024-03-09 15:01:22,350] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: True
[2024-03-09 15:01:22,350] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False
[2024-03-09 15:01:41,320] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-03-09 15:01:41,321] [INFO] [utils.py:803:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-03-09 15:01:41,321] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 53.6 GB, percent = 14.2%
[2024-03-09 15:01:48,797] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-03-09 15:01:48,798] [INFO] [utils.py:803:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-03-09 15:01:48,798] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 107.23 GB, percent = 28.5%
[2024-03-09 15:01:48,798] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized
[2024-03-09 15:01:49,142] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-03-09 15:01:49,143] [INFO] [utils.py:803:see_memory_usage] MA 20.76 GB         Max_MA 20.76 GB         CA 20.76 GB         Max_CA 21 GB 
[2024-03-09 15:01:49,143] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 110.5 GB, percent = 29.3%
[2024-03-09 15:01:49,146] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-03-09 15:01:49,146] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-03-09 15:01:49,146] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-03-09 15:01:49,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2024-03-09 15:01:49,155] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-03-09 15:01:49,157] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-03-09 15:01:49,158] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-03-09 15:01:49,158] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-03-09 15:01:49,158] [INFO] [config.py:978:print]   amp_params ................... False
[2024-03-09 15:01:49,161] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-03-09 15:01:49,161] [INFO] [config.py:978:print]   bfloat16_enabled ............. True
[2024-03-09 15:01:49,162] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-03-09 15:01:49,162] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-03-09 15:01:49,162] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-03-09 15:01:49,162] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff347b696c0>
[2024-03-09 15:01:49,162] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-03-09 15:01:49,163] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-03-09 15:01:49,163] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-03-09 15:01:49,163] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-03-09 15:01:49,163] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-03-09 15:01:49,163] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-03-09 15:01:49,163] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   dump_state ................... False
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... None
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-03-09 15:01:49,164] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-03-09 15:01:49,165] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-03-09 15:01:49,165] [INFO] [config.py:978:print]   fp16_auto_cast ............... None
[2024-03-09 15:01:49,166] [INFO] [config.py:978:print]   fp16_enabled ................. False
[2024-03-09 15:01:49,166] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-03-09 15:01:49,166] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-03-09 15:01:49,166] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-03-09 15:01:49,166] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 8
[2024-03-09 15:01:49,166] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-03-09 15:01:49,166] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-03-09 15:01:49,166] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-03-09 15:01:49,167] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 1
[2024-03-09 15:01:49,167] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-03-09 15:01:49,167] [INFO] [config.py:978:print]   loss_scale ................... 1.0
[2024-03-09 15:01:49,167] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-03-09 15:01:49,167] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-03-09 15:01:49,167] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-03-09 15:01:49,168] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-03-09 15:01:49,168] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-03-09 15:01:49,169] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-03-09 15:01:49,169] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-03-09 15:01:49,169] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-03-09 15:01:49,169] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-03-09 15:01:49,169] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-03-09 15:01:49,169] [INFO] [config.py:978:print]   pld_params ................... False
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   train_batch_size ............. 256
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  8
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-03-09 15:01:49,170] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-03-09 15:01:49,171] [INFO] [config.py:978:print]   world_size ................... 4
[2024-03-09 15:01:49,171] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-03-09 15:01:49,171] [INFO] [config.py:978:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-03-09 15:01:49,172] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-03-09 15:01:49,172] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-03-09 15:01:49,172] [INFO] [config.py:978:print]   zero_optimization_stage ...... 2
[2024-03-09 15:01:49,173] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 8, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }
}
[INFO|trainer.py:1747] 2024-03-09 15:01:49,174 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-03-09 15:01:49,175 >>   Num examples = 981,401
[INFO|trainer.py:1749] 2024-03-09 15:01:49,175 >>   Num Epochs = 1
[INFO|trainer.py:1750] 2024-03-09 15:01:49,175 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-03-09 15:01:49,176 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:1754] 2024-03-09 15:01:49,176 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1755] 2024-03-09 15:01:49,176 >>   Total optimization steps = 3,833
[INFO|trainer.py:1756] 2024-03-09 15:01:49,188 >>   Number of trainable parameters = 4,328,783,872
  0%|          | 0/3833 [00:00<?, ?it/s]  0%|          | 1/3833 [01:27<92:40:51, 87.07s/it]  0%|          | 2/3833 [02:50<90:21:54, 84.92s/it]  0%|          | 3/3833 [04:14<89:58:44, 84.58s/it]  0%|          | 4/3833 [05:38<89:43:39, 84.36s/it]  0%|          | 5/3833 [07:02<89:21:05, 84.03s/it]  0%|          | 6/3833 [08:25<89:11:19, 83.90s/it]  0%|          | 7/3833 [09:49<88:59:01, 83.73s/it]  0%|          | 8/3833 [11:11<88:36:55, 83.40s/it]  0%|          | 9/3833 [12:34<88:29:33, 83.31s/it]  0%|          | 10/3833 [13:58<88:23:22, 83.23s/it]                                                    {'loss': 2.4843, 'learning_rate': 4.9999160289297196e-05, 'epoch': 0.0}
  0%|          | 10/3833 [13:58<88:23:22, 83.23s/it]  0%|          | 11/3833 [15:20<88:10:55, 83.06s/it]  0%|          | 12/3833 [16:43<88:03:53, 82.97s/it]  0%|          | 13/3833 [18:06<88:04:23, 83.00s/it]  0%|          | 14/3833 [19:29<87:54:24, 82.87s/it]  0%|          | 15/3833 [20:51<87:53:47, 82.88s/it]  0%|          | 16/3833 [22:14<87:41:06, 82.70s/it]  0%|          | 17/3833 [23:36<87:38:24, 82.68s/it]  0%|          | 18/3833 [24:58<87:21:31, 82.44s/it]  0%|          | 19/3833 [26:20<87:13:01, 82.32s/it]  1%|          | 20/3833 [27:43<87:10:11, 82.30s/it]                                                    {'loss': 2.1782, 'learning_rate': 4.999664121359792e-05, 'epoch': 0.01}
  1%|          | 20/3833 [27:43<87:10:11, 82.30s/it]  1%|          | 21/3833 [29:04<86:57:14, 82.12s/it]  1%|          | 22/3833 [30:26<86:56:05, 82.12s/it]  1%|          | 23/3833 [31:49<86:58:06, 82.17s/it]  1%|          | 24/3833 [33:10<86:45:56, 82.00s/it]  1%|          | 25/3833 [34:32<86:41:18, 81.95s/it]  1%|          | 26/3833 [35:53<86:24:45, 81.71s/it]  1%|          | 27/3833 [37:15<86:15:49, 81.59s/it]  1%|          | 28/3833 [38:36<86:16:19, 81.62s/it]  1%|          | 29/3833 [39:58<86:14:49, 81.62s/it]  1%|          | 30/3833 [41:20<86:15:47, 81.66s/it]                                                    {'loss': 2.0768, 'learning_rate': 4.999244294212574e-05, 'epoch': 0.01}
  1%|          | 30/3833 [41:20<86:15:47, 81.66s/it]  1%|          | 31/3833 [42:41<86:10:06, 81.59s/it]  1%|          | 32/3833 [44:02<86:03:30, 81.51s/it]  1%|          | 33/3833 [45:24<86:05:37, 81.56s/it]  1%|          | 34/3833 [46:46<86:13:06, 81.70s/it]  1%|          | 35/3833 [48:08<86:14:21, 81.74s/it]  1%|          | 36/3833 [49:29<86:07:28, 81.66s/it]  1%|          | 37/3833 [50:52<86:15:34, 81.81s/it]  1%|          | 38/3833 [52:14<86:17:55, 81.86s/it]  1%|          | 39/3833 [53:35<86:12:19, 81.80s/it]  1%|          | 40/3833 [54:57<86:16:15, 81.88s/it]                                                    {'loss': 1.9916, 'learning_rate': 4.9986565756907345e-05, 'epoch': 0.01}
  1%|          | 40/3833 [54:57<86:16:15, 81.88s/it]  1%|          | 41/3833 [56:19<86:02:57, 81.69s/it]  1%|          | 42/3833 [57:40<85:52:17, 81.55s/it]  1%|          | 43/3833 [59:01<85:47:12, 81.49s/it]  1%|          | 44/3833 [1:00:22<85:42:55, 81.44s/it]  1%|          | 45/3833 [1:01:44<85:43:27, 81.47s/it]  1%|          | 46/3833 [1:03:05<85:39:21, 81.43s/it]  1%|          | 47/3833 [1:04:27<85:35:56, 81.39s/it]  1%|▏         | 48/3833 [1:05:49<85:57:11, 81.75s/it]  1%|▏         | 49/3833 [1:07:11<85:51:14, 81.68s/it]  1%|▏         | 50/3833 [1:08:32<85:50:57, 81.70s/it]                                                      {'loss': 1.9408, 'learning_rate': 4.997901005275356e-05, 'epoch': 0.01}
  1%|▏         | 50/3833 [1:08:32<85:50:57, 81.70s/it]  1%|▏         | 51/3833 [1:09:54<85:51:38, 81.73s/it]  1%|▏         | 52/3833 [1:11:16<85:51:14, 81.74s/it]  1%|▏         | 53/3833 [1:12:38<85:59:54, 81.90s/it]  1%|▏         | 54/3833 [1:14:01<86:07:59, 82.05s/it]  1%|▏         | 55/3833 [1:15:23<86:07:20, 82.06s/it]  1%|▏         | 56/3833 [1:16:45<86:01:10, 81.99s/it]  1%|▏         | 57/3833 [1:18:07<86:16:03, 82.25s/it]  2%|▏         | 58/3833 [1:19:29<86:06:37, 82.12s/it]  2%|▏         | 59/3833 [1:20:51<85:59:13, 82.02s/it]  2%|▏         | 60/3833 [1:22:13<85:54:43, 81.97s/it]                                                      {'loss': 1.9133, 'learning_rate': 4.9969776337232844e-05, 'epoch': 0.02}
  2%|▏         | 60/3833 [1:22:13<85:54:43, 81.97s/it]  2%|▏         | 61/3833 [1:23:35<85:54:34, 81.99s/it]  2%|▏         | 62/3833 [1:24:57<85:47:55, 81.91s/it]  2%|▏         | 63/3833 [1:26:21<86:23:16, 82.49s/it]  2%|▏         | 64/3833 [1:27:42<86:00:59, 82.16s/it]  2%|▏         | 65/3833 [1:29:04<85:52:34, 82.05s/it]  2%|▏         | 66/3833 [1:30:25<85:40:41, 81.88s/it]  2%|▏         | 67/3833 [1:31:47<85:33:15, 81.78s/it]  2%|▏         | 68/3833 [1:33:08<85:29:39, 81.75s/it]  2%|▏         | 69/3833 [1:34:30<85:22:39, 81.66s/it]  2%|▏         | 70/3833 [1:35:52<85:23:21, 81.69s/it]                                                      {'loss': 1.8816, 'learning_rate': 4.995886523063716e-05, 'epoch': 0.02}
  2%|▏         | 70/3833 [1:35:52<85:23:21, 81.69s/it]  2%|▏         | 71/3833 [1:37:14<85:36:07, 81.92s/it]  2%|▏         | 72/3833 [1:38:36<85:42:54, 82.05s/it]  2%|▏         | 73/3833 [1:39:58<85:37:46, 81.99s/it]  2%|▏         | 74/3833 [1:41:20<85:34:21, 81.95s/it]  2%|▏         | 75/3833 [1:42:42<85:28:51, 81.89s/it]  2%|▏         | 76/3833 [1:44:04<85:27:28, 81.89s/it]  2%|▏         | 77/3833 [1:45:29<86:36:21, 83.01s/it]  2%|▏         | 78/3833 [1:46:52<86:26:40, 82.88s/it]  2%|▏         | 79/3833 [1:48:21<88:26:51, 84.82s/it]  2%|▏         | 80/3833 [1:49:43<87:34:19, 84.00s/it]                                                      {'loss': 1.8472, 'learning_rate': 4.9946277465940366e-05, 'epoch': 0.02}
  2%|▏         | 80/3833 [1:49:43<87:34:19, 84.00s/it]  2%|▏         | 81/3833 [1:51:06<87:08:12, 83.61s/it]  2%|▏         | 82/3833 [1:52:28<86:40:05, 83.18s/it]  2%|▏         | 83/3833 [1:53:50<86:16:32, 82.82s/it]  2%|▏         | 84/3833 [1:55:17<87:20:57, 83.88s/it]  2%|▏         | 85/3833 [1:56:39<86:49:32, 83.40s/it]  2%|▏         | 86/3833 [1:58:01<86:26:40, 83.05s/it]  2%|▏         | 87/3833 [1:59:23<86:09:28, 82.80s/it]  2%|▏         | 88/3833 [2:00:46<85:57:00, 82.62s/it]  2%|▏         | 89/3833 [2:02:08<85:54:00, 82.60s/it]  2%|▏         | 90/3833 [2:03:31<86:05:17, 82.80s/it]                                                      {'loss': 1.8018, 'learning_rate': 4.9932013888748904e-05, 'epoch': 0.02}
  2%|▏         | 90/3833 [2:03:31<86:05:17, 82.80s/it]  2%|▏         | 91/3833 [2:04:54<85:59:30, 82.73s/it]  2%|▏         | 92/3833 [2:06:18<86:32:29, 83.28s/it]  2%|▏         | 93/3833 [2:07:41<86:18:31, 83.08s/it]  2%|▏         | 94/3833 [2:09:04<86:10:48, 82.98s/it]  2%|▏         | 95/3833 [2:10:27<86:11:18, 83.01s/it]  3%|▎         | 96/3833 [2:11:52<86:42:55, 83.54s/it]  3%|▎         | 97/3833 [2:13:15<86:29:36, 83.34s/it]  3%|▎         | 98/3833 [2:14:37<86:09:29, 83.04s/it]  3%|▎         | 99/3833 [2:15:59<85:55:55, 82.85s/it]  3%|▎         | 100/3833 [2:17:22<85:48:05, 82.74s/it]                                                       {'loss': 1.7793, 'learning_rate': 4.991607545724507e-05, 'epoch': 0.03}
  3%|▎         | 100/3833 [2:17:22<85:48:05, 82.74s/it]  3%|▎         | 101/3833 [2:18:44<85:41:33, 82.66s/it]  3%|▎         | 102/3833 [2:20:07<85:40:44, 82.67s/it]  3%|▎         | 103/3833 [2:21:29<85:29:11, 82.51s/it]  3%|▎         | 104/3833 [2:22:52<85:29:49, 82.54s/it]  3%|▎         | 105/3833 [2:24:14<85:28:15, 82.54s/it]  3%|▎         | 106/3833 [2:25:38<85:53:53, 82.97s/it]  3%|▎         | 107/3833 [2:27:01<85:40:45, 82.78s/it]  3%|▎         | 108/3833 [2:28:23<85:33:44, 82.69s/it]  3%|▎         | 109/3833 [2:29:45<85:27:19, 82.61s/it]  3%|▎         | 110/3833 [2:31:08<85:25:03, 82.60s/it]                                                       {'loss': 1.7677, 'learning_rate': 4.989846324212256e-05, 'epoch': 0.03}
  3%|▎         | 110/3833 [2:31:08<85:25:03, 82.60s/it]  3%|▎         | 111/3833 [2:32:30<85:13:21, 82.43s/it]  3%|▎         | 112/3833 [2:33:52<85:11:09, 82.42s/it]  3%|▎         | 113/3833 [2:35:15<85:07:57, 82.39s/it]  3%|▎         | 114/3833 [2:36:37<85:05:57, 82.38s/it]  3%|▎         | 115/3833 [2:38:00<85:05:10, 82.39s/it]  3%|▎         | 116/3833 [2:39:22<85:07:53, 82.45s/it]  3%|▎         | 117/3833 [2:40:44<85:02:57, 82.39s/it]  3%|▎         | 118/3833 [2:42:07<85:03:43, 82.43s/it]  3%|▎         | 119/3833 [2:43:29<84:57:26, 82.35s/it]  3%|▎         | 120/3833 [2:44:51<84:54:41, 82.33s/it]                                                       {'loss': 1.7388, 'learning_rate': 4.987917842651464e-05, 'epoch': 0.03}
  3%|▎         | 120/3833 [2:44:51<84:54:41, 82.33s/it]  3%|▎         | 121/3833 [2:46:14<84:51:46, 82.30s/it]  3%|▎         | 122/3833 [2:47:36<84:49:49, 82.29s/it]  3%|▎         | 123/3833 [2:48:59<84:56:46, 82.43s/it]  3%|▎         | 124/3833 [2:50:21<84:50:41, 82.35s/it]  3%|▎         | 125/3833 [2:51:44<85:02:27, 82.56s/it]  3%|▎         | 126/3833 [2:53:06<84:51:18, 82.41s/it]  3%|▎         | 127/3833 [2:54:28<84:52:08, 82.44s/it]  3%|▎         | 128/3833 [2:55:51<84:48:28, 82.40s/it]  3%|▎         | 129/3833 [2:57:13<84:44:03, 82.36s/it]  3%|▎         | 130/3833 [2:58:35<84:41:55, 82.34s/it]                                                       {'loss': 1.7299, 'learning_rate': 4.985822230591458e-05, 'epoch': 0.03}
  3%|▎         | 130/3833 [2:58:35<84:41:55, 82.34s/it]  3%|▎         | 131/3833 [2:59:57<84:29:04, 82.16s/it]  3%|▎         | 132/3833 [3:01:19<84:24:28, 82.10s/it]  3%|▎         | 133/3833 [3:02:41<84:25:16, 82.14s/it]  3%|▎         | 134/3833 [3:04:03<84:20:43, 82.09s/it]  4%|▎         | 135/3833 [3:05:25<84:16:56, 82.05s/it]  4%|▎         | 136/3833 [3:06:48<84:29:39, 82.28s/it]  4%|▎         | 137/3833 [3:08:10<84:24:19, 82.21s/it]  4%|▎         | 138/3833 [3:09:32<84:23:22, 82.22s/it]  4%|▎         | 139/3833 [3:10:55<84:37:20, 82.47s/it]  4%|▎         | 140/3833 [3:12:18<84:33:59, 82.44s/it]                                                       {'loss': 1.7115, 'learning_rate': 4.983559628808868e-05, 'epoch': 0.04}
  4%|▎         | 140/3833 [3:12:18<84:33:59, 82.44s/it]  4%|▎         | 141/3833 [3:13:40<84:22:21, 82.27s/it]  4%|▎         | 142/3833 [3:15:02<84:20:42, 82.27s/it]  4%|▎         | 143/3833 [3:16:24<84:16:49, 82.22s/it]  4%|▍         | 144/3833 [3:17:46<84:12:17, 82.17s/it]  4%|▍         | 145/3833 [3:19:08<84:07:09, 82.11s/it]  4%|▍         | 146/3833 [3:20:30<83:59:16, 82.01s/it]  4%|▍         | 147/3833 [3:21:52<84:00:36, 82.05s/it]  4%|▍         | 148/3833 [3:23:14<84:00:10, 82.07s/it]  4%|▍         | 149/3833 [3:24:36<84:06:25, 82.19s/it]  4%|▍         | 150/3833 [3:25:58<84:00:26, 82.11s/it]                                                       {'loss': 1.7036, 'learning_rate': 4.98113018929817e-05, 'epoch': 0.04}
  4%|▍         | 150/3833 [3:25:58<84:00:26, 82.11s/it]  4%|▍         | 151/3833 [3:27:20<83:54:34, 82.04s/it]  4%|▍         | 152/3833 [3:28:42<83:54:26, 82.06s/it]  4%|▍         | 153/3833 [3:30:05<83:58:12, 82.14s/it]  4%|▍         | 154/3833 [3:31:27<83:55:13, 82.12s/it]  4%|▍         | 155/3833 [3:32:49<83:50:53, 82.07s/it]  4%|▍         | 156/3833 [3:34:11<83:56:27, 82.18s/it]  4%|▍         | 157/3833 [3:35:33<83:47:54, 82.07s/it]  4%|▍         | 158/3833 [3:36:55<83:55:21, 82.21s/it]  4%|▍         | 159/3833 [3:38:18<84:02:42, 82.35s/it]  4%|▍         | 160/3833 [3:39:41<84:07:34, 82.45s/it]                                                       {'loss': 1.6781, 'learning_rate': 4.9785340752614715e-05, 'epoch': 0.04}
  4%|▍         | 160/3833 [3:39:41<84:07:34, 82.45s/it]  4%|▍         | 161/3833 [3:41:04<84:11:49, 82.55s/it]  4%|▍         | 162/3833 [3:42:26<84:06:40, 82.48s/it]  4%|▍         | 163/3833 [3:43:49<84:10:14, 82.57s/it]  4%|▍         | 164/3833 [3:45:11<84:02:29, 82.46s/it]  4%|▍         | 165/3833 [3:46:33<83:58:49, 82.42s/it]  4%|▍         | 166/3833 [3:47:55<83:52:25, 82.34s/it]  4%|▍         | 167/3833 [3:49:17<83:43:06, 82.21s/it]  4%|▍         | 168/3833 [3:50:39<83:33:13, 82.07s/it]  4%|▍         | 169/3833 [3:52:01<83:31:04, 82.06s/it]  4%|▍         | 170/3833 [3:53:23<83:26:54, 82.01s/it]                                                       {'loss': 1.6681, 'learning_rate': 4.975771461097553e-05, 'epoch': 0.04}
  4%|▍         | 170/3833 [3:53:23<83:26:54, 82.01s/it]  4%|▍         | 171/3833 [3:54:45<83:21:42, 81.95s/it]  4%|▍         | 172/3833 [3:56:07<83:18:27, 81.92s/it]  5%|▍         | 173/3833 [3:57:29<83:16:02, 81.90s/it]  5%|▍         | 174/3833 [3:58:50<83:13:09, 81.88s/it]  5%|▍         | 175/3833 [4:00:12<83:12:56, 81.90s/it]  5%|▍         | 176/3833 [4:01:34<83:14:57, 81.95s/it]  5%|▍         | 177/3833 [4:02:56<83:13:29, 81.95s/it]  5%|▍         | 178/3833 [4:04:18<83:10:01, 81.92s/it]  5%|▍         | 179/3833 [4:05:40<83:08:18, 81.91s/it]  5%|▍         | 180/3833 [4:07:03<83:18:05, 82.09s/it]                                                       {'loss': 1.6718, 'learning_rate': 4.9728425323901474e-05, 'epoch': 0.05}
  5%|▍         | 180/3833 [4:07:03<83:18:05, 82.09s/it]  5%|▍         | 181/3833 [4:08:25<83:15:41, 82.08s/it]  5%|▍         | 182/3833 [4:09:47<83:12:16, 82.04s/it]  5%|▍         | 183/3833 [4:11:08<83:01:57, 81.90s/it]  5%|▍         | 184/3833 [4:12:30<83:01:05, 81.90s/it]  5%|▍         | 185/3833 [4:13:52<83:04:25, 81.98s/it]  5%|▍         | 186/3833 [4:15:14<83:01:16, 81.95s/it]  5%|▍         | 187/3833 [4:16:36<83:01:27, 81.98s/it]  5%|▍         | 188/3833 [4:17:58<83:00:06, 81.98s/it]  5%|▍         | 189/3833 [4:19:20<83:00:29, 82.01s/it]  5%|▍         | 190/3833 [4:20:42<83:00:18, 82.03s/it]                                                       {'loss': 1.6567, 'learning_rate': 4.9697474858954786e-05, 'epoch': 0.05}
  5%|▍         | 190/3833 [4:20:42<83:00:18, 82.03s/it]  5%|▍         | 191/3833 [4:22:05<83:07:57, 82.17s/it]  5%|▌         | 192/3833 [4:23:27<82:59:22, 82.06s/it]  5%|▌         | 193/3833 [4:24:49<83:11:25, 82.28s/it]  5%|▌         | 194/3833 [4:26:11<83:02:13, 82.15s/it]  5%|▌         | 195/3833 [4:27:34<83:04:28, 82.21s/it]  5%|▌         | 196/3833 [4:28:56<83:01:19, 82.18s/it]  5%|▌         | 197/3833 [4:30:18<82:57:29, 82.14s/it]  5%|▌         | 198/3833 [4:31:40<82:54:07, 82.10s/it]  5%|▌         | 199/3833 [4:33:02<82:58:31, 82.20s/it]  5%|▌         | 200/3833 [4:34:24<82:51:36, 82.11s/it]                                                       {'loss': 1.6507, 'learning_rate': 4.9664865295290394e-05, 'epoch': 0.05}
  5%|▌         | 200/3833 [4:34:24<82:51:36, 82.11s/it]  5%|▌         | 201/3833 [4:35:46<82:46:22, 82.04s/it]  5%|▌         | 202/3833 [4:37:08<82:36:58, 81.91s/it]  5%|▌         | 203/3833 [4:38:29<82:35:00, 81.90s/it]  5%|▌         | 204/3833 [4:39:51<82:35:48, 81.94s/it]  5%|▌         | 205/3833 [4:41:13<82:30:46, 81.88s/it]  5%|▌         | 206/3833 [4:42:35<82:26:32, 81.83s/it]  5%|▌         | 207/3833 [4:43:57<82:26:26, 81.85s/it]  5%|▌         | 208/3833 [4:45:18<82:21:27, 81.79s/it]  5%|▌         | 209/3833 [4:46:40<82:21:23, 81.81s/it]  5%|▌         | 210/3833 [4:48:02<82:23:34, 81.87s/it]                                                       {'loss': 1.641, 'learning_rate': 4.963059882351627e-05, 'epoch': 0.05}
  5%|▌         | 210/3833 [4:48:02<82:23:34, 81.87s/it]  6%|▌         | 211/3833 [4:49:24<82:21:01, 81.85s/it]  6%|▌         | 212/3833 [4:50:46<82:28:02, 81.99s/it]  6%|▌         | 213/3833 [4:52:08<82:28:10, 82.01s/it]  6%|▌         | 214/3833 [4:53:31<82:31:17, 82.09s/it]  6%|▌         | 215/3833 [4:54:53<82:25:45, 82.02s/it]  6%|▌         | 216/3833 [4:56:14<82:19:59, 81.95s/it]  6%|▌         | 217/3833 [4:57:37<82:29:17, 82.12s/it]  6%|▌         | 218/3833 [4:58:59<82:23:15, 82.05s/it]  6%|▌         | 219/3833 [5:00:21<82:29:54, 82.18s/it]  6%|▌         | 220/3833 [5:01:43<82:20:30, 82.05s/it]                                                       {'loss': 1.6222, 'learning_rate': 4.959467774554627e-05, 'epoch': 0.06}
  6%|▌         | 220/3833 [5:01:43<82:20:30, 82.05s/it]  6%|▌         | 221/3833 [5:03:05<82:18:08, 82.03s/it]  6%|▌         | 222/3833 [5:04:27<82:16:48, 82.03s/it]  6%|▌         | 223/3833 [5:05:49<82:21:51, 82.14s/it]  6%|▌         | 224/3833 [5:07:11<82:17:27, 82.09s/it]  6%|▌         | 225/3833 [5:08:33<82:09:15, 81.97s/it]  6%|▌         | 226/3833 [5:09:56<82:17:07, 82.13s/it]  6%|▌         | 227/3833 [5:11:18<82:14:56, 82.11s/it]  6%|▌         | 228/3833 [5:12:41<82:29:03, 82.37s/it]  6%|▌         | 229/3833 [5:14:03<82:24:56, 82.32s/it]  6%|▌         | 230/3833 [5:15:25<82:20:40, 82.28s/it]                                                       {'loss': 1.6235, 'learning_rate': 4.955710447444547e-05, 'epoch': 0.06}
  6%|▌         | 230/3833 [5:15:25<82:20:40, 82.28s/it]  6%|▌         | 231/3833 [5:16:47<82:15:26, 82.21s/it]  6%|▌         | 232/3833 [5:18:10<82:23:59, 82.38s/it]  6%|▌         | 233/3833 [5:19:32<82:21:45, 82.36s/it]  6%|▌         | 234/3833 [5:20:54<82:12:16, 82.23s/it]  6%|▌         | 235/3833 [5:22:16<82:11:47, 82.24s/it]  6%|▌         | 236/3833 [5:23:39<82:11:25, 82.26s/it]  6%|▌         | 237/3833 [5:25:01<82:20:29, 82.43s/it]  6%|▌         | 238/3833 [5:26:24<82:20:03, 82.45s/it]  6%|▌         | 239/3833 [5:27:46<82:10:21, 82.31s/it]  6%|▋         | 240/3833 [5:29:08<81:59:02, 82.14s/it]                                                       {'loss': 1.6161, 'learning_rate': 4.951788153426811e-05, 'epoch': 0.06}
  6%|▋         | 240/3833 [5:29:08<81:59:02, 82.14s/it]  6%|▋         | 241/3833 [5:30:30<81:51:47, 82.05s/it]  6%|▋         | 242/3833 [5:31:52<81:50:39, 82.05s/it]  6%|▋         | 243/3833 [5:33:14<81:49:07, 82.05s/it]  6%|▋         | 244/3833 [5:34:36<81:49:53, 82.08s/it]  6%|▋         | 245/3833 [5:35:58<81:44:40, 82.02s/it]  6%|▋         | 246/3833 [5:37:20<81:42:44, 82.01s/it]  6%|▋         | 247/3833 [5:38:42<81:47:04, 82.10s/it]  6%|▋         | 248/3833 [5:40:04<81:38:47, 81.99s/it]  6%|▋         | 249/3833 [5:41:26<81:36:44, 81.98s/it]  7%|▋         | 250/3833 [5:42:47<81:30:23, 81.89s/it]                                                       {'loss': 1.6188, 'learning_rate': 4.947701155988799e-05, 'epoch': 0.07}
  7%|▋         | 250/3833 [5:42:47<81:30:23, 81.89s/it]  7%|▋         | 251/3833 [5:44:09<81:27:27, 81.87s/it]  7%|▋         | 252/3833 [5:45:31<81:26:02, 81.87s/it]  7%|▋         | 253/3833 [5:46:53<81:25:37, 81.88s/it]  7%|▋         | 254/3833 [5:48:15<81:23:31, 81.87s/it]  7%|▋         | 255/3833 [5:49:37<81:20:38, 81.84s/it]  7%|▋         | 256/3833 [5:50:58<81:15:55, 81.79s/it]  7%|▋         | 257/3833 [5:52:20<81:20:40, 81.89s/it]  7%|▋         | 258/3833 [5:53:42<81:19:59, 81.90s/it]  7%|▋         | 259/3833 [5:55:04<81:17:45, 81.89s/it]  7%|▋         | 260/3833 [5:56:26<81:13:18, 81.84s/it]                                                       {'loss': 1.6083, 'learning_rate': 4.943449729682152e-05, 'epoch': 0.07}
  7%|▋         | 260/3833 [5:56:26<81:13:18, 81.84s/it]  7%|▋         | 261/3833 [5:57:48<81:17:32, 81.93s/it]  7%|▋         | 262/3833 [5:59:10<81:20:25, 82.00s/it]  7%|▋         | 263/3833 [6:00:33<81:35:34, 82.28s/it]  7%|▋         | 264/3833 [6:01:55<81:26:58, 82.16s/it]  7%|▋         | 265/3833 [6:03:17<81:21:02, 82.08s/it]  7%|▋         | 266/3833 [6:04:39<81:15:53, 82.02s/it]  7%|▋         | 267/3833 [6:06:00<81:09:36, 81.93s/it]  7%|▋         | 268/3833 [6:07:22<81:02:40, 81.84s/it]  7%|▋         | 269/3833 [6:08:44<81:08:34, 81.96s/it]  7%|▋         | 270/3833 [6:10:07<81:18:04, 82.15s/it]                                                       {'loss': 1.6019, 'learning_rate': 4.9390341601043235e-05, 'epoch': 0.07}
  7%|▋         | 270/3833 [6:10:07<81:18:04, 82.15s/it]  7%|▋         | 271/3833 [6:11:29<81:12:56, 82.08s/it]  7%|▋         | 272/3833 [6:12:50<81:02:47, 81.93s/it]  7%|▋         | 273/3833 [6:14:12<80:59:37, 81.90s/it]  7%|▋         | 274/3833 [6:15:35<81:08:54, 82.08s/it]  7%|▋         | 275/3833 [6:16:57<81:02:39, 82.00s/it]  7%|▋         | 276/3833 [6:18:19<81:07:19, 82.10s/it]  7%|▋         | 277/3833 [6:19:41<81:07:22, 82.13s/it]  7%|▋         | 278/3833 [6:21:03<80:56:29, 81.97s/it]  7%|▋         | 279/3833 [6:22:25<80:54:15, 81.95s/it]  7%|▋         | 280/3833 [6:23:47<80:59:16, 82.06s/it]                                                       {'loss': 1.6003, 'learning_rate': 4.934454743879394e-05, 'epoch': 0.07}
  7%|▋         | 280/3833 [6:23:47<80:59:16, 82.06s/it]  7%|▋         | 281/3833 [6:25:09<80:54:37, 82.00s/it]  7%|▋         | 282/3833 [6:26:31<81:03:17, 82.17s/it]  7%|▋         | 283/3833 [6:27:53<80:52:57, 82.02s/it]  7%|▋         | 284/3833 [6:29:15<80:50:35, 82.00s/it]  7%|▋         | 285/3833 [6:30:37<80:40:43, 81.86s/it]  7%|▋         | 286/3833 [6:31:58<80:41:29, 81.90s/it]  7%|▋         | 287/3833 [6:33:20<80:41:32, 81.92s/it]  8%|▊         | 288/3833 [6:34:43<80:46:19, 82.03s/it]  8%|▊         | 289/3833 [6:36:05<80:49:04, 82.09s/it]  8%|▊         | 290/3833 [6:37:28<80:56:17, 82.24s/it]                                                       {'loss': 1.5895, 'learning_rate': 4.929711788638152e-05, 'epoch': 0.08}
  8%|▊         | 290/3833 [6:37:28<80:56:17, 82.24s/it]  8%|▊         | 291/3833 [6:38:49<80:44:40, 82.07s/it]  8%|▊         | 292/3833 [6:40:12<80:47:09, 82.13s/it]  8%|▊         | 293/3833 [6:41:34<80:43:21, 82.09s/it]  8%|▊         | 294/3833 [6:42:56<80:40:29, 82.07s/it]  8%|▊         | 295/3833 [6:44:17<80:34:51, 81.99s/it]  8%|▊         | 296/3833 [6:45:39<80:34:54, 82.02s/it]  8%|▊         | 297/3833 [6:47:01<80:32:39, 82.00s/it]  8%|▊         | 298/3833 [6:48:23<80:26:29, 81.92s/it]  8%|▊         | 299/3833 [6:49:45<80:23:44, 81.90s/it]  8%|▊         | 300/3833 [6:51:07<80:17:58, 81.82s/it]                                                       {'loss': 1.5818, 'learning_rate': 4.924805612997418e-05, 'epoch': 0.08}
  8%|▊         | 300/3833 [6:51:07<80:17:58, 81.82s/it]  8%|▊         | 301/3833 [6:52:29<80:18:37, 81.86s/it]  8%|▊         | 302/3833 [6:53:50<80:14:37, 81.81s/it]  8%|▊         | 303/3833 [6:55:12<80:13:25, 81.81s/it]  8%|▊         | 304/3833 [6:56:34<80:10:27, 81.79s/it]  8%|▊         | 305/3833 [6:57:55<80:03:26, 81.69s/it]  8%|▊         | 306/3833 [6:59:17<79:57:53, 81.62s/it]  8%|▊         | 307/3833 [7:00:38<79:58:46, 81.66s/it]  8%|▊         | 308/3833 [7:02:01<80:16:10, 81.98s/it]  8%|▊         | 309/3833 [7:03:23<80:16:06, 82.00s/it]  8%|▊         | 310/3833 [7:04:45<80:15:52, 82.02s/it]                                                       {'loss': 1.5737, 'learning_rate': 4.919736546538648e-05, 'epoch': 0.08}
  8%|▊         | 310/3833 [7:04:45<80:15:52, 82.02s/it]  8%|▊         | 311/3833 [7:06:07<80:16:15, 82.05s/it]  8%|▊         | 312/3833 [7:07:29<80:13:27, 82.02s/it]  8%|▊         | 313/3833 [7:08:51<80:11:08, 82.01s/it]  8%|▊         | 314/3833 [7:10:13<80:07:47, 81.97s/it]  8%|▊         | 315/3833 [7:11:35<80:04:57, 81.95s/it]  8%|▊         | 316/3833 [7:12:57<79:59:26, 81.88s/it]  8%|▊         | 317/3833 [7:14:19<79:55:31, 81.83s/it]  8%|▊         | 318/3833 [7:15:40<79:48:18, 81.74s/it]  8%|▊         | 319/3833 [7:17:02<79:50:56, 81.80s/it]  8%|▊         | 320/3833 [7:18:24<79:50:29, 81.82s/it]                                                       {'loss': 1.5796, 'learning_rate': 4.91450492978579e-05, 'epoch': 0.08}
  8%|▊         | 320/3833 [7:18:24<79:50:29, 81.82s/it]  8%|▊         | 321/3833 [7:19:46<79:57:47, 81.97s/it]  8%|▊         | 322/3833 [7:21:08<79:54:47, 81.94s/it]  8%|▊         | 323/3833 [7:22:30<79:52:03, 81.92s/it]  8%|▊         | 324/3833 [7:23:52<79:51:54, 81.94s/it]  8%|▊         | 325/3833 [7:25:14<79:47:56, 81.89s/it]  9%|▊         | 326/3833 [7:26:36<79:56:40, 82.06s/it]  9%|▊         | 327/3833 [7:27:58<79:50:19, 81.98s/it]  9%|▊         | 328/3833 [7:29:20<79:51:23, 82.02s/it]  9%|▊         | 329/3833 [7:30:42<79:44:54, 81.93s/it]  9%|▊         | 330/3833 [7:32:03<79:39:06, 81.86s/it]                                                       {'loss': 1.5753, 'learning_rate': 4.909111114182412e-05, 'epoch': 0.09}
  9%|▊         | 330/3833 [7:32:04<79:39:06, 81.86s/it]  9%|▊         | 331/3833 [7:33:25<79:32:58, 81.78s/it]  9%|▊         | 332/3833 [7:34:47<79:32:45, 81.80s/it]  9%|▊         | 333/3833 [7:36:09<79:29:52, 81.77s/it]  9%|▊         | 334/3833 [7:37:31<79:31:34, 81.82s/it]  9%|▊         | 335/3833 [7:38:52<79:29:12, 81.80s/it]  9%|▉         | 336/3833 [7:40:14<79:24:26, 81.75s/it]  9%|▉         | 337/3833 [7:41:36<79:23:40, 81.76s/it]  9%|▉         | 338/3833 [7:42:58<79:24:32, 81.79s/it]  9%|▉         | 339/3833 [7:44:19<79:19:52, 81.74s/it]  9%|▉         | 340/3833 [7:45:41<79:25:40, 81.86s/it]                                                       {'loss': 1.5732, 'learning_rate': 4.903555462068088e-05, 'epoch': 0.09}
  9%|▉         | 340/3833 [7:45:41<79:25:40, 81.86s/it]  9%|▉         | 341/3833 [7:47:03<79:23:45, 81.85s/it]  9%|▉         | 342/3833 [7:48:25<79:17:08, 81.76s/it]  9%|▉         | 343/3833 [7:49:46<79:12:30, 81.71s/it]  9%|▉         | 344/3833 [7:51:08<79:07:33, 81.64s/it]  9%|▉         | 345/3833 [7:52:30<79:16:54, 81.83s/it]  9%|▉         | 346/3833 [7:53:52<79:15:34, 81.83s/it]  9%|▉         | 347/3833 [7:55:14<79:11:59, 81.79s/it]  9%|▉         | 348/3833 [7:56:36<79:12:51, 81.83s/it]  9%|▉         | 349/3833 [7:57:57<79:12:28, 81.85s/it]  9%|▉         | 350/3833 [7:59:19<79:11:56, 81.86s/it]                                                       {'loss': 1.5559, 'learning_rate': 4.897838346654061e-05, 'epoch': 0.09}
  9%|▉         | 350/3833 [7:59:19<79:11:56, 81.86s/it]  9%|▉         | 351/3833 [8:00:41<79:08:29, 81.82s/it]  9%|▉         | 352/3833 [8:02:03<79:02:12, 81.74s/it]  9%|▉         | 353/3833 [8:03:24<78:55:01, 81.64s/it]  9%|▉         | 354/3833 [8:04:46<78:57:59, 81.71s/it]  9%|▉         | 355/3833 [8:06:07<78:53:34, 81.66s/it]  9%|▉         | 356/3833 [8:07:29<78:57:02, 81.74s/it]  9%|▉         | 357/3833 [8:08:51<78:53:27, 81.71s/it]  9%|▉         | 358/3833 [8:10:13<79:02:32, 81.89s/it]  9%|▉         | 359/3833 [8:11:35<79:04:01, 81.93s/it]  9%|▉         | 360/3833 [8:12:57<79:01:40, 81.92s/it]                                                       {'loss': 1.5488, 'learning_rate': 4.891960151998173e-05, 'epoch': 0.09}
  9%|▉         | 360/3833 [8:12:57<79:01:40, 81.92s/it]  9%|▉         | 361/3833 [8:14:19<79:06:31, 82.03s/it]  9%|▉         | 362/3833 [8:15:41<79:03:45, 82.00s/it]  9%|▉         | 363/3833 [8:17:03<78:56:22, 81.90s/it]  9%|▉         | 364/3833 [8:18:25<78:56:37, 81.92s/it] 10%|▉         | 365/3833 [8:19:47<78:47:45, 81.80s/it] 10%|▉         | 366/3833 [8:21:08<78:44:05, 81.76s/it] 10%|▉         | 367/3833 [8:22:30<78:44:00, 81.78s/it] 10%|▉         | 368/3833 [8:23:52<78:45:08, 81.82s/it] 10%|▉         | 369/3833 [8:25:14<78:40:17, 81.76s/it] 10%|▉         | 370/3833 [8:26:35<78:41:31, 81.81s/it]                                                       {'loss': 1.5627, 'learning_rate': 4.885921272979059e-05, 'epoch': 0.1}
 10%|▉         | 370/3833 [8:26:36<78:41:31, 81.81s/it] 10%|▉         | 371/3833 [8:27:58<78:44:37, 81.88s/it] 10%|▉         | 372/3833 [8:29:19<78:39:40, 81.82s/it] 10%|▉         | 373/3833 [8:30:41<78:34:05, 81.75s/it] 10%|▉         | 374/3833 [8:32:02<78:27:04, 81.65s/it] 10%|▉         | 375/3833 [8:33:24<78:28:48, 81.70s/it] 10%|▉         | 376/3833 [8:34:46<78:26:16, 81.68s/it] 10%|▉         | 377/3833 [8:36:07<78:24:52, 81.68s/it] 10%|▉         | 378/3833 [8:37:30<78:35:07, 81.88s/it] 10%|▉         | 379/3833 [8:38:52<78:36:35, 81.93s/it] 10%|▉         | 380/3833 [8:40:14<78:35:36, 81.94s/it]                                                       {'loss': 1.5542, 'learning_rate': 4.8797221152696295e-05, 'epoch': 0.1}
 10%|▉         | 380/3833 [8:40:14<78:35:36, 81.94s/it] 10%|▉         | 381/3833 [8:41:36<78:44:13, 82.11s/it] 10%|▉         | 382/3833 [8:42:58<78:35:04, 81.98s/it] 10%|▉         | 383/3833 [8:44:20<78:29:11, 81.90s/it] 10%|█         | 384/3833 [8:45:42<78:35:24, 82.03s/it] 10%|█         | 385/3833 [8:47:04<78:40:28, 82.14s/it] 10%|█         | 386/3833 [8:48:27<78:52:10, 82.37s/it] 10%|█         | 387/3833 [8:49:49<78:46:18, 82.29s/it] 10%|█         | 388/3833 [8:51:12<78:44:00, 82.28s/it] 10%|█         | 389/3833 [8:52:33<78:35:30, 82.15s/it] 10%|█         | 390/3833 [8:53:55<78:31:30, 82.11s/it]                                                       {'loss': 1.5482, 'learning_rate': 4.873363095309807e-05, 'epoch': 0.1}
 10%|█         | 390/3833 [8:53:56<78:31:30, 82.11s/it] 10%|█         | 391/3833 [8:55:18<78:37:40, 82.24s/it] 10%|█         | 392/3833 [8:56:40<78:35:24, 82.22s/it] 10%|█         | 393/3833 [8:58:03<78:42:16, 82.37s/it] 10%|█         | 394/3833 [8:59:25<78:43:00, 82.40s/it] 10%|█         | 395/3833 [9:00:48<78:44:04, 82.44s/it] 10%|█         | 396/3833 [9:02:10<78:40:20, 82.40s/it] 10%|█         | 397/3833 [9:03:32<78:32:14, 82.29s/it] 10%|█         | 398/3833 [9:04:54<78:29:36, 82.26s/it] 10%|█         | 399/3833 [9:06:17<78:25:47, 82.22s/it] 10%|█         | 400/3833 [9:07:38<78:17:33, 82.10s/it]                                                       {'loss': 1.5447, 'learning_rate': 4.8668446402785636e-05, 'epoch': 0.1}
 10%|█         | 400/3833 [9:07:38<78:17:33, 82.10s/it] 10%|█         | 401/3833 [9:09:01<78:17:43, 82.13s/it] 10%|█         | 402/3833 [9:10:23<78:15:16, 82.11s/it] 11%|█         | 403/3833 [9:11:46<78:26:44, 82.33s/it] 11%|█         | 404/3833 [9:13:08<78:19:54, 82.24s/it] 11%|█         | 405/3833 [9:14:30<78:30:22, 82.45s/it] 11%|█         | 406/3833 [9:15:52<78:21:10, 82.31s/it] 11%|█         | 407/3833 [9:17:15<78:21:10, 82.33s/it] 11%|█         | 408/3833 [9:18:37<78:17:34, 82.29s/it] 11%|█         | 409/3833 [9:20:00<78:19:26, 82.35s/it] 11%|█         | 410/3833 [9:21:21<78:10:12, 82.21s/it]                                                       {'loss': 1.5337, 'learning_rate': 4.860167188065215e-05, 'epoch': 0.11}
 11%|█         | 410/3833 [9:21:21<78:10:12, 82.21s/it] 11%|█         | 411/3833 [9:22:44<78:13:33, 82.30s/it] 11%|█         | 412/3833 [9:24:06<78:08:52, 82.24s/it] 11%|█         | 413/3833 [9:25:29<78:12:27, 82.32s/it] 11%|█         | 414/3833 [9:26:51<78:11:04, 82.32s/it] 11%|█         | 415/3833 [9:28:13<78:10:03, 82.33s/it] 11%|█         | 416/3833 [9:29:35<78:07:14, 82.30s/it] 11%|█         | 417/3833 [9:30:58<78:06:39, 82.32s/it] 11%|█         | 418/3833 [9:32:20<78:07:10, 82.35s/it] 11%|█         | 419/3833 [9:33:43<78:14:01, 82.50s/it] 11%|█         | 420/3833 [9:35:05<78:10:12, 82.45s/it]                                                       {'loss': 1.5432, 'learning_rate': 4.8533311872400095e-05, 'epoch': 0.11}
 11%|█         | 420/3833 [9:35:05<78:10:12, 82.45s/it] 11%|█         | 421/3833 [9:36:28<78:11:46, 82.50s/it] 11%|█         | 422/3833 [9:37:51<78:19:31, 82.67s/it] 11%|█         | 423/3833 [9:39:13<78:13:07, 82.58s/it] 11%|█         | 424/3833 [9:40:36<78:07:39, 82.51s/it] 11%|█         | 425/3833 [9:41:58<78:02:28, 82.44s/it] 11%|█         | 426/3833 [9:43:21<78:03:07, 82.47s/it] 11%|█         | 427/3833 [9:44:43<77:59:21, 82.43s/it] 11%|█         | 428/3833 [9:46:05<77:57:45, 82.43s/it] 11%|█         | 429/3833 [9:47:28<77:55:35, 82.41s/it] 11%|█         | 430/3833 [9:48:50<77:54:13, 82.41s/it]                                                       {'loss': 1.5356, 'learning_rate': 4.84633709702399e-05, 'epoch': 0.11}
 11%|█         | 430/3833 [9:48:50<77:54:13, 82.41s/it] 11%|█         | 431/3833 [9:50:12<77:50:18, 82.37s/it] 11%|█▏        | 432/3833 [9:51:35<77:44:32, 82.29s/it] 11%|█▏        | 433/3833 [9:52:57<77:45:02, 82.32s/it] 11%|█▏        | 434/3833 [9:54:19<77:38:05, 82.23s/it] 11%|█▏        | 435/3833 [9:55:41<77:40:30, 82.29s/it] 11%|█▏        | 436/3833 [9:57:03<77:33:40, 82.20s/it] 11%|█▏        | 437/3833 [9:58:26<77:31:57, 82.19s/it] 11%|█▏        | 438/3833 [9:59:48<77:34:50, 82.27s/it] 11%|█▏        | 439/3833 [10:01:10<77:34:28, 82.28s/it] 11%|█▏        | 440/3833 [10:02:32<77:30:20, 82.23s/it]                                                        {'loss': 1.5251, 'learning_rate': 4.8391853872581505e-05, 'epoch': 0.11}
 11%|█▏        | 440/3833 [10:02:32<77:30:20, 82.23s/it] 12%|█▏        | 441/3833 [10:03:55<77:31:36, 82.28s/it] 12%|█▏        | 442/3833 [10:05:17<77:36:31, 82.39s/it] 12%|█▏        | 443/3833 [10:06:40<77:33:39, 82.37s/it] 12%|█▏        | 444/3833 [10:08:02<77:29:10, 82.31s/it] 12%|█▏        | 445/3833 [10:09:25<77:31:41, 82.38s/it] 12%|█▏        | 446/3833 [10:10:47<77:29:09, 82.36s/it] 12%|█▏        | 447/3833 [10:12:10<77:37:26, 82.53s/it] 12%|█▏        | 448/3833 [10:13:32<77:36:08, 82.53s/it] 12%|█▏        | 449/3833 [10:14:55<77:37:04, 82.57s/it] 12%|█▏        | 450/3833 [10:16:17<77:33:26, 82.53s/it]                                                        {'loss': 1.5288, 'learning_rate': 4.831876538371869e-05, 'epoch': 0.12}
 12%|█▏        | 450/3833 [10:16:17<77:33:26, 82.53s/it] 12%|█▏        | 451/3833 [10:17:40<77:30:45, 82.51s/it] 12%|█▏        | 452/3833 [10:19:02<77:26:48, 82.46s/it] 12%|█▏        | 453/3833 [10:20:25<77:23:59, 82.44s/it] 12%|█▏        | 454/3833 [10:21:47<77:24:33, 82.47s/it] 12%|█▏        | 455/3833 [10:23:10<77:21:39, 82.45s/it] 12%|█▏        | 456/3833 [10:24:32<77:13:11, 82.32s/it] 12%|█▏        | 457/3833 [10:25:53<77:03:22, 82.17s/it] 12%|█▏        | 458/3833 [10:27:15<76:56:19, 82.07s/it] 12%|█▏        | 459/3833 [10:28:37<76:55:10, 82.07s/it] 12%|█▏        | 460/3833 [10:30:00<76:57:29, 82.14s/it]                                                        {'loss': 1.5352, 'learning_rate': 4.824411041350637e-05, 'epoch': 0.12}
 12%|█▏        | 460/3833 [10:30:00<76:57:29, 82.14s/it] 12%|█▏        | 461/3833 [10:31:22<76:57:36, 82.16s/it] 12%|█▏        | 462/3833 [10:32:44<76:54:58, 82.14s/it] 12%|█▏        | 463/3833 [10:34:06<76:49:05, 82.06s/it] 12%|█▏        | 464/3833 [10:35:28<76:51:32, 82.13s/it] 12%|█▏        | 465/3833 [10:36:50<76:55:47, 82.23s/it] 12%|█▏        | 466/3833 [10:38:13<76:59:06, 82.31s/it] 12%|█▏        | 467/3833 [10:39:35<76:52:51, 82.23s/it] 12%|█▏        | 468/3833 [10:40:57<76:45:23, 82.12s/it] 12%|█▏        | 469/3833 [10:42:19<76:45:47, 82.15s/it] 12%|█▏        | 470/3833 [10:43:41<76:41:23, 82.09s/it]                                                        {'loss': 1.5177, 'learning_rate': 4.816789397703073e-05, 'epoch': 0.12}
 12%|█▏        | 470/3833 [10:43:41<76:41:23, 82.09s/it] 12%|█▏        | 471/3833 [10:45:03<76:40:54, 82.11s/it] 12%|█▏        | 472/3833 [10:46:25<76:39:34, 82.11s/it] 12%|█▏        | 473/3833 [10:47:47<76:34:35, 82.05s/it] 12%|█▏        | 474/3833 [10:49:10<76:39:36, 82.16s/it] 12%|█▏        | 475/3833 [10:50:32<76:40:09, 82.19s/it] 12%|█▏        | 476/3833 [10:51:54<76:38:25, 82.19s/it] 12%|█▏        | 477/3833 [10:53:16<76:37:11, 82.19s/it] 12%|█▏        | 478/3833 [10:54:38<76:30:55, 82.10s/it] 12%|█▏        | 479/3833 [10:56:00<76:24:31, 82.01s/it] 13%|█▎        | 480/3833 [10:57:22<76:20:46, 81.97s/it]                                                        {'loss': 1.5191, 'learning_rate': 4.8090121194272386e-05, 'epoch': 0.13}
 13%|█▎        | 480/3833 [10:57:22<76:20:46, 81.97s/it] 13%|█▎        | 481/3833 [10:58:45<76:32:20, 82.20s/it] 13%|█▎        | 482/3833 [11:00:07<76:27:02, 82.13s/it] 13%|█▎        | 483/3833 [11:01:28<76:17:05, 81.98s/it] 13%|█▎        | 484/3833 [11:02:50<76:09:47, 81.87s/it] 13%|█▎        | 485/3833 [11:04:11<76:02:30, 81.77s/it] 13%|█▎        | 486/3833 [11:05:33<76:01:27, 81.77s/it] 13%|█▎        | 487/3833 [11:06:55<75:58:05, 81.74s/it] 13%|█▎        | 488/3833 [11:08:17<75:57:59, 81.76s/it] 13%|█▎        | 489/3833 [11:09:38<75:58:30, 81.79s/it] 13%|█▎        | 490/3833 [11:11:00<75:52:16, 81.70s/it]                                                        {'loss': 1.5261, 'learning_rate': 4.801079728976238e-05, 'epoch': 0.13}
 13%|█▎        | 490/3833 [11:11:00<75:52:16, 81.70s/it] 13%|█▎        | 491/3833 [11:12:22<75:52:13, 81.73s/it] 13%|█▎        | 492/3833 [11:13:43<75:49:37, 81.71s/it] 13%|█▎        | 493/3833 [11:15:05<75:45:36, 81.66s/it] 13%|█▎        | 494/3833 [11:16:27<75:44:35, 81.66s/it] 13%|█▎        | 495/3833 [11:17:48<75:41:18, 81.63s/it] 13%|█▎        | 496/3833 [11:19:10<75:43:00, 81.68s/it] 13%|█▎        | 497/3833 [11:20:32<75:38:58, 81.64s/it] 13%|█▎        | 498/3833 [11:21:53<75:41:13, 81.70s/it] 13%|█▎        | 499/3833 [11:23:15<75:39:47, 81.70s/it] 13%|█▎        | 500/3833 [11:24:37<75:38:13, 81.70s/it]                                                        {'loss': 1.5252, 'learning_rate': 4.792992759223123e-05, 'epoch': 0.13}
 13%|█▎        | 500/3833 [11:24:37<75:38:13, 81.70s/it] 13%|█▎        | 501/3833 [11:25:59<75:45:21, 81.85s/it] 13%|█▎        | 502/3833 [11:27:21<75:40:54, 81.79s/it] 13%|█▎        | 503/3833 [11:28:42<75:38:12, 81.77s/it] 13%|█▎        | 504/3833 [11:30:04<75:34:52, 81.73s/it] 13%|█▎        | 505/3833 [11:31:26<75:32:04, 81.71s/it] 13%|█▎        | 506/3833 [11:32:48<75:34:27, 81.78s/it] 13%|█▎        | 507/3833 [11:34:09<75:30:28, 81.73s/it] 13%|█▎        | 508/3833 [11:35:31<75:28:32, 81.72s/it] 13%|█▎        | 509/3833 [11:36:52<75:24:46, 81.67s/it] 13%|█▎        | 510/3833 [11:38:14<75:27:22, 81.75s/it]                                                        {'loss': 1.5171, 'learning_rate': 4.784751753425097e-05, 'epoch': 0.13}
 13%|█▎        | 510/3833 [11:38:14<75:27:22, 81.75s/it] 13%|█▎        | 511/3833 [11:39:36<75:28:47, 81.80s/it] 13%|█▎        | 512/3833 [11:40:58<75:24:47, 81.75s/it] 13%|█▎        | 513/3833 [11:42:20<75:22:27, 81.73s/it] 13%|█▎        | 514/3833 [11:43:41<75:23:09, 81.77s/it] 13%|█▎        | 515/3833 [11:45:03<75:24:36, 81.82s/it] 13%|█▎        | 516/3833 [11:46:25<75:22:00, 81.80s/it] 13%|█▎        | 517/3833 [11:47:47<75:28:53, 81.95s/it] 14%|█▎        | 518/3833 [11:49:09<75:26:48, 81.93s/it] 14%|█▎        | 519/3833 [11:50:31<75:22:40, 81.88s/it] 14%|█▎        | 520/3833 [11:51:53<75:18:07, 81.83s/it]                                                        {'loss': 1.5117, 'learning_rate': 4.776357265187025e-05, 'epoch': 0.14}
 14%|█▎        | 520/3833 [11:51:53<75:18:07, 81.83s/it] 14%|█▎        | 521/3833 [11:53:15<75:16:36, 81.82s/it] 14%|█▎        | 522/3833 [11:54:36<75:11:54, 81.76s/it] 14%|█▎        | 523/3833 [11:55:58<75:07:58, 81.72s/it] 14%|█▎        | 524/3833 [11:57:19<75:04:43, 81.68s/it] 14%|█▎        | 525/3833 [11:58:41<75:06:44, 81.74s/it] 14%|█▎        | 526/3833 [12:00:03<75:07:40, 81.78s/it] 14%|█▎        | 527/3833 [12:01:25<75:10:14, 81.86s/it] 14%|█▍        | 528/3833 [12:02:47<75:07:42, 81.83s/it] 14%|█▍        | 529/3833 [12:04:09<75:02:55, 81.77s/it] 14%|█▍        | 530/3833 [12:05:30<74:57:58, 81.71s/it]                                                        {'loss': 1.5104, 'learning_rate': 4.767809858424234e-05, 'epoch': 0.14}
 14%|█▍        | 530/3833 [12:05:30<74:57:58, 81.71s/it] 14%|█▍        | 531/3833 [12:06:53<75:08:58, 81.93s/it] 14%|█▍        | 532/3833 [12:08:15<75:06:21, 81.91s/it] 14%|█▍        | 533/3833 [12:09:36<75:02:47, 81.87s/it] 14%|█▍        | 534/3833 [12:10:58<74:59:47, 81.84s/it] 14%|█▍        | 535/3833 [12:12:20<74:56:03, 81.80s/it] 14%|█▍        | 536/3833 [12:13:42<74:56:19, 81.83s/it] 14%|█▍        | 537/3833 [12:15:04<74:58:46, 81.90s/it] 14%|█▍        | 538/3833 [12:16:26<74:57:51, 81.90s/it] 14%|█▍        | 539/3833 [12:17:47<74:52:16, 81.83s/it] 14%|█▍        | 540/3833 [12:19:09<74:46:44, 81.75s/it]                                                        {'loss': 1.5218, 'learning_rate': 4.759110107324639e-05, 'epoch': 0.14}
 14%|█▍        | 540/3833 [12:19:09<74:46:44, 81.75s/it] 14%|█▍        | 541/3833 [12:20:31<74:49:49, 81.83s/it] 14%|█▍        | 542/3833 [12:21:52<74:45:13, 81.77s/it] 14%|█▍        | 543/3833 [12:23:14<74:45:26, 81.80s/it] 14%|█▍        | 544/3833 [12:24:36<74:44:04, 81.80s/it] 14%|█▍        | 545/3833 [12:25:58<74:43:18, 81.81s/it] 14%|█▍        | 546/3833 [12:27:20<74:40:02, 81.78s/it] 14%|█▍        | 547/3833 [12:28:41<74:35:50, 81.73s/it] 14%|█▍        | 548/3833 [12:30:03<74:34:47, 81.73s/it] 14%|█▍        | 549/3833 [12:31:25<74:35:09, 81.76s/it] 14%|█▍        | 550/3833 [12:32:47<74:34:03, 81.77s/it]                                                        {'loss': 1.5094, 'learning_rate': 4.750258596310171e-05, 'epoch': 0.14}
 14%|█▍        | 550/3833 [12:32:47<74:34:03, 81.77s/it] 14%|█▍        | 551/3833 [12:34:08<74:32:54, 81.77s/it] 14%|█▍        | 552/3833 [12:35:30<74:33:08, 81.80s/it] 14%|█▍        | 553/3833 [12:36:52<74:30:31, 81.78s/it] 14%|█▍        | 554/3833 [12:38:14<74:34:32, 81.88s/it] 14%|█▍        | 555/3833 [12:39:36<74:39:30, 81.99s/it] 15%|█▍        | 556/3833 [12:40:58<74:38:06, 81.99s/it] 15%|█▍        | 557/3833 [12:42:20<74:31:57, 81.90s/it] 15%|█▍        | 558/3833 [12:43:42<74:29:13, 81.88s/it] 15%|█▍        | 559/3833 [12:45:04<74:33:12, 81.98s/it] 15%|█▍        | 560/3833 [12:46:26<74:34:03, 82.02s/it]                                                        {'loss': 1.5052, 'learning_rate': 4.74125591999751e-05, 'epoch': 0.15}
 15%|█▍        | 560/3833 [12:46:26<74:34:03, 82.02s/it] 15%|█▍        | 561/3833 [12:47:48<74:31:42, 82.00s/it] 15%|█▍        | 562/3833 [12:49:11<74:36:58, 82.12s/it] 15%|█▍        | 563/3833 [12:50:33<74:33:08, 82.08s/it] 15%|█▍        | 564/3833 [12:51:55<74:31:12, 82.07s/it] 15%|█▍        | 565/3833 [12:53:17<74:29:26, 82.06s/it] 15%|█▍        | 566/3833 [12:54:39<74:26:44, 82.03s/it] 15%|█▍        | 567/3833 [12:56:01<74:27:19, 82.07s/it] 15%|█▍        | 568/3833 [12:57:23<74:22:47, 82.01s/it] 15%|█▍        | 569/3833 [12:58:45<74:20:17, 81.99s/it] 15%|█▍        | 570/3833 [13:00:07<74:20:11, 82.01s/it]                                                        {'loss': 1.5065, 'learning_rate': 4.732102683158151e-05, 'epoch': 0.15}
 15%|█▍        | 570/3833 [13:00:07<74:20:11, 82.01s/it] 15%|█▍        | 571/3833 [13:01:29<74:31:20, 82.24s/it] 15%|█▍        | 572/3833 [13:02:51<74:25:33, 82.16s/it] 15%|█▍        | 573/3833 [13:04:14<74:28:43, 82.25s/it] 15%|█▍        | 574/3833 [13:05:36<74:21:50, 82.15s/it] 15%|█▌        | 575/3833 [13:06:58<74:21:43, 82.17s/it] 15%|█▌        | 576/3833 [13:08:20<74:14:36, 82.06s/it] 15%|█▌        | 577/3833 [13:09:42<74:14:52, 82.09s/it] 15%|█▌        | 578/3833 [13:11:04<74:17:53, 82.17s/it] 15%|█▌        | 579/3833 [13:12:26<74:11:25, 82.08s/it] 15%|█▌        | 580/3833 [13:13:48<74:04:17, 81.97s/it]                                                        {'loss': 1.5084, 'learning_rate': 4.722799500677766e-05, 'epoch': 0.15}
 15%|█▌        | 580/3833 [13:13:48<74:04:17, 81.97s/it] 15%|█▌        | 581/3833 [13:15:10<74:00:36, 81.93s/it] 15%|█▌        | 582/3833 [13:16:31<73:54:28, 81.84s/it] 15%|█▌        | 583/3833 [13:17:53<73:54:08, 81.86s/it] 15%|█▌        | 584/3833 [13:19:15<73:58:18, 81.96s/it] 15%|█▌        | 585/3833 [13:20:38<73:59:07, 82.00s/it] 15%|█▌        | 586/3833 [13:22:00<73:58:33, 82.02s/it] 15%|█▌        | 587/3833 [13:23:21<73:52:29, 81.93s/it] 15%|█▌        | 588/3833 [13:24:43<73:48:12, 81.88s/it] 15%|█▌        | 589/3833 [13:26:05<73:49:01, 81.92s/it] 15%|█▌        | 590/3833 [13:27:27<73:43:10, 81.83s/it]                                                        {'loss': 1.5025, 'learning_rate': 4.713346997514909e-05, 'epoch': 0.15}
 15%|█▌        | 590/3833 [13:27:27<73:43:10, 81.83s/it] 15%|█▌        | 591/3833 [13:28:49<73:45:22, 81.90s/it] 15%|█▌        | 592/3833 [13:30:11<73:42:52, 81.88s/it] 15%|█▌        | 593/3833 [13:31:32<73:37:58, 81.81s/it] 15%|█▌        | 594/3833 [13:32:54<73:38:58, 81.86s/it] 16%|█▌        | 595/3833 [13:34:16<73:37:43, 81.86s/it] 16%|█▌        | 596/3833 [13:35:38<73:31:26, 81.77s/it] 16%|█▌        | 597/3833 [13:37:00<73:37:37, 81.91s/it] 16%|█▌        | 598/3833 [13:38:22<73:35:00, 81.89s/it] 16%|█▌        | 599/3833 [13:39:44<73:35:47, 81.93s/it] 16%|█▌        | 600/3833 [13:41:06<73:32:42, 81.89s/it]                                                        {'loss': 1.5032, 'learning_rate': 4.703745808659027e-05, 'epoch': 0.16}
 16%|█▌        | 600/3833 [13:41:06<73:32:42, 81.89s/it] 16%|█▌        | 601/3833 [13:42:27<73:25:55, 81.79s/it] 16%|█▌        | 602/3833 [13:43:49<73:28:59, 81.88s/it] 16%|█▌        | 603/3833 [13:45:11<73:28:54, 81.90s/it] 16%|█▌        | 604/3833 [13:46:33<73:22:39, 81.81s/it] 16%|█▌        | 605/3833 [13:47:55<73:24:37, 81.87s/it] 16%|█▌        | 606/3833 [13:49:17<73:21:19, 81.83s/it] 16%|█▌        | 607/3833 [13:50:38<73:19:44, 81.83s/it] 16%|█▌        | 608/3833 [13:52:00<73:19:57, 81.86s/it] 16%|█▌        | 609/3833 [13:53:22<73:20:36, 81.90s/it] 16%|█▌        | 610/3833 [13:54:45<73:24:35, 82.00s/it]                                                        {'loss': 1.4915, 'learning_rate': 4.6939965790878e-05, 'epoch': 0.16}
 16%|█▌        | 610/3833 [13:54:45<73:24:35, 82.00s/it] 16%|█▌        | 611/3833 [13:56:07<73:24:10, 82.01s/it] 16%|█▌        | 612/3833 [13:57:28<73:16:40, 81.90s/it] 16%|█▌        | 613/3833 [13:58:51<73:24:17, 82.07s/it] 16%|█▌        | 614/3833 [14:00:12<73:17:53, 81.97s/it] 16%|█▌        | 615/3833 [14:01:34<73:15:01, 81.95s/it] 16%|█▌        | 616/3833 [14:02:56<73:14:00, 81.95s/it] 16%|█▌        | 617/3833 [14:04:18<73:10:41, 81.92s/it] 16%|█▌        | 618/3833 [14:05:40<73:07:23, 81.88s/it] 16%|█▌        | 619/3833 [14:07:02<73:04:14, 81.85s/it] 16%|█▌        | 620/3833 [14:08:23<73:00:19, 81.80s/it]                                                        {'loss': 1.4952, 'learning_rate': 4.6840999637238246e-05, 'epoch': 0.16}
 16%|█▌        | 620/3833 [14:08:23<73:00:19, 81.80s/it] 16%|█▌        | 621/3833 [14:09:45<73:00:18, 81.82s/it] 16%|█▌        | 622/3833 [14:11:07<73:03:56, 81.92s/it] 16%|█▋        | 623/3833 [14:12:29<72:57:20, 81.82s/it] 16%|█▋        | 624/3833 [14:13:51<73:02:13, 81.94s/it] 16%|█▋        | 625/3833 [14:15:13<72:57:15, 81.87s/it] 16%|█▋        | 626/3833 [14:16:35<72:53:09, 81.82s/it] 16%|█▋        | 627/3833 [14:17:57<72:58:35, 81.95s/it] 16%|█▋        | 628/3833 [14:19:19<73:03:54, 82.07s/it] 16%|█▋        | 629/3833 [14:20:41<72:56:47, 81.96s/it] 16%|█▋        | 630/3833 [14:22:03<72:58:14, 82.02s/it]                                                        {'loss': 1.4846, 'learning_rate': 4.6740566273906065e-05, 'epoch': 0.16}
 16%|█▋        | 630/3833 [14:22:03<72:58:14, 82.02s/it] 16%|█▋        | 631/3833 [14:23:25<72:50:49, 81.90s/it] 16%|█▋        | 632/3833 [14:24:46<72:47:25, 81.86s/it] 17%|█▋        | 633/3833 [14:26:09<72:56:03, 82.05s/it] 17%|█▋        | 634/3833 [14:27:31<72:52:19, 82.01s/it] 17%|█▋        | 635/3833 [14:28:53<72:47:31, 81.94s/it] 17%|█▋        | 636/3833 [14:30:14<72:43:11, 81.89s/it] 17%|█▋        | 637/3833 [14:31:36<72:44:10, 81.93s/it] 17%|█▋        | 638/3833 [14:32:58<72:43:37, 81.95s/it] 17%|█▋        | 639/3833 [14:34:20<72:35:15, 81.81s/it] 17%|█▋        | 640/3833 [14:35:42<72:38:29, 81.90s/it]                                                        {'loss': 1.4793, 'learning_rate': 4.663867244767907e-05, 'epoch': 0.17}
 17%|█▋        | 640/3833 [14:35:42<72:38:29, 81.90s/it] 17%|█▋        | 641/3833 [14:37:04<72:35:46, 81.88s/it] 17%|█▋        | 642/3833 [14:38:26<72:31:56, 81.83s/it] 17%|█▋        | 643/3833 [14:39:47<72:29:54, 81.82s/it] 17%|█▋        | 644/3833 [14:41:09<72:28:45, 81.82s/it] 17%|█▋        | 645/3833 [14:42:31<72:26:04, 81.80s/it] 17%|█▋        | 646/3833 [14:43:53<72:31:35, 81.93s/it] 17%|█▋        | 647/3833 [14:45:15<72:28:55, 81.90s/it] 17%|█▋        | 648/3833 [14:46:37<72:32:36, 82.00s/it] 17%|█▋        | 649/3833 [14:47:59<72:27:22, 81.92s/it] 17%|█▋        | 650/3833 [14:49:20<72:19:51, 81.81s/it]                                                        {'loss': 1.4935, 'learning_rate': 4.653532500346417e-05, 'epoch': 0.17}
 17%|█▋        | 650/3833 [14:49:21<72:19:51, 81.81s/it] 17%|█▋        | 651/3833 [14:50:42<72:19:48, 81.83s/it] 17%|█▋        | 652/3833 [14:52:04<72:19:52, 81.86s/it] 17%|█▋        | 653/3833 [14:53:26<72:17:16, 81.84s/it] 17%|█▋        | 654/3833 [14:54:48<72:18:06, 81.88s/it] 17%|█▋        | 655/3833 [14:56:10<72:14:15, 81.83s/it] 17%|█▋        | 656/3833 [14:57:32<72:11:40, 81.81s/it] 17%|█▋        | 657/3833 [14:58:54<72:17:14, 81.94s/it] 17%|█▋        | 658/3833 [15:00:16<72:15:20, 81.93s/it] 17%|█▋        | 659/3833 [15:01:38<72:19:31, 82.03s/it] 17%|█▋        | 660/3833 [15:03:00<72:13:17, 81.94s/it]                                                        {'loss': 1.4845, 'learning_rate': 4.6430530883817776e-05, 'epoch': 0.17}
 17%|█▋        | 660/3833 [15:03:00<72:13:17, 81.94s/it] 17%|█▋        | 661/3833 [15:04:22<72:12:34, 81.95s/it] 17%|█▋        | 662/3833 [15:05:44<72:15:27, 82.03s/it] 17%|█▋        | 663/3833 [15:07:06<72:09:41, 81.95s/it] 17%|█▋        | 664/3833 [15:08:27<72:04:42, 81.88s/it] 17%|█▋        | 665/3833 [15:09:49<72:06:51, 81.95s/it] 17%|█▋        | 666/3833 [15:11:11<72:06:19, 81.96s/it] 17%|█▋        | 667/3833 [15:12:33<72:05:33, 81.98s/it] 17%|█▋        | 668/3833 [15:13:55<72:00:58, 81.91s/it] 17%|█▋        | 669/3833 [15:15:17<71:55:01, 81.83s/it] 17%|█▋        | 670/3833 [15:16:39<71:56:04, 81.87s/it]                                                        {'loss': 1.4877, 'learning_rate': 4.6324297128479385e-05, 'epoch': 0.17}
 17%|█▋        | 670/3833 [15:16:39<71:56:04, 81.87s/it] 18%|█▊        | 671/3833 [15:18:00<71:51:30, 81.81s/it] 18%|█▊        | 672/3833 [15:19:22<71:48:32, 81.78s/it] 18%|█▊        | 673/3833 [15:20:44<71:53:15, 81.90s/it] 18%|█▊        | 674/3833 [15:22:07<71:57:53, 82.01s/it] 18%|█▊        | 675/3833 [15:23:29<71:55:30, 81.99s/it] 18%|█▊        | 676/3833 [15:24:51<71:55:21, 82.02s/it] 18%|█▊        | 677/3833 [15:26:13<71:52:37, 81.99s/it] 18%|█▊        | 678/3833 [15:27:35<71:51:25, 81.99s/it] 18%|█▊        | 679/3833 [15:28:56<71:48:30, 81.96s/it] 18%|█▊        | 680/3833 [15:30:18<71:40:12, 81.83s/it]                                                        {'loss': 1.476, 'learning_rate': 4.6216630873898714e-05, 'epoch': 0.18}
 18%|█▊        | 680/3833 [15:30:18<71:40:12, 81.83s/it] 18%|█▊        | 681/3833 [15:31:40<71:39:35, 81.85s/it] 18%|█▊        | 682/3833 [15:33:01<71:33:21, 81.75s/it] 18%|█▊        | 683/3833 [15:34:23<71:29:19, 81.70s/it] 18%|█▊        | 684/3833 [15:35:45<71:33:13, 81.80s/it] 18%|█▊        | 685/3833 [15:37:07<71:30:23, 81.77s/it] 18%|█▊        | 686/3833 [15:38:28<71:28:16, 81.76s/it] 18%|█▊        | 687/3833 [15:39:50<71:28:25, 81.79s/it] 18%|█▊        | 688/3833 [15:41:12<71:26:06, 81.77s/it] 18%|█▊        | 689/3833 [15:42:34<71:25:37, 81.79s/it] 18%|█▊        | 690/3833 [15:43:56<71:23:34, 81.77s/it]                                                        {'loss': 1.4757, 'learning_rate': 4.6107539352756255e-05, 'epoch': 0.18}
 18%|█▊        | 690/3833 [15:43:56<71:23:34, 81.77s/it] 18%|█▊        | 691/3833 [15:45:17<71:21:15, 81.76s/it] 18%|█▊        | 692/3833 [15:46:40<71:26:38, 81.88s/it] 18%|█▊        | 693/3833 [15:48:01<71:26:27, 81.91s/it] 18%|█▊        | 694/3833 [15:49:23<71:24:06, 81.89s/it] 18%|█▊        | 695/3833 [15:50:45<71:22:29, 81.88s/it] 18%|█▊        | 696/3833 [15:52:07<71:16:38, 81.80s/it] 18%|█▊        | 697/3833 [15:53:29<71:20:35, 81.90s/it] 18%|█▊        | 698/3833 [15:54:51<71:18:05, 81.88s/it] 18%|█▊        | 699/3833 [15:56:12<71:11:38, 81.78s/it] 18%|█▊        | 700/3833 [15:57:35<71:19:43, 81.96s/it]                                                        {'loss': 1.4758, 'learning_rate': 4.5997029893477464e-05, 'epoch': 0.18}
 18%|█▊        | 700/3833 [15:57:35<71:19:43, 81.96s/it] 18%|█▊        | 701/3833 [15:58:56<71:13:50, 81.87s/it] 18%|█▊        | 702/3833 [16:00:18<71:06:58, 81.77s/it] 18%|█▊        | 703/3833 [16:01:40<71:11:18, 81.88s/it] 18%|█▊        | 704/3833 [16:03:02<71:06:08, 81.81s/it] 18%|█▊        | 705/3833 [16:04:23<71:02:08, 81.75s/it] 18%|█▊        | 706/3833 [16:05:45<71:02:22, 81.79s/it] 18%|█▊        | 707/3833 [16:07:07<71:08:47, 81.93s/it] 18%|█▊        | 708/3833 [16:08:30<71:09:33, 81.98s/it] 18%|█▊        | 709/3833 [16:09:51<71:07:12, 81.96s/it] 19%|█▊        | 710/3833 [16:11:13<70:59:14, 81.83s/it]                                                        {'loss': 1.4847, 'learning_rate': 4.5885109919740375e-05, 'epoch': 0.19}
 19%|█▊        | 710/3833 [16:11:13<70:59:14, 81.83s/it] 19%|█▊        | 711/3833 [16:12:35<71:02:59, 81.93s/it] 19%|█▊        | 712/3833 [16:13:57<70:59:17, 81.88s/it] 19%|█▊        | 713/3833 [16:15:18<70:52:43, 81.78s/it] 19%|█▊        | 714/3833 [16:16:40<70:53:35, 81.83s/it] 19%|█▊        | 715/3833 [16:18:02<70:51:19, 81.81s/it] 19%|█▊        | 716/3833 [16:19:24<70:49:31, 81.80s/it] 19%|█▊        | 717/3833 [16:20:46<70:51:02, 81.86s/it] 19%|█▊        | 718/3833 [16:22:08<70:53:13, 81.92s/it] 19%|█▉        | 719/3833 [16:23:30<70:50:32, 81.90s/it] 19%|█▉        | 720/3833 [16:24:52<70:50:40, 81.93s/it]                                                        {'loss': 1.4691, 'learning_rate': 4.577178694997698e-05, 'epoch': 0.19}
 19%|█▉        | 720/3833 [16:24:52<70:50:40, 81.93s/it] 19%|█▉        | 721/3833 [16:26:14<70:47:22, 81.89s/it] 19%|█▉        | 722/3833 [16:27:36<70:46:25, 81.90s/it] 19%|█▉        | 723/3833 [16:28:57<70:41:57, 81.84s/it] 19%|█▉        | 724/3833 [16:30:19<70:38:31, 81.80s/it] 19%|█▉        | 725/3833 [16:31:41<70:41:33, 81.88s/it] 19%|█▉        | 726/3833 [16:33:03<70:41:28, 81.91s/it] 19%|█▉        | 727/3833 [16:34:25<70:45:27, 82.01s/it] 19%|█▉        | 728/3833 [16:35:47<70:37:27, 81.88s/it] 19%|█▉        | 729/3833 [16:37:09<70:38:03, 81.92s/it] 19%|█▉        | 730/3833 [16:38:31<70:34:14, 81.87s/it]                                                        {'loss': 1.471, 'learning_rate': 4.565706859686812e-05, 'epoch': 0.19}
 19%|█▉        | 730/3833 [16:38:31<70:34:14, 81.87s/it] 19%|█▉        | 731/3833 [16:39:53<70:37:34, 81.96s/it] 19%|█▉        | 732/3833 [16:41:15<70:33:39, 81.92s/it] 19%|█▉        | 733/3833 [16:42:36<70:27:23, 81.82s/it] 19%|█▉        | 734/3833 [16:43:58<70:32:29, 81.95s/it] 19%|█▉        | 735/3833 [16:45:20<70:25:06, 81.83s/it] 19%|█▉        | 736/3833 [16:46:42<70:20:31, 81.77s/it] 19%|█▉        | 737/3833 [16:48:03<70:20:36, 81.79s/it] 19%|█▉        | 738/3833 [16:49:25<70:15:53, 81.73s/it] 19%|█▉        | 739/3833 [16:50:47<70:15:20, 81.75s/it] 19%|█▉        | 740/3833 [16:52:09<70:17:27, 81.81s/it]                                                        {'loss': 1.4682, 'learning_rate': 4.554096256683212e-05, 'epoch': 0.19}
 19%|█▉        | 740/3833 [16:52:09<70:17:27, 81.81s/it] 19%|█▉        | 741/3833 [16:53:31<70:18:37, 81.86s/it] 19%|█▉        | 742/3833 [16:54:53<70:22:15, 81.96s/it] 19%|█▉        | 743/3833 [16:56:15<70:20:54, 81.96s/it] 19%|█▉        | 744/3833 [16:57:37<70:15:43, 81.89s/it] 19%|█▉        | 745/3833 [16:58:58<70:14:03, 81.88s/it] 19%|█▉        | 746/3833 [17:00:20<70:09:37, 81.82s/it] 19%|█▉        | 747/3833 [17:01:42<70:05:09, 81.76s/it] 20%|█▉        | 748/3833 [17:03:04<70:07:13, 81.83s/it] 20%|█▉        | 749/3833 [17:04:25<70:03:38, 81.78s/it] 20%|█▉        | 750/3833 [17:05:47<70:05:47, 81.85s/it]                                                        {'loss': 1.4708, 'learning_rate': 4.542347665950707e-05, 'epoch': 0.2}
 20%|█▉        | 750/3833 [17:05:47<70:05:47, 81.85s/it] 20%|█▉        | 751/3833 [17:07:09<70:05:10, 81.87s/it] 20%|█▉        | 752/3833 [17:08:31<69:57:46, 81.75s/it] 20%|█▉        | 753/3833 [17:09:53<70:07:13, 81.96s/it] 20%|█▉        | 754/3833 [17:11:15<70:03:02, 81.90s/it] 20%|█▉        | 755/3833 [17:12:37<69:58:57, 81.85s/it] 20%|█▉        | 756/3833 [17:13:59<69:58:45, 81.87s/it] 20%|█▉        | 757/3833 [17:15:21<69:57:16, 81.87s/it] 20%|█▉        | 758/3833 [17:16:43<69:56:58, 81.89s/it] 20%|█▉        | 759/3833 [17:18:04<69:50:53, 81.80s/it] 20%|█▉        | 760/3833 [17:19:26<69:44:47, 81.71s/it]                                                        {'loss': 1.4658, 'learning_rate': 4.5304618767226854e-05, 'epoch': 0.2}
 20%|█▉        | 760/3833 [17:19:26<69:44:47, 81.71s/it] 20%|█▉        | 761/3833 [17:20:48<69:49:24, 81.82s/it] 20%|█▉        | 762/3833 [17:22:09<69:44:31, 81.76s/it] 20%|█▉        | 763/3833 [17:23:31<69:46:28, 81.82s/it] 20%|█▉        | 764/3833 [17:24:53<69:45:55, 81.84s/it] 20%|█▉        | 765/3833 [17:26:15<69:42:43, 81.80s/it] 20%|█▉        | 766/3833 [17:27:37<69:41:09, 81.80s/it] 20%|██        | 767/3833 [17:28:59<69:42:44, 81.85s/it] 20%|██        | 768/3833 [17:30:21<69:42:33, 81.88s/it] 20%|██        | 769/3833 [17:31:43<69:45:21, 81.96s/it] 20%|██        | 770/3833 [17:33:05<69:49:19, 82.06s/it]                                                        {'loss': 1.4608, 'learning_rate': 4.518439687449103e-05, 'epoch': 0.2}
 20%|██        | 770/3833 [17:33:05<69:49:19, 82.06s/it] 20%|██        | 771/3833 [17:34:27<69:44:47, 82.00s/it] 20%|██        | 772/3833 [17:35:49<69:41:00, 81.95s/it] 20%|██        | 773/3833 [17:37:10<69:34:13, 81.85s/it] 20%|██        | 774/3833 [17:38:32<69:29:57, 81.79s/it] 20%|██        | 775/3833 [17:39:54<69:28:59, 81.80s/it] 20%|██        | 776/3833 [17:41:16<69:29:22, 81.83s/it] 20%|██        | 777/3833 [17:42:38<69:28:00, 81.83s/it] 20%|██        | 778/3833 [17:43:59<69:28:27, 81.87s/it] 20%|██        | 779/3833 [17:45:21<69:27:36, 81.88s/it] 20%|██        | 780/3833 [17:46:43<69:23:44, 81.83s/it]                                                        {'loss': 1.4575, 'learning_rate': 4.5062819057428383e-05, 'epoch': 0.2}
 20%|██        | 780/3833 [17:46:43<69:23:44, 81.83s/it] 20%|██        | 781/3833 [17:48:05<69:22:33, 81.83s/it] 20%|██        | 782/3833 [17:49:27<69:19:22, 81.80s/it] 20%|██        | 783/3833 [17:50:49<69:22:32, 81.89s/it] 20%|██        | 784/3833 [17:52:10<69:17:51, 81.82s/it] 20%|██        | 785/3833 [17:53:32<69:14:27, 81.78s/it] 21%|██        | 786/3833 [17:54:54<69:15:11, 81.82s/it] 21%|██        | 787/3833 [17:56:16<69:10:48, 81.76s/it] 21%|██        | 788/3833 [17:57:38<69:12:59, 81.83s/it] 21%|██        | 789/3833 [17:58:59<69:11:28, 81.83s/it] 21%|██        | 790/3833 [18:00:21<69:11:34, 81.86s/it]                                                        {'loss': 1.4682, 'learning_rate': 4.4939893483254476e-05, 'epoch': 0.21}
 21%|██        | 790/3833 [18:00:21<69:11:34, 81.86s/it] 21%|██        | 791/3833 [18:01:43<69:11:39, 81.89s/it] 21%|██        | 792/3833 [18:03:05<69:06:11, 81.81s/it] 21%|██        | 793/3833 [18:04:27<69:03:10, 81.77s/it] 21%|██        | 794/3833 [18:05:49<69:07:03, 81.88s/it] 21%|██        | 795/3833 [18:07:11<69:04:04, 81.84s/it] 21%|██        | 796/3833 [18:08:32<69:00:56, 81.81s/it] 21%|██        | 797/3833 [18:09:54<69:01:00, 81.84s/it] 21%|██        | 798/3833 [18:11:16<69:01:49, 81.88s/it] 21%|██        | 799/3833 [18:12:38<69:00:51, 81.89s/it] 21%|██        | 800/3833 [18:14:00<68:57:37, 81.85s/it]                                                        {'loss': 1.4625, 'learning_rate': 4.481562840972291e-05, 'epoch': 0.21}
 21%|██        | 800/3833 [18:14:00<68:57:37, 81.85s/it] 21%|██        | 801/3833 [18:15:21<68:51:35, 81.76s/it] 21%|██        | 802/3833 [18:16:43<68:54:45, 81.85s/it] 21%|██        | 803/3833 [18:18:05<68:53:54, 81.86s/it] 21%|██        | 804/3833 [18:19:27<68:56:40, 81.94s/it] 21%|██        | 805/3833 [18:20:50<69:00:33, 82.05s/it] 21%|██        | 806/3833 [18:22:12<68:56:08, 81.99s/it] 21%|██        | 807/3833 [18:23:34<68:54:11, 81.97s/it] 21%|██        | 808/3833 [18:24:56<68:55:13, 82.02s/it] 21%|██        | 809/3833 [18:26:17<68:49:11, 81.93s/it] 21%|██        | 810/3833 [18:27:39<68:49:44, 81.97s/it]                                                        {'loss': 1.4648, 'learning_rate': 4.4690032184570676e-05, 'epoch': 0.21}
 21%|██        | 810/3833 [18:27:39<68:49:44, 81.97s/it] 21%|██        | 811/3833 [18:29:01<68:46:57, 81.94s/it] 21%|██        | 812/3833 [18:30:23<68:42:44, 81.88s/it] 21%|██        | 813/3833 [18:31:45<68:45:06, 81.96s/it] 21%|██        | 814/3833 [18:33:07<68:45:22, 81.99s/it] 21%|██▏       | 815/3833 [18:34:29<68:38:44, 81.88s/it] 21%|██▏       | 816/3833 [18:35:51<68:39:06, 81.92s/it] 21%|██▏       | 817/3833 [18:37:13<68:39:55, 81.96s/it] 21%|██▏       | 818/3833 [18:38:35<68:33:32, 81.86s/it] 21%|██▏       | 819/3833 [18:39:56<68:31:41, 81.85s/it] 21%|██▏       | 820/3833 [18:41:18<68:29:37, 81.84s/it]                                                        {'loss': 1.4653, 'learning_rate': 4.456311324495733e-05, 'epoch': 0.21}
 21%|██▏       | 820/3833 [18:41:18<68:29:37, 81.84s/it] 21%|██▏       | 821/3833 [18:42:40<68:25:48, 81.79s/it] 21%|██▏       | 822/3833 [18:44:02<68:22:51, 81.76s/it] 21%|██▏       | 823/3833 [18:45:23<68:22:12, 81.77s/it] 21%|██▏       | 824/3833 [18:46:45<68:25:53, 81.87s/it] 22%|██▏       | 825/3833 [18:48:08<68:31:33, 82.01s/it] 22%|██▏       | 826/3833 [18:49:30<68:25:45, 81.92s/it] 22%|██▏       | 827/3833 [18:50:52<68:30:11, 82.04s/it] 22%|██▏       | 828/3833 [18:52:14<68:24:59, 81.96s/it] 22%|██▏       | 829/3833 [18:53:36<68:23:26, 81.96s/it] 22%|██▏       | 830/3833 [18:54:58<68:22:12, 81.96s/it]                                                        {'loss': 1.4568, 'learning_rate': 4.443488011689823e-05, 'epoch': 0.22}
 22%|██▏       | 830/3833 [18:54:58<68:22:12, 81.96s/it] 22%|██▏       | 831/3833 [18:56:19<68:15:09, 81.85s/it] 22%|██▏       | 832/3833 [18:57:41<68:12:54, 81.83s/it] 22%|██▏       | 833/3833 [18:59:03<68:10:14, 81.80s/it] 22%|██▏       | 834/3833 [19:00:24<68:08:25, 81.80s/it] 22%|██▏       | 835/3833 [19:01:46<68:10:41, 81.87s/it] 22%|██▏       | 836/3833 [19:03:08<68:10:01, 81.88s/it] 22%|██▏       | 837/3833 [19:04:30<68:05:39, 81.82s/it] 22%|██▏       | 838/3833 [19:05:52<68:10:13, 81.94s/it] 22%|██▏       | 839/3833 [19:07:14<68:05:24, 81.87s/it] 22%|██▏       | 840/3833 [19:08:36<68:07:20, 81.94s/it]                                                        {'loss': 1.4502, 'learning_rate': 4.430534141469177e-05, 'epoch': 0.22}
 22%|██▏       | 840/3833 [19:08:36<68:07:20, 81.94s/it] 22%|██▏       | 841/3833 [19:09:58<68:12:20, 82.07s/it] 22%|██▏       | 842/3833 [19:11:20<68:06:14, 81.97s/it] 22%|██▏       | 843/3833 [19:12:42<68:05:24, 81.98s/it] 22%|██▏       | 844/3833 [19:14:04<68:05:19, 82.01s/it] 22%|██▏       | 845/3833 [19:15:26<68:00:03, 81.93s/it] 22%|██▏       | 846/3833 [19:16:48<68:02:52, 82.01s/it] 22%|██▏       | 847/3833 [19:18:10<68:00:43, 82.00s/it] 22%|██▏       | 848/3833 [19:19:33<68:06:31, 82.14s/it] 22%|██▏       | 849/3833 [19:20:55<68:04:59, 82.14s/it] 22%|██▏       | 850/3833 [19:22:17<68:04:41, 82.16s/it]                                                        {'loss': 1.467, 'learning_rate': 4.4174505840340746e-05, 'epoch': 0.22}
 22%|██▏       | 850/3833 [19:22:18<68:04:41, 82.16s/it] 22%|██▏       | 851/3833 [19:23:40<68:12:16, 82.34s/it] 22%|██▏       | 852/3833 [19:25:02<68:03:37, 82.19s/it] 22%|██▏       | 853/3833 [19:26:23<67:55:40, 82.06s/it] 22%|██▏       | 854/3833 [19:27:45<67:53:45, 82.05s/it] 22%|██▏       | 855/3833 [19:29:07<67:48:45, 81.98s/it] 22%|██▏       | 856/3833 [19:30:30<68:01:38, 82.26s/it] 22%|██▏       | 857/3833 [19:31:52<67:50:28, 82.07s/it] 22%|██▏       | 858/3833 [19:33:14<67:51:02, 82.11s/it] 22%|██▏       | 859/3833 [19:34:36<67:46:52, 82.05s/it] 22%|██▏       | 860/3833 [19:35:58<67:46:01, 82.06s/it]                                                        {'loss': 1.4474, 'learning_rate': 4.404238218296771e-05, 'epoch': 0.22}
 22%|██▏       | 860/3833 [19:35:58<67:46:01, 82.06s/it] 22%|██▏       | 861/3833 [19:37:20<67:43:35, 82.04s/it] 22%|██▏       | 862/3833 [19:38:41<67:35:05, 81.89s/it] 23%|██▎       | 863/3833 [19:40:03<67:34:34, 81.91s/it] 23%|██▎       | 864/3833 [19:41:26<67:35:57, 81.97s/it] 23%|██▎       | 865/3833 [19:42:48<67:34:54, 81.97s/it] 23%|██▎       | 866/3833 [19:44:09<67:32:08, 81.94s/it] 23%|██▎       | 867/3833 [19:45:32<67:34:05, 82.01s/it] 23%|██▎       | 868/3833 [19:46:53<67:28:40, 81.93s/it] 23%|██▎       | 869/3833 [19:48:15<67:24:16, 81.87s/it] 23%|██▎       | 870/3833 [19:49:37<67:25:30, 81.92s/it]                                                        {'loss': 1.4501, 'learning_rate': 4.3908979318224605e-05, 'epoch': 0.23}
 23%|██▎       | 870/3833 [19:49:37<67:25:30, 81.92s/it] 23%|██▎       | 871/3833 [19:50:59<67:23:08, 81.90s/it] 23%|██▎       | 872/3833 [19:52:21<67:21:31, 81.90s/it] 23%|██▎       | 873/3833 [19:53:42<67:14:39, 81.78s/it] 23%|██▎       | 874/3833 [19:55:04<67:12:52, 81.77s/it] 23%|██▎       | 875/3833 [19:56:26<67:12:22, 81.79s/it] 23%|██▎       | 876/3833 [19:57:48<67:08:28, 81.74s/it] 23%|██▎       | 877/3833 [19:59:09<67:07:40, 81.75s/it] 23%|██▎       | 878/3833 [20:00:31<67:08:10, 81.79s/it] 23%|██▎       | 879/3833 [20:01:53<67:04:42, 81.75s/it] 23%|██▎       | 880/3833 [20:03:15<67:03:31, 81.75s/it]                                                        {'loss': 1.4503, 'learning_rate': 4.377430620769649e-05, 'epoch': 0.23}
 23%|██▎       | 880/3833 [20:03:15<67:03:31, 81.75s/it] 23%|██▎       | 881/3833 [20:04:36<67:02:43, 81.76s/it] 23%|██▎       | 882/3833 [20:05:58<67:04:22, 81.82s/it] 23%|██▎       | 883/3833 [20:07:20<67:02:23, 81.81s/it] 23%|██▎       | 884/3833 [20:08:42<66:56:06, 81.71s/it] 23%|██▎       | 885/3833 [20:10:03<66:53:29, 81.69s/it] 23%|██▎       | 886/3833 [20:11:25<66:59:46, 81.84s/it] 23%|██▎       | 887/3833 [20:12:47<67:01:22, 81.90s/it] 23%|██▎       | 888/3833 [20:14:09<66:58:03, 81.86s/it] 23%|██▎       | 889/3833 [20:15:31<66:57:23, 81.88s/it] 23%|██▎       | 890/3833 [20:16:53<66:53:31, 81.83s/it]                                                        {'loss': 1.452, 'learning_rate': 4.3638371898299555e-05, 'epoch': 0.23}
 23%|██▎       | 890/3833 [20:16:53<66:53:31, 81.83s/it] 23%|██▎       | 891/3833 [20:18:15<66:50:35, 81.79s/it] 23%|██▎       | 892/3833 [20:19:36<66:48:18, 81.77s/it] 23%|██▎       | 893/3833 [20:20:58<66:46:09, 81.76s/it] 23%|██▎       | 894/3833 [20:22:20<66:44:53, 81.76s/it] 23%|██▎       | 895/3833 [20:23:41<66:39:42, 81.68s/it] 23%|██▎       | 896/3833 [20:25:03<66:39:19, 81.70s/it] 23%|██▎       | 897/3833 [20:26:25<66:44:11, 81.83s/it] 23%|██▎       | 898/3833 [20:27:47<66:39:19, 81.76s/it] 23%|██▎       | 899/3833 [20:29:09<66:40:11, 81.80s/it] 23%|██▎       | 900/3833 [20:30:31<66:41:56, 81.87s/it]                                                        {'loss': 1.4556, 'learning_rate': 4.350118552167335e-05, 'epoch': 0.23}
 23%|██▎       | 900/3833 [20:30:31<66:41:56, 81.87s/it] 24%|██▎       | 901/3833 [20:31:53<66:41:52, 81.89s/it] 24%|██▎       | 902/3833 [20:33:14<66:37:44, 81.84s/it] 24%|██▎       | 903/3833 [20:34:37<66:43:10, 81.98s/it] 24%|██▎       | 904/3833 [20:35:59<66:40:39, 81.95s/it] 24%|██▎       | 905/3833 [20:37:21<66:39:55, 81.97s/it] 24%|██▎       | 906/3833 [20:38:42<66:35:31, 81.90s/it] 24%|██▎       | 907/3833 [20:40:04<66:30:47, 81.83s/it] 24%|██▎       | 908/3833 [20:41:26<66:30:41, 81.86s/it] 24%|██▎       | 909/3833 [20:42:48<66:27:13, 81.82s/it] 24%|██▎       | 910/3833 [20:44:10<66:29:30, 81.89s/it]                                                        {'loss': 1.4454, 'learning_rate': 4.336275629356738e-05, 'epoch': 0.24}
 24%|██▎       | 910/3833 [20:44:10<66:29:30, 81.89s/it] 24%|██▍       | 911/3833 [20:45:32<66:27:26, 81.88s/it] 24%|██▍       | 912/3833 [20:46:53<66:20:36, 81.77s/it] 24%|██▍       | 913/3833 [20:48:15<66:19:59, 81.78s/it] 24%|██▍       | 914/3833 [20:49:37<66:17:10, 81.75s/it] 24%|██▍       | 915/3833 [20:50:58<66:14:41, 81.73s/it] 24%|██▍       | 916/3833 [20:52:20<66:12:35, 81.71s/it] 24%|██▍       | 917/3833 [20:53:42<66:09:44, 81.68s/it] 24%|██▍       | 918/3833 [20:55:03<66:12:24, 81.76s/it] 24%|██▍       | 919/3833 [20:56:26<66:20:01, 81.95s/it] 24%|██▍       | 920/3833 [20:57:48<66:21:09, 82.00s/it]                                                        {'loss': 1.4535, 'learning_rate': 4.3223093513221996e-05, 'epoch': 0.24}
 24%|██▍       | 920/3833 [20:57:48<66:21:09, 82.00s/it] 24%|██▍       | 921/3833 [20:59:11<66:35:28, 82.32s/it] 24%|██▍       | 922/3833 [21:00:34<66:42:25, 82.50s/it] 24%|██▍       | 923/3833 [21:01:57<66:50:11, 82.68s/it] 24%|██▍       | 924/3833 [21:03:20<66:48:57, 82.69s/it] 24%|██▍       | 925/3833 [21:04:43<66:49:41, 82.73s/it] 24%|██▍       | 926/3833 [21:06:06<66:55:15, 82.87s/it] 24%|██▍       | 927/3833 [21:07:28<66:51:00, 82.81s/it] 24%|██▍       | 928/3833 [21:08:51<66:49:04, 82.80s/it] 24%|██▍       | 929/3833 [21:10:14<66:46:05, 82.77s/it] 24%|██▍       | 930/3833 [21:11:37<66:49:41, 82.87s/it]                                                        {'loss': 1.4443, 'learning_rate': 4.308220656274371e-05, 'epoch': 0.24}
 24%|██▍       | 930/3833 [21:11:37<66:49:41, 82.87s/it] 24%|██▍       | 931/3833 [21:13:00<66:44:31, 82.80s/it] 24%|██▍       | 932/3833 [21:14:22<66:40:44, 82.75s/it] 24%|██▍       | 933/3833 [21:15:45<66:34:45, 82.65s/it] 24%|██▍       | 934/3833 [21:17:08<66:36:21, 82.71s/it] 24%|██▍       | 935/3833 [21:18:32<66:54:18, 83.11s/it] 24%|██▍       | 936/3833 [21:19:55<67:01:38, 83.29s/it] 24%|██▍       | 937/3833 [21:21:18<66:56:42, 83.22s/it] 24%|██▍       | 938/3833 [21:22:42<66:57:53, 83.27s/it] 24%|██▍       | 939/3833 [21:24:04<66:44:54, 83.03s/it] 25%|██▍       | 940/3833 [21:25:27<66:41:41, 82.99s/it]                                                        {'loss': 1.4504, 'learning_rate': 4.2940104906474945e-05, 'epoch': 0.25}
 25%|██▍       | 940/3833 [21:25:27<66:41:41, 82.99s/it] 25%|██▍       | 941/3833 [21:26:51<66:50:09, 83.20s/it] 25%|██▍       | 942/3833 [21:28:14<66:52:19, 83.27s/it] 25%|██▍       | 943/3833 [21:29:37<66:42:21, 83.09s/it] 25%|██▍       | 944/3833 [21:31:01<66:47:27, 83.23s/it] 25%|██▍       | 945/3833 [21:32:24<66:45:39, 83.22s/it] 25%|██▍       | 946/3833 [21:33:46<66:37:55, 83.09s/it] 25%|██▍       | 947/3833 [21:35:10<66:36:27, 83.09s/it] 25%|██▍       | 948/3833 [21:36:32<66:30:22, 82.99s/it] 25%|██▍       | 949/3833 [21:37:55<66:24:16, 82.89s/it] 25%|██▍       | 950/3833 [21:39:17<66:15:16, 82.73s/it]                                                        {'loss': 1.4439, 'learning_rate': 4.2796798090358234e-05, 'epoch': 0.25}
 25%|██▍       | 950/3833 [21:39:17<66:15:16, 82.73s/it] 25%|██▍       | 951/3833 [21:40:40<66:16:38, 82.79s/it] 25%|██▍       | 952/3833 [21:42:02<65:59:57, 82.47s/it] 25%|██▍       | 953/3833 [21:43:25<66:04:47, 82.60s/it] 25%|██▍       | 954/3833 [21:44:48<66:03:58, 82.61s/it] 25%|██▍       | 955/3833 [21:46:10<65:57:21, 82.50s/it] 25%|██▍       | 956/3833 [21:47:32<65:54:45, 82.48s/it] 25%|██▍       | 957/3833 [21:48:55<65:58:20, 82.58s/it] 25%|██▍       | 958/3833 [21:50:17<65:50:30, 82.45s/it] 25%|██▌       | 959/3833 [21:51:40<65:51:39, 82.50s/it] 25%|██▌       | 960/3833 [21:53:02<65:48:03, 82.45s/it]                                                        {'loss': 1.4426, 'learning_rate': 4.265229574129494e-05, 'epoch': 0.25}
 25%|██▌       | 960/3833 [21:53:02<65:48:03, 82.45s/it] 25%|██▌       | 961/3833 [21:54:25<65:54:29, 82.61s/it] 25%|██▌       | 962/3833 [21:55:48<65:51:54, 82.59s/it] 25%|██▌       | 963/3833 [21:57:11<66:02:33, 82.84s/it] 25%|██▌       | 964/3833 [21:58:34<65:56:35, 82.75s/it] 25%|██▌       | 965/3833 [21:59:56<65:54:55, 82.74s/it] 25%|██▌       | 966/3833 [22:01:19<65:56:54, 82.81s/it] 25%|██▌       | 967/3833 [22:02:42<65:54:43, 82.79s/it] 25%|██▌       | 968/3833 [22:04:05<65:50:41, 82.74s/it] 25%|██▌       | 969/3833 [22:05:28<65:55:00, 82.86s/it] 25%|██▌       | 970/3833 [22:06:51<65:52:48, 82.84s/it]                                                        {'loss': 1.4358, 'learning_rate': 4.2506607566498627e-05, 'epoch': 0.25}
 25%|██▌       | 970/3833 [22:06:51<65:52:48, 82.84s/it] 25%|██▌       | 971/3833 [22:08:13<65:49:14, 82.79s/it] 25%|██▌       | 972/3833 [22:09:36<65:41:43, 82.66s/it] 25%|██▌       | 973/3833 [22:10:58<65:36:28, 82.58s/it] 25%|██▌       | 974/3833 [22:12:21<65:36:21, 82.61s/it] 25%|██▌       | 975/3833 [22:13:44<65:38:28, 82.68s/it] 25%|██▌       | 976/3833 [22:15:06<65:33:13, 82.60s/it] 25%|██▌       | 977/3833 [22:16:29<65:40:56, 82.79s/it] 26%|██▌       | 978/3833 [22:17:52<65:38:53, 82.78s/it] 26%|██▌       | 979/3833 [22:19:14<65:29:06, 82.60s/it] 26%|██▌       | 980/3833 [22:20:37<65:36:03, 82.78s/it]                                                        {'loss': 1.4368, 'learning_rate': 4.235974335284283e-05, 'epoch': 0.26}
 26%|██▌       | 980/3833 [22:20:37<65:36:03, 82.78s/it] 26%|██▌       | 981/3833 [22:22:00<65:32:15, 82.73s/it] 26%|██▌       | 982/3833 [22:23:23<65:28:31, 82.68s/it] 26%|██▌       | 983/3833 [22:24:45<65:24:56, 82.63s/it] 26%|██▌       | 984/3833 [22:26:07<65:20:26, 82.56s/it] 26%|██▌       | 985/3833 [22:27:30<65:17:19, 82.53s/it] 26%|██▌       | 986/3833 [22:28:52<65:08:20, 82.37s/it] 26%|██▌       | 987/3833 [22:30:14<65:09:45, 82.43s/it] 26%|██▌       | 988/3833 [22:31:37<65:08:14, 82.42s/it] 26%|██▌       | 989/3833 [22:33:00<65:10:13, 82.49s/it] 26%|██▌       | 990/3833 [22:34:21<65:00:45, 82.32s/it]                                                        {'loss': 1.4401, 'learning_rate': 4.221171296620374e-05, 'epoch': 0.26}
 26%|██▌       | 990/3833 [22:34:21<65:00:45, 82.32s/it] 26%|██▌       | 991/3833 [22:35:44<65:09:24, 82.53s/it] 26%|██▌       | 992/3833 [22:37:07<65:03:09, 82.43s/it] 26%|██▌       | 993/3833 [22:38:29<65:01:04, 82.42s/it] 26%|██▌       | 994/3833 [22:39:52<65:01:26, 82.45s/it] 26%|██▌       | 995/3833 [22:41:13<64:52:28, 82.29s/it] 26%|██▌       | 996/3833 [22:42:36<64:58:31, 82.45s/it] 26%|██▌       | 997/3833 [22:44:00<65:09:35, 82.71s/it] 26%|██▌       | 998/3833 [22:45:22<65:06:00, 82.67s/it] 26%|██▌       | 999/3833 [22:46:44<64:58:50, 82.54s/it] 26%|██▌       | 1000/3833 [22:48:08<65:08:03, 82.77s/it]                                                         {'loss': 1.4332, 'learning_rate': 4.206252635079734e-05, 'epoch': 0.26}
 26%|██▌       | 1000/3833 [22:48:08<65:08:03, 82.77s/it][INFO|trainer.py:2979] 2024-03-10 13:50:20,770 >> Saving model checkpoint to /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1/tmp-checkpoint-1000
/home/nfs02/wangzj/public_code/hitsz/peft/src/peft/utils/save_and_load.py:151: UserWarning: Could not find a config file in /home/nfs02/model/llama2/hf/Llama-2-7b-hf - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2435] 2024-03-10 13:51:53,343 >> tokenizer config file saved in /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-03-10 13:51:53,393 >> Special tokens file saved in /home/wangzj/LLaMA-Factory/llama-pt/ckpts/moe-zhisen-real-top1/tmp-checkpoint-1000/special_tokens_map.json
 26%|██▌       | 1001/3833 [22:51:27<92:29:48, 117.58s/it] 26%|██▌       | 1002/3833 [22:52:49<84:09:42, 107.02s/it] 26%|██▌       | 1003/3833 [22:54:11<78:11:27, 99.47s/it]  26%|██▌       | 1004/3833 [22:55:33<74:01:17, 94.20s/it] 26%|██▌       | 1005/3833 [22:56:55<71:10:25, 90.60s/it] 26%|██▌       | 1006/3833 [22:58:16<68:59:10, 87.85s/it] 26%|██▋       | 1007/3833 [22:59:38<67:30:20, 85.99s/it] 26%|██▋       | 1008/3833 [23:01:00<66:34:25, 84.84s/it] 26%|██▋       | 1009/3833 [23:02:22<65:48:34, 83.89s/it] 26%|██▋       | 1010/3833 [23:03:44<65:19:44, 83.31s/it]                                                         {'loss': 1.4393, 'learning_rate': 4.191219352851145e-05, 'epoch': 0.26}
 26%|██▋       | 1010/3833 [23:03:44<65:19:44, 83.31s/it] 26%|██▋       | 1011/3833 [23:05:05<64:50:35, 82.72s/it] 26%|██▋       | 1012/3833 [23:06:27<64:36:23, 82.45s/it] 26%|██▋       | 1013/3833 [23:07:49<64:25:25, 82.24s/it] 26%|██▋       | 1014/3833 [23:09:10<64:15:41, 82.06s/it] 26%|██▋       | 1015/3833 [23:10:32<64:12:30, 82.03s/it] 27%|██▋       | 1016/3833 [23:11:55<64:14:15, 82.09s/it] 27%|██▋       | 1017/3833 [23:13:16<64:08:27, 82.00s/it] 27%|██▋       | 1018/3833 [23:14:39<64:11:50, 82.10s/it] 27%|██▋       | 1019/3833 [23:16:01<64:07:18, 82.03s/it] 27%|██▋       | 1020/3833 [23:17:22<64:04:58, 82.01s/it]                                                         {'loss': 1.4372, 'learning_rate': 4.1760724598232456e-05, 'epoch': 0.27}
 27%|██▋       | 1020/3833 [23:17:23<64:04:58, 82.01s/it] 27%|██▋       | 1021/3833 [23:18:44<64:01:26, 81.97s/it] 27%|██▋       | 1022/3833 [23:20:06<63:56:51, 81.90s/it] 27%|██▋       | 1023/3833 [23:21:28<63:53:04, 81.84s/it] 27%|██▋       | 1024/3833 [23:22:50<63:55:02, 81.92s/it] 27%|██▋       | 1025/3833 [23:24:12<63:53:11, 81.91s/it] 27%|██▋       | 1026/3833 [23:25:34<63:54:00, 81.95s/it] 27%|██▋       | 1027/3833 [23:26:56<63:52:58, 81.96s/it] 27%|██▋       | 1028/3833 [23:28:18<63:48:43, 81.90s/it] 27%|██▋       | 1029/3833 [23:29:39<63:44:28, 81.84s/it] 27%|██▋       | 1030/3833 [23:31:01<63:45:20, 81.88s/it]                                                         {'loss': 1.427, 'learning_rate': 4.160812973516692e-05, 'epoch': 0.27}
 27%|██▋       | 1030/3833 [23:31:01<63:45:20, 81.88s/it] 27%|██▋       | 1031/3833 [23:32:23<63:40:44, 81.81s/it] 27%|██▋       | 1032/3833 [23:33:45<63:41:21, 81.86s/it] 27%|██▋       | 1033/3833 [23:35:08<63:52:36, 82.13s/it] 27%|██▋       | 1034/3833 [23:36:29<63:47:50, 82.05s/it] 27%|██▋       | 1035/3833 [23:37:51<63:45:13, 82.03s/it] 27%|██▋       | 1036/3833 [23:39:13<63:41:26, 81.98s/it] 27%|██▋       | 1037/3833 [23:40:35<63:40:13, 81.98s/it] 27%|██▋       | 1038/3833 [23:41:57<63:38:03, 81.96s/it] 27%|██▋       | 1039/3833 [23:43:19<63:37:09, 81.97s/it] 27%|██▋       | 1040/3833 [23:44:41<63:38:42, 82.03s/it]                                                         {'loss': 1.4421, 'learning_rate': 4.1454419190158e-05, 'epoch': 0.27}
 27%|██▋       | 1040/3833 [23:44:41<63:38:42, 82.03s/it] 27%|██▋       | 1041/3833 [23:46:03<63:34:14, 81.97s/it] 27%|██▋       | 1042/3833 [23:47:25<63:37:02, 82.06s/it] 27%|██▋       | 1043/3833 [23:48:48<63:35:57, 82.06s/it] 27%|██▋       | 1044/3833 [23:50:10<63:38:24, 82.15s/it] 27%|██▋       | 1045/3833 [23:51:32<63:32:16, 82.04s/it] 27%|██▋       | 1046/3833 [23:52:54<63:34:11, 82.11s/it] 27%|██▋       | 1047/3833 [23:54:16<63:30:13, 82.06s/it] 27%|██▋       | 1048/3833 [23:55:38<63:32:11, 82.13s/it] 27%|██▋       | 1049/3833 [23:57:01<63:33:38, 82.19s/it] 27%|██▋       | 1050/3833 [23:58:23<63:37:59, 82.31s/it]                                                         {'loss': 1.4342, 'learning_rate': 4.12996032889969e-05, 'epoch': 0.27}
 27%|██▋       | 1050/3833 [23:58:23<63:37:59, 82.31s/it] 27%|██▋       | 1051/3833 [23:59:46<63:40:26, 82.40s/it] 27%|██▋       | 1052/3833 [24:01:08<63:32:01, 82.24s/it] 27%|██▋       | 1053/3833 [24:02:30<63:31:26, 82.26s/it] 27%|██▋       | 1054/3833 [24:03:52<63:31:35, 82.29s/it] 28%|██▊       | 1055/3833 [24:05:14<63:24:29, 82.17s/it] 28%|██▊       | 1056/3833 [24:06:36<63:20:14, 82.11s/it] 28%|██▊       | 1057/3833 [24:07:58<63:19:26, 82.12s/it] 28%|██▊       | 1058/3833 [24:09:21<63:26:25, 82.30s/it] 28%|██▊       | 1059/3833 [24:10:43<63:21:28, 82.22s/it] 28%|██▊       | 1060/3833 [24:12:05<63:19:45, 82.22s/it]                                                         {'loss': 1.4324, 'learning_rate': 4.1143692431729144e-05, 'epoch': 0.28}
 28%|██▊       | 1060/3833 [24:12:05<63:19:45, 82.22s/it] 28%|██▊       | 1061/3833 [24:13:27<63:14:42, 82.14s/it] 28%|██▊       | 1062/3833 [24:14:50<63:16:34, 82.21s/it] 28%|██▊       | 1063/3833 [24:16:11<63:11:07, 82.12s/it] 28%|██▊       | 1064/3833 [24:17:34<63:09:29, 82.11s/it] 28%|██▊       | 1065/3833 [24:18:56<63:15:11, 82.27s/it] 28%|██▊       | 1066/3833 [24:20:18<63:12:32, 82.24s/it] 28%|██▊       | 1067/3833 [24:21:40<63:07:07, 82.15s/it] 28%|██▊       | 1068/3833 [24:23:02<63:05:47, 82.15s/it] 28%|██▊       | 1069/3833 [24:24:25<63:03:49, 82.14s/it] 28%|██▊       | 1070/3833 [24:25:47<63:00:08, 82.09s/it]                                                         {'loss': 1.4343, 'learning_rate': 4.0986697091955974e-05, 'epoch': 0.28}
 28%|██▊       | 1070/3833 [24:25:47<63:00:08, 82.09s/it] 28%|██▊       | 1071/3833 [24:27:09<62:57:47, 82.07s/it] 28%|██▊       | 1072/3833 [24:28:31<63:02:36, 82.20s/it] 28%|██▊       | 1073/3833 [24:29:53<62:56:00, 82.09s/it] 28%|██▊       | 1074/3833 [24:31:15<62:59:02, 82.18s/it] 28%|██▊       | 1075/3833 [24:32:37<62:56:51, 82.17s/it] 28%|██▊       | 1076/3833 [24:34:00<62:59:56, 82.26s/it] 28%|██▊       | 1077/3833 [24:35:22<62:59:29, 82.28s/it] 28%|██▊       | 1078/3833 [24:36:45<62:59:14, 82.31s/it] 28%|██▊       | 1079/3833 [24:38:07<62:56:33, 82.28s/it] 28%|██▊       | 1080/3833 [24:39:29<62:52:01, 82.21s/it]                                                         {'loss': 1.4342, 'learning_rate': 4.082862781613075e-05, 'epoch': 0.28}
 28%|██▊       | 1080/3833 [24:39:29<62:52:01, 82.21s/it] 28%|██▊       | 1081/3833 [24:40:51<62:46:49, 82.13s/it] 28%|██▊       | 1082/3833 [24:42:13<62:50:59, 82.25s/it] 28%|██▊       | 1083/3833 [24:43:37<63:09:13, 82.67s/it] 28%|██▊       | 1084/3833 [24:44:59<62:57:26, 82.45s/it] 28%|██▊       | 1085/3833 [24:46:22<62:58:37, 82.50s/it] 28%|██▊       | 1086/3833 [24:47:44<63:02:49, 82.62s/it] 28%|██▊       | 1087/3833 [24:49:08<63:08:18, 82.77s/it]